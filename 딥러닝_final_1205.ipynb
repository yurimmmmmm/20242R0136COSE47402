{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFGDeyPjYoue",
    "outputId": "a21d0cb9-17c2-456b-9f92-191342f5f606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '20242R0136COSE47402'...\n",
      "remote: Enumerating objects: 41, done.\u001b[K\n",
      "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
      "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
      "remote: Total 41 (delta 13), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (41/41), 29.41 MiB | 11.74 MiB/s, done.\n",
      "Resolving deltas: 100% (13/13), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://{token}@github.com/{username}/{repo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxIQwsMDZAHS",
    "outputId": "537cd464-4c9b-4d6b-93e4-19f937b4edd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/DAPT\n"
     ]
    }
   ],
   "source": [
    "cd \"/content/drive/My Drive/DAPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AXXBHS-iZKLe",
    "outputId": "1dc6fb11-d282-4321-d8bc-c9b69eeda16b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh index: 100% (230/230), done.\n",
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 1 commit.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31m.gitignore\u001b[m\n",
      "\t\u001b[31mDATA/\u001b[m\n",
      "\t\u001b[31mclip/__pycache__/\u001b[m\n",
      "\t\u001b[31mdatasets/__pycache__/\u001b[m\n",
      "\t\u001b[31moutput/\u001b[m\n",
      "\t\u001b[31mtrainers/__pycache__/\u001b[m\n",
      "\n",
      "\n",
      "It took 2.05 seconds to enumerate untracked files. 'status -uno'\n",
      "may speed it up, but you have to be careful not to forget to add\n",
      "new files yourself (see 'git help status').\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPVZmim26uzW"
   },
   "outputs": [],
   "source": [
    "!echo \"*.gdoc\" >> .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9RF16o_ZjlI"
   },
   "outputs": [],
   "source": [
    "!git add .\n",
    "# !git commit -m \"Ignore .gdoc files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-VJKHp2ZviZ"
   },
   "outputs": [],
   "source": [
    "!git commit -a -m \"Commit using Google Colab \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHEaabsQjnxB"
   },
   "outputs": [],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_Fu7pB5A68Z",
    "outputId": "e4dd3c19-aa11-430d-c0bf-505a0bbdec99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MirVO57eaZGK",
    "outputId": "b5a361c0-205f-499c-f405-42964561c831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive\n"
     ]
    }
   ],
   "source": [
    "%cd \"/content/drive/My Drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcE0KBJTB8K8",
    "outputId": "4fa51e45-6f80-4899-bd4d-358c62803efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive\n",
      "fatal: destination path 'DAPT' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# %cd \"/content/drive/My Drive\"\n",
    "# !git clone https://github.com/mlvlab/DAPT.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc7pyXviqRds"
   },
   "source": [
    "# 새 섹션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "c0ot1hSRqdqH",
    "outputId": "0275850d-39de-444d-b768-ae6720cfa704"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huc--Bv8CWwn",
    "outputId": "6ee1dc0d-bf8a-4852-fb50-c48504acbbb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/DAPT\n"
     ]
    }
   ],
   "source": [
    "cd \"DAPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuAnHxxTBHYw"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/KaiyangZhou/Dassl.pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDrjvvKVadl0",
    "outputId": "3c99b136-7a6e-470e-aa55-d37df8bb2b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/DAPT/Dassl.pytorch\n"
     ]
    }
   ],
   "source": [
    " cd \"./Dassl.pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCx7c6eHCwUg",
    "outputId": "bf71a40f-4f63-4099-850c-4cb97943eac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flake8==3.7.9 (from -r requirements.txt (line 1))\n",
      "  Downloading flake8-3.7.9-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting yapf==0.29.0 (from -r requirements.txt (line 2))\n",
      "  Downloading yapf-0.29.0-py2.py3-none-any.whl.metadata (30 kB)\n",
      "Collecting isort==4.3.21 (from -r requirements.txt (line 3))\n",
      "  Downloading isort-4.3.21-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting yacs (from -r requirements.txt (line 4))\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (5.2.0)\n",
      "Collecting tb-nightly (from -r requirements.txt (line 6))\n",
      "  Downloading tb_nightly-2.19.0a20241121-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.5.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.66.6)\n",
      "Collecting ftfy (from -r requirements.txt (line 11))\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2024.9.11)\n",
      "Collecting wilds==1.2.2 (from -r requirements.txt (line 13))\n",
      "  Downloading wilds-1.2.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.9.0)\n",
      "Collecting entrypoints<0.4.0,>=0.3.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Downloading entrypoints-0.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pyflakes<2.2.0,>=2.1.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Downloading pyflakes-2.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pycodestyle<2.6.0,>=2.5.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Downloading pycodestyle-2.5.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting mccabe<0.7.0,>=0.6.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.26.4)\n",
      "Collecting ogb>=1.2.6 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
      "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting outdated>=0.2.0 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.2.2)\n",
      "Requirement already satisfied: pillow>=7.2.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (11.0.0)\n",
      "Requirement already satisfied: pytz>=2020.4 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2024.2)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.20.1+cu121)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (4.12.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.68.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (3.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (4.25.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (3.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.5.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements.txt (line 11)) (0.2.13)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (2.2.3)\n",
      "Collecting littleutils (from outdated>=0.2.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
      "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2024.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly->-r requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 5)) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (1.7.1)\n",
      "Downloading flake8-3.7.9-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yapf-0.29.0-py2.py3-none-any.whl (185 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.3/185.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wilds-1.2.2-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading tb_nightly-2.19.0a20241121-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Downloading pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
      "Installing collected packages: yapf, mccabe, yacs, pyflakes, pycodestyle, littleutils, isort, ftfy, entrypoints, tb-nightly, outdated, flake8, ogb, wilds\n",
      "  Attempting uninstall: entrypoints\n",
      "    Found existing installation: entrypoints 0.4\n",
      "    Uninstalling entrypoints-0.4:\n",
      "      Successfully uninstalled entrypoints-0.4\n",
      "Successfully installed entrypoints-0.3 flake8-3.7.9 ftfy-6.3.1 isort-4.3.21 littleutils-0.2.4 mccabe-0.6.1 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.5.0 pyflakes-2.1.1 tb-nightly-2.19.0a20241121 wilds-1.2.2 yacs-0.1.8 yapf-0.29.0\n",
      "running develop\n",
      "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` and ``easy_install``.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  easy_install.initialize_options(self)\n",
      "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "running egg_info\n",
      "writing dassl.egg-info/PKG-INFO\n",
      "writing dependency_links to dassl.egg-info/dependency_links.txt\n",
      "writing requirements to dassl.egg-info/requires.txt\n",
      "writing top-level names to dassl.egg-info/top_level.txt\n",
      "reading manifest file 'dassl.egg-info/SOURCES.txt'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'dassl.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "Creating /usr/local/lib/python3.10/dist-packages/dassl.egg-link (link to .)\n",
      "Adding dassl 0.6.3 to easy-install.pth file\n",
      "\n",
      "Installed /content/drive/My Drive/DAPT/Dassl.pytorch\n",
      "Processing dependencies for dassl==0.6.3\n",
      "Searching for tabulate==0.9.0\n",
      "Best match: tabulate 0.9.0\n",
      "Adding tabulate 0.9.0 to easy-install.pth file\n",
      "Installing tabulate script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for wilds==1.2.2\n",
      "Best match: wilds 1.2.2\n",
      "Adding wilds 1.2.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for regex==2024.9.11\n",
      "Best match: regex 2024.9.11\n",
      "Adding regex 2024.9.11 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for ftfy==6.3.1\n",
      "Best match: ftfy 6.3.1\n",
      "Adding ftfy 6.3.1 to easy-install.pth file\n",
      "Installing ftfy script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for tqdm==4.66.6\n",
      "Best match: tqdm 4.66.6\n",
      "Adding tqdm 4.66.6 to easy-install.pth file\n",
      "Installing tqdm script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for scikit-learn==1.5.2\n",
      "Best match: scikit-learn 1.5.2\n",
      "Adding scikit-learn 1.5.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for scipy==1.13.1\n",
      "Best match: scipy 1.13.1\n",
      "Adding scipy 1.13.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for future==1.0.0\n",
      "Best match: future 1.0.0\n",
      "Adding future 1.0.0 to easy-install.pth file\n",
      "Installing futurize script to /usr/local/bin\n",
      "Installing pasteurize script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for tb-nightly==2.19.0a20241121\n",
      "Best match: tb-nightly 2.19.0a20241121\n",
      "Adding tb-nightly 2.19.0a20241121 to easy-install.pth file\n",
      "Installing tensorboard script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for gdown==5.2.0\n",
      "Best match: gdown 5.2.0\n",
      "Adding gdown 5.2.0 to easy-install.pth file\n",
      "Installing gdown script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for yacs==0.1.8\n",
      "Best match: yacs 0.1.8\n",
      "Adding yacs 0.1.8 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for isort==4.3.21\n",
      "Best match: isort 4.3.21\n",
      "Adding isort 4.3.21 to easy-install.pth file\n",
      "Installing isort script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for yapf==0.29.0\n",
      "Best match: yapf 0.29.0\n",
      "Adding yapf 0.29.0 to easy-install.pth file\n",
      "Installing yapf script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for flake8==3.7.9\n",
      "Best match: flake8 3.7.9\n",
      "Adding flake8 3.7.9 to easy-install.pth file\n",
      "Installing flake8 script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for torchvision==0.20.1+cu121\n",
      "Best match: torchvision 0.20.1+cu121\n",
      "Adding torchvision 0.20.1+cu121 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for torch==2.5.1+cu121\n",
      "Best match: torch 2.5.1+cu121\n",
      "Adding torch 2.5.1+cu121 to easy-install.pth file\n",
      "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
      "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
      "Installing torchfrtrace script to /usr/local/bin\n",
      "Installing torchrun script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for pytz==2024.2\n",
      "Best match: pytz 2024.2\n",
      "Adding pytz 2024.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for pillow==11.0.0\n",
      "Best match: pillow 11.0.0\n",
      "Adding pillow 11.0.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for pandas==2.2.2\n",
      "Best match: pandas 2.2.2\n",
      "Adding pandas 2.2.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for outdated==0.2.2\n",
      "Best match: outdated 0.2.2\n",
      "Adding outdated 0.2.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for ogb==1.3.6\n",
      "Best match: ogb 1.3.6\n",
      "Adding ogb 1.3.6 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for numpy==1.26.4\n",
      "Best match: numpy 1.26.4\n",
      "Adding numpy 1.26.4 to easy-install.pth file\n",
      "Installing f2py script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for wcwidth==0.2.13\n",
      "Best match: wcwidth 0.2.13\n",
      "Adding wcwidth 0.2.13 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for threadpoolctl==3.5.0\n",
      "Best match: threadpoolctl 3.5.0\n",
      "Adding threadpoolctl 3.5.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for joblib==1.4.2\n",
      "Best match: joblib 1.4.2\n",
      "Adding joblib 1.4.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for werkzeug==3.1.3\n",
      "Best match: werkzeug 3.1.3\n",
      "Adding werkzeug 3.1.3 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for tensorboard-data-server==0.7.2\n",
      "Best match: tensorboard-data-server 0.7.2\n",
      "Adding tensorboard-data-server 0.7.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for six==1.16.0\n",
      "Best match: six 1.16.0\n",
      "Adding six 1.16.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for setuptools==75.1.0\n",
      "Best match: setuptools 75.1.0\n",
      "Adding setuptools 75.1.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for protobuf==4.25.5\n",
      "Best match: protobuf 4.25.5\n",
      "Adding protobuf 4.25.5 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for packaging==24.2\n",
      "Best match: packaging 24.2\n",
      "Adding packaging 24.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for Markdown==3.7\n",
      "Best match: Markdown 3.7\n",
      "Adding Markdown 3.7 to easy-install.pth file\n",
      "Installing markdown_py script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for grpcio==1.68.0\n",
      "Best match: grpcio 1.68.0\n",
      "Adding grpcio 1.68.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for absl-py==1.4.0\n",
      "Best match: absl-py 1.4.0\n",
      "Adding absl-py 1.4.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for requests==2.32.3\n",
      "Best match: requests 2.32.3\n",
      "Adding requests 2.32.3 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for filelock==3.16.1\n",
      "Best match: filelock 3.16.1\n",
      "Adding filelock 3.16.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for beautifulsoup4==4.12.3\n",
      "Best match: beautifulsoup4 4.12.3\n",
      "Adding beautifulsoup4 4.12.3 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for PyYAML==6.0.2\n",
      "Best match: PyYAML 6.0.2\n",
      "Adding PyYAML 6.0.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for mccabe==0.6.1\n",
      "Best match: mccabe 0.6.1\n",
      "Adding mccabe 0.6.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for pycodestyle==2.5.0\n",
      "Best match: pycodestyle 2.5.0\n",
      "Adding pycodestyle 2.5.0 to easy-install.pth file\n",
      "Installing pycodestyle script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for pyflakes==2.1.1\n",
      "Best match: pyflakes 2.1.1\n",
      "Adding pyflakes 2.1.1 to easy-install.pth file\n",
      "Installing pyflakes script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for entrypoints==0.3\n",
      "Best match: entrypoints 0.3\n",
      "Adding entrypoints 0.3 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for sympy==1.13.1\n",
      "Best match: sympy 1.13.1\n",
      "Adding sympy 1.13.1 to easy-install.pth file\n",
      "Installing isympy script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for fsspec==2024.10.0\n",
      "Best match: fsspec 2024.10.0\n",
      "Adding fsspec 2024.10.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for jinja2==3.1.4\n",
      "Best match: jinja2 3.1.4\n",
      "Adding jinja2 3.1.4 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for networkx==3.4.2\n",
      "Best match: networkx 3.4.2\n",
      "Adding networkx 3.4.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for typing-extensions==4.12.2\n",
      "Best match: typing-extensions 4.12.2\n",
      "Adding typing-extensions 4.12.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages/setuptools/_vendor\n",
      "Searching for tzdata==2024.2\n",
      "Best match: tzdata 2024.2\n",
      "Adding tzdata 2024.2 to easy-install.pth file\n",
      "detected new path './setuptools/_vendor'\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for python-dateutil==2.8.2\n",
      "Best match: python-dateutil 2.8.2\n",
      "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for littleutils==0.2.4\n",
      "Best match: littleutils 0.2.4\n",
      "Adding littleutils 0.2.4 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for urllib3==2.2.3\n",
      "Best match: urllib3 2.2.3\n",
      "Adding urllib3 2.2.3 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for MarkupSafe==3.0.2\n",
      "Best match: MarkupSafe 3.0.2\n",
      "Adding MarkupSafe 3.0.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for PySocks==1.7.1\n",
      "Best match: PySocks 1.7.1\n",
      "Adding PySocks 1.7.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for certifi==2024.8.30\n",
      "Best match: certifi 2024.8.30\n",
      "Adding certifi 2024.8.30 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for idna==3.10\n",
      "Best match: idna 3.10\n",
      "Adding idna 3.10 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for charset-normalizer==3.4.0\n",
      "Best match: charset-normalizer 3.4.0\n",
      "Adding charset-normalizer 3.4.0 to easy-install.pth file\n",
      "Installing normalizer script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for soupsieve==2.6\n",
      "Best match: soupsieve 2.6\n",
      "Adding soupsieve 2.6 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Searching for mpmath==1.3.0\n",
      "Best match: mpmath 1.3.0\n",
      "Adding mpmath 1.3.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages\n",
      "Finished processing dependencies for dassl==0.6.3\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.3.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
      "Collecting torchcam\n",
      "  Downloading torchcam-0.4.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchcam) (1.26.4)\n",
      "Requirement already satisfied: Pillow!=9.2.0,>=8.4.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (11.0.0)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (2.8.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.0.0->torchcam) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.0->torchcam) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->torchcam) (3.0.2)\n",
      "Downloading torchcam-0.4.0-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchcam\n",
      "Successfully installed torchcam-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!python setup.py develop\n",
    "!pip install regex\n",
    "!pip install ftfy\n",
    "!pip install tqdm\n",
    "!pip install torchcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSJvsvvGC1Ni",
    "outputId": "091b3104-7581-4a7c-965b-d044749d1bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/DAPT\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gldJlKllGDan",
    "outputId": "31291732-d4ca-4df8-e3bb-fe65c11a8be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=791f453090d646694451ce42d216bb20d1d8fa029e2a67adcbd76102b9813aa2\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  tree\n",
      "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
      "Need to get 47.9 kB of archives.\n",
      "After this operation, 116 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
      "Fetched 47.9 kB in 1s (85.1 kB/s)\n",
      "Selecting previously unselected package tree.\n",
      "(Reading database ... 123630 files and directories currently installed.)\n",
      "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
      "Unpacking tree (2.0.2-1) ...\n",
      "Setting up tree (2.0.2-1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "!apt-get install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWG4pEyXJyq7"
   },
   "outputs": [],
   "source": [
    "# !python train.py --root ../ --seed 1 --trainer RPO --dataset-config-file configs/datasets/eurosat.yaml --config-file configs/trainers/RPO/main_K24.yaml --output-dir output/rpo/base2new/train_base/eurosat/shots_16/RPO/main_K24/ep1512= DATASET.NUM_SHOTS 16 DATASET.SUBSAMPLE_CLASSES base TRAINER.RPO.K 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vRZlFAYfX6v",
    "outputId": "09c04413-15a5-47c0-daa4-6dbc3a2333f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "디렉토리 구조:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "DATA_DIR = \"DATA/eurosat\"\n",
    "print(\"\\n디렉토리 구조:\")\n",
    "os.system(f\"tree {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFGTiBqfXZUP",
    "outputId": "96c1c72a-19d1-4228-cc98-4d43e1f0321f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJ_D5AiOW8aQ",
    "outputId": "0db882f3-277a-4816-b811-4849a05bc91d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/DAPT\n"
     ]
    }
   ],
   "source": [
    "%cd ../MyDrive/DAPT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OI594A7odD9p",
    "outputId": "9bd12a3b-3211-4e3a-cd4c-a45d41f37ff9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive/DAPT'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pW9fktbZfvgv",
    "outputId": "605d4cff-2024-494e-ac32-3967f366e189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2750  split_fewshot  split_zhou_EuroSAT.json\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/MyDrive/DAPT/DATA/eurosat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3v-vdjQGorpk"
   },
   "source": [
    "# Eurosat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMZtXDswwJqS"
   },
   "source": [
    "##Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Tc1--NsgbQO",
    "outputId": "65177221-f8a3-4204-97db-942eb9a8b6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 16:25:55.726917: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 16:25:55.746411: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 16:25:55.752194: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 16:25:55.766397: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 16:25:56.991710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_16-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  160\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.txt_ctx', 'prompt_learner.vis_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "EuroSAT (SHOTS: 16)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#eurosat prototype-16shots-seed1\n",
    "\n",
    "!python train.py \\\n",
    "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "            --seed 1 \\\n",
    "            --trainer DAPT \\\n",
    "            --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "            --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "            DATASET.NUM_SHOTS 16 \\\n",
    "            TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1uGbt9Ut-p3",
    "outputId": "f21ebab4-79ee-464c-ec0a-77611b44f936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:01:49.703294: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:01:49.723352: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:01:49.729262: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:01:49.743466: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:01:50.763708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '8', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 8\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  80\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.vis_ctx', 'prompt_learner.txt_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "EuroSAT (SHOTS: 8)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#eurosat prototype-8shots-seed1\n",
    "\n",
    "!python3 train.py \\\n",
    " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    " --seed 1 \\\n",
    " --trainer DAPT \\\n",
    " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    " --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    " DATASET.NUM_SHOTS 8 \\\n",
    " TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTDAxN44PzlZ",
    "outputId": "d72f8fd1-c76f-4658-ac36-a5af8966ed77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:59:33.931688: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:59:33.951513: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:59:33.957420: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:59:33.972126: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:59:34.981118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '8', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 8\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Creating a 8-shot dataset\n",
      "Creating a 4-shot dataset\n",
      "Saving preprocessed few-shot data to /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_2.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  80\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "EuroSAT (SHOTS: 8)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#eurosat prototype-8shots-seed2\n",
    "\n",
    "!python3 train.py \\\n",
    " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    " --seed 2 \\\n",
    " --trainer DAPT \\\n",
    " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    " --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    " DATASET.NUM_SHOTS 8 \\\n",
    " TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AaEnoiCkP2Np",
    "outputId": "a715c755-f84b-40be-e030-7838dae6b814"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 16:00:23.086320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 16:00:23.106290: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 16:00:23.112246: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 16:00:23.126276: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 16:00:24.143050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '8', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 8\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Creating a 8-shot dataset\n",
      "Creating a 4-shot dataset\n",
      "Saving preprocessed few-shot data to /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_3.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  80\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "EuroSAT (SHOTS: 8)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#eurosat prototype-8shots-seed3\n",
    "\n",
    "!python3 train.py \\\n",
    " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    " --seed 3 \\\n",
    " --trainer DAPT \\\n",
    " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    " --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    " DATASET.NUM_SHOTS 8 \\\n",
    " TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5W7OVsIvYFm",
    "outputId": "e2b2b49c-ecd3-48cf-93d4-e5ce656f6003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:02:38.575094: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:02:38.610095: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:02:38.619845: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:02:38.644310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:02:40.182538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 4\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  40\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.vis_ctx', 'prompt_learner.txt_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "EuroSAT (SHOTS: 4)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#eurosat prototype-4shots-seed1\n",
    "\n",
    "!python3 train.py \\\n",
    " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    " --seed 1 \\\n",
    " --trainer DAPT \\\n",
    " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    " DATASET.NUM_SHOTS 4 \\\n",
    " TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHMTdDz3JfqT",
    "outputId": "06e34117-ff81-4188-f70d-da6370032cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:31:52.828153: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:31:52.847522: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:31:52.853460: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:31:52.868277: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:31:53.951621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 4\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Creating a 4-shot dataset\n",
      "Creating a 4-shot dataset\n",
      "Saving preprocessed few-shot data to /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_2.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  40\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "EuroSAT (SHOTS: 4)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#eurosat prototype-4shots-seed2\n",
    "\n",
    "!python3 train.py \\\n",
    " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    " --seed 2 \\\n",
    " --trainer DAPT \\\n",
    " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    " DATASET.NUM_SHOTS 4 \\\n",
    " TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dmEkpJZJ55Q",
    "outputId": "f9667867-9ab5-424f-df80-c78079201dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:33:39.507814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:33:39.527492: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:33:39.533460: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:33:39.547756: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:33:40.575038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 4\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Creating a 4-shot dataset\n",
      "Creating a 4-shot dataset\n",
      "Saving preprocessed few-shot data to /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_3.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  40\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "EuroSAT (SHOTS: 4)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#eurosat prototype-4shots-seed3\n",
    "\n",
    "!python3 train.py \\\n",
    " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    " --seed 3 \\\n",
    " --trainer DAPT \\\n",
    " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    " DATASET.NUM_SHOTS 4 \\\n",
    " TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0NR797HOvcfw",
    "outputId": "4c2a5e37-1436-4194-c750-a1db43671be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:03:14.993982: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:03:15.013106: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:03:15.019265: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:03:15.033382: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:03:16.060554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  20\n",
      "# val      20\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.txt_ctx', 'prompt_learner.vis_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "EuroSAT (SHOTS: 2)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#eurosat prototype-2shots-seed1\n",
    "\n",
    "!python3 train.py \\\n",
    " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    " --seed 1 \\\n",
    " --trainer DAPT \\\n",
    " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    " DATASET.NUM_SHOTS 2 \\\n",
    " TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QWMQypILHS1",
    "outputId": "46053654-4cad-49a4-b383-18f5d70f11b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:40:28.546588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:40:28.568259: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:40:28.574590: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:40:28.589401: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:40:29.619936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Creating a 2-shot dataset\n",
      "Creating a 2-shot dataset\n",
      "Saving preprocessed few-shot data to /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_2.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  20\n",
      "# val      20\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "EuroSAT (SHOTS: 2)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#eurosat prototype-2shots-seed2\n",
    "\n",
    "!python3 train.py \\\n",
    " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    " --seed 2 \\\n",
    " --trainer DAPT \\\n",
    " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    " DATASET.NUM_SHOTS 2 \\\n",
    " TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4p9-jk2JLMZD",
    "outputId": "e411752b-6e2a-4ad5-d1ec-605ac579fbc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:40:55.382532: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:40:55.402452: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:40:55.408299: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:40:55.422099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:40:56.428080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Creating a 2-shot dataset\n",
      "Creating a 2-shot dataset\n",
      "Saving preprocessed few-shot data to /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_3.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  20\n",
      "# val      20\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "EuroSAT (SHOTS: 2)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#eurosat prototype-2shots-seed3\n",
    "\n",
    "!python3 train.py \\\n",
    " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    " --seed 3 \\\n",
    " --trainer DAPT \\\n",
    " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    " DATASET.NUM_SHOTS 2 \\\n",
    " TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6jIDWhlvl0O",
    "outputId": "8470711f-c427-48f9-b702-6b45adc22dc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:03:43.998958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:03:44.018477: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:03:44.024401: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:03:44.038285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:03:45.056503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '1', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 1\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 50\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_1-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  10\n",
      "# val      10\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.txt_ctx', 'prompt_learner.vis_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "EuroSAT (SHOTS: 1)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#eurosat prototype-1shots-seed1\n",
    "\n",
    "!python3 train.py \\\n",
    " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    " --seed 1 \\\n",
    " --trainer DAPT \\\n",
    " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    " --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
    " DATASET.NUM_SHOTS 1 \\\n",
    " TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LU6JtYkHunM2"
   },
   "source": [
    "##Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGklAx4EhWsX",
    "outputId": "f979dead-daa8-402a-c7d8-34c7d2be998e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:01:41.301988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:01:41.322888: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:01:41.328895: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:01:41.343114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:01:42.348175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16']\n",
      "output_dir: output/eurosat/DAPT/vit_b16_16shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/eurosat/DAPT/vit_b16_16shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_16-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  160\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/eurosat/DAPT/vit_b16_16shots/seed1/tensorboard)\n",
      "epoch [1/200] batch [5/5] time 0.254 (0.906) data 0.000 (0.332) loss 11.3727 (11.5112) acc 6.2500 (10.0000) lr 2.0000e+01 eta 0:15:01\n",
      "epoch [2/200] batch [5/5] time 0.258 (0.474) data 0.000 (0.222) loss 9.9746 (10.2003) acc 9.3750 (7.5000) lr 1.9999e+01 eta 0:07:48\n",
      "epoch [3/200] batch [5/5] time 0.255 (0.432) data 0.000 (0.180) loss 9.7693 (9.8531) acc 9.3750 (6.8750) lr 1.9995e+01 eta 0:07:05\n",
      "epoch [4/200] batch [5/5] time 0.254 (0.447) data 0.000 (0.195) loss 9.7230 (9.7074) acc 6.2500 (7.5000) lr 1.9989e+01 eta 0:07:18\n",
      "epoch [5/200] batch [5/5] time 0.251 (0.444) data 0.000 (0.192) loss 9.6226 (9.6196) acc 0.0000 (5.0000) lr 1.9980e+01 eta 0:07:13\n",
      "epoch [6/200] batch [5/5] time 0.261 (0.481) data 0.000 (0.219) loss 9.3626 (9.4812) acc 9.3750 (12.5000) lr 1.9969e+01 eta 0:07:46\n",
      "epoch [7/200] batch [5/5] time 0.268 (0.581) data 0.000 (0.311) loss 8.8567 (9.1467) acc 25.0000 (12.5000) lr 1.9956e+01 eta 0:09:20\n",
      "epoch [8/200] batch [5/5] time 0.254 (0.464) data 0.000 (0.210) loss 5.5669 (7.5054) acc 6.2500 (16.2500) lr 1.9940e+01 eta 0:07:25\n",
      "epoch [9/200] batch [5/5] time 0.253 (0.456) data 0.000 (0.202) loss 10.0880 (10.1076) acc 9.3750 (7.5000) lr 1.9921e+01 eta 0:07:15\n",
      "epoch [10/200] batch [5/5] time 0.254 (0.447) data 0.000 (0.184) loss 8.9414 (9.4035) acc 9.3750 (10.0000) lr 1.9900e+01 eta 0:07:04\n",
      "epoch [11/200] batch [5/5] time 0.254 (0.443) data 0.000 (0.190) loss 7.3760 (7.9826) acc 9.3750 (10.6250) lr 1.9877e+01 eta 0:06:59\n",
      "epoch [12/200] batch [5/5] time 0.256 (0.494) data 0.001 (0.238) loss 5.9303 (6.3471) acc 15.6250 (15.6250) lr 1.9851e+01 eta 0:07:44\n",
      "epoch [13/200] batch [5/5] time 0.262 (0.571) data 0.000 (0.293) loss 4.9451 (5.1904) acc 18.7500 (18.7500) lr 1.9823e+01 eta 0:08:53\n",
      "epoch [14/200] batch [5/5] time 0.255 (0.495) data 0.000 (0.239) loss 4.0103 (4.4721) acc 9.3750 (11.2500) lr 1.9792e+01 eta 0:07:40\n",
      "epoch [15/200] batch [5/5] time 0.263 (0.447) data 0.000 (0.191) loss 6.9367 (5.4600) acc 9.3750 (16.2500) lr 1.9759e+01 eta 0:06:53\n",
      "epoch [16/200] batch [5/5] time 0.256 (0.445) data 0.000 (0.189) loss 8.6148 (10.0724) acc 6.2500 (6.8750) lr 1.9724e+01 eta 0:06:49\n",
      "epoch [17/200] batch [5/5] time 0.256 (0.444) data 0.000 (0.183) loss 4.2595 (7.5933) acc 21.8750 (15.6250) lr 1.9686e+01 eta 0:06:46\n",
      "epoch [18/200] batch [5/5] time 0.262 (0.537) data 0.002 (0.272) loss 3.1157 (3.4743) acc 46.8750 (32.5000) lr 1.9646e+01 eta 0:08:08\n",
      "epoch [19/200] batch [5/5] time 0.262 (0.591) data 0.000 (0.323) loss 2.3592 (2.8220) acc 50.0000 (37.5000) lr 1.9603e+01 eta 0:08:55\n",
      "epoch [20/200] batch [5/5] time 0.262 (0.474) data 0.000 (0.214) loss 2.2935 (2.5462) acc 53.1250 (48.1250) lr 1.9558e+01 eta 0:07:06\n",
      "epoch [21/200] batch [5/5] time 0.260 (0.438) data 0.000 (0.175) loss 1.8957 (2.2684) acc 62.5000 (46.8750) lr 1.9511e+01 eta 0:06:32\n",
      "epoch [22/200] batch [5/5] time 0.257 (0.480) data 0.000 (0.221) loss 3.6155 (2.7378) acc 40.6250 (47.5000) lr 1.9461e+01 eta 0:07:06\n",
      "epoch [23/200] batch [5/5] time 0.261 (0.448) data 0.000 (0.189) loss 2.7424 (3.2688) acc 56.2500 (48.7500) lr 1.9409e+01 eta 0:06:36\n",
      "epoch [24/200] batch [5/5] time 0.260 (0.523) data 0.000 (0.262) loss 1.5222 (2.3325) acc 78.1250 (63.1250) lr 1.9354e+01 eta 0:07:40\n",
      "epoch [25/200] batch [5/5] time 0.275 (0.549) data 0.000 (0.272) loss 1.6631 (1.6449) acc 56.2500 (66.2500) lr 1.9298e+01 eta 0:08:00\n",
      "epoch [26/200] batch [5/5] time 0.262 (0.490) data 0.000 (0.229) loss 1.4200 (1.3905) acc 65.6250 (71.8750) lr 1.9239e+01 eta 0:07:06\n",
      "epoch [27/200] batch [5/5] time 0.261 (0.454) data 0.000 (0.194) loss 1.9479 (1.5911) acc 59.3750 (63.7500) lr 1.9178e+01 eta 0:06:32\n",
      "epoch [28/200] batch [5/5] time 0.258 (0.436) data 0.000 (0.171) loss 1.4660 (1.3776) acc 68.7500 (65.6250) lr 1.9114e+01 eta 0:06:15\n",
      "epoch [29/200] batch [5/5] time 0.259 (0.447) data 0.000 (0.182) loss 6.6763 (3.3830) acc 25.0000 (45.0000) lr 1.9048e+01 eta 0:06:22\n",
      "epoch [30/200] batch [5/5] time 0.260 (0.583) data 0.000 (0.319) loss 2.1796 (3.1937) acc 53.1250 (53.1250) lr 1.8980e+01 eta 0:08:15\n",
      "epoch [31/200] batch [5/5] time 0.273 (0.588) data 0.000 (0.310) loss 1.8673 (1.9081) acc 59.3750 (59.3750) lr 1.8910e+01 eta 0:08:17\n",
      "epoch [32/200] batch [5/5] time 0.260 (0.494) data 0.000 (0.233) loss 1.5317 (1.8143) acc 68.7500 (61.8750) lr 1.8838e+01 eta 0:06:54\n",
      "epoch [33/200] batch [5/5] time 0.258 (0.433) data 0.000 (0.163) loss 1.1975 (1.3125) acc 75.0000 (72.5000) lr 1.8763e+01 eta 0:06:01\n",
      "epoch [34/200] batch [5/5] time 0.263 (0.476) data 0.000 (0.210) loss 1.4098 (1.2295) acc 65.6250 (78.1250) lr 1.8686e+01 eta 0:06:35\n",
      "epoch [35/200] batch [5/5] time 0.262 (0.439) data 0.000 (0.173) loss 1.8024 (1.9097) acc 62.5000 (59.3750) lr 1.8607e+01 eta 0:06:02\n",
      "epoch [36/200] batch [5/5] time 0.266 (0.516) data 0.000 (0.248) loss 2.6184 (2.2786) acc 46.8750 (50.6250) lr 1.8526e+01 eta 0:07:02\n",
      "epoch [37/200] batch [5/5] time 0.272 (0.592) data 0.000 (0.315) loss 2.2869 (3.7051) acc 62.5000 (46.8750) lr 1.8443e+01 eta 0:08:02\n",
      "epoch [38/200] batch [5/5] time 0.262 (0.469) data 0.000 (0.203) loss 2.0218 (2.0050) acc 59.3750 (65.6250) lr 1.8358e+01 eta 0:06:19\n",
      "epoch [39/200] batch [5/5] time 0.262 (0.451) data 0.000 (0.188) loss 1.9644 (1.9104) acc 56.2500 (59.3750) lr 1.8271e+01 eta 0:06:02\n",
      "epoch [40/200] batch [5/5] time 0.263 (0.460) data 0.000 (0.195) loss 1.3878 (1.6319) acc 68.7500 (61.2500) lr 1.8181e+01 eta 0:06:07\n",
      "epoch [41/200] batch [5/5] time 0.268 (0.448) data 0.000 (0.176) loss 1.3708 (1.3601) acc 71.8750 (75.6250) lr 1.8090e+01 eta 0:05:56\n",
      "epoch [42/200] batch [5/5] time 0.267 (0.527) data 0.000 (0.259) loss 1.7355 (1.2953) acc 68.7500 (77.5000) lr 1.7997e+01 eta 0:06:56\n",
      "epoch [43/200] batch [5/5] time 0.271 (0.603) data 0.000 (0.323) loss 1.3929 (1.5739) acc 78.1250 (70.0000) lr 1.7902e+01 eta 0:07:53\n",
      "epoch [44/200] batch [5/5] time 0.263 (0.471) data 0.000 (0.204) loss 12.0946 (6.3728) acc 15.6250 (38.7500) lr 1.7804e+01 eta 0:06:07\n",
      "epoch [45/200] batch [5/5] time 0.263 (0.461) data 0.000 (0.194) loss 2.4530 (3.8665) acc 53.1250 (60.0000) lr 1.7705e+01 eta 0:05:57\n",
      "epoch [46/200] batch [5/5] time 0.263 (0.446) data 0.000 (0.178) loss 1.7040 (1.8596) acc 68.7500 (67.5000) lr 1.7604e+01 eta 0:05:43\n",
      "epoch [47/200] batch [5/5] time 0.265 (0.446) data 0.000 (0.174) loss 1.7204 (1.6770) acc 65.6250 (68.7500) lr 1.7501e+01 eta 0:05:41\n",
      "epoch [48/200] batch [5/5] time 0.267 (0.508) data 0.000 (0.211) loss 1.0903 (1.5420) acc 81.2500 (68.7500) lr 1.7396e+01 eta 0:06:25\n",
      "epoch [49/200] batch [5/5] time 0.274 (0.590) data 0.000 (0.312) loss 1.3273 (1.4280) acc 68.7500 (71.2500) lr 1.7290e+01 eta 0:07:25\n",
      "epoch [50/200] batch [5/5] time 0.268 (0.480) data 0.000 (0.211) loss 1.0033 (1.1014) acc 84.3750 (80.6250) lr 1.7181e+01 eta 0:06:00\n",
      "epoch [51/200] batch [5/5] time 0.267 (0.460) data 0.000 (0.191) loss 1.7378 (1.5729) acc 53.1250 (65.6250) lr 1.7071e+01 eta 0:05:42\n",
      "epoch [52/200] batch [5/5] time 0.269 (0.446) data 0.000 (0.175) loss 2.4477 (2.3749) acc 56.2500 (58.1250) lr 1.6959e+01 eta 0:05:29\n",
      "epoch [53/200] batch [5/5] time 0.269 (0.457) data 0.000 (0.188) loss 2.9026 (2.6877) acc 25.0000 (44.3750) lr 1.6845e+01 eta 0:05:36\n",
      "epoch [54/200] batch [5/5] time 0.272 (0.555) data 0.000 (0.283) loss 2.0791 (1.8480) acc 43.7500 (59.3750) lr 1.6730e+01 eta 0:06:44\n",
      "epoch [55/200] batch [5/5] time 0.278 (0.630) data 0.001 (0.340) loss 1.3585 (1.4582) acc 71.8750 (73.7500) lr 1.6613e+01 eta 0:07:36\n",
      "epoch [56/200] batch [5/5] time 0.267 (0.472) data 0.000 (0.202) loss 0.8848 (0.9853) acc 84.3750 (81.8750) lr 1.6494e+01 eta 0:05:39\n",
      "epoch [57/200] batch [5/5] time 0.270 (0.459) data 0.000 (0.182) loss 1.2363 (1.1230) acc 68.7500 (75.6250) lr 1.6374e+01 eta 0:05:28\n",
      "epoch [58/200] batch [5/5] time 0.266 (0.456) data 0.000 (0.183) loss 1.3293 (1.2583) acc 68.7500 (70.0000) lr 1.6252e+01 eta 0:05:23\n",
      "epoch [59/200] batch [5/5] time 0.270 (0.469) data 0.000 (0.196) loss 1.6399 (1.3629) acc 62.5000 (71.8750) lr 1.6129e+01 eta 0:05:30\n",
      "epoch [60/200] batch [5/5] time 0.269 (0.553) data 0.000 (0.280) loss 1.5299 (1.3495) acc 71.8750 (74.3750) lr 1.6004e+01 eta 0:06:27\n",
      "epoch [61/200] batch [5/5] time 0.279 (0.612) data 0.000 (0.329) loss 4.2939 (2.3144) acc 37.5000 (56.2500) lr 1.5878e+01 eta 0:07:05\n",
      "epoch [62/200] batch [5/5] time 0.272 (0.480) data 0.000 (0.208) loss 1.6238 (2.3001) acc 71.8750 (59.3750) lr 1.5750e+01 eta 0:05:31\n",
      "epoch [63/200] batch [5/5] time 0.270 (0.468) data 0.000 (0.194) loss 1.1954 (1.4838) acc 78.1250 (72.5000) lr 1.5621e+01 eta 0:05:20\n",
      "epoch [64/200] batch [5/5] time 0.271 (0.458) data 0.000 (0.184) loss 1.0884 (1.2507) acc 78.1250 (75.6250) lr 1.5490e+01 eta 0:05:11\n",
      "epoch [65/200] batch [5/5] time 0.268 (0.465) data 0.000 (0.192) loss 1.1866 (1.1760) acc 78.1250 (76.2500) lr 1.5358e+01 eta 0:05:14\n",
      "epoch [66/200] batch [5/5] time 0.273 (0.536) data 0.000 (0.252) loss 1.1993 (1.0040) acc 68.7500 (78.7500) lr 1.5225e+01 eta 0:05:59\n",
      "epoch [67/200] batch [5/5] time 0.277 (0.622) data 0.000 (0.340) loss 1.3211 (1.1670) acc 71.8750 (76.8750) lr 1.5090e+01 eta 0:06:53\n",
      "epoch [68/200] batch [5/5] time 0.271 (0.479) data 0.000 (0.204) loss 2.2226 (1.5738) acc 59.3750 (66.2500) lr 1.4955e+01 eta 0:05:16\n",
      "epoch [69/200] batch [5/5] time 0.270 (0.447) data 0.000 (0.169) loss 1.2919 (1.4872) acc 87.5000 (73.1250) lr 1.4818e+01 eta 0:04:52\n",
      "epoch [70/200] batch [5/5] time 0.272 (0.463) data 0.000 (0.188) loss 1.2734 (1.3951) acc 84.3750 (76.8750) lr 1.4679e+01 eta 0:05:00\n",
      "epoch [71/200] batch [5/5] time 0.277 (0.472) data 0.000 (0.195) loss 1.5451 (1.3080) acc 59.3750 (71.8750) lr 1.4540e+01 eta 0:05:04\n",
      "epoch [72/200] batch [5/5] time 0.279 (0.569) data 0.000 (0.289) loss 1.3870 (1.1405) acc 75.0000 (81.2500) lr 1.4399e+01 eta 0:06:04\n",
      "epoch [73/200] batch [5/5] time 0.284 (0.609) data 0.000 (0.322) loss 1.3720 (1.3323) acc 71.8750 (70.0000) lr 1.4258e+01 eta 0:06:26\n",
      "epoch [74/200] batch [5/5] time 0.273 (0.486) data 0.000 (0.209) loss 1.1543 (1.0392) acc 71.8750 (77.5000) lr 1.4115e+01 eta 0:05:05\n",
      "epoch [75/200] batch [5/5] time 0.272 (0.456) data 0.000 (0.175) loss 0.9056 (1.1203) acc 81.2500 (75.6250) lr 1.3971e+01 eta 0:04:45\n",
      "epoch [76/200] batch [5/5] time 0.273 (0.475) data 0.000 (0.198) loss 1.1962 (0.8992) acc 81.2500 (85.6250) lr 1.3827e+01 eta 0:04:54\n",
      "epoch [77/200] batch [5/5] time 0.274 (0.473) data 0.000 (0.195) loss 1.2102 (1.2675) acc 78.1250 (71.8750) lr 1.3681e+01 eta 0:04:50\n",
      "epoch [78/200] batch [5/5] time 0.287 (0.601) data 0.000 (0.291) loss 1.5602 (1.4245) acc 75.0000 (70.6250) lr 1.3535e+01 eta 0:06:06\n",
      "epoch [79/200] batch [5/5] time 0.286 (0.652) data 0.000 (0.360) loss 1.8685 (1.5401) acc 46.8750 (65.6250) lr 1.3387e+01 eta 0:06:34\n",
      "epoch [80/200] batch [5/5] time 0.276 (0.524) data 0.000 (0.246) loss 1.3536 (1.2498) acc 62.5000 (75.6250) lr 1.3239e+01 eta 0:05:14\n",
      "epoch [81/200] batch [5/5] time 0.278 (0.458) data 0.000 (0.178) loss 1.5664 (1.0629) acc 71.8750 (79.3750) lr 1.3090e+01 eta 0:04:32\n",
      "epoch [82/200] batch [5/5] time 0.274 (0.473) data 0.000 (0.194) loss 1.3963 (1.4824) acc 68.7500 (68.7500) lr 1.2940e+01 eta 0:04:39\n",
      "epoch [83/200] batch [5/5] time 0.275 (0.484) data 0.000 (0.205) loss 1.0443 (1.1402) acc 78.1250 (73.7500) lr 1.2790e+01 eta 0:04:43\n",
      "epoch [84/200] batch [5/5] time 0.280 (0.584) data 0.000 (0.298) loss 0.8966 (1.0750) acc 81.2500 (79.3750) lr 1.2639e+01 eta 0:05:38\n",
      "epoch [85/200] batch [5/5] time 0.279 (0.507) data 0.000 (0.227) loss 1.3843 (0.8533) acc 71.8750 (83.7500) lr 1.2487e+01 eta 0:04:51\n",
      "epoch [86/200] batch [5/5] time 0.276 (0.477) data 0.000 (0.196) loss 1.3774 (1.4365) acc 75.0000 (70.0000) lr 1.2334e+01 eta 0:04:31\n",
      "epoch [87/200] batch [5/5] time 0.264 (0.475) data 0.000 (0.197) loss 1.2543 (1.0872) acc 71.8750 (81.2500) lr 1.2181e+01 eta 0:04:28\n",
      "epoch [88/200] batch [5/5] time 0.274 (0.476) data 0.000 (0.195) loss 1.1974 (0.9926) acc 68.7500 (78.1250) lr 1.2028e+01 eta 0:04:26\n",
      "epoch [89/200] batch [5/5] time 0.280 (0.553) data 0.000 (0.265) loss 1.4432 (1.2618) acc 68.7500 (67.5000) lr 1.1874e+01 eta 0:05:06\n",
      "epoch [90/200] batch [5/5] time 0.291 (0.642) data 0.000 (0.351) loss 1.5282 (1.5336) acc 62.5000 (63.7500) lr 1.1719e+01 eta 0:05:53\n",
      "epoch [91/200] batch [5/5] time 0.279 (0.463) data 0.000 (0.176) loss 1.1628 (1.2918) acc 68.7500 (70.0000) lr 1.1564e+01 eta 0:04:12\n",
      "epoch [92/200] batch [5/5] time 0.279 (0.479) data 0.000 (0.196) loss 1.3754 (1.1032) acc 68.7500 (76.8750) lr 1.1409e+01 eta 0:04:18\n",
      "epoch [93/200] batch [5/5] time 0.280 (0.448) data 0.000 (0.163) loss 0.8065 (1.0698) acc 84.3750 (76.2500) lr 1.1253e+01 eta 0:03:59\n",
      "epoch [94/200] batch [5/5] time 0.282 (0.471) data 0.000 (0.185) loss 1.1016 (1.1336) acc 75.0000 (76.8750) lr 1.1097e+01 eta 0:04:09\n",
      "epoch [95/200] batch [5/5] time 0.283 (0.564) data 0.000 (0.276) loss 1.2910 (1.1119) acc 78.1250 (81.2500) lr 1.0941e+01 eta 0:04:56\n",
      "epoch [96/200] batch [5/5] time 0.293 (0.624) data 0.000 (0.332) loss 0.7335 (0.8576) acc 87.5000 (84.3750) lr 1.0785e+01 eta 0:05:24\n",
      "epoch [97/200] batch [5/5] time 0.280 (0.472) data 0.000 (0.187) loss 1.0555 (0.9459) acc 84.3750 (81.8750) lr 1.0628e+01 eta 0:04:03\n",
      "epoch [98/200] batch [5/5] time 0.280 (0.483) data 0.000 (0.199) loss 0.7928 (0.8161) acc 78.1250 (83.1250) lr 1.0471e+01 eta 0:04:06\n",
      "epoch [99/200] batch [5/5] time 0.280 (0.481) data 0.000 (0.197) loss 1.0358 (0.9911) acc 68.7500 (78.1250) lr 1.0314e+01 eta 0:04:02\n",
      "epoch [100/200] batch [5/5] time 0.280 (0.469) data 0.000 (0.182) loss 0.9678 (0.8384) acc 81.2500 (85.0000) lr 1.0157e+01 eta 0:03:54\n",
      "epoch [101/200] batch [5/5] time 0.284 (0.592) data 0.000 (0.304) loss 1.5329 (1.0514) acc 68.7500 (79.3750) lr 1.0000e+01 eta 0:04:52\n",
      "epoch [102/200] batch [5/5] time 0.278 (0.508) data 0.000 (0.224) loss 1.6473 (1.1222) acc 56.2500 (75.0000) lr 9.8429e+00 eta 0:04:08\n",
      "epoch [103/200] batch [5/5] time 0.280 (0.472) data 0.000 (0.189) loss 0.9545 (1.1379) acc 84.3750 (76.8750) lr 9.6859e+00 eta 0:03:49\n",
      "epoch [104/200] batch [5/5] time 0.279 (0.465) data 0.000 (0.183) loss 1.4898 (1.0499) acc 65.6250 (78.7500) lr 9.5289e+00 eta 0:03:43\n",
      "epoch [105/200] batch [5/5] time 0.278 (0.475) data 0.000 (0.192) loss 1.1814 (1.1533) acc 71.8750 (72.5000) lr 9.3721e+00 eta 0:03:45\n",
      "epoch [106/200] batch [5/5] time 0.282 (0.559) data 0.000 (0.265) loss 0.6210 (0.8545) acc 90.6250 (81.2500) lr 9.2154e+00 eta 0:04:22\n",
      "epoch [107/200] batch [5/5] time 0.293 (0.608) data 0.000 (0.316) loss 0.9826 (0.7783) acc 75.0000 (87.5000) lr 9.0589e+00 eta 0:04:42\n",
      "epoch [108/200] batch [5/5] time 0.276 (0.486) data 0.000 (0.201) loss 0.6352 (0.8372) acc 87.5000 (84.3750) lr 8.9027e+00 eta 0:03:43\n",
      "epoch [109/200] batch [5/5] time 0.276 (0.463) data 0.000 (0.182) loss 0.6101 (0.8616) acc 90.6250 (83.7500) lr 8.7467e+00 eta 0:03:30\n",
      "epoch [110/200] batch [5/5] time 0.277 (0.467) data 0.000 (0.186) loss 0.7578 (0.7770) acc 87.5000 (85.6250) lr 8.5910e+00 eta 0:03:29\n",
      "epoch [111/200] batch [5/5] time 0.276 (0.444) data 0.000 (0.161) loss 1.0238 (0.7781) acc 81.2500 (88.7500) lr 8.4357e+00 eta 0:03:17\n",
      "epoch [112/200] batch [5/5] time 0.278 (0.555) data 0.001 (0.270) loss 1.1650 (0.9025) acc 75.0000 (79.3750) lr 8.2807e+00 eta 0:04:04\n",
      "epoch [113/200] batch [5/5] time 0.290 (0.626) data 0.000 (0.328) loss 0.8801 (1.0841) acc 81.2500 (78.1250) lr 8.1262e+00 eta 0:04:32\n",
      "epoch [114/200] batch [5/5] time 0.280 (0.473) data 0.000 (0.191) loss 1.2315 (1.0791) acc 68.7500 (75.6250) lr 7.9721e+00 eta 0:03:23\n",
      "epoch [115/200] batch [5/5] time 0.278 (0.468) data 0.000 (0.186) loss 1.3485 (1.0717) acc 62.5000 (76.8750) lr 7.8186e+00 eta 0:03:18\n",
      "epoch [116/200] batch [5/5] time 0.279 (0.471) data 0.000 (0.188) loss 1.3365 (1.1395) acc 68.7500 (75.6250) lr 7.6655e+00 eta 0:03:17\n",
      "epoch [117/200] batch [5/5] time 0.275 (0.482) data 0.000 (0.201) loss 1.4130 (1.0841) acc 75.0000 (80.6250) lr 7.5131e+00 eta 0:03:19\n",
      "epoch [118/200] batch [5/5] time 0.280 (0.577) data 0.001 (0.290) loss 0.7377 (0.8631) acc 90.6250 (85.6250) lr 7.3613e+00 eta 0:03:56\n",
      "epoch [119/200] batch [5/5] time 0.280 (0.551) data 0.000 (0.269) loss 1.0684 (0.9909) acc 84.3750 (82.5000) lr 7.2101e+00 eta 0:03:43\n",
      "epoch [120/200] batch [5/5] time 0.278 (0.469) data 0.000 (0.188) loss 1.2333 (1.0509) acc 65.6250 (73.7500) lr 7.0596e+00 eta 0:03:07\n",
      "epoch [121/200] batch [5/5] time 0.281 (0.484) data 0.000 (0.202) loss 0.6085 (0.8757) acc 93.7500 (84.3750) lr 6.9098e+00 eta 0:03:11\n",
      "epoch [122/200] batch [5/5] time 0.280 (0.481) data 0.000 (0.198) loss 1.3084 (1.1955) acc 78.1250 (77.5000) lr 6.7608e+00 eta 0:03:07\n",
      "epoch [123/200] batch [5/5] time 0.287 (0.589) data 0.000 (0.298) loss 0.6054 (0.7766) acc 93.7500 (86.2500) lr 6.6126e+00 eta 0:03:46\n",
      "epoch [124/200] batch [5/5] time 0.276 (0.500) data 0.000 (0.219) loss 0.9624 (0.9050) acc 81.2500 (83.1250) lr 6.4653e+00 eta 0:03:10\n",
      "epoch [125/200] batch [5/5] time 0.280 (0.460) data 0.000 (0.176) loss 0.4779 (0.8600) acc 90.6250 (81.8750) lr 6.3188e+00 eta 0:02:52\n",
      "epoch [126/200] batch [5/5] time 0.279 (0.459) data 0.000 (0.175) loss 0.6450 (0.7806) acc 84.3750 (82.5000) lr 6.1732e+00 eta 0:02:49\n",
      "epoch [127/200] batch [5/5] time 0.280 (0.476) data 0.000 (0.192) loss 0.9577 (0.8582) acc 75.0000 (83.1250) lr 6.0285e+00 eta 0:02:53\n",
      "epoch [128/200] batch [5/5] time 0.282 (0.565) data 0.000 (0.278) loss 0.6671 (0.6789) acc 84.3750 (87.5000) lr 5.8849e+00 eta 0:03:23\n",
      "epoch [129/200] batch [5/5] time 0.290 (0.613) data 0.000 (0.316) loss 0.9478 (1.0716) acc 71.8750 (76.8750) lr 5.7422e+00 eta 0:03:37\n",
      "epoch [130/200] batch [5/5] time 0.278 (0.473) data 0.000 (0.187) loss 0.9640 (1.0696) acc 78.1250 (80.0000) lr 5.6006e+00 eta 0:02:45\n",
      "epoch [131/200] batch [5/5] time 0.282 (0.482) data 0.000 (0.199) loss 0.5765 (0.9526) acc 90.6250 (80.6250) lr 5.4601e+00 eta 0:02:46\n",
      "epoch [132/200] batch [5/5] time 0.278 (0.468) data 0.000 (0.185) loss 0.6455 (1.0124) acc 96.8750 (83.1250) lr 5.3207e+00 eta 0:02:39\n",
      "epoch [133/200] batch [5/5] time 0.277 (0.466) data 0.000 (0.181) loss 0.4800 (0.6950) acc 100.0000 (87.5000) lr 5.1825e+00 eta 0:02:35\n",
      "epoch [134/200] batch [5/5] time 0.280 (0.581) data 0.000 (0.294) loss 0.5622 (0.6719) acc 90.6250 (85.6250) lr 5.0454e+00 eta 0:03:11\n",
      "epoch [135/200] batch [5/5] time 0.279 (0.551) data 0.000 (0.270) loss 0.7108 (0.7563) acc 87.5000 (88.7500) lr 4.9096e+00 eta 0:02:58\n",
      "epoch [136/200] batch [5/5] time 0.276 (0.479) data 0.000 (0.198) loss 0.5736 (0.5586) acc 90.6250 (90.6250) lr 4.7750e+00 eta 0:02:33\n",
      "epoch [137/200] batch [5/5] time 0.279 (0.463) data 0.000 (0.180) loss 0.6374 (0.5532) acc 78.1250 (90.0000) lr 4.6417e+00 eta 0:02:25\n",
      "epoch [138/200] batch [5/5] time 0.277 (0.482) data 0.000 (0.201) loss 0.6091 (0.7035) acc 90.6250 (85.6250) lr 4.5098e+00 eta 0:02:29\n",
      "epoch [139/200] batch [5/5] time 0.281 (0.531) data 0.000 (0.247) loss 0.9012 (0.7082) acc 87.5000 (89.3750) lr 4.3792e+00 eta 0:02:41\n",
      "epoch [140/200] batch [5/5] time 0.291 (0.600) data 0.000 (0.310) loss 0.9526 (0.6137) acc 84.3750 (88.7500) lr 4.2499e+00 eta 0:02:59\n",
      "epoch [141/200] batch [5/5] time 0.277 (0.474) data 0.000 (0.190) loss 0.5798 (0.7688) acc 90.6250 (84.3750) lr 4.1221e+00 eta 0:02:19\n",
      "epoch [142/200] batch [5/5] time 0.277 (0.448) data 0.000 (0.163) loss 1.3186 (1.1911) acc 75.0000 (77.5000) lr 3.9958e+00 eta 0:02:09\n",
      "epoch [143/200] batch [5/5] time 0.280 (0.471) data 0.000 (0.187) loss 1.3125 (1.5550) acc 75.0000 (67.5000) lr 3.8709e+00 eta 0:02:14\n",
      "epoch [144/200] batch [5/5] time 0.282 (0.471) data 0.000 (0.186) loss 1.0921 (1.0733) acc 78.1250 (78.1250) lr 3.7476e+00 eta 0:02:11\n",
      "epoch [145/200] batch [5/5] time 0.284 (0.574) data 0.000 (0.289) loss 1.2237 (1.0513) acc 81.2500 (81.8750) lr 3.6258e+00 eta 0:02:37\n",
      "epoch [146/200] batch [5/5] time 0.283 (0.608) data 0.000 (0.319) loss 0.5654 (0.7541) acc 93.7500 (87.5000) lr 3.5055e+00 eta 0:02:44\n",
      "epoch [147/200] batch [5/5] time 0.277 (0.479) data 0.000 (0.193) loss 0.8460 (0.8244) acc 87.5000 (84.3750) lr 3.3869e+00 eta 0:02:06\n",
      "epoch [148/200] batch [5/5] time 0.276 (0.469) data 0.000 (0.185) loss 0.8952 (0.6895) acc 90.6250 (91.8750) lr 3.2699e+00 eta 0:02:01\n",
      "epoch [149/200] batch [5/5] time 0.283 (0.453) data 0.000 (0.162) loss 0.5645 (0.6448) acc 93.7500 (90.6250) lr 3.1545e+00 eta 0:01:55\n",
      "epoch [150/200] batch [5/5] time 0.278 (0.466) data 0.000 (0.184) loss 0.7164 (0.6190) acc 87.5000 (90.6250) lr 3.0409e+00 eta 0:01:56\n",
      "epoch [151/200] batch [5/5] time 0.279 (0.577) data 0.000 (0.292) loss 0.4535 (0.4993) acc 90.6250 (95.0000) lr 2.9289e+00 eta 0:02:21\n",
      "epoch [152/200] batch [5/5] time 0.280 (0.611) data 0.000 (0.317) loss 0.5249 (0.5569) acc 87.5000 (90.6250) lr 2.8187e+00 eta 0:02:26\n",
      "epoch [153/200] batch [5/5] time 0.276 (0.464) data 0.000 (0.182) loss 0.6393 (0.5785) acc 93.7500 (90.0000) lr 2.7103e+00 eta 0:01:49\n",
      "epoch [154/200] batch [5/5] time 0.279 (0.477) data 0.000 (0.195) loss 0.5289 (0.6315) acc 93.7500 (90.0000) lr 2.6037e+00 eta 0:01:49\n",
      "epoch [155/200] batch [5/5] time 0.277 (0.478) data 0.001 (0.196) loss 0.4269 (0.5584) acc 93.7500 (92.5000) lr 2.4989e+00 eta 0:01:47\n",
      "epoch [156/200] batch [5/5] time 0.278 (0.477) data 0.001 (0.194) loss 0.8638 (0.7985) acc 84.3750 (85.6250) lr 2.3959e+00 eta 0:01:45\n",
      "epoch [157/200] batch [5/5] time 0.281 (0.609) data 0.000 (0.327) loss 0.8051 (0.8766) acc 84.3750 (81.2500) lr 2.2949e+00 eta 0:02:10\n",
      "epoch [158/200] batch [5/5] time 0.279 (0.503) data 0.000 (0.221) loss 0.4718 (0.6830) acc 93.7500 (88.7500) lr 2.1957e+00 eta 0:01:45\n",
      "epoch [159/200] batch [5/5] time 0.276 (0.464) data 0.000 (0.182) loss 0.4979 (0.5265) acc 93.7500 (93.7500) lr 2.0984e+00 eta 0:01:35\n",
      "epoch [160/200] batch [5/5] time 0.280 (0.475) data 0.000 (0.194) loss 0.4852 (0.4967) acc 87.5000 (91.2500) lr 2.0032e+00 eta 0:01:34\n",
      "epoch [161/200] batch [5/5] time 0.278 (0.463) data 0.000 (0.180) loss 0.4686 (0.5162) acc 90.6250 (92.5000) lr 1.9098e+00 eta 0:01:30\n",
      "epoch [162/200] batch [5/5] time 0.280 (0.562) data 0.001 (0.279) loss 0.6546 (0.6251) acc 87.5000 (86.8750) lr 1.8185e+00 eta 0:01:46\n",
      "epoch [163/200] batch [5/5] time 0.289 (0.620) data 0.000 (0.328) loss 0.3675 (0.4998) acc 100.0000 (92.5000) lr 1.7292e+00 eta 0:01:54\n",
      "epoch [164/200] batch [5/5] time 0.281 (0.523) data 0.000 (0.240) loss 0.5777 (0.5984) acc 90.6250 (89.3750) lr 1.6419e+00 eta 0:01:34\n",
      "epoch [165/200] batch [5/5] time 0.276 (0.477) data 0.000 (0.196) loss 0.4412 (0.4372) acc 93.7500 (92.5000) lr 1.5567e+00 eta 0:01:23\n",
      "epoch [166/200] batch [5/5] time 0.280 (0.480) data 0.000 (0.199) loss 0.3703 (0.4732) acc 96.8750 (92.5000) lr 1.4736e+00 eta 0:01:21\n",
      "epoch [167/200] batch [5/5] time 0.276 (0.484) data 0.000 (0.204) loss 0.2988 (0.4353) acc 100.0000 (91.8750) lr 1.3926e+00 eta 0:01:19\n",
      "epoch [168/200] batch [5/5] time 0.285 (0.633) data 0.000 (0.342) loss 0.4648 (0.4335) acc 90.6250 (93.7500) lr 1.3137e+00 eta 0:01:41\n",
      "epoch [169/200] batch [5/5] time 0.279 (0.500) data 0.000 (0.218) loss 0.2960 (0.4100) acc 100.0000 (95.0000) lr 1.2369e+00 eta 0:01:17\n",
      "epoch [170/200] batch [5/5] time 0.277 (0.462) data 0.000 (0.178) loss 0.3495 (0.3605) acc 96.8750 (96.2500) lr 1.1623e+00 eta 0:01:09\n",
      "epoch [171/200] batch [5/5] time 0.286 (0.482) data 0.000 (0.198) loss 0.2987 (0.4581) acc 100.0000 (95.0000) lr 1.0899e+00 eta 0:01:09\n",
      "epoch [172/200] batch [5/5] time 0.279 (0.469) data 0.000 (0.187) loss 0.4923 (0.4755) acc 96.8750 (93.7500) lr 1.0197e+00 eta 0:01:05\n",
      "epoch [173/200] batch [5/5] time 0.280 (0.566) data 0.000 (0.282) loss 0.3808 (0.4609) acc 93.7500 (92.5000) lr 9.5173e-01 eta 0:01:16\n",
      "epoch [174/200] batch [5/5] time 0.291 (0.598) data 0.000 (0.288) loss 0.3228 (0.3782) acc 96.8750 (96.2500) lr 8.8597e-01 eta 0:01:17\n",
      "epoch [175/200] batch [5/5] time 0.277 (0.511) data 0.000 (0.230) loss 0.3159 (0.4099) acc 96.8750 (93.1250) lr 8.2245e-01 eta 0:01:03\n",
      "epoch [176/200] batch [5/5] time 0.276 (0.495) data 0.000 (0.213) loss 0.2695 (0.3927) acc 100.0000 (96.8750) lr 7.6120e-01 eta 0:00:59\n",
      "epoch [177/200] batch [5/5] time 0.275 (0.469) data 0.000 (0.187) loss 0.6634 (0.4690) acc 90.6250 (94.3750) lr 7.0224e-01 eta 0:00:53\n",
      "epoch [178/200] batch [5/5] time 0.280 (0.447) data 0.000 (0.164) loss 0.2829 (0.4165) acc 96.8750 (93.1250) lr 6.4556e-01 eta 0:00:49\n",
      "epoch [179/200] batch [5/5] time 0.292 (0.619) data 0.000 (0.329) loss 0.3607 (0.3894) acc 93.7500 (95.0000) lr 5.9119e-01 eta 0:01:04\n",
      "epoch [180/200] batch [5/5] time 0.280 (0.504) data 0.000 (0.222) loss 0.4510 (0.3451) acc 96.8750 (98.1250) lr 5.3915e-01 eta 0:00:50\n",
      "epoch [181/200] batch [5/5] time 0.274 (0.471) data 0.000 (0.187) loss 0.3882 (0.3564) acc 96.8750 (95.0000) lr 4.8943e-01 eta 0:00:44\n",
      "epoch [182/200] batch [5/5] time 0.277 (0.486) data 0.000 (0.205) loss 0.3302 (0.2667) acc 93.7500 (98.7500) lr 4.4207e-01 eta 0:00:43\n",
      "epoch [183/200] batch [5/5] time 0.275 (0.469) data 0.000 (0.180) loss 0.2376 (0.3235) acc 100.0000 (97.5000) lr 3.9706e-01 eta 0:00:39\n",
      "epoch [184/200] batch [5/5] time 0.277 (0.563) data 0.001 (0.279) loss 0.3216 (0.3245) acc 93.7500 (96.8750) lr 3.5443e-01 eta 0:00:45\n",
      "epoch [185/200] batch [5/5] time 0.290 (0.637) data 0.000 (0.344) loss 0.6016 (0.3837) acc 87.5000 (93.7500) lr 3.1417e-01 eta 0:00:47\n",
      "epoch [186/200] batch [5/5] time 0.276 (0.503) data 0.000 (0.222) loss 0.3476 (0.2838) acc 96.8750 (98.1250) lr 2.7630e-01 eta 0:00:35\n",
      "epoch [187/200] batch [5/5] time 0.275 (0.458) data 0.000 (0.173) loss 0.3864 (0.3747) acc 93.7500 (96.2500) lr 2.4083e-01 eta 0:00:29\n",
      "epoch [188/200] batch [5/5] time 0.278 (0.488) data 0.000 (0.206) loss 0.4864 (0.3348) acc 87.5000 (95.0000) lr 2.0777e-01 eta 0:00:29\n",
      "epoch [189/200] batch [5/5] time 0.275 (0.470) data 0.000 (0.189) loss 0.2715 (0.3725) acc 96.8750 (95.6250) lr 1.7713e-01 eta 0:00:25\n",
      "epoch [190/200] batch [5/5] time 0.278 (0.580) data 0.000 (0.297) loss 0.2643 (0.3402) acc 96.8750 (95.6250) lr 1.4891e-01 eta 0:00:28\n",
      "epoch [191/200] batch [5/5] time 0.276 (0.549) data 0.000 (0.268) loss 0.2252 (0.2884) acc 100.0000 (96.8750) lr 1.2312e-01 eta 0:00:24\n",
      "epoch [192/200] batch [5/5] time 0.275 (0.505) data 0.000 (0.224) loss 0.4198 (0.3609) acc 93.7500 (95.6250) lr 9.9763e-02 eta 0:00:20\n",
      "epoch [193/200] batch [5/5] time 0.277 (0.464) data 0.000 (0.183) loss 0.2633 (0.2535) acc 96.8750 (97.5000) lr 7.8853e-02 eta 0:00:16\n",
      "epoch [194/200] batch [5/5] time 0.277 (0.461) data 0.000 (0.180) loss 0.2269 (0.2417) acc 100.0000 (99.3750) lr 6.0390e-02 eta 0:00:13\n",
      "epoch [195/200] batch [5/5] time 0.277 (0.535) data 0.000 (0.252) loss 0.2388 (0.2596) acc 100.0000 (98.1250) lr 4.4380e-02 eta 0:00:13\n",
      "epoch [196/200] batch [5/5] time 0.293 (0.619) data 0.000 (0.325) loss 0.2335 (0.2793) acc 100.0000 (96.8750) lr 3.0827e-02 eta 0:00:12\n",
      "epoch [197/200] batch [5/5] time 0.292 (0.618) data 0.000 (0.316) loss 0.4555 (0.3505) acc 93.7500 (95.6250) lr 1.9733e-02 eta 0:00:09\n",
      "epoch [198/200] batch [5/5] time 0.274 (0.521) data 0.000 (0.240) loss 0.2275 (0.2872) acc 100.0000 (96.8750) lr 1.1101e-02 eta 0:00:05\n",
      "epoch [199/200] batch [5/5] time 0.277 (0.463) data 0.000 (0.182) loss 0.3335 (0.3184) acc 96.8750 (96.8750) lr 4.9344e-03 eta 0:00:02\n",
      "epoch [200/200] batch [5/5] time 0.280 (0.467) data 0.000 (0.187) loss 0.2583 (0.2821) acc 96.8750 (97.5000) lr 1.2337e-03 eta 0:00:00\n",
      "Checkpoint saved to output/eurosat/DAPT/vit_b16_16shots/seed1/prompt_learner/model.pth.tar-200\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [05:53<00:00,  4.36s/it]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 7,383\n",
      "* accuracy: 91.1%\n",
      "* error: 8.9%\n",
      "* macro_f1: 90.9%\n",
      "Elapsed: 0:14:58\n"
     ]
    }
   ],
   "source": [
    "#eurosat-16shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "        --output-dir output/eurosat/DAPT/vit_b16_16shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R407bmfCvusQ",
    "outputId": "66ed9e62-e913-400b-8ce3-77340f4ade55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:42:57.971649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:42:58.004215: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:42:58.014314: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:42:58.037184: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:42:59.325948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '8']\n",
      "output_dir: output/eurosat/DAPT/vit_b16_8shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 8\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/eurosat/DAPT/vit_b16_8shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  80\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/eurosat/DAPT/vit_b16_8shots/seed1/tensorboard)\n",
      "epoch [1/200] batch [1/2] time 1.934 (1.934) data 0.570 (0.570) loss 11.7227 (11.7227) acc 12.5000 (12.5000) lr 1.0000e-05 eta 0:12:51\n",
      "epoch [1/200] batch [2/2] time 0.250 (1.092) data 0.001 (0.285) loss 11.4763 (11.5995) acc 9.3750 (10.9375) lr 2.0000e+01 eta 0:07:14\n",
      "epoch [2/200] batch [1/2] time 0.743 (0.743) data 0.494 (0.494) loss 11.6592 (11.6592) acc 9.3750 (9.3750) lr 2.0000e+01 eta 0:04:54\n",
      "epoch [2/200] batch [2/2] time 0.251 (0.497) data 0.001 (0.247) loss 10.8984 (11.2788) acc 6.2500 (7.8125) lr 1.9999e+01 eta 0:03:16\n",
      "epoch [3/200] batch [1/2] time 0.739 (0.739) data 0.487 (0.487) loss 10.6030 (10.6030) acc 0.0000 (0.0000) lr 1.9999e+01 eta 0:04:51\n",
      "epoch [3/200] batch [2/2] time 0.250 (0.494) data 0.001 (0.244) loss 10.3674 (10.4852) acc 0.0000 (0.0000) lr 1.9995e+01 eta 0:03:14\n",
      "epoch [4/200] batch [1/2] time 0.741 (0.741) data 0.487 (0.487) loss 10.2961 (10.2961) acc 3.1250 (3.1250) lr 1.9995e+01 eta 0:04:51\n",
      "epoch [4/200] batch [2/2] time 0.252 (0.496) data 0.000 (0.244) loss 10.2397 (10.2679) acc 9.3750 (6.2500) lr 1.9989e+01 eta 0:03:14\n",
      "epoch [5/200] batch [1/2] time 0.729 (0.729) data 0.479 (0.479) loss 10.2097 (10.2097) acc 9.3750 (9.3750) lr 1.9989e+01 eta 0:04:44\n",
      "epoch [5/200] batch [2/2] time 0.250 (0.489) data 0.000 (0.240) loss 10.1612 (10.1854) acc 9.3750 (9.3750) lr 1.9980e+01 eta 0:03:10\n",
      "epoch [6/200] batch [1/2] time 0.745 (0.745) data 0.493 (0.493) loss 10.1278 (10.1278) acc 12.5000 (12.5000) lr 1.9980e+01 eta 0:04:49\n",
      "epoch [6/200] batch [2/2] time 0.263 (0.504) data 0.001 (0.247) loss 10.1173 (10.1225) acc 9.3750 (10.9375) lr 1.9969e+01 eta 0:03:15\n",
      "epoch [7/200] batch [1/2] time 1.061 (1.061) data 0.809 (0.809) loss 10.0505 (10.0505) acc 9.3750 (9.3750) lr 1.9969e+01 eta 0:06:50\n",
      "epoch [7/200] batch [2/2] time 0.251 (0.656) data 0.001 (0.405) loss 10.0347 (10.0426) acc 9.3750 (9.3750) lr 1.9956e+01 eta 0:04:13\n",
      "epoch [8/200] batch [1/2] time 1.094 (1.094) data 0.841 (0.841) loss 10.0461 (10.0461) acc 6.2500 (6.2500) lr 1.9956e+01 eta 0:07:01\n",
      "epoch [8/200] batch [2/2] time 0.254 (0.674) data 0.001 (0.421) loss 9.9512 (9.9986) acc 3.1250 (4.6875) lr 1.9940e+01 eta 0:04:18\n",
      "epoch [9/200] batch [1/2] time 0.924 (0.924) data 0.675 (0.675) loss 9.9453 (9.9453) acc 0.0000 (0.0000) lr 1.9940e+01 eta 0:05:53\n",
      "epoch [9/200] batch [2/2] time 0.250 (0.587) data 0.000 (0.338) loss 9.8645 (9.9049) acc 12.5000 (6.2500) lr 1.9921e+01 eta 0:03:44\n",
      "epoch [10/200] batch [1/2] time 0.741 (0.741) data 0.493 (0.493) loss 9.9218 (9.9218) acc 3.1250 (3.1250) lr 1.9921e+01 eta 0:04:42\n",
      "epoch [10/200] batch [2/2] time 0.252 (0.496) data 0.000 (0.247) loss 9.8635 (9.8927) acc 6.2500 (4.6875) lr 1.9900e+01 eta 0:03:08\n",
      "epoch [11/200] batch [1/2] time 0.716 (0.716) data 0.465 (0.465) loss 9.8008 (9.8008) acc 3.1250 (3.1250) lr 1.9900e+01 eta 0:04:31\n",
      "epoch [11/200] batch [2/2] time 0.253 (0.484) data 0.001 (0.233) loss 9.7165 (9.7587) acc 3.1250 (3.1250) lr 1.9877e+01 eta 0:03:03\n",
      "epoch [12/200] batch [1/2] time 0.834 (0.834) data 0.572 (0.572) loss 9.7204 (9.7204) acc 3.1250 (3.1250) lr 1.9877e+01 eta 0:05:14\n",
      "epoch [12/200] batch [2/2] time 0.251 (0.542) data 0.000 (0.286) loss 9.5940 (9.6572) acc 15.6250 (9.3750) lr 1.9851e+01 eta 0:03:23\n",
      "epoch [13/200] batch [1/2] time 0.731 (0.731) data 0.480 (0.480) loss 9.4969 (9.4969) acc 15.6250 (15.6250) lr 1.9851e+01 eta 0:04:34\n",
      "epoch [13/200] batch [2/2] time 0.254 (0.493) data 0.001 (0.240) loss 9.4661 (9.4815) acc 3.1250 (9.3750) lr 1.9823e+01 eta 0:03:04\n",
      "epoch [14/200] batch [1/2] time 0.747 (0.747) data 0.493 (0.493) loss 9.2718 (9.2718) acc 25.0000 (25.0000) lr 1.9823e+01 eta 0:04:38\n",
      "epoch [14/200] batch [2/2] time 0.251 (0.499) data 0.001 (0.247) loss 9.2385 (9.2552) acc 9.3750 (17.1875) lr 1.9792e+01 eta 0:03:05\n",
      "epoch [15/200] batch [1/2] time 0.739 (0.739) data 0.487 (0.487) loss 8.9786 (8.9786) acc 12.5000 (12.5000) lr 1.9792e+01 eta 0:04:34\n",
      "epoch [15/200] batch [2/2] time 0.252 (0.496) data 0.001 (0.244) loss 8.7771 (8.8779) acc 15.6250 (14.0625) lr 1.9759e+01 eta 0:03:03\n",
      "epoch [16/200] batch [1/2] time 0.754 (0.754) data 0.503 (0.503) loss 8.4289 (8.4289) acc 6.2500 (6.2500) lr 1.9759e+01 eta 0:04:38\n",
      "epoch [16/200] batch [2/2] time 0.255 (0.504) data 0.001 (0.252) loss 8.0306 (8.2298) acc 15.6250 (10.9375) lr 1.9724e+01 eta 0:03:05\n",
      "epoch [17/200] batch [1/2] time 0.739 (0.739) data 0.503 (0.503) loss 7.3165 (7.3165) acc 12.5000 (12.5000) lr 1.9724e+01 eta 0:04:31\n",
      "epoch [17/200] batch [2/2] time 0.245 (0.492) data 0.000 (0.252) loss 6.7456 (7.0311) acc 12.5000 (12.5000) lr 1.9686e+01 eta 0:02:59\n",
      "epoch [18/200] batch [1/2] time 0.992 (0.992) data 0.740 (0.740) loss 5.9562 (5.9562) acc 15.6250 (15.6250) lr 1.9686e+01 eta 0:06:02\n",
      "epoch [18/200] batch [2/2] time 0.260 (0.626) data 0.001 (0.370) loss 5.7804 (5.8683) acc 6.2500 (10.9375) lr 1.9646e+01 eta 0:03:47\n",
      "epoch [19/200] batch [1/2] time 1.048 (1.048) data 0.793 (0.793) loss 5.0854 (5.0854) acc 15.6250 (15.6250) lr 1.9646e+01 eta 0:06:20\n",
      "epoch [19/200] batch [2/2] time 0.257 (0.653) data 0.001 (0.397) loss 6.6019 (5.8436) acc 6.2500 (10.9375) lr 1.9603e+01 eta 0:03:56\n",
      "epoch [20/200] batch [1/2] time 1.088 (1.088) data 0.833 (0.833) loss 11.8818 (11.8818) acc 9.3750 (9.3750) lr 1.9603e+01 eta 0:06:32\n",
      "epoch [20/200] batch [2/2] time 0.255 (0.672) data 0.000 (0.417) loss 10.8960 (11.3889) acc 3.1250 (6.2500) lr 1.9558e+01 eta 0:04:01\n",
      "epoch [21/200] batch [1/2] time 0.739 (0.739) data 0.488 (0.488) loss 9.6707 (9.6707) acc 15.6250 (15.6250) lr 1.9558e+01 eta 0:04:25\n",
      "epoch [21/200] batch [2/2] time 0.255 (0.497) data 0.001 (0.244) loss 8.4064 (9.0386) acc 9.3750 (12.5000) lr 1.9511e+01 eta 0:02:57\n",
      "epoch [22/200] batch [1/2] time 0.748 (0.748) data 0.494 (0.494) loss 7.6750 (7.6750) acc 9.3750 (9.3750) lr 1.9511e+01 eta 0:04:26\n",
      "epoch [22/200] batch [2/2] time 0.254 (0.501) data 0.000 (0.247) loss 6.9640 (7.3195) acc 3.1250 (6.2500) lr 1.9461e+01 eta 0:02:58\n",
      "epoch [23/200] batch [1/2] time 0.759 (0.759) data 0.507 (0.507) loss 6.2847 (6.2847) acc 21.8750 (21.8750) lr 1.9461e+01 eta 0:04:29\n",
      "epoch [23/200] batch [2/2] time 0.254 (0.506) data 0.000 (0.254) loss 6.5582 (6.4215) acc 3.1250 (12.5000) lr 1.9409e+01 eta 0:02:59\n",
      "epoch [24/200] batch [1/2] time 0.753 (0.753) data 0.504 (0.504) loss 5.6821 (5.6821) acc 15.6250 (15.6250) lr 1.9409e+01 eta 0:04:25\n",
      "epoch [24/200] batch [2/2] time 0.254 (0.504) data 0.001 (0.252) loss 5.5076 (5.5949) acc 3.1250 (9.3750) lr 1.9354e+01 eta 0:02:57\n",
      "epoch [25/200] batch [1/2] time 0.748 (0.748) data 0.499 (0.499) loss 5.1315 (5.1315) acc 12.5000 (12.5000) lr 1.9354e+01 eta 0:04:22\n",
      "epoch [25/200] batch [2/2] time 0.251 (0.500) data 0.000 (0.250) loss 5.0492 (5.0904) acc 3.1250 (7.8125) lr 1.9298e+01 eta 0:02:54\n",
      "epoch [26/200] batch [1/2] time 0.747 (0.747) data 0.495 (0.495) loss 4.8253 (4.8253) acc 9.3750 (9.3750) lr 1.9298e+01 eta 0:04:20\n",
      "epoch [26/200] batch [2/2] time 0.254 (0.500) data 0.000 (0.248) loss 4.9704 (4.8978) acc 6.2500 (7.8125) lr 1.9239e+01 eta 0:02:54\n",
      "epoch [27/200] batch [1/2] time 0.746 (0.746) data 0.493 (0.493) loss 4.5084 (4.5084) acc 9.3750 (9.3750) lr 1.9239e+01 eta 0:04:18\n",
      "epoch [27/200] batch [2/2] time 0.253 (0.500) data 0.000 (0.247) loss 4.5403 (4.5244) acc 6.2500 (7.8125) lr 1.9178e+01 eta 0:02:52\n",
      "epoch [28/200] batch [1/2] time 0.726 (0.726) data 0.474 (0.474) loss 4.4766 (4.4766) acc 12.5000 (12.5000) lr 1.9178e+01 eta 0:04:10\n",
      "epoch [28/200] batch [2/2] time 0.255 (0.490) data 0.000 (0.237) loss 4.1552 (4.3159) acc 12.5000 (12.5000) lr 1.9114e+01 eta 0:02:48\n",
      "epoch [29/200] batch [1/2] time 0.890 (0.890) data 0.637 (0.637) loss 4.0333 (4.0333) acc 21.8750 (21.8750) lr 1.9114e+01 eta 0:05:05\n",
      "epoch [29/200] batch [2/2] time 0.255 (0.572) data 0.001 (0.319) loss 4.2336 (4.1334) acc 6.2500 (14.0625) lr 1.9048e+01 eta 0:03:15\n",
      "epoch [30/200] batch [1/2] time 1.088 (1.088) data 0.834 (0.834) loss 3.9206 (3.9206) acc 28.1250 (28.1250) lr 1.9048e+01 eta 0:06:11\n",
      "epoch [30/200] batch [2/2] time 0.258 (0.673) data 0.001 (0.417) loss 3.9740 (3.9473) acc 18.7500 (23.4375) lr 1.8980e+01 eta 0:03:48\n",
      "epoch [31/200] batch [1/2] time 1.122 (1.122) data 0.864 (0.864) loss 3.7327 (3.7327) acc 18.7500 (18.7500) lr 1.8980e+01 eta 0:06:20\n",
      "epoch [31/200] batch [2/2] time 0.265 (0.693) data 0.001 (0.432) loss 3.6565 (3.6946) acc 15.6250 (17.1875) lr 1.8910e+01 eta 0:03:54\n",
      "epoch [32/200] batch [1/2] time 0.868 (0.868) data 0.617 (0.617) loss 3.5986 (3.5986) acc 12.5000 (12.5000) lr 1.8910e+01 eta 0:04:52\n",
      "epoch [32/200] batch [2/2] time 0.257 (0.562) data 0.000 (0.309) loss 3.3580 (3.4783) acc 21.8750 (17.1875) lr 1.8838e+01 eta 0:03:08\n",
      "epoch [33/200] batch [1/2] time 0.754 (0.754) data 0.503 (0.503) loss 3.4839 (3.4839) acc 21.8750 (21.8750) lr 1.8838e+01 eta 0:04:12\n",
      "epoch [33/200] batch [2/2] time 0.253 (0.503) data 0.000 (0.252) loss 3.6932 (3.5886) acc 6.2500 (14.0625) lr 1.8763e+01 eta 0:02:48\n",
      "epoch [34/200] batch [1/2] time 0.748 (0.748) data 0.495 (0.495) loss 4.5113 (4.5113) acc 9.3750 (9.3750) lr 1.8763e+01 eta 0:04:09\n",
      "epoch [34/200] batch [2/2] time 0.262 (0.505) data 0.001 (0.248) loss 4.8498 (4.6806) acc 15.6250 (12.5000) lr 1.8686e+01 eta 0:02:47\n",
      "epoch [35/200] batch [1/2] time 0.754 (0.754) data 0.498 (0.498) loss 4.8874 (4.8874) acc 9.3750 (9.3750) lr 1.8686e+01 eta 0:04:09\n",
      "epoch [35/200] batch [2/2] time 0.252 (0.503) data 0.000 (0.249) loss 5.5950 (5.2412) acc 6.2500 (7.8125) lr 1.8607e+01 eta 0:02:45\n",
      "epoch [36/200] batch [1/2] time 0.722 (0.722) data 0.471 (0.471) loss 5.5239 (5.5239) acc 15.6250 (15.6250) lr 1.8607e+01 eta 0:03:57\n",
      "epoch [36/200] batch [2/2] time 0.256 (0.489) data 0.000 (0.236) loss 6.3617 (5.9428) acc 9.3750 (12.5000) lr 1.8526e+01 eta 0:02:40\n",
      "epoch [37/200] batch [1/2] time 0.738 (0.738) data 0.483 (0.483) loss 7.9213 (7.9213) acc 9.3750 (9.3750) lr 1.8526e+01 eta 0:04:01\n",
      "epoch [37/200] batch [2/2] time 0.253 (0.495) data 0.000 (0.242) loss 10.6580 (9.2897) acc 6.2500 (7.8125) lr 1.8443e+01 eta 0:02:41\n",
      "epoch [38/200] batch [1/2] time 0.744 (0.744) data 0.492 (0.492) loss 11.6310 (11.6310) acc 12.5000 (12.5000) lr 1.8443e+01 eta 0:04:01\n",
      "epoch [38/200] batch [2/2] time 0.256 (0.500) data 0.000 (0.246) loss 10.0013 (10.8161) acc 9.3750 (10.9375) lr 1.8358e+01 eta 0:02:41\n",
      "epoch [39/200] batch [1/2] time 0.743 (0.743) data 0.485 (0.485) loss 7.4711 (7.4711) acc 12.5000 (12.5000) lr 1.8358e+01 eta 0:04:00\n",
      "epoch [39/200] batch [2/2] time 0.256 (0.500) data 0.000 (0.243) loss 5.7472 (6.6091) acc 15.6250 (14.0625) lr 1.8271e+01 eta 0:02:40\n",
      "epoch [40/200] batch [1/2] time 0.765 (0.765) data 0.511 (0.511) loss 5.5807 (5.5807) acc 9.3750 (9.3750) lr 1.8271e+01 eta 0:04:05\n",
      "epoch [40/200] batch [2/2] time 0.256 (0.511) data 0.001 (0.256) loss 6.2507 (5.9157) acc 12.5000 (10.9375) lr 1.8181e+01 eta 0:02:43\n",
      "epoch [41/200] batch [1/2] time 1.073 (1.073) data 0.810 (0.810) loss 4.1956 (4.1956) acc 31.2500 (31.2500) lr 1.8181e+01 eta 0:05:42\n",
      "epoch [41/200] batch [2/2] time 0.256 (0.665) data 0.001 (0.406) loss 4.7566 (4.4761) acc 12.5000 (21.8750) lr 1.8090e+01 eta 0:03:31\n",
      "epoch [42/200] batch [1/2] time 1.045 (1.045) data 0.789 (0.789) loss 4.5050 (4.5050) acc 6.2500 (6.2500) lr 1.8090e+01 eta 0:05:31\n",
      "epoch [42/200] batch [2/2] time 0.262 (0.654) data 0.001 (0.395) loss 3.8079 (4.1564) acc 12.5000 (9.3750) lr 1.7997e+01 eta 0:03:26\n",
      "epoch [43/200] batch [1/2] time 0.834 (0.834) data 0.580 (0.580) loss 4.1240 (4.1240) acc 15.6250 (15.6250) lr 1.7997e+01 eta 0:04:22\n",
      "epoch [43/200] batch [2/2] time 0.254 (0.544) data 0.001 (0.290) loss 3.4901 (3.8071) acc 34.3750 (25.0000) lr 1.7902e+01 eta 0:02:50\n",
      "epoch [44/200] batch [1/2] time 0.749 (0.749) data 0.496 (0.496) loss 3.2897 (3.2897) acc 31.2500 (31.2500) lr 1.7902e+01 eta 0:03:54\n",
      "epoch [44/200] batch [2/2] time 0.255 (0.502) data 0.000 (0.248) loss 3.0270 (3.1584) acc 25.0000 (28.1250) lr 1.7804e+01 eta 0:02:36\n",
      "epoch [45/200] batch [1/2] time 0.746 (0.746) data 0.489 (0.489) loss 3.1942 (3.1942) acc 18.7500 (18.7500) lr 1.7804e+01 eta 0:03:51\n",
      "epoch [45/200] batch [2/2] time 0.257 (0.501) data 0.000 (0.245) loss 3.3490 (3.2716) acc 9.3750 (14.0625) lr 1.7705e+01 eta 0:02:35\n",
      "epoch [46/200] batch [1/2] time 0.735 (0.735) data 0.480 (0.480) loss 2.7386 (2.7386) acc 34.3750 (34.3750) lr 1.7705e+01 eta 0:03:47\n",
      "epoch [46/200] batch [2/2] time 0.258 (0.497) data 0.000 (0.240) loss 2.7004 (2.7195) acc 31.2500 (32.8125) lr 1.7604e+01 eta 0:02:32\n",
      "epoch [47/200] batch [1/2] time 0.747 (0.747) data 0.494 (0.494) loss 2.6133 (2.6133) acc 34.3750 (34.3750) lr 1.7604e+01 eta 0:03:49\n",
      "epoch [47/200] batch [2/2] time 0.259 (0.503) data 0.000 (0.247) loss 2.6674 (2.6404) acc 28.1250 (31.2500) lr 1.7501e+01 eta 0:02:33\n",
      "epoch [48/200] batch [1/2] time 0.731 (0.731) data 0.474 (0.474) loss 2.2779 (2.2779) acc 40.6250 (40.6250) lr 1.7501e+01 eta 0:03:42\n",
      "epoch [48/200] batch [2/2] time 0.257 (0.494) data 0.001 (0.237) loss 2.7220 (2.4999) acc 28.1250 (34.3750) lr 1.7396e+01 eta 0:02:30\n",
      "epoch [49/200] batch [1/2] time 0.736 (0.736) data 0.477 (0.477) loss 1.9853 (1.9853) acc 50.0000 (50.0000) lr 1.7396e+01 eta 0:03:42\n",
      "epoch [49/200] batch [2/2] time 0.258 (0.497) data 0.001 (0.239) loss 2.2793 (2.1323) acc 50.0000 (50.0000) lr 1.7290e+01 eta 0:02:30\n",
      "epoch [50/200] batch [1/2] time 1.020 (1.020) data 0.757 (0.757) loss 2.3912 (2.3912) acc 40.6250 (40.6250) lr 1.7290e+01 eta 0:05:07\n",
      "epoch [50/200] batch [2/2] time 0.262 (0.641) data 0.001 (0.379) loss 2.4532 (2.4222) acc 28.1250 (34.3750) lr 1.7181e+01 eta 0:03:12\n",
      "epoch [51/200] batch [1/2] time 1.007 (1.007) data 0.742 (0.742) loss 2.2566 (2.2566) acc 59.3750 (59.3750) lr 1.7181e+01 eta 0:05:01\n",
      "epoch [51/200] batch [2/2] time 0.265 (0.636) data 0.003 (0.373) loss 2.7888 (2.5227) acc 28.1250 (43.7500) lr 1.7071e+01 eta 0:03:09\n",
      "epoch [52/200] batch [1/2] time 1.232 (1.232) data 0.959 (0.959) loss 2.1290 (2.1290) acc 43.7500 (43.7500) lr 1.7071e+01 eta 0:06:05\n",
      "epoch [52/200] batch [2/2] time 0.280 (0.756) data 0.002 (0.480) loss 2.6769 (2.4029) acc 18.7500 (31.2500) lr 1.6959e+01 eta 0:03:43\n",
      "epoch [53/200] batch [1/2] time 1.112 (1.112) data 0.852 (0.852) loss 2.3657 (2.3657) acc 46.8750 (46.8750) lr 1.6959e+01 eta 0:05:28\n",
      "epoch [53/200] batch [2/2] time 0.263 (0.687) data 0.001 (0.427) loss 3.2981 (2.8319) acc 21.8750 (34.3750) lr 1.6845e+01 eta 0:03:22\n",
      "epoch [54/200] batch [1/2] time 1.077 (1.077) data 0.820 (0.820) loss 2.9235 (2.9235) acc 37.5000 (37.5000) lr 1.6845e+01 eta 0:05:15\n",
      "epoch [54/200] batch [2/2] time 0.259 (0.668) data 0.000 (0.410) loss 3.7657 (3.3446) acc 12.5000 (25.0000) lr 1.6730e+01 eta 0:03:15\n",
      "epoch [55/200] batch [1/2] time 0.738 (0.738) data 0.482 (0.482) loss 4.2775 (4.2775) acc 25.0000 (25.0000) lr 1.6730e+01 eta 0:03:34\n",
      "epoch [55/200] batch [2/2] time 0.257 (0.498) data 0.001 (0.241) loss 4.9538 (4.6156) acc 15.6250 (20.3125) lr 1.6613e+01 eta 0:02:24\n",
      "epoch [56/200] batch [1/2] time 0.741 (0.741) data 0.484 (0.484) loss 3.9613 (3.9613) acc 12.5000 (12.5000) lr 1.6613e+01 eta 0:03:34\n",
      "epoch [56/200] batch [2/2] time 0.256 (0.499) data 0.001 (0.242) loss 4.1129 (4.0371) acc 9.3750 (10.9375) lr 1.6494e+01 eta 0:02:23\n",
      "epoch [57/200] batch [1/2] time 0.736 (0.736) data 0.478 (0.478) loss 3.7170 (3.7170) acc 25.0000 (25.0000) lr 1.6494e+01 eta 0:03:31\n",
      "epoch [57/200] batch [2/2] time 0.259 (0.498) data 0.001 (0.239) loss 3.2711 (3.4941) acc 25.0000 (25.0000) lr 1.6374e+01 eta 0:02:22\n",
      "epoch [58/200] batch [1/2] time 0.753 (0.753) data 0.496 (0.496) loss 3.5138 (3.5138) acc 25.0000 (25.0000) lr 1.6374e+01 eta 0:03:34\n",
      "epoch [58/200] batch [2/2] time 0.258 (0.506) data 0.001 (0.248) loss 2.9979 (3.2558) acc 18.7500 (21.8750) lr 1.6252e+01 eta 0:02:23\n",
      "epoch [59/200] batch [1/2] time 0.744 (0.744) data 0.489 (0.489) loss 3.1024 (3.1024) acc 15.6250 (15.6250) lr 1.6252e+01 eta 0:03:30\n",
      "epoch [59/200] batch [2/2] time 0.260 (0.502) data 0.001 (0.245) loss 2.6313 (2.8669) acc 12.5000 (14.0625) lr 1.6129e+01 eta 0:02:21\n",
      "epoch [60/200] batch [1/2] time 0.950 (0.950) data 0.695 (0.695) loss 2.5768 (2.5768) acc 21.8750 (21.8750) lr 1.6129e+01 eta 0:04:27\n",
      "epoch [60/200] batch [2/2] time 0.259 (0.604) data 0.000 (0.348) loss 2.3456 (2.4612) acc 40.6250 (31.2500) lr 1.6004e+01 eta 0:02:49\n",
      "epoch [61/200] batch [1/2] time 0.731 (0.731) data 0.472 (0.472) loss 1.9320 (1.9320) acc 56.2500 (56.2500) lr 1.6004e+01 eta 0:03:24\n",
      "epoch [61/200] batch [2/2] time 0.257 (0.494) data 0.001 (0.236) loss 2.0373 (1.9846) acc 53.1250 (54.6875) lr 1.5878e+01 eta 0:02:17\n",
      "epoch [62/200] batch [1/2] time 0.733 (0.733) data 0.477 (0.477) loss 1.7673 (1.7673) acc 62.5000 (62.5000) lr 1.5878e+01 eta 0:03:23\n",
      "epoch [62/200] batch [2/2] time 0.260 (0.496) data 0.001 (0.239) loss 1.8165 (1.7919) acc 56.2500 (59.3750) lr 1.5750e+01 eta 0:02:17\n",
      "epoch [63/200] batch [1/2] time 0.899 (0.899) data 0.611 (0.611) loss 1.6377 (1.6377) acc 68.7500 (68.7500) lr 1.5750e+01 eta 0:04:07\n",
      "epoch [63/200] batch [2/2] time 0.262 (0.581) data 0.001 (0.306) loss 1.7191 (1.6784) acc 56.2500 (62.5000) lr 1.5621e+01 eta 0:02:39\n",
      "epoch [64/200] batch [1/2] time 1.069 (1.069) data 0.812 (0.812) loss 1.4793 (1.4793) acc 65.6250 (65.6250) lr 1.5621e+01 eta 0:04:51\n",
      "epoch [64/200] batch [2/2] time 0.260 (0.664) data 0.001 (0.406) loss 1.6732 (1.5762) acc 59.3750 (62.5000) lr 1.5490e+01 eta 0:03:00\n",
      "epoch [65/200] batch [1/2] time 1.043 (1.043) data 0.778 (0.778) loss 1.8836 (1.8836) acc 50.0000 (50.0000) lr 1.5490e+01 eta 0:04:42\n",
      "epoch [65/200] batch [2/2] time 0.263 (0.653) data 0.001 (0.389) loss 1.4557 (1.6697) acc 56.2500 (53.1250) lr 1.5358e+01 eta 0:02:56\n",
      "epoch [66/200] batch [1/2] time 0.841 (0.841) data 0.585 (0.585) loss 1.1705 (1.1705) acc 78.1250 (78.1250) lr 1.5358e+01 eta 0:03:46\n",
      "epoch [66/200] batch [2/2] time 0.262 (0.551) data 0.001 (0.293) loss 1.1301 (1.1503) acc 75.0000 (76.5625) lr 1.5225e+01 eta 0:02:27\n",
      "epoch [67/200] batch [1/2] time 0.756 (0.756) data 0.496 (0.496) loss 1.1723 (1.1723) acc 78.1250 (78.1250) lr 1.5225e+01 eta 0:03:21\n",
      "epoch [67/200] batch [2/2] time 0.260 (0.508) data 0.000 (0.248) loss 1.1856 (1.1790) acc 75.0000 (76.5625) lr 1.5090e+01 eta 0:02:15\n",
      "epoch [68/200] batch [1/2] time 0.760 (0.760) data 0.506 (0.506) loss 0.9210 (0.9210) acc 84.3750 (84.3750) lr 1.5090e+01 eta 0:03:21\n",
      "epoch [68/200] batch [2/2] time 0.259 (0.510) data 0.000 (0.253) loss 1.4408 (1.1809) acc 68.7500 (76.5625) lr 1.4955e+01 eta 0:02:14\n",
      "epoch [69/200] batch [1/2] time 0.763 (0.763) data 0.506 (0.506) loss 1.2309 (1.2309) acc 71.8750 (71.8750) lr 1.4955e+01 eta 0:03:20\n",
      "epoch [69/200] batch [2/2] time 0.262 (0.512) data 0.000 (0.253) loss 1.2027 (1.2168) acc 78.1250 (75.0000) lr 1.4818e+01 eta 0:02:14\n",
      "epoch [70/200] batch [1/2] time 0.745 (0.745) data 0.487 (0.487) loss 0.9720 (0.9720) acc 87.5000 (87.5000) lr 1.4818e+01 eta 0:03:14\n",
      "epoch [70/200] batch [2/2] time 0.262 (0.504) data 0.001 (0.244) loss 1.1969 (1.0844) acc 75.0000 (81.2500) lr 1.4679e+01 eta 0:02:10\n",
      "epoch [71/200] batch [1/2] time 0.740 (0.740) data 0.480 (0.480) loss 1.1669 (1.1669) acc 75.0000 (75.0000) lr 1.4679e+01 eta 0:03:11\n",
      "epoch [71/200] batch [2/2] time 0.261 (0.501) data 0.000 (0.240) loss 1.3266 (1.2468) acc 65.6250 (70.3125) lr 1.4540e+01 eta 0:02:09\n",
      "epoch [72/200] batch [1/2] time 0.735 (0.735) data 0.476 (0.476) loss 1.2077 (1.2077) acc 75.0000 (75.0000) lr 1.4540e+01 eta 0:03:08\n",
      "epoch [72/200] batch [2/2] time 0.258 (0.496) data 0.000 (0.238) loss 1.3587 (1.2832) acc 68.7500 (71.8750) lr 1.4399e+01 eta 0:02:07\n",
      "epoch [73/200] batch [1/2] time 0.733 (0.733) data 0.473 (0.473) loss 1.0630 (1.0630) acc 84.3750 (84.3750) lr 1.4399e+01 eta 0:03:06\n",
      "epoch [73/200] batch [2/2] time 0.260 (0.497) data 0.000 (0.237) loss 1.0166 (1.0398) acc 90.6250 (87.5000) lr 1.4258e+01 eta 0:02:06\n",
      "epoch [74/200] batch [1/2] time 0.766 (0.766) data 0.508 (0.508) loss 1.1967 (1.1967) acc 78.1250 (78.1250) lr 1.4258e+01 eta 0:03:13\n",
      "epoch [74/200] batch [2/2] time 0.266 (0.516) data 0.000 (0.254) loss 0.9805 (1.0886) acc 84.3750 (81.2500) lr 1.4115e+01 eta 0:02:09\n",
      "epoch [75/200] batch [1/2] time 1.069 (1.069) data 0.805 (0.805) loss 1.0445 (1.0445) acc 81.2500 (81.2500) lr 1.4115e+01 eta 0:04:28\n",
      "epoch [75/200] batch [2/2] time 0.263 (0.666) data 0.001 (0.403) loss 1.2961 (1.1703) acc 75.0000 (78.1250) lr 1.3971e+01 eta 0:02:46\n",
      "epoch [76/200] batch [1/2] time 1.014 (1.014) data 0.752 (0.752) loss 1.1092 (1.1092) acc 81.2500 (81.2500) lr 1.3971e+01 eta 0:04:12\n",
      "epoch [76/200] batch [2/2] time 0.266 (0.640) data 0.002 (0.377) loss 1.6398 (1.3745) acc 59.3750 (70.3125) lr 1.3827e+01 eta 0:02:38\n",
      "epoch [77/200] batch [1/2] time 0.901 (0.901) data 0.645 (0.645) loss 0.8589 (0.8589) acc 87.5000 (87.5000) lr 1.3827e+01 eta 0:03:42\n",
      "epoch [77/200] batch [2/2] time 0.263 (0.582) data 0.001 (0.323) loss 1.0223 (0.9406) acc 75.0000 (81.2500) lr 1.3681e+01 eta 0:02:23\n",
      "epoch [78/200] batch [1/2] time 0.740 (0.740) data 0.482 (0.482) loss 0.9952 (0.9952) acc 84.3750 (84.3750) lr 1.3681e+01 eta 0:03:01\n",
      "epoch [78/200] batch [2/2] time 0.262 (0.501) data 0.000 (0.241) loss 1.1289 (1.0620) acc 78.1250 (81.2500) lr 1.3535e+01 eta 0:02:02\n",
      "epoch [79/200] batch [1/2] time 0.743 (0.743) data 0.485 (0.485) loss 1.0203 (1.0203) acc 93.7500 (93.7500) lr 1.3535e+01 eta 0:03:00\n",
      "epoch [79/200] batch [2/2] time 0.262 (0.503) data 0.000 (0.243) loss 1.0602 (1.0403) acc 75.0000 (84.3750) lr 1.3387e+01 eta 0:02:01\n",
      "epoch [80/200] batch [1/2] time 0.742 (0.742) data 0.483 (0.483) loss 0.8279 (0.8279) acc 87.5000 (87.5000) lr 1.3387e+01 eta 0:02:58\n",
      "epoch [80/200] batch [2/2] time 0.261 (0.501) data 0.001 (0.242) loss 0.9583 (0.8931) acc 78.1250 (82.8125) lr 1.3239e+01 eta 0:02:00\n",
      "epoch [81/200] batch [1/2] time 0.731 (0.731) data 0.484 (0.484) loss 0.9570 (0.9570) acc 75.0000 (75.0000) lr 1.3239e+01 eta 0:02:54\n",
      "epoch [81/200] batch [2/2] time 0.259 (0.495) data 0.000 (0.242) loss 1.2297 (1.0934) acc 78.1250 (76.5625) lr 1.3090e+01 eta 0:01:57\n",
      "epoch [82/200] batch [1/2] time 0.744 (0.744) data 0.485 (0.485) loss 1.0745 (1.0745) acc 81.2500 (81.2500) lr 1.3090e+01 eta 0:02:56\n",
      "epoch [82/200] batch [2/2] time 0.261 (0.503) data 0.001 (0.243) loss 0.9269 (1.0007) acc 84.3750 (82.8125) lr 1.2940e+01 eta 0:01:58\n",
      "epoch [83/200] batch [1/2] time 0.767 (0.767) data 0.508 (0.508) loss 0.6488 (0.6488) acc 90.6250 (90.6250) lr 1.2940e+01 eta 0:03:00\n",
      "epoch [83/200] batch [2/2] time 0.259 (0.513) data 0.001 (0.254) loss 0.8655 (0.7572) acc 87.5000 (89.0625) lr 1.2790e+01 eta 0:02:00\n",
      "epoch [84/200] batch [1/2] time 0.756 (0.756) data 0.493 (0.493) loss 0.9266 (0.9266) acc 87.5000 (87.5000) lr 1.2790e+01 eta 0:02:56\n",
      "epoch [84/200] batch [2/2] time 0.265 (0.510) data 0.001 (0.247) loss 0.6716 (0.7991) acc 87.5000 (87.5000) lr 1.2639e+01 eta 0:01:58\n",
      "epoch [85/200] batch [1/2] time 0.769 (0.769) data 0.512 (0.512) loss 0.5360 (0.5360) acc 96.8750 (96.8750) lr 1.2639e+01 eta 0:02:57\n",
      "epoch [85/200] batch [2/2] time 0.262 (0.516) data 0.000 (0.256) loss 0.6777 (0.6069) acc 93.7500 (95.3125) lr 1.2487e+01 eta 0:01:58\n",
      "epoch [86/200] batch [1/2] time 1.046 (1.046) data 0.783 (0.783) loss 0.7132 (0.7132) acc 87.5000 (87.5000) lr 1.2487e+01 eta 0:03:59\n",
      "epoch [86/200] batch [2/2] time 0.265 (0.655) data 0.001 (0.392) loss 0.5666 (0.6399) acc 87.5000 (87.5000) lr 1.2334e+01 eta 0:02:29\n",
      "epoch [87/200] batch [1/2] time 1.080 (1.080) data 0.817 (0.817) loss 0.5385 (0.5385) acc 100.0000 (100.0000) lr 1.2334e+01 eta 0:04:05\n",
      "epoch [87/200] batch [2/2] time 0.264 (0.672) data 0.001 (0.409) loss 0.8525 (0.6955) acc 87.5000 (93.7500) lr 1.2181e+01 eta 0:02:31\n",
      "epoch [88/200] batch [1/2] time 0.901 (0.901) data 0.644 (0.644) loss 0.6061 (0.6061) acc 90.6250 (90.6250) lr 1.2181e+01 eta 0:03:22\n",
      "epoch [88/200] batch [2/2] time 0.259 (0.580) data 0.000 (0.322) loss 0.9453 (0.7757) acc 81.2500 (85.9375) lr 1.2028e+01 eta 0:02:09\n",
      "epoch [89/200] batch [1/2] time 0.748 (0.748) data 0.488 (0.488) loss 0.7051 (0.7051) acc 87.5000 (87.5000) lr 1.2028e+01 eta 0:02:46\n",
      "epoch [89/200] batch [2/2] time 0.260 (0.504) data 0.000 (0.244) loss 0.5290 (0.6171) acc 90.6250 (89.0625) lr 1.1874e+01 eta 0:01:51\n",
      "epoch [90/200] batch [1/2] time 0.767 (0.767) data 0.505 (0.505) loss 0.7208 (0.7208) acc 87.5000 (87.5000) lr 1.1874e+01 eta 0:02:49\n",
      "epoch [90/200] batch [2/2] time 0.262 (0.515) data 0.000 (0.253) loss 0.5748 (0.6478) acc 93.7500 (90.6250) lr 1.1719e+01 eta 0:01:53\n",
      "epoch [91/200] batch [1/2] time 0.756 (0.756) data 0.499 (0.499) loss 0.7906 (0.7906) acc 75.0000 (75.0000) lr 1.1719e+01 eta 0:02:45\n",
      "epoch [91/200] batch [2/2] time 0.261 (0.509) data 0.000 (0.250) loss 1.0242 (0.9074) acc 84.3750 (79.6875) lr 1.1564e+01 eta 0:01:50\n",
      "epoch [92/200] batch [1/2] time 0.766 (0.766) data 0.508 (0.508) loss 1.3117 (1.3117) acc 78.1250 (78.1250) lr 1.1564e+01 eta 0:02:46\n",
      "epoch [92/200] batch [2/2] time 0.264 (0.515) data 0.000 (0.254) loss 1.4188 (1.3652) acc 62.5000 (70.3125) lr 1.1409e+01 eta 0:01:51\n",
      "epoch [93/200] batch [1/2] time 0.761 (0.761) data 0.502 (0.502) loss 1.5261 (1.5261) acc 75.0000 (75.0000) lr 1.1409e+01 eta 0:02:43\n",
      "epoch [93/200] batch [2/2] time 0.260 (0.511) data 0.000 (0.251) loss 1.0565 (1.2913) acc 84.3750 (79.6875) lr 1.1253e+01 eta 0:01:49\n",
      "epoch [94/200] batch [1/2] time 0.818 (0.818) data 0.552 (0.552) loss 0.9506 (0.9506) acc 81.2500 (81.2500) lr 1.1253e+01 eta 0:02:54\n",
      "epoch [94/200] batch [2/2] time 0.262 (0.540) data 0.000 (0.276) loss 0.7686 (0.8596) acc 90.6250 (85.9375) lr 1.1097e+01 eta 0:01:54\n",
      "epoch [95/200] batch [1/2] time 0.782 (0.782) data 0.519 (0.519) loss 1.4393 (1.4393) acc 75.0000 (75.0000) lr 1.1097e+01 eta 0:02:45\n",
      "epoch [95/200] batch [2/2] time 0.276 (0.529) data 0.000 (0.260) loss 1.4489 (1.4441) acc 68.7500 (71.8750) lr 1.0941e+01 eta 0:01:51\n",
      "epoch [96/200] batch [1/2] time 0.758 (0.758) data 0.500 (0.500) loss 1.1917 (1.1917) acc 84.3750 (84.3750) lr 1.0941e+01 eta 0:02:38\n",
      "epoch [96/200] batch [2/2] time 0.267 (0.512) data 0.000 (0.250) loss 0.8650 (1.0284) acc 84.3750 (84.3750) lr 1.0785e+01 eta 0:01:46\n",
      "epoch [97/200] batch [1/2] time 1.090 (1.090) data 0.821 (0.821) loss 0.7250 (0.7250) acc 96.8750 (96.8750) lr 1.0785e+01 eta 0:03:45\n",
      "epoch [97/200] batch [2/2] time 0.265 (0.678) data 0.001 (0.411) loss 0.7844 (0.7547) acc 87.5000 (92.1875) lr 1.0628e+01 eta 0:02:19\n",
      "epoch [98/200] batch [1/2] time 1.054 (1.054) data 0.793 (0.793) loss 0.8291 (0.8291) acc 84.3750 (84.3750) lr 1.0628e+01 eta 0:03:36\n",
      "epoch [98/200] batch [2/2] time 0.263 (0.659) data 0.001 (0.397) loss 0.7407 (0.7849) acc 87.5000 (85.9375) lr 1.0471e+01 eta 0:02:14\n",
      "epoch [99/200] batch [1/2] time 0.946 (0.946) data 0.685 (0.685) loss 1.0248 (1.0248) acc 84.3750 (84.3750) lr 1.0471e+01 eta 0:03:11\n",
      "epoch [99/200] batch [2/2] time 0.262 (0.604) data 0.000 (0.343) loss 0.9651 (0.9949) acc 78.1250 (81.2500) lr 1.0314e+01 eta 0:02:01\n",
      "epoch [100/200] batch [1/2] time 0.750 (0.750) data 0.488 (0.488) loss 0.9212 (0.9212) acc 84.3750 (84.3750) lr 1.0314e+01 eta 0:02:30\n",
      "epoch [100/200] batch [2/2] time 0.260 (0.505) data 0.001 (0.244) loss 0.6702 (0.7957) acc 87.5000 (85.9375) lr 1.0157e+01 eta 0:01:41\n",
      "epoch [101/200] batch [1/2] time 0.743 (0.743) data 0.480 (0.480) loss 0.8766 (0.8766) acc 87.5000 (87.5000) lr 1.0157e+01 eta 0:02:27\n",
      "epoch [101/200] batch [2/2] time 0.264 (0.504) data 0.000 (0.240) loss 0.5495 (0.7131) acc 90.6250 (89.0625) lr 1.0000e+01 eta 0:01:39\n",
      "epoch [102/200] batch [1/2] time 0.817 (0.817) data 0.559 (0.559) loss 1.1186 (1.1186) acc 75.0000 (75.0000) lr 1.0000e+01 eta 0:02:41\n",
      "epoch [102/200] batch [2/2] time 0.261 (0.539) data 0.000 (0.280) loss 0.7524 (0.9355) acc 90.6250 (82.8125) lr 9.8429e+00 eta 0:01:45\n",
      "epoch [103/200] batch [1/2] time 0.848 (0.848) data 0.579 (0.579) loss 0.6126 (0.6126) acc 90.6250 (90.6250) lr 9.8429e+00 eta 0:02:45\n",
      "epoch [103/200] batch [2/2] time 0.265 (0.557) data 0.001 (0.290) loss 0.5500 (0.5813) acc 90.6250 (90.6250) lr 9.6859e+00 eta 0:01:48\n",
      "epoch [104/200] batch [1/2] time 0.757 (0.757) data 0.496 (0.496) loss 0.8734 (0.8734) acc 84.3750 (84.3750) lr 9.6859e+00 eta 0:02:26\n",
      "epoch [104/200] batch [2/2] time 0.263 (0.510) data 0.001 (0.248) loss 0.6749 (0.7741) acc 90.6250 (87.5000) lr 9.5289e+00 eta 0:01:37\n",
      "epoch [105/200] batch [1/2] time 0.755 (0.755) data 0.497 (0.497) loss 1.8177 (1.8177) acc 71.8750 (71.8750) lr 9.5289e+00 eta 0:02:24\n",
      "epoch [105/200] batch [2/2] time 0.263 (0.509) data 0.000 (0.249) loss 0.5497 (1.1837) acc 93.7500 (82.8125) lr 9.3721e+00 eta 0:01:36\n",
      "epoch [106/200] batch [1/2] time 0.773 (0.773) data 0.514 (0.514) loss 0.9707 (0.9707) acc 78.1250 (78.1250) lr 9.3721e+00 eta 0:02:26\n",
      "epoch [106/200] batch [2/2] time 0.262 (0.518) data 0.000 (0.257) loss 0.7710 (0.8708) acc 81.2500 (79.6875) lr 9.2154e+00 eta 0:01:37\n",
      "epoch [107/200] batch [1/2] time 0.764 (0.764) data 0.502 (0.502) loss 0.8295 (0.8295) acc 87.5000 (87.5000) lr 9.2154e+00 eta 0:02:22\n",
      "epoch [107/200] batch [2/2] time 0.269 (0.517) data 0.001 (0.251) loss 1.2682 (1.0488) acc 78.1250 (82.8125) lr 9.0589e+00 eta 0:01:36\n",
      "epoch [108/200] batch [1/2] time 1.066 (1.066) data 0.792 (0.792) loss 0.6988 (0.6988) acc 87.5000 (87.5000) lr 9.0589e+00 eta 0:03:17\n",
      "epoch [108/200] batch [2/2] time 0.267 (0.666) data 0.001 (0.396) loss 0.7023 (0.7005) acc 90.6250 (89.0625) lr 8.9027e+00 eta 0:02:02\n",
      "epoch [109/200] batch [1/2] time 1.078 (1.078) data 0.817 (0.817) loss 0.7628 (0.7628) acc 87.5000 (87.5000) lr 8.9027e+00 eta 0:03:17\n",
      "epoch [109/200] batch [2/2] time 0.266 (0.672) data 0.001 (0.409) loss 0.7665 (0.7646) acc 84.3750 (85.9375) lr 8.7467e+00 eta 0:02:02\n",
      "epoch [110/200] batch [1/2] time 0.959 (0.959) data 0.698 (0.698) loss 0.8623 (0.8623) acc 90.6250 (90.6250) lr 8.7467e+00 eta 0:02:53\n",
      "epoch [110/200] batch [2/2] time 0.266 (0.612) data 0.000 (0.349) loss 1.0041 (0.9332) acc 84.3750 (87.5000) lr 8.5910e+00 eta 0:01:50\n",
      "epoch [111/200] batch [1/2] time 0.810 (0.810) data 0.548 (0.548) loss 1.1747 (1.1747) acc 84.3750 (84.3750) lr 8.5910e+00 eta 0:02:24\n",
      "epoch [111/200] batch [2/2] time 0.265 (0.537) data 0.000 (0.274) loss 0.6097 (0.8922) acc 90.6250 (87.5000) lr 8.4357e+00 eta 0:01:35\n",
      "epoch [112/200] batch [1/2] time 0.795 (0.795) data 0.535 (0.535) loss 0.6620 (0.6620) acc 87.5000 (87.5000) lr 8.4357e+00 eta 0:02:20\n",
      "epoch [112/200] batch [2/2] time 0.264 (0.529) data 0.001 (0.268) loss 0.9374 (0.7997) acc 84.3750 (85.9375) lr 8.2807e+00 eta 0:01:33\n",
      "epoch [113/200] batch [1/2] time 0.780 (0.780) data 0.519 (0.519) loss 0.7834 (0.7834) acc 87.5000 (87.5000) lr 8.2807e+00 eta 0:02:16\n",
      "epoch [113/200] batch [2/2] time 0.260 (0.520) data 0.000 (0.260) loss 0.7332 (0.7583) acc 90.6250 (89.0625) lr 8.1262e+00 eta 0:01:30\n",
      "epoch [114/200] batch [1/2] time 0.766 (0.766) data 0.505 (0.505) loss 0.5070 (0.5070) acc 100.0000 (100.0000) lr 8.1262e+00 eta 0:02:12\n",
      "epoch [114/200] batch [2/2] time 0.263 (0.515) data 0.000 (0.253) loss 0.7703 (0.6387) acc 81.2500 (90.6250) lr 7.9721e+00 eta 0:01:28\n",
      "epoch [115/200] batch [1/2] time 0.775 (0.775) data 0.512 (0.512) loss 0.5228 (0.5228) acc 93.7500 (93.7500) lr 7.9721e+00 eta 0:02:12\n",
      "epoch [115/200] batch [2/2] time 0.266 (0.520) data 0.000 (0.256) loss 1.1180 (0.8204) acc 78.1250 (85.9375) lr 7.8186e+00 eta 0:01:28\n",
      "epoch [116/200] batch [1/2] time 0.752 (0.752) data 0.487 (0.487) loss 0.4614 (0.4614) acc 100.0000 (100.0000) lr 7.8186e+00 eta 0:02:07\n",
      "epoch [116/200] batch [2/2] time 0.265 (0.509) data 0.001 (0.244) loss 0.9345 (0.6979) acc 87.5000 (93.7500) lr 7.6655e+00 eta 0:01:25\n",
      "epoch [117/200] batch [1/2] time 0.741 (0.741) data 0.477 (0.477) loss 0.5898 (0.5898) acc 93.7500 (93.7500) lr 7.6655e+00 eta 0:02:03\n",
      "epoch [117/200] batch [2/2] time 0.267 (0.504) data 0.001 (0.239) loss 0.8805 (0.7352) acc 87.5000 (90.6250) lr 7.5131e+00 eta 0:01:23\n",
      "epoch [118/200] batch [1/2] time 0.766 (0.766) data 0.506 (0.506) loss 0.7711 (0.7711) acc 87.5000 (87.5000) lr 7.5131e+00 eta 0:02:06\n",
      "epoch [118/200] batch [2/2] time 0.271 (0.518) data 0.001 (0.253) loss 0.8524 (0.8118) acc 81.2500 (84.3750) lr 7.3613e+00 eta 0:01:24\n",
      "epoch [119/200] batch [1/2] time 1.021 (1.021) data 0.733 (0.733) loss 0.6784 (0.6784) acc 87.5000 (87.5000) lr 7.3613e+00 eta 0:02:46\n",
      "epoch [119/200] batch [2/2] time 0.268 (0.644) data 0.000 (0.367) loss 0.4734 (0.5759) acc 100.0000 (93.7500) lr 7.2101e+00 eta 0:01:44\n",
      "epoch [120/200] batch [1/2] time 1.085 (1.085) data 0.818 (0.818) loss 0.5551 (0.5551) acc 96.8750 (96.8750) lr 7.2101e+00 eta 0:02:54\n",
      "epoch [120/200] batch [2/2] time 0.270 (0.677) data 0.001 (0.409) loss 0.7215 (0.6383) acc 87.5000 (92.1875) lr 7.0596e+00 eta 0:01:48\n",
      "epoch [121/200] batch [1/2] time 1.010 (1.010) data 0.747 (0.747) loss 0.7843 (0.7843) acc 81.2500 (81.2500) lr 7.0596e+00 eta 0:02:40\n",
      "epoch [121/200] batch [2/2] time 0.268 (0.639) data 0.001 (0.374) loss 0.5834 (0.6839) acc 93.7500 (87.5000) lr 6.9098e+00 eta 0:01:40\n",
      "epoch [122/200] batch [1/2] time 0.747 (0.747) data 0.482 (0.482) loss 0.5416 (0.5416) acc 90.6250 (90.6250) lr 6.9098e+00 eta 0:01:57\n",
      "epoch [122/200] batch [2/2] time 0.269 (0.508) data 0.000 (0.241) loss 0.5568 (0.5492) acc 93.7500 (92.1875) lr 6.7608e+00 eta 0:01:19\n",
      "epoch [123/200] batch [1/2] time 0.755 (0.755) data 0.492 (0.492) loss 0.5057 (0.5057) acc 90.6250 (90.6250) lr 6.7608e+00 eta 0:01:57\n",
      "epoch [123/200] batch [2/2] time 0.267 (0.511) data 0.001 (0.246) loss 0.3480 (0.4268) acc 100.0000 (95.3125) lr 6.6126e+00 eta 0:01:18\n",
      "epoch [124/200] batch [1/2] time 0.750 (0.750) data 0.488 (0.488) loss 0.7126 (0.7126) acc 90.6250 (90.6250) lr 6.6126e+00 eta 0:01:54\n",
      "epoch [124/200] batch [2/2] time 0.266 (0.508) data 0.001 (0.244) loss 0.4513 (0.5820) acc 96.8750 (93.7500) lr 6.4653e+00 eta 0:01:17\n",
      "epoch [125/200] batch [1/2] time 0.762 (0.762) data 0.498 (0.498) loss 0.6833 (0.6833) acc 81.2500 (81.2500) lr 6.4653e+00 eta 0:01:55\n",
      "epoch [125/200] batch [2/2] time 0.266 (0.514) data 0.000 (0.249) loss 0.6754 (0.6794) acc 93.7500 (87.5000) lr 6.3188e+00 eta 0:01:17\n",
      "epoch [126/200] batch [1/2] time 0.763 (0.763) data 0.501 (0.501) loss 0.7211 (0.7211) acc 84.3750 (84.3750) lr 6.3188e+00 eta 0:01:53\n",
      "epoch [126/200] batch [2/2] time 0.269 (0.516) data 0.000 (0.251) loss 0.4460 (0.5836) acc 96.8750 (90.6250) lr 6.1732e+00 eta 0:01:16\n",
      "epoch [127/200] batch [1/2] time 0.849 (0.849) data 0.586 (0.586) loss 0.8507 (0.8507) acc 84.3750 (84.3750) lr 6.1732e+00 eta 0:02:04\n",
      "epoch [127/200] batch [2/2] time 0.268 (0.559) data 0.000 (0.293) loss 0.5141 (0.6824) acc 93.7500 (89.0625) lr 6.0285e+00 eta 0:01:21\n",
      "epoch [128/200] batch [1/2] time 0.758 (0.758) data 0.494 (0.494) loss 0.4994 (0.4994) acc 93.7500 (93.7500) lr 6.0285e+00 eta 0:01:49\n",
      "epoch [128/200] batch [2/2] time 0.267 (0.512) data 0.000 (0.247) loss 0.5486 (0.5240) acc 96.8750 (95.3125) lr 5.8849e+00 eta 0:01:13\n",
      "epoch [129/200] batch [1/2] time 0.756 (0.756) data 0.495 (0.495) loss 0.4231 (0.4231) acc 96.8750 (96.8750) lr 5.8849e+00 eta 0:01:48\n",
      "epoch [129/200] batch [2/2] time 0.271 (0.514) data 0.001 (0.248) loss 0.5944 (0.5088) acc 93.7500 (95.3125) lr 5.7422e+00 eta 0:01:12\n",
      "epoch [130/200] batch [1/2] time 1.065 (1.065) data 0.796 (0.796) loss 0.3480 (0.3480) acc 100.0000 (100.0000) lr 5.7422e+00 eta 0:02:30\n",
      "epoch [130/200] batch [2/2] time 0.270 (0.667) data 0.001 (0.398) loss 0.3794 (0.3637) acc 96.8750 (98.4375) lr 5.6006e+00 eta 0:01:33\n",
      "epoch [131/200] batch [1/2] time 1.092 (1.092) data 0.815 (0.815) loss 0.4025 (0.4025) acc 96.8750 (96.8750) lr 5.6006e+00 eta 0:02:31\n",
      "epoch [131/200] batch [2/2] time 0.270 (0.681) data 0.002 (0.409) loss 0.3685 (0.3855) acc 96.8750 (96.8750) lr 5.4601e+00 eta 0:01:33\n",
      "epoch [132/200] batch [1/2] time 0.962 (0.962) data 0.700 (0.700) loss 0.9150 (0.9150) acc 90.6250 (90.6250) lr 5.4601e+00 eta 0:02:11\n",
      "epoch [132/200] batch [2/2] time 0.267 (0.615) data 0.000 (0.350) loss 0.8328 (0.8739) acc 75.0000 (82.8125) lr 5.3207e+00 eta 0:01:23\n",
      "epoch [133/200] batch [1/2] time 0.764 (0.764) data 0.499 (0.499) loss 0.5635 (0.5635) acc 90.6250 (90.6250) lr 5.3207e+00 eta 0:01:43\n",
      "epoch [133/200] batch [2/2] time 0.267 (0.516) data 0.000 (0.250) loss 0.6026 (0.5830) acc 87.5000 (89.0625) lr 5.1825e+00 eta 0:01:09\n",
      "epoch [134/200] batch [1/2] time 0.761 (0.761) data 0.494 (0.494) loss 0.6934 (0.6934) acc 90.6250 (90.6250) lr 5.1825e+00 eta 0:01:41\n",
      "epoch [134/200] batch [2/2] time 0.268 (0.515) data 0.000 (0.247) loss 0.5841 (0.6388) acc 90.6250 (90.6250) lr 5.0454e+00 eta 0:01:07\n",
      "epoch [135/200] batch [1/2] time 0.800 (0.800) data 0.537 (0.537) loss 0.3664 (0.3664) acc 100.0000 (100.0000) lr 5.0454e+00 eta 0:01:44\n",
      "epoch [135/200] batch [2/2] time 0.269 (0.534) data 0.001 (0.269) loss 0.4514 (0.4089) acc 96.8750 (98.4375) lr 4.9096e+00 eta 0:01:09\n",
      "epoch [136/200] batch [1/2] time 0.776 (0.776) data 0.509 (0.509) loss 0.6098 (0.6098) acc 90.6250 (90.6250) lr 4.9096e+00 eta 0:01:40\n",
      "epoch [136/200] batch [2/2] time 0.267 (0.521) data 0.001 (0.255) loss 0.4224 (0.5161) acc 96.8750 (93.7500) lr 4.7750e+00 eta 0:01:06\n",
      "epoch [137/200] batch [1/2] time 0.753 (0.753) data 0.489 (0.489) loss 1.0361 (1.0361) acc 75.0000 (75.0000) lr 4.7750e+00 eta 0:01:35\n",
      "epoch [137/200] batch [2/2] time 0.268 (0.511) data 0.001 (0.245) loss 1.2016 (1.1189) acc 78.1250 (76.5625) lr 4.6417e+00 eta 0:01:04\n",
      "epoch [138/200] batch [1/2] time 0.781 (0.781) data 0.513 (0.513) loss 0.9759 (0.9759) acc 71.8750 (71.8750) lr 4.6417e+00 eta 0:01:37\n",
      "epoch [138/200] batch [2/2] time 0.269 (0.525) data 0.001 (0.257) loss 0.7379 (0.8569) acc 81.2500 (76.5625) lr 4.5098e+00 eta 0:01:05\n",
      "epoch [139/200] batch [1/2] time 0.778 (0.778) data 0.511 (0.511) loss 0.6951 (0.6951) acc 87.5000 (87.5000) lr 4.5098e+00 eta 0:01:35\n",
      "epoch [139/200] batch [2/2] time 0.268 (0.523) data 0.000 (0.256) loss 0.8788 (0.7870) acc 87.5000 (87.5000) lr 4.3792e+00 eta 0:01:03\n",
      "epoch [140/200] batch [1/2] time 0.766 (0.766) data 0.499 (0.499) loss 0.6388 (0.6388) acc 90.6250 (90.6250) lr 4.3792e+00 eta 0:01:32\n",
      "epoch [140/200] batch [2/2] time 0.272 (0.519) data 0.001 (0.250) loss 0.4885 (0.5636) acc 96.8750 (93.7500) lr 4.2499e+00 eta 0:01:02\n",
      "epoch [141/200] batch [1/2] time 1.058 (1.058) data 0.790 (0.790) loss 0.5780 (0.5780) acc 93.7500 (93.7500) lr 4.2499e+00 eta 0:02:05\n",
      "epoch [141/200] batch [2/2] time 0.268 (0.663) data 0.001 (0.395) loss 0.4342 (0.5061) acc 100.0000 (96.8750) lr 4.1221e+00 eta 0:01:18\n",
      "epoch [142/200] batch [1/2] time 1.161 (1.161) data 0.884 (0.884) loss 0.5644 (0.5644) acc 96.8750 (96.8750) lr 4.1221e+00 eta 0:02:15\n",
      "epoch [142/200] batch [2/2] time 0.280 (0.720) data 0.001 (0.443) loss 0.6898 (0.6271) acc 90.6250 (93.7500) lr 3.9958e+00 eta 0:01:23\n",
      "epoch [143/200] batch [1/2] time 0.907 (0.907) data 0.642 (0.642) loss 0.6486 (0.6486) acc 90.6250 (90.6250) lr 3.9958e+00 eta 0:01:44\n",
      "epoch [143/200] batch [2/2] time 0.270 (0.588) data 0.001 (0.321) loss 0.4533 (0.5510) acc 93.7500 (92.1875) lr 3.8709e+00 eta 0:01:07\n",
      "epoch [144/200] batch [1/2] time 0.767 (0.767) data 0.502 (0.502) loss 0.3778 (0.3778) acc 100.0000 (100.0000) lr 3.8709e+00 eta 0:01:26\n",
      "epoch [144/200] batch [2/2] time 0.271 (0.519) data 0.000 (0.251) loss 0.5666 (0.4722) acc 90.6250 (95.3125) lr 3.7476e+00 eta 0:00:58\n",
      "epoch [145/200] batch [1/2] time 0.770 (0.770) data 0.507 (0.507) loss 0.4650 (0.4650) acc 96.8750 (96.8750) lr 3.7476e+00 eta 0:01:25\n",
      "epoch [145/200] batch [2/2] time 0.270 (0.520) data 0.000 (0.254) loss 0.3617 (0.4133) acc 100.0000 (98.4375) lr 3.6258e+00 eta 0:00:57\n",
      "epoch [146/200] batch [1/2] time 0.770 (0.770) data 0.503 (0.503) loss 0.4479 (0.4479) acc 96.8750 (96.8750) lr 3.6258e+00 eta 0:01:23\n",
      "epoch [146/200] batch [2/2] time 0.268 (0.519) data 0.000 (0.252) loss 0.3892 (0.4185) acc 96.8750 (96.8750) lr 3.5055e+00 eta 0:00:56\n",
      "epoch [147/200] batch [1/2] time 0.885 (0.885) data 0.621 (0.621) loss 0.4365 (0.4365) acc 93.7500 (93.7500) lr 3.5055e+00 eta 0:01:34\n",
      "epoch [147/200] batch [2/2] time 0.268 (0.576) data 0.000 (0.311) loss 0.7284 (0.5824) acc 87.5000 (90.6250) lr 3.3869e+00 eta 0:01:01\n",
      "epoch [148/200] batch [1/2] time 0.749 (0.749) data 0.482 (0.482) loss 0.3974 (0.3974) acc 96.8750 (96.8750) lr 3.3869e+00 eta 0:01:18\n",
      "epoch [148/200] batch [2/2] time 0.270 (0.509) data 0.000 (0.241) loss 0.4985 (0.4480) acc 90.6250 (93.7500) lr 3.2699e+00 eta 0:00:52\n",
      "epoch [149/200] batch [1/2] time 0.763 (0.763) data 0.496 (0.496) loss 0.5710 (0.5710) acc 93.7500 (93.7500) lr 3.2699e+00 eta 0:01:18\n",
      "epoch [149/200] batch [2/2] time 0.270 (0.517) data 0.000 (0.248) loss 0.6260 (0.5985) acc 84.3750 (89.0625) lr 3.1545e+00 eta 0:00:52\n",
      "epoch [150/200] batch [1/2] time 0.779 (0.779) data 0.515 (0.515) loss 0.3796 (0.3796) acc 96.8750 (96.8750) lr 3.1545e+00 eta 0:01:18\n",
      "epoch [150/200] batch [2/2] time 0.271 (0.525) data 0.000 (0.258) loss 0.3139 (0.3467) acc 100.0000 (98.4375) lr 3.0409e+00 eta 0:00:52\n",
      "epoch [151/200] batch [1/2] time 0.807 (0.807) data 0.538 (0.538) loss 0.5168 (0.5168) acc 93.7500 (93.7500) lr 3.0409e+00 eta 0:01:19\n",
      "epoch [151/200] batch [2/2] time 0.270 (0.539) data 0.001 (0.269) loss 0.4017 (0.4593) acc 93.7500 (93.7500) lr 2.9289e+00 eta 0:00:52\n",
      "epoch [152/200] batch [1/2] time 1.059 (1.059) data 0.791 (0.791) loss 0.4436 (0.4436) acc 96.8750 (96.8750) lr 2.9289e+00 eta 0:01:42\n",
      "epoch [152/200] batch [2/2] time 0.272 (0.665) data 0.001 (0.396) loss 0.5234 (0.4835) acc 90.6250 (93.7500) lr 2.8187e+00 eta 0:01:03\n",
      "epoch [153/200] batch [1/2] time 1.116 (1.116) data 0.847 (0.847) loss 0.5635 (0.5635) acc 93.7500 (93.7500) lr 2.8187e+00 eta 0:01:46\n",
      "epoch [153/200] batch [2/2] time 0.272 (0.694) data 0.001 (0.424) loss 0.3497 (0.4566) acc 100.0000 (96.8750) lr 2.7103e+00 eta 0:01:05\n",
      "epoch [154/200] batch [1/2] time 0.969 (0.969) data 0.705 (0.705) loss 0.3078 (0.3078) acc 100.0000 (100.0000) lr 2.7103e+00 eta 0:01:30\n",
      "epoch [154/200] batch [2/2] time 0.268 (0.618) data 0.001 (0.353) loss 0.2973 (0.3025) acc 100.0000 (100.0000) lr 2.6037e+00 eta 0:00:56\n",
      "epoch [155/200] batch [1/2] time 0.784 (0.784) data 0.518 (0.518) loss 0.3258 (0.3258) acc 96.8750 (96.8750) lr 2.6037e+00 eta 0:01:11\n",
      "epoch [155/200] batch [2/2] time 0.270 (0.527) data 0.001 (0.259) loss 0.3774 (0.3516) acc 96.8750 (96.8750) lr 2.4989e+00 eta 0:00:47\n",
      "epoch [156/200] batch [1/2] time 0.783 (0.783) data 0.517 (0.517) loss 0.7872 (0.7872) acc 90.6250 (90.6250) lr 2.4989e+00 eta 0:01:09\n",
      "epoch [156/200] batch [2/2] time 0.268 (0.526) data 0.000 (0.259) loss 0.3475 (0.5674) acc 100.0000 (95.3125) lr 2.3959e+00 eta 0:00:46\n",
      "epoch [157/200] batch [1/2] time 0.753 (0.753) data 0.487 (0.487) loss 0.5188 (0.5188) acc 96.8750 (96.8750) lr 2.3959e+00 eta 0:01:05\n",
      "epoch [157/200] batch [2/2] time 0.267 (0.510) data 0.000 (0.244) loss 0.7093 (0.6140) acc 87.5000 (92.1875) lr 2.2949e+00 eta 0:00:43\n",
      "epoch [158/200] batch [1/2] time 0.819 (0.819) data 0.550 (0.550) loss 0.2866 (0.2866) acc 100.0000 (100.0000) lr 2.2949e+00 eta 0:01:09\n",
      "epoch [158/200] batch [2/2] time 0.269 (0.544) data 0.001 (0.275) loss 0.4732 (0.3799) acc 96.8750 (98.4375) lr 2.1957e+00 eta 0:00:45\n",
      "epoch [159/200] batch [1/2] time 0.773 (0.773) data 0.507 (0.507) loss 0.4640 (0.4640) acc 93.7500 (93.7500) lr 2.1957e+00 eta 0:01:04\n",
      "epoch [159/200] batch [2/2] time 0.270 (0.521) data 0.001 (0.254) loss 0.3883 (0.4261) acc 93.7500 (93.7500) lr 2.0984e+00 eta 0:00:42\n",
      "epoch [160/200] batch [1/2] time 0.771 (0.771) data 0.506 (0.506) loss 0.4732 (0.4732) acc 90.6250 (90.6250) lr 2.0984e+00 eta 0:01:02\n",
      "epoch [160/200] batch [2/2] time 0.268 (0.519) data 0.000 (0.253) loss 0.2779 (0.3755) acc 100.0000 (95.3125) lr 2.0032e+00 eta 0:00:41\n",
      "epoch [161/200] batch [1/2] time 0.746 (0.746) data 0.480 (0.480) loss 0.3024 (0.3024) acc 96.8750 (96.8750) lr 2.0032e+00 eta 0:00:58\n",
      "epoch [161/200] batch [2/2] time 0.268 (0.507) data 0.000 (0.240) loss 0.3074 (0.3049) acc 96.8750 (96.8750) lr 1.9098e+00 eta 0:00:39\n",
      "epoch [162/200] batch [1/2] time 0.776 (0.776) data 0.507 (0.507) loss 0.3701 (0.3701) acc 96.8750 (96.8750) lr 1.9098e+00 eta 0:00:59\n",
      "epoch [162/200] batch [2/2] time 0.274 (0.525) data 0.001 (0.254) loss 0.5007 (0.4354) acc 87.5000 (92.1875) lr 1.8185e+00 eta 0:00:39\n",
      "epoch [163/200] batch [1/2] time 1.089 (1.089) data 0.823 (0.823) loss 0.3719 (0.3719) acc 96.8750 (96.8750) lr 1.8185e+00 eta 0:01:21\n",
      "epoch [163/200] batch [2/2] time 0.273 (0.681) data 0.001 (0.412) loss 0.3353 (0.3536) acc 96.8750 (96.8750) lr 1.7292e+00 eta 0:00:50\n",
      "epoch [164/200] batch [1/2] time 1.130 (1.130) data 0.859 (0.859) loss 0.3391 (0.3391) acc 100.0000 (100.0000) lr 1.7292e+00 eta 0:01:22\n",
      "epoch [164/200] batch [2/2] time 0.273 (0.701) data 0.001 (0.430) loss 0.2713 (0.3052) acc 100.0000 (100.0000) lr 1.6419e+00 eta 0:00:50\n",
      "epoch [165/200] batch [1/2] time 0.909 (0.909) data 0.641 (0.641) loss 0.2736 (0.2736) acc 96.8750 (96.8750) lr 1.6419e+00 eta 0:01:04\n",
      "epoch [165/200] batch [2/2] time 0.269 (0.589) data 0.001 (0.321) loss 0.6854 (0.4795) acc 87.5000 (92.1875) lr 1.5567e+00 eta 0:00:41\n",
      "epoch [166/200] batch [1/2] time 0.756 (0.756) data 0.489 (0.489) loss 0.2909 (0.2909) acc 100.0000 (100.0000) lr 1.5567e+00 eta 0:00:52\n",
      "epoch [166/200] batch [2/2] time 0.268 (0.512) data 0.001 (0.245) loss 0.6016 (0.4463) acc 90.6250 (95.3125) lr 1.4736e+00 eta 0:00:34\n",
      "epoch [167/200] batch [1/2] time 0.766 (0.766) data 0.499 (0.499) loss 0.3359 (0.3359) acc 100.0000 (100.0000) lr 1.4736e+00 eta 0:00:51\n",
      "epoch [167/200] batch [2/2] time 0.270 (0.518) data 0.000 (0.250) loss 0.3435 (0.3397) acc 96.8750 (98.4375) lr 1.3926e+00 eta 0:00:34\n",
      "epoch [168/200] batch [1/2] time 0.767 (0.767) data 0.501 (0.501) loss 0.3520 (0.3520) acc 100.0000 (100.0000) lr 1.3926e+00 eta 0:00:49\n",
      "epoch [168/200] batch [2/2] time 0.269 (0.518) data 0.000 (0.251) loss 0.2975 (0.3248) acc 100.0000 (100.0000) lr 1.3137e+00 eta 0:00:33\n",
      "epoch [169/200] batch [1/2] time 0.760 (0.760) data 0.497 (0.497) loss 0.4733 (0.4733) acc 93.7500 (93.7500) lr 1.3137e+00 eta 0:00:47\n",
      "epoch [169/200] batch [2/2] time 0.270 (0.515) data 0.000 (0.249) loss 0.4975 (0.4854) acc 90.6250 (92.1875) lr 1.2369e+00 eta 0:00:31\n",
      "epoch [170/200] batch [1/2] time 0.781 (0.781) data 0.514 (0.514) loss 0.4639 (0.4639) acc 93.7500 (93.7500) lr 1.2369e+00 eta 0:00:47\n",
      "epoch [170/200] batch [2/2] time 0.272 (0.526) data 0.001 (0.257) loss 0.3909 (0.4274) acc 96.8750 (95.3125) lr 1.1623e+00 eta 0:00:31\n",
      "epoch [171/200] batch [1/2] time 0.750 (0.750) data 0.483 (0.483) loss 0.2892 (0.2892) acc 100.0000 (100.0000) lr 1.1623e+00 eta 0:00:44\n",
      "epoch [171/200] batch [2/2] time 0.271 (0.511) data 0.000 (0.242) loss 0.3225 (0.3059) acc 100.0000 (100.0000) lr 1.0899e+00 eta 0:00:29\n",
      "epoch [172/200] batch [1/2] time 0.742 (0.742) data 0.475 (0.475) loss 0.3962 (0.3962) acc 96.8750 (96.8750) lr 1.0899e+00 eta 0:00:42\n",
      "epoch [172/200] batch [2/2] time 0.272 (0.507) data 0.000 (0.238) loss 0.4017 (0.3990) acc 90.6250 (93.7500) lr 1.0197e+00 eta 0:00:28\n",
      "epoch [173/200] batch [1/2] time 0.764 (0.764) data 0.498 (0.498) loss 0.2569 (0.2569) acc 100.0000 (100.0000) lr 1.0197e+00 eta 0:00:42\n",
      "epoch [173/200] batch [2/2] time 0.277 (0.520) data 0.001 (0.249) loss 0.3966 (0.3267) acc 93.7500 (96.8750) lr 9.5173e-01 eta 0:00:28\n",
      "epoch [174/200] batch [1/2] time 1.066 (1.066) data 0.793 (0.793) loss 0.2546 (0.2546) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:56\n",
      "epoch [174/200] batch [2/2] time 0.274 (0.670) data 0.001 (0.397) loss 0.2518 (0.2532) acc 100.0000 (100.0000) lr 8.8597e-01 eta 0:00:34\n",
      "epoch [175/200] batch [1/2] time 1.104 (1.104) data 0.831 (0.831) loss 0.2508 (0.2508) acc 100.0000 (100.0000) lr 8.8597e-01 eta 0:00:56\n",
      "epoch [175/200] batch [2/2] time 0.279 (0.692) data 0.000 (0.416) loss 0.3606 (0.3057) acc 96.8750 (98.4375) lr 8.2245e-01 eta 0:00:34\n",
      "epoch [176/200] batch [1/2] time 0.890 (0.890) data 0.621 (0.621) loss 0.2614 (0.2614) acc 100.0000 (100.0000) lr 8.2245e-01 eta 0:00:43\n",
      "epoch [176/200] batch [2/2] time 0.268 (0.579) data 0.000 (0.311) loss 0.2769 (0.2692) acc 100.0000 (100.0000) lr 7.6120e-01 eta 0:00:27\n",
      "epoch [177/200] batch [1/2] time 0.744 (0.744) data 0.475 (0.475) loss 0.3866 (0.3866) acc 90.6250 (90.6250) lr 7.6120e-01 eta 0:00:34\n",
      "epoch [177/200] batch [2/2] time 0.272 (0.508) data 0.000 (0.238) loss 0.3328 (0.3597) acc 96.8750 (93.7500) lr 7.0224e-01 eta 0:00:23\n",
      "epoch [178/200] batch [1/2] time 0.839 (0.839) data 0.570 (0.570) loss 0.3548 (0.3548) acc 96.8750 (96.8750) lr 7.0224e-01 eta 0:00:37\n",
      "epoch [178/200] batch [2/2] time 0.273 (0.556) data 0.000 (0.285) loss 0.2766 (0.3157) acc 100.0000 (98.4375) lr 6.4556e-01 eta 0:00:24\n",
      "epoch [179/200] batch [1/2] time 0.749 (0.749) data 0.478 (0.478) loss 0.3601 (0.3601) acc 96.8750 (96.8750) lr 6.4556e-01 eta 0:00:32\n",
      "epoch [179/200] batch [2/2] time 0.271 (0.510) data 0.001 (0.239) loss 0.6018 (0.4810) acc 93.7500 (95.3125) lr 5.9119e-01 eta 0:00:21\n",
      "epoch [180/200] batch [1/2] time 0.740 (0.740) data 0.467 (0.467) loss 0.3219 (0.3219) acc 96.8750 (96.8750) lr 5.9119e-01 eta 0:00:30\n",
      "epoch [180/200] batch [2/2] time 0.271 (0.506) data 0.001 (0.234) loss 0.3578 (0.3398) acc 96.8750 (96.8750) lr 5.3915e-01 eta 0:00:20\n",
      "epoch [181/200] batch [1/2] time 0.754 (0.754) data 0.488 (0.488) loss 0.2242 (0.2242) acc 100.0000 (100.0000) lr 5.3915e-01 eta 0:00:29\n",
      "epoch [181/200] batch [2/2] time 0.273 (0.514) data 0.001 (0.244) loss 0.3206 (0.2724) acc 96.8750 (98.4375) lr 4.8943e-01 eta 0:00:19\n",
      "epoch [182/200] batch [1/2] time 0.757 (0.757) data 0.487 (0.487) loss 0.2337 (0.2337) acc 100.0000 (100.0000) lr 4.8943e-01 eta 0:00:28\n",
      "epoch [182/200] batch [2/2] time 0.273 (0.515) data 0.001 (0.244) loss 0.2364 (0.2351) acc 100.0000 (100.0000) lr 4.4207e-01 eta 0:00:18\n",
      "epoch [183/200] batch [1/2] time 0.776 (0.776) data 0.510 (0.510) loss 0.3095 (0.3095) acc 96.8750 (96.8750) lr 4.4207e-01 eta 0:00:27\n",
      "epoch [183/200] batch [2/2] time 0.273 (0.524) data 0.000 (0.255) loss 0.2654 (0.2874) acc 100.0000 (98.4375) lr 3.9706e-01 eta 0:00:17\n",
      "epoch [184/200] batch [1/2] time 0.760 (0.760) data 0.491 (0.491) loss 0.2555 (0.2555) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:25\n",
      "epoch [184/200] batch [2/2] time 0.275 (0.518) data 0.001 (0.246) loss 0.3888 (0.3222) acc 93.7500 (96.8750) lr 3.5443e-01 eta 0:00:16\n",
      "epoch [185/200] batch [1/2] time 1.086 (1.086) data 0.817 (0.817) loss 0.2887 (0.2887) acc 96.8750 (96.8750) lr 3.5443e-01 eta 0:00:33\n",
      "epoch [185/200] batch [2/2] time 0.274 (0.680) data 0.001 (0.409) loss 0.3450 (0.3168) acc 96.8750 (96.8750) lr 3.1417e-01 eta 0:00:20\n",
      "epoch [186/200] batch [1/2] time 1.074 (1.074) data 0.798 (0.798) loss 0.3335 (0.3335) acc 96.8750 (96.8750) lr 3.1417e-01 eta 0:00:31\n",
      "epoch [186/200] batch [2/2] time 0.276 (0.675) data 0.001 (0.399) loss 0.2917 (0.3126) acc 96.8750 (96.8750) lr 2.7630e-01 eta 0:00:18\n",
      "epoch [187/200] batch [1/2] time 0.880 (0.880) data 0.610 (0.610) loss 0.4454 (0.4454) acc 90.6250 (90.6250) lr 2.7630e-01 eta 0:00:23\n",
      "epoch [187/200] batch [2/2] time 0.271 (0.576) data 0.000 (0.305) loss 0.2696 (0.3575) acc 100.0000 (95.3125) lr 2.4083e-01 eta 0:00:14\n",
      "epoch [188/200] batch [1/2] time 0.775 (0.775) data 0.506 (0.506) loss 0.2701 (0.2701) acc 96.8750 (96.8750) lr 2.4083e-01 eta 0:00:19\n",
      "epoch [188/200] batch [2/2] time 0.274 (0.524) data 0.000 (0.253) loss 0.2681 (0.2691) acc 100.0000 (98.4375) lr 2.0777e-01 eta 0:00:12\n",
      "epoch [189/200] batch [1/2] time 0.771 (0.771) data 0.502 (0.502) loss 0.2139 (0.2139) acc 100.0000 (100.0000) lr 2.0777e-01 eta 0:00:17\n",
      "epoch [189/200] batch [2/2] time 0.273 (0.522) data 0.000 (0.251) loss 0.2212 (0.2175) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:11\n",
      "epoch [190/200] batch [1/2] time 0.768 (0.768) data 0.501 (0.501) loss 0.4609 (0.4609) acc 93.7500 (93.7500) lr 1.7713e-01 eta 0:00:16\n",
      "epoch [190/200] batch [2/2] time 0.273 (0.521) data 0.000 (0.251) loss 0.2202 (0.3405) acc 100.0000 (96.8750) lr 1.4891e-01 eta 0:00:10\n",
      "epoch [191/200] batch [1/2] time 0.750 (0.750) data 0.481 (0.481) loss 0.3708 (0.3708) acc 93.7500 (93.7500) lr 1.4891e-01 eta 0:00:14\n",
      "epoch [191/200] batch [2/2] time 0.273 (0.512) data 0.000 (0.241) loss 0.2616 (0.3162) acc 100.0000 (96.8750) lr 1.2312e-01 eta 0:00:09\n",
      "epoch [192/200] batch [1/2] time 0.766 (0.766) data 0.496 (0.496) loss 0.3193 (0.3193) acc 96.8750 (96.8750) lr 1.2312e-01 eta 0:00:13\n",
      "epoch [192/200] batch [2/2] time 0.273 (0.519) data 0.001 (0.248) loss 0.2726 (0.2960) acc 96.8750 (96.8750) lr 9.9763e-02 eta 0:00:08\n",
      "epoch [193/200] batch [1/2] time 0.757 (0.757) data 0.489 (0.489) loss 0.2131 (0.2131) acc 100.0000 (100.0000) lr 9.9763e-02 eta 0:00:11\n",
      "epoch [193/200] batch [2/2] time 0.274 (0.515) data 0.001 (0.245) loss 0.3308 (0.2719) acc 96.8750 (98.4375) lr 7.8853e-02 eta 0:00:07\n",
      "epoch [194/200] batch [1/2] time 0.777 (0.777) data 0.507 (0.507) loss 0.2342 (0.2342) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:10\n",
      "epoch [194/200] batch [2/2] time 0.272 (0.524) data 0.000 (0.254) loss 0.4026 (0.3184) acc 93.7500 (96.8750) lr 6.0390e-02 eta 0:00:06\n",
      "epoch [195/200] batch [1/2] time 0.775 (0.775) data 0.505 (0.505) loss 0.3549 (0.3549) acc 93.7500 (93.7500) lr 6.0390e-02 eta 0:00:08\n",
      "epoch [195/200] batch [2/2] time 0.281 (0.528) data 0.001 (0.253) loss 0.2357 (0.2953) acc 100.0000 (96.8750) lr 4.4380e-02 eta 0:00:05\n",
      "epoch [196/200] batch [1/2] time 1.101 (1.101) data 0.829 (0.829) loss 0.2190 (0.2190) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:09\n",
      "epoch [196/200] batch [2/2] time 0.273 (0.687) data 0.001 (0.415) loss 0.3134 (0.2662) acc 96.8750 (98.4375) lr 3.0827e-02 eta 0:00:05\n",
      "epoch [197/200] batch [1/2] time 1.117 (1.117) data 0.844 (0.844) loss 0.3423 (0.3423) acc 96.8750 (96.8750) lr 3.0827e-02 eta 0:00:07\n",
      "epoch [197/200] batch [2/2] time 0.274 (0.695) data 0.001 (0.422) loss 0.3528 (0.3476) acc 96.8750 (96.8750) lr 1.9733e-02 eta 0:00:04\n",
      "epoch [198/200] batch [1/2] time 0.861 (0.861) data 0.590 (0.590) loss 0.4047 (0.4047) acc 93.7500 (93.7500) lr 1.9733e-02 eta 0:00:04\n",
      "epoch [198/200] batch [2/2] time 0.280 (0.570) data 0.001 (0.295) loss 0.2652 (0.3350) acc 100.0000 (96.8750) lr 1.1101e-02 eta 0:00:02\n",
      "epoch [199/200] batch [1/2] time 0.779 (0.779) data 0.510 (0.510) loss 0.2860 (0.2860) acc 96.8750 (96.8750) lr 1.1101e-02 eta 0:00:02\n",
      "epoch [199/200] batch [2/2] time 0.275 (0.527) data 0.000 (0.255) loss 0.3379 (0.3119) acc 96.8750 (96.8750) lr 4.9344e-03 eta 0:00:01\n",
      "epoch [200/200] batch [1/2] time 0.773 (0.773) data 0.502 (0.502) loss 0.3574 (0.3574) acc 93.7500 (93.7500) lr 4.9344e-03 eta 0:00:00\n",
      "epoch [200/200] batch [2/2] time 0.274 (0.524) data 0.000 (0.251) loss 0.2315 (0.2944) acc 100.0000 (96.8750) lr 1.2337e-03 eta 0:00:00\n",
      "Checkpoint saved to output/eurosat/DAPT/vit_b16_8shots/seed1/prompt_learner/model.pth.tar-200\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:58<00:00,  1.39it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 7,039\n",
      "* accuracy: 86.9%\n",
      "* error: 13.1%\n",
      "* macro_f1: 86.8%\n",
      "Elapsed: 0:05:16\n"
     ]
    }
   ],
   "source": [
    "#eurosat-8shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "        --output-dir output/eurosat/DAPT/vit_b16_8shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kodoK_RLP-gQ",
    "outputId": "c13e4cc7-b41d-462a-dc9d-8212564bd28c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 16:02:53.339730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 16:02:53.360717: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 16:02:53.366611: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 16:02:53.380679: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 16:02:54.400684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '8']\n",
      "output_dir: output/eurosat/DAPT/vit_b16_8shots/seed2\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 8\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/eurosat/DAPT/vit_b16_8shots/seed2\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_2.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  80\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/eurosat/DAPT/vit_b16_8shots/seed2/tensorboard)\n",
      "epoch [1/200] batch [1/2] time 1.948 (1.948) data 0.594 (0.594) loss 11.1543 (11.1543) acc 25.0000 (25.0000) lr 1.0000e-05 eta 0:12:57\n",
      "epoch [1/200] batch [2/2] time 0.250 (1.099) data 0.001 (0.297) loss 11.5512 (11.3527) acc 12.5000 (18.7500) lr 2.0000e+01 eta 0:07:17\n",
      "epoch [2/200] batch [1/2] time 0.793 (0.793) data 0.543 (0.543) loss 11.2128 (11.2128) acc 18.7500 (18.7500) lr 2.0000e+01 eta 0:05:14\n",
      "epoch [2/200] batch [2/2] time 0.252 (0.522) data 0.001 (0.272) loss 10.8974 (11.0551) acc 6.2500 (12.5000) lr 1.9999e+01 eta 0:03:26\n",
      "epoch [3/200] batch [1/2] time 0.790 (0.790) data 0.539 (0.539) loss 9.9540 (9.9540) acc 9.3750 (9.3750) lr 1.9999e+01 eta 0:05:11\n",
      "epoch [3/200] batch [2/2] time 0.251 (0.520) data 0.000 (0.270) loss 9.6032 (9.7786) acc 12.5000 (10.9375) lr 1.9995e+01 eta 0:03:24\n",
      "epoch [4/200] batch [1/2] time 0.758 (0.758) data 0.509 (0.509) loss 9.6332 (9.6332) acc 3.1250 (3.1250) lr 1.9995e+01 eta 0:04:57\n",
      "epoch [4/200] batch [2/2] time 0.254 (0.506) data 0.000 (0.255) loss 9.4707 (9.5520) acc 15.6250 (9.3750) lr 1.9989e+01 eta 0:03:18\n",
      "epoch [5/200] batch [1/2] time 0.763 (0.763) data 0.513 (0.513) loss 9.3831 (9.3831) acc 12.5000 (12.5000) lr 1.9989e+01 eta 0:04:58\n",
      "epoch [5/200] batch [2/2] time 0.251 (0.507) data 0.000 (0.257) loss 9.3217 (9.3524) acc 18.7500 (15.6250) lr 1.9980e+01 eta 0:03:17\n",
      "epoch [6/200] batch [1/2] time 1.024 (1.024) data 0.771 (0.771) loss 9.3034 (9.3034) acc 15.6250 (15.6250) lr 1.9980e+01 eta 0:06:38\n",
      "epoch [6/200] batch [2/2] time 0.251 (0.638) data 0.001 (0.386) loss 9.3085 (9.3059) acc 3.1250 (9.3750) lr 1.9969e+01 eta 0:04:07\n",
      "epoch [7/200] batch [1/2] time 1.150 (1.150) data 0.898 (0.898) loss 9.1732 (9.1732) acc 3.1250 (3.1250) lr 1.9969e+01 eta 0:07:24\n",
      "epoch [7/200] batch [2/2] time 0.256 (0.703) data 0.001 (0.449) loss 9.1898 (9.1815) acc 9.3750 (6.2500) lr 1.9956e+01 eta 0:04:31\n",
      "epoch [8/200] batch [1/2] time 1.084 (1.084) data 0.832 (0.832) loss 9.0626 (9.0626) acc 12.5000 (12.5000) lr 1.9956e+01 eta 0:06:57\n",
      "epoch [8/200] batch [2/2] time 0.263 (0.673) data 0.000 (0.416) loss 8.9684 (9.0155) acc 25.0000 (18.7500) lr 1.9940e+01 eta 0:04:18\n",
      "epoch [9/200] batch [1/2] time 0.762 (0.762) data 0.508 (0.508) loss 8.9460 (8.9460) acc 25.0000 (25.0000) lr 1.9940e+01 eta 0:04:51\n",
      "epoch [9/200] batch [2/2] time 0.251 (0.507) data 0.000 (0.254) loss 8.9400 (8.9430) acc 28.1250 (26.5625) lr 1.9921e+01 eta 0:03:13\n",
      "epoch [10/200] batch [1/2] time 0.777 (0.777) data 0.526 (0.526) loss 8.7706 (8.7706) acc 18.7500 (18.7500) lr 1.9921e+01 eta 0:04:55\n",
      "epoch [10/200] batch [2/2] time 0.251 (0.514) data 0.000 (0.263) loss 8.9327 (8.8516) acc 6.2500 (12.5000) lr 1.9900e+01 eta 0:03:15\n",
      "epoch [11/200] batch [1/2] time 0.768 (0.768) data 0.520 (0.520) loss 8.6189 (8.6189) acc 18.7500 (18.7500) lr 1.9900e+01 eta 0:04:51\n",
      "epoch [11/200] batch [2/2] time 0.253 (0.511) data 0.000 (0.260) loss 8.6364 (8.6277) acc 12.5000 (15.6250) lr 1.9877e+01 eta 0:03:13\n",
      "epoch [12/200] batch [1/2] time 0.764 (0.764) data 0.515 (0.515) loss 8.4098 (8.4098) acc 34.3750 (34.3750) lr 1.9877e+01 eta 0:04:48\n",
      "epoch [12/200] batch [2/2] time 0.253 (0.509) data 0.000 (0.258) loss 8.4828 (8.4463) acc 25.0000 (29.6875) lr 1.9851e+01 eta 0:03:11\n",
      "epoch [13/200] batch [1/2] time 0.772 (0.772) data 0.516 (0.516) loss 8.2485 (8.2485) acc 18.7500 (18.7500) lr 1.9851e+01 eta 0:04:49\n",
      "epoch [13/200] batch [2/2] time 0.255 (0.513) data 0.000 (0.258) loss 8.1121 (8.1803) acc 25.0000 (21.8750) lr 1.9823e+01 eta 0:03:12\n",
      "epoch [14/200] batch [1/2] time 0.773 (0.773) data 0.523 (0.523) loss 7.9446 (7.9446) acc 21.8750 (21.8750) lr 1.9823e+01 eta 0:04:48\n",
      "epoch [14/200] batch [2/2] time 0.253 (0.513) data 0.000 (0.262) loss 7.8115 (7.8780) acc 15.6250 (18.7500) lr 1.9792e+01 eta 0:03:10\n",
      "epoch [15/200] batch [1/2] time 0.770 (0.770) data 0.520 (0.520) loss 7.5362 (7.5362) acc 28.1250 (28.1250) lr 1.9792e+01 eta 0:04:45\n",
      "epoch [15/200] batch [2/2] time 0.253 (0.512) data 0.000 (0.260) loss 7.4195 (7.4779) acc 37.5000 (32.8125) lr 1.9759e+01 eta 0:03:09\n",
      "epoch [16/200] batch [1/2] time 0.802 (0.802) data 0.551 (0.551) loss 6.8373 (6.8373) acc 25.0000 (25.0000) lr 1.9759e+01 eta 0:04:55\n",
      "epoch [16/200] batch [2/2] time 0.257 (0.530) data 0.001 (0.276) loss 6.5070 (6.6721) acc 46.8750 (35.9375) lr 1.9724e+01 eta 0:03:14\n",
      "epoch [17/200] batch [1/2] time 1.114 (1.114) data 0.859 (0.859) loss 6.3144 (6.3144) acc 50.0000 (50.0000) lr 1.9724e+01 eta 0:06:48\n",
      "epoch [17/200] batch [2/2] time 0.256 (0.685) data 0.001 (0.430) loss 6.2495 (6.2819) acc 28.1250 (39.0625) lr 1.9686e+01 eta 0:04:10\n",
      "epoch [18/200] batch [1/2] time 1.065 (1.065) data 0.800 (0.800) loss 6.7625 (6.7625) acc 12.5000 (12.5000) lr 1.9686e+01 eta 0:06:28\n",
      "epoch [18/200] batch [2/2] time 0.258 (0.662) data 0.001 (0.400) loss 6.4693 (6.6159) acc 12.5000 (12.5000) lr 1.9646e+01 eta 0:04:00\n",
      "epoch [19/200] batch [1/2] time 0.897 (0.897) data 0.643 (0.643) loss 5.7286 (5.7286) acc 12.5000 (12.5000) lr 1.9646e+01 eta 0:05:25\n",
      "epoch [19/200] batch [2/2] time 0.255 (0.576) data 0.001 (0.322) loss 4.8272 (5.2779) acc 9.3750 (10.9375) lr 1.9603e+01 eta 0:03:28\n",
      "epoch [20/200] batch [1/2] time 0.760 (0.760) data 0.508 (0.508) loss 10.4257 (10.4257) acc 15.6250 (15.6250) lr 1.9603e+01 eta 0:04:34\n",
      "epoch [20/200] batch [2/2] time 0.254 (0.507) data 0.000 (0.254) loss 7.1500 (8.7878) acc 6.2500 (10.9375) lr 1.9558e+01 eta 0:03:02\n",
      "epoch [21/200] batch [1/2] time 0.780 (0.780) data 0.530 (0.530) loss 5.6684 (5.6684) acc 0.0000 (0.0000) lr 1.9558e+01 eta 0:04:40\n",
      "epoch [21/200] batch [2/2] time 0.253 (0.517) data 0.000 (0.265) loss 5.3232 (5.4958) acc 6.2500 (3.1250) lr 1.9511e+01 eta 0:03:05\n",
      "epoch [22/200] batch [1/2] time 0.769 (0.769) data 0.518 (0.518) loss 5.0651 (5.0651) acc 9.3750 (9.3750) lr 1.9511e+01 eta 0:04:34\n",
      "epoch [22/200] batch [2/2] time 0.254 (0.512) data 0.000 (0.259) loss 4.8135 (4.9393) acc 6.2500 (7.8125) lr 1.9461e+01 eta 0:03:02\n",
      "epoch [23/200] batch [1/2] time 0.772 (0.772) data 0.523 (0.523) loss 4.6881 (4.6881) acc 18.7500 (18.7500) lr 1.9461e+01 eta 0:04:34\n",
      "epoch [23/200] batch [2/2] time 0.254 (0.513) data 0.000 (0.262) loss 4.5026 (4.5954) acc 9.3750 (14.0625) lr 1.9409e+01 eta 0:03:01\n",
      "epoch [24/200] batch [1/2] time 0.777 (0.777) data 0.523 (0.523) loss 4.3877 (4.3877) acc 15.6250 (15.6250) lr 1.9409e+01 eta 0:04:34\n",
      "epoch [24/200] batch [2/2] time 0.255 (0.516) data 0.001 (0.262) loss 4.2753 (4.3315) acc 9.3750 (12.5000) lr 1.9354e+01 eta 0:03:01\n",
      "epoch [25/200] batch [1/2] time 0.782 (0.782) data 0.530 (0.530) loss 4.0289 (4.0289) acc 34.3750 (34.3750) lr 1.9354e+01 eta 0:04:34\n",
      "epoch [25/200] batch [2/2] time 0.255 (0.518) data 0.001 (0.265) loss 3.9770 (4.0030) acc 34.3750 (34.3750) lr 1.9298e+01 eta 0:03:01\n",
      "epoch [26/200] batch [1/2] time 0.771 (0.771) data 0.519 (0.519) loss 3.8690 (3.8690) acc 28.1250 (28.1250) lr 1.9298e+01 eta 0:04:29\n",
      "epoch [26/200] batch [2/2] time 0.275 (0.523) data 0.001 (0.260) loss 3.9023 (3.8856) acc 15.6250 (21.8750) lr 1.9239e+01 eta 0:03:02\n",
      "epoch [27/200] batch [1/2] time 0.762 (0.762) data 0.508 (0.508) loss 3.7737 (3.7737) acc 43.7500 (43.7500) lr 1.9239e+01 eta 0:04:24\n",
      "epoch [27/200] batch [2/2] time 0.258 (0.510) data 0.000 (0.254) loss 3.7514 (3.7625) acc 18.7500 (31.2500) lr 1.9178e+01 eta 0:02:56\n",
      "epoch [28/200] batch [1/2] time 1.136 (1.136) data 0.880 (0.880) loss 3.2923 (3.2923) acc 37.5000 (37.5000) lr 1.9178e+01 eta 0:06:32\n",
      "epoch [28/200] batch [2/2] time 0.255 (0.696) data 0.001 (0.440) loss 3.8301 (3.5612) acc 21.8750 (29.6875) lr 1.9114e+01 eta 0:03:59\n",
      "epoch [29/200] batch [1/2] time 1.129 (1.129) data 0.874 (0.874) loss 3.9358 (3.9358) acc 21.8750 (21.8750) lr 1.9114e+01 eta 0:06:27\n",
      "epoch [29/200] batch [2/2] time 0.257 (0.693) data 0.001 (0.437) loss 3.7796 (3.8577) acc 9.3750 (15.6250) lr 1.9048e+01 eta 0:03:57\n",
      "epoch [30/200] batch [1/2] time 0.886 (0.886) data 0.634 (0.634) loss 3.5865 (3.5865) acc 15.6250 (15.6250) lr 1.9048e+01 eta 0:05:02\n",
      "epoch [30/200] batch [2/2] time 0.253 (0.569) data 0.001 (0.317) loss 3.3050 (3.4457) acc 34.3750 (25.0000) lr 1.8980e+01 eta 0:03:13\n",
      "epoch [31/200] batch [1/2] time 0.770 (0.770) data 0.514 (0.514) loss 3.3279 (3.3279) acc 28.1250 (28.1250) lr 1.8980e+01 eta 0:04:21\n",
      "epoch [31/200] batch [2/2] time 0.254 (0.512) data 0.000 (0.257) loss 3.3167 (3.3223) acc 15.6250 (21.8750) lr 1.8910e+01 eta 0:02:53\n",
      "epoch [32/200] batch [1/2] time 0.767 (0.767) data 0.513 (0.513) loss 3.3236 (3.3236) acc 18.7500 (18.7500) lr 1.8910e+01 eta 0:04:18\n",
      "epoch [32/200] batch [2/2] time 0.257 (0.512) data 0.000 (0.257) loss 3.1547 (3.2391) acc 34.3750 (26.5625) lr 1.8838e+01 eta 0:02:52\n",
      "epoch [33/200] batch [1/2] time 0.762 (0.762) data 0.509 (0.509) loss 2.8432 (2.8432) acc 50.0000 (50.0000) lr 1.8838e+01 eta 0:04:15\n",
      "epoch [33/200] batch [2/2] time 0.257 (0.510) data 0.000 (0.255) loss 2.8993 (2.8713) acc 25.0000 (37.5000) lr 1.8763e+01 eta 0:02:50\n",
      "epoch [34/200] batch [1/2] time 0.795 (0.795) data 0.542 (0.542) loss 2.6369 (2.6369) acc 31.2500 (31.2500) lr 1.8763e+01 eta 0:04:24\n",
      "epoch [34/200] batch [2/2] time 0.256 (0.526) data 0.000 (0.271) loss 2.5859 (2.6114) acc 25.0000 (28.1250) lr 1.8686e+01 eta 0:02:54\n",
      "epoch [35/200] batch [1/2] time 0.877 (0.877) data 0.623 (0.623) loss 2.3890 (2.3890) acc 34.3750 (34.3750) lr 1.8686e+01 eta 0:04:50\n",
      "epoch [35/200] batch [2/2] time 0.257 (0.567) data 0.000 (0.312) loss 2.9047 (2.6468) acc 25.0000 (29.6875) lr 1.8607e+01 eta 0:03:07\n",
      "epoch [36/200] batch [1/2] time 0.766 (0.766) data 0.509 (0.509) loss 3.4386 (3.4386) acc 28.1250 (28.1250) lr 1.8607e+01 eta 0:04:12\n",
      "epoch [36/200] batch [2/2] time 0.258 (0.512) data 0.001 (0.255) loss 8.4579 (5.9482) acc 18.7500 (23.4375) lr 1.8526e+01 eta 0:02:47\n",
      "epoch [37/200] batch [1/2] time 0.766 (0.766) data 0.509 (0.509) loss 8.9529 (8.9529) acc 6.2500 (6.2500) lr 1.8526e+01 eta 0:04:10\n",
      "epoch [37/200] batch [2/2] time 0.254 (0.510) data 0.000 (0.255) loss 10.3769 (9.6649) acc 12.5000 (9.3750) lr 1.8443e+01 eta 0:02:46\n",
      "epoch [38/200] batch [1/2] time 0.772 (0.772) data 0.513 (0.513) loss 10.6537 (10.6537) acc 12.5000 (12.5000) lr 1.8443e+01 eta 0:04:10\n",
      "epoch [38/200] batch [2/2] time 0.255 (0.514) data 0.001 (0.257) loss 9.0645 (9.8591) acc 15.6250 (14.0625) lr 1.8358e+01 eta 0:02:46\n",
      "epoch [39/200] batch [1/2] time 1.124 (1.124) data 0.867 (0.867) loss 7.9019 (7.9019) acc 6.2500 (6.2500) lr 1.8358e+01 eta 0:06:02\n",
      "epoch [39/200] batch [2/2] time 0.255 (0.689) data 0.001 (0.434) loss 5.8747 (6.8883) acc 0.0000 (3.1250) lr 1.8271e+01 eta 0:03:42\n",
      "epoch [40/200] batch [1/2] time 1.162 (1.162) data 0.902 (0.902) loss 4.9769 (4.9769) acc 9.3750 (9.3750) lr 1.8271e+01 eta 0:06:13\n",
      "epoch [40/200] batch [2/2] time 0.259 (0.710) data 0.001 (0.451) loss 4.2942 (4.6355) acc 9.3750 (9.3750) lr 1.8181e+01 eta 0:03:47\n",
      "epoch [41/200] batch [1/2] time 0.823 (0.823) data 0.570 (0.570) loss 3.9258 (3.9258) acc 15.6250 (15.6250) lr 1.8181e+01 eta 0:04:22\n",
      "epoch [41/200] batch [2/2] time 0.254 (0.539) data 0.000 (0.285) loss 3.8035 (3.8647) acc 15.6250 (15.6250) lr 1.8090e+01 eta 0:02:51\n",
      "epoch [42/200] batch [1/2] time 0.773 (0.773) data 0.516 (0.516) loss 3.6529 (3.6529) acc 9.3750 (9.3750) lr 1.8090e+01 eta 0:04:05\n",
      "epoch [42/200] batch [2/2] time 0.258 (0.516) data 0.000 (0.258) loss 3.5812 (3.6171) acc 9.3750 (9.3750) lr 1.7997e+01 eta 0:02:42\n",
      "epoch [43/200] batch [1/2] time 0.780 (0.780) data 0.522 (0.522) loss 3.4411 (3.4411) acc 25.0000 (25.0000) lr 1.7997e+01 eta 0:04:05\n",
      "epoch [43/200] batch [2/2] time 0.257 (0.518) data 0.001 (0.261) loss 3.4715 (3.4563) acc 9.3750 (17.1875) lr 1.7902e+01 eta 0:02:42\n",
      "epoch [44/200] batch [1/2] time 0.783 (0.783) data 0.528 (0.528) loss 3.1344 (3.1344) acc 31.2500 (31.2500) lr 1.7902e+01 eta 0:04:05\n",
      "epoch [44/200] batch [2/2] time 0.255 (0.519) data 0.001 (0.264) loss 3.2657 (3.2001) acc 21.8750 (26.5625) lr 1.7804e+01 eta 0:02:41\n",
      "epoch [45/200] batch [1/2] time 0.765 (0.765) data 0.510 (0.510) loss 3.0685 (3.0685) acc 18.7500 (18.7500) lr 1.7804e+01 eta 0:03:57\n",
      "epoch [45/200] batch [2/2] time 0.257 (0.511) data 0.001 (0.255) loss 3.2026 (3.1355) acc 9.3750 (14.0625) lr 1.7705e+01 eta 0:02:38\n",
      "epoch [46/200] batch [1/2] time 0.757 (0.757) data 0.504 (0.504) loss 2.9793 (2.9793) acc 6.2500 (6.2500) lr 1.7705e+01 eta 0:03:54\n",
      "epoch [46/200] batch [2/2] time 0.256 (0.507) data 0.000 (0.252) loss 3.0007 (2.9900) acc 9.3750 (7.8125) lr 1.7604e+01 eta 0:02:36\n",
      "epoch [47/200] batch [1/2] time 0.782 (0.782) data 0.526 (0.526) loss 3.0631 (3.0631) acc 9.3750 (9.3750) lr 1.7604e+01 eta 0:04:00\n",
      "epoch [47/200] batch [2/2] time 0.255 (0.519) data 0.000 (0.263) loss 3.0738 (3.0684) acc 15.6250 (12.5000) lr 1.7501e+01 eta 0:02:38\n",
      "epoch [48/200] batch [1/2] time 0.777 (0.777) data 0.521 (0.521) loss 2.7762 (2.7762) acc 25.0000 (25.0000) lr 1.7501e+01 eta 0:03:56\n",
      "epoch [48/200] batch [2/2] time 0.257 (0.517) data 0.000 (0.261) loss 2.8706 (2.8234) acc 9.3750 (17.1875) lr 1.7396e+01 eta 0:02:37\n",
      "epoch [49/200] batch [1/2] time 0.778 (0.778) data 0.518 (0.518) loss 2.6348 (2.6348) acc 28.1250 (28.1250) lr 1.7396e+01 eta 0:03:55\n",
      "epoch [49/200] batch [2/2] time 0.259 (0.519) data 0.001 (0.260) loss 2.9515 (2.7931) acc 3.1250 (15.6250) lr 1.7290e+01 eta 0:02:36\n",
      "epoch [50/200] batch [1/2] time 1.167 (1.167) data 0.910 (0.910) loss 2.7468 (2.7468) acc 34.3750 (34.3750) lr 1.7290e+01 eta 0:05:51\n",
      "epoch [50/200] batch [2/2] time 0.257 (0.712) data 0.001 (0.455) loss 3.2587 (3.0027) acc 28.1250 (31.2500) lr 1.7181e+01 eta 0:03:33\n",
      "epoch [51/200] batch [1/2] time 1.198 (1.198) data 0.939 (0.939) loss 2.7637 (2.7637) acc 18.7500 (18.7500) lr 1.7181e+01 eta 0:05:58\n",
      "epoch [51/200] batch [2/2] time 0.267 (0.733) data 0.001 (0.470) loss 3.3266 (3.0451) acc 9.3750 (14.0625) lr 1.7071e+01 eta 0:03:38\n",
      "epoch [52/200] batch [1/2] time 0.856 (0.856) data 0.597 (0.597) loss 3.2432 (3.2432) acc 15.6250 (15.6250) lr 1.7071e+01 eta 0:04:14\n",
      "epoch [52/200] batch [2/2] time 0.258 (0.557) data 0.000 (0.299) loss 3.2679 (3.2555) acc 25.0000 (20.3125) lr 1.6959e+01 eta 0:02:44\n",
      "epoch [53/200] batch [1/2] time 0.783 (0.783) data 0.529 (0.529) loss 3.3032 (3.3032) acc 12.5000 (12.5000) lr 1.6959e+01 eta 0:03:51\n",
      "epoch [53/200] batch [2/2] time 0.259 (0.521) data 0.001 (0.265) loss 4.0383 (3.6707) acc 9.3750 (10.9375) lr 1.6845e+01 eta 0:02:33\n",
      "epoch [54/200] batch [1/2] time 0.783 (0.783) data 0.527 (0.527) loss 3.5856 (3.5856) acc 3.1250 (3.1250) lr 1.6845e+01 eta 0:03:49\n",
      "epoch [54/200] batch [2/2] time 0.256 (0.520) data 0.001 (0.264) loss 4.2289 (3.9072) acc 12.5000 (7.8125) lr 1.6730e+01 eta 0:02:31\n",
      "epoch [55/200] batch [1/2] time 0.812 (0.812) data 0.557 (0.557) loss 4.6534 (4.6534) acc 3.1250 (3.1250) lr 1.6730e+01 eta 0:03:56\n",
      "epoch [55/200] batch [2/2] time 0.257 (0.535) data 0.001 (0.279) loss 4.5093 (4.5814) acc 6.2500 (4.6875) lr 1.6613e+01 eta 0:02:35\n",
      "epoch [56/200] batch [1/2] time 0.801 (0.801) data 0.545 (0.545) loss 3.4885 (3.4885) acc 15.6250 (15.6250) lr 1.6613e+01 eta 0:03:51\n",
      "epoch [56/200] batch [2/2] time 0.259 (0.530) data 0.001 (0.273) loss 3.4712 (3.4799) acc 3.1250 (9.3750) lr 1.6494e+01 eta 0:02:32\n",
      "epoch [57/200] batch [1/2] time 0.802 (0.802) data 0.546 (0.546) loss 3.3722 (3.3722) acc 21.8750 (21.8750) lr 1.6494e+01 eta 0:03:50\n",
      "epoch [57/200] batch [2/2] time 0.259 (0.531) data 0.000 (0.273) loss 10.0684 (6.7203) acc 15.6250 (18.7500) lr 1.6374e+01 eta 0:02:31\n",
      "epoch [58/200] batch [1/2] time 0.770 (0.770) data 0.515 (0.515) loss 3.4705 (3.4705) acc 6.2500 (6.2500) lr 1.6374e+01 eta 0:03:39\n",
      "epoch [58/200] batch [2/2] time 0.259 (0.514) data 0.000 (0.258) loss 4.2730 (3.8717) acc 12.5000 (9.3750) lr 1.6252e+01 eta 0:02:26\n",
      "epoch [59/200] batch [1/2] time 0.805 (0.805) data 0.550 (0.550) loss 3.9451 (3.9451) acc 18.7500 (18.7500) lr 1.6252e+01 eta 0:03:47\n",
      "epoch [59/200] batch [2/2] time 0.259 (0.532) data 0.001 (0.275) loss 4.4402 (4.1926) acc 9.3750 (14.0625) lr 1.6129e+01 eta 0:02:30\n",
      "epoch [60/200] batch [1/2] time 0.924 (0.924) data 0.666 (0.666) loss 2.9644 (2.9644) acc 28.1250 (28.1250) lr 1.6129e+01 eta 0:04:19\n",
      "epoch [60/200] batch [2/2] time 0.263 (0.593) data 0.001 (0.333) loss 2.9970 (2.9807) acc 18.7500 (23.4375) lr 1.6004e+01 eta 0:02:46\n",
      "epoch [61/200] batch [1/2] time 1.134 (1.134) data 0.878 (0.878) loss 2.7701 (2.7701) acc 34.3750 (34.3750) lr 1.6004e+01 eta 0:05:16\n",
      "epoch [61/200] batch [2/2] time 0.262 (0.698) data 0.001 (0.439) loss 2.5972 (2.6836) acc 21.8750 (28.1250) lr 1.5878e+01 eta 0:03:14\n",
      "epoch [62/200] batch [1/2] time 1.135 (1.135) data 0.864 (0.864) loss 3.0688 (3.0688) acc 28.1250 (28.1250) lr 1.5878e+01 eta 0:05:14\n",
      "epoch [62/200] batch [2/2] time 0.262 (0.699) data 0.001 (0.432) loss 3.3753 (3.2221) acc 25.0000 (26.5625) lr 1.5750e+01 eta 0:03:12\n",
      "epoch [63/200] batch [1/2] time 0.855 (0.855) data 0.598 (0.598) loss 3.0826 (3.0826) acc 12.5000 (12.5000) lr 1.5750e+01 eta 0:03:55\n",
      "epoch [63/200] batch [2/2] time 0.258 (0.557) data 0.000 (0.299) loss 3.0495 (3.0660) acc 12.5000 (12.5000) lr 1.5621e+01 eta 0:02:32\n",
      "epoch [64/200] batch [1/2] time 0.788 (0.788) data 0.530 (0.530) loss 3.3665 (3.3665) acc 12.5000 (12.5000) lr 1.5621e+01 eta 0:03:35\n",
      "epoch [64/200] batch [2/2] time 0.259 (0.524) data 0.001 (0.265) loss 3.1008 (3.2336) acc 9.3750 (10.9375) lr 1.5490e+01 eta 0:02:22\n",
      "epoch [65/200] batch [1/2] time 0.814 (0.814) data 0.559 (0.559) loss 3.1881 (3.1881) acc 3.1250 (3.1250) lr 1.5490e+01 eta 0:03:40\n",
      "epoch [65/200] batch [2/2] time 0.257 (0.535) data 0.001 (0.280) loss 3.3195 (3.2538) acc 15.6250 (9.3750) lr 1.5358e+01 eta 0:02:24\n",
      "epoch [66/200] batch [1/2] time 0.831 (0.831) data 0.576 (0.576) loss 3.0258 (3.0258) acc 18.7500 (18.7500) lr 1.5358e+01 eta 0:03:43\n",
      "epoch [66/200] batch [2/2] time 0.261 (0.546) data 0.000 (0.288) loss 3.2159 (3.1209) acc 6.2500 (12.5000) lr 1.5225e+01 eta 0:02:26\n",
      "epoch [67/200] batch [1/2] time 0.767 (0.767) data 0.509 (0.509) loss 2.9680 (2.9680) acc 18.7500 (18.7500) lr 1.5225e+01 eta 0:03:24\n",
      "epoch [67/200] batch [2/2] time 0.259 (0.513) data 0.001 (0.255) loss 2.9731 (2.9706) acc 12.5000 (15.6250) lr 1.5090e+01 eta 0:02:16\n",
      "epoch [68/200] batch [1/2] time 0.795 (0.795) data 0.538 (0.538) loss 2.9357 (2.9357) acc 15.6250 (15.6250) lr 1.5090e+01 eta 0:03:30\n",
      "epoch [68/200] batch [2/2] time 0.259 (0.527) data 0.001 (0.270) loss 3.0498 (2.9927) acc 6.2500 (10.9375) lr 1.4955e+01 eta 0:02:19\n",
      "epoch [69/200] batch [1/2] time 0.791 (0.791) data 0.531 (0.531) loss 2.8743 (2.8743) acc 9.3750 (9.3750) lr 1.4955e+01 eta 0:03:27\n",
      "epoch [69/200] batch [2/2] time 0.260 (0.525) data 0.001 (0.266) loss 3.0090 (2.9416) acc 18.7500 (14.0625) lr 1.4818e+01 eta 0:02:17\n",
      "epoch [70/200] batch [1/2] time 0.802 (0.802) data 0.545 (0.545) loss 3.0351 (3.0351) acc 12.5000 (12.5000) lr 1.4818e+01 eta 0:03:29\n",
      "epoch [70/200] batch [2/2] time 0.260 (0.531) data 0.001 (0.273) loss 2.9303 (2.9827) acc 12.5000 (12.5000) lr 1.4679e+01 eta 0:02:17\n",
      "epoch [71/200] batch [1/2] time 0.878 (0.878) data 0.618 (0.618) loss 2.7252 (2.7252) acc 15.6250 (15.6250) lr 1.4679e+01 eta 0:03:47\n",
      "epoch [71/200] batch [2/2] time 0.261 (0.569) data 0.001 (0.309) loss 4.3762 (3.5507) acc 6.2500 (10.9375) lr 1.4540e+01 eta 0:02:26\n",
      "epoch [72/200] batch [1/2] time 1.108 (1.108) data 0.850 (0.850) loss 3.0630 (3.0630) acc 12.5000 (12.5000) lr 1.4540e+01 eta 0:04:44\n",
      "epoch [72/200] batch [2/2] time 0.262 (0.685) data 0.001 (0.425) loss 3.3357 (3.1993) acc 9.3750 (10.9375) lr 1.4399e+01 eta 0:02:55\n",
      "epoch [73/200] batch [1/2] time 1.211 (1.211) data 0.951 (0.951) loss 3.2106 (3.2106) acc 15.6250 (15.6250) lr 1.4399e+01 eta 0:05:08\n",
      "epoch [73/200] batch [2/2] time 0.262 (0.736) data 0.001 (0.476) loss 3.0484 (3.1295) acc 12.5000 (14.0625) lr 1.4258e+01 eta 0:03:07\n",
      "epoch [74/200] batch [1/2] time 0.837 (0.837) data 0.578 (0.578) loss 2.9831 (2.9831) acc 12.5000 (12.5000) lr 1.4258e+01 eta 0:03:31\n",
      "epoch [74/200] batch [2/2] time 0.260 (0.549) data 0.001 (0.289) loss 2.8238 (2.9035) acc 21.8750 (17.1875) lr 1.4115e+01 eta 0:02:18\n",
      "epoch [75/200] batch [1/2] time 0.813 (0.813) data 0.555 (0.555) loss 2.8564 (2.8564) acc 12.5000 (12.5000) lr 1.4115e+01 eta 0:03:23\n",
      "epoch [75/200] batch [2/2] time 0.260 (0.536) data 0.001 (0.278) loss 2.8179 (2.8372) acc 31.2500 (21.8750) lr 1.3971e+01 eta 0:02:14\n",
      "epoch [76/200] batch [1/2] time 0.787 (0.787) data 0.528 (0.528) loss 3.3049 (3.3049) acc 31.2500 (31.2500) lr 1.3971e+01 eta 0:03:16\n",
      "epoch [76/200] batch [2/2] time 0.259 (0.523) data 0.000 (0.264) loss 2.8531 (3.0790) acc 18.7500 (25.0000) lr 1.3827e+01 eta 0:02:09\n",
      "epoch [77/200] batch [1/2] time 0.800 (0.800) data 0.543 (0.543) loss 2.8600 (2.8600) acc 31.2500 (31.2500) lr 1.3827e+01 eta 0:03:17\n",
      "epoch [77/200] batch [2/2] time 0.262 (0.531) data 0.000 (0.272) loss 3.3414 (3.1007) acc 12.5000 (21.8750) lr 1.3681e+01 eta 0:02:10\n",
      "epoch [78/200] batch [1/2] time 0.814 (0.814) data 0.555 (0.555) loss 4.2851 (4.2851) acc 15.6250 (15.6250) lr 1.3681e+01 eta 0:03:19\n",
      "epoch [78/200] batch [2/2] time 0.260 (0.537) data 0.000 (0.278) loss 3.8972 (4.0911) acc 6.2500 (10.9375) lr 1.3535e+01 eta 0:02:10\n",
      "epoch [79/200] batch [1/2] time 0.820 (0.820) data 0.558 (0.558) loss 3.9138 (3.9138) acc 15.6250 (15.6250) lr 1.3535e+01 eta 0:03:19\n",
      "epoch [79/200] batch [2/2] time 0.260 (0.540) data 0.001 (0.279) loss 3.8648 (3.8893) acc 6.2500 (10.9375) lr 1.3387e+01 eta 0:02:10\n",
      "epoch [80/200] batch [1/2] time 0.793 (0.793) data 0.534 (0.534) loss 3.6089 (3.6089) acc 12.5000 (12.5000) lr 1.3387e+01 eta 0:03:11\n",
      "epoch [80/200] batch [2/2] time 0.260 (0.527) data 0.001 (0.267) loss 3.3860 (3.4975) acc 0.0000 (6.2500) lr 1.3239e+01 eta 0:02:06\n",
      "epoch [81/200] batch [1/2] time 0.772 (0.772) data 0.514 (0.514) loss 3.3434 (3.3434) acc 9.3750 (9.3750) lr 1.3239e+01 eta 0:03:04\n",
      "epoch [81/200] batch [2/2] time 0.264 (0.518) data 0.001 (0.257) loss 3.2319 (3.2877) acc 6.2500 (7.8125) lr 1.3090e+01 eta 0:02:03\n",
      "epoch [82/200] batch [1/2] time 0.929 (0.929) data 0.652 (0.652) loss 3.1356 (3.1356) acc 21.8750 (21.8750) lr 1.3090e+01 eta 0:03:40\n",
      "epoch [82/200] batch [2/2] time 0.265 (0.597) data 0.001 (0.326) loss 3.0245 (3.0801) acc 34.3750 (28.1250) lr 1.2940e+01 eta 0:02:20\n",
      "epoch [83/200] batch [1/2] time 1.120 (1.120) data 0.861 (0.861) loss 2.9623 (2.9623) acc 21.8750 (21.8750) lr 1.2940e+01 eta 0:04:23\n",
      "epoch [83/200] batch [2/2] time 0.267 (0.694) data 0.001 (0.431) loss 2.9156 (2.9389) acc 9.3750 (15.6250) lr 1.2790e+01 eta 0:02:42\n",
      "epoch [84/200] batch [1/2] time 1.133 (1.133) data 0.860 (0.860) loss 2.8399 (2.8399) acc 21.8750 (21.8750) lr 1.2790e+01 eta 0:04:23\n",
      "epoch [84/200] batch [2/2] time 0.270 (0.701) data 0.001 (0.430) loss 2.7279 (2.7839) acc 21.8750 (21.8750) lr 1.2639e+01 eta 0:02:42\n",
      "epoch [85/200] batch [1/2] time 0.881 (0.881) data 0.619 (0.619) loss 2.7595 (2.7595) acc 18.7500 (18.7500) lr 1.2639e+01 eta 0:03:23\n",
      "epoch [85/200] batch [2/2] time 0.262 (0.572) data 0.000 (0.310) loss 2.6047 (2.6821) acc 25.0000 (21.8750) lr 1.2487e+01 eta 0:02:11\n",
      "epoch [86/200] batch [1/2] time 0.898 (0.898) data 0.637 (0.637) loss 2.8774 (2.8774) acc 21.8750 (21.8750) lr 1.2487e+01 eta 0:03:25\n",
      "epoch [86/200] batch [2/2] time 0.262 (0.580) data 0.001 (0.319) loss 2.8160 (2.8467) acc 21.8750 (21.8750) lr 1.2334e+01 eta 0:02:12\n",
      "epoch [87/200] batch [1/2] time 0.806 (0.806) data 0.545 (0.545) loss 2.6323 (2.6323) acc 34.3750 (34.3750) lr 1.2334e+01 eta 0:03:02\n",
      "epoch [87/200] batch [2/2] time 0.261 (0.534) data 0.001 (0.273) loss 2.6796 (2.6559) acc 25.0000 (29.6875) lr 1.2181e+01 eta 0:02:00\n",
      "epoch [88/200] batch [1/2] time 0.800 (0.800) data 0.541 (0.541) loss 2.2846 (2.2846) acc 43.7500 (43.7500) lr 1.2181e+01 eta 0:03:00\n",
      "epoch [88/200] batch [2/2] time 0.264 (0.532) data 0.000 (0.271) loss 2.1996 (2.2421) acc 37.5000 (40.6250) lr 1.2028e+01 eta 0:01:59\n",
      "epoch [89/200] batch [1/2] time 0.789 (0.789) data 0.526 (0.526) loss 2.4025 (2.4025) acc 25.0000 (25.0000) lr 1.2028e+01 eta 0:02:56\n",
      "epoch [89/200] batch [2/2] time 0.263 (0.526) data 0.001 (0.263) loss 3.3084 (2.8554) acc 15.6250 (20.3125) lr 1.1874e+01 eta 0:01:56\n",
      "epoch [90/200] batch [1/2] time 0.820 (0.820) data 0.559 (0.559) loss 2.8500 (2.8500) acc 15.6250 (15.6250) lr 1.1874e+01 eta 0:03:01\n",
      "epoch [90/200] batch [2/2] time 0.261 (0.540) data 0.001 (0.280) loss 4.2363 (3.5431) acc 9.3750 (12.5000) lr 1.1719e+01 eta 0:01:58\n",
      "epoch [91/200] batch [1/2] time 0.798 (0.798) data 0.539 (0.539) loss 3.3212 (3.3212) acc 21.8750 (21.8750) lr 1.1719e+01 eta 0:02:54\n",
      "epoch [91/200] batch [2/2] time 0.264 (0.531) data 0.001 (0.270) loss 3.3103 (3.3157) acc 12.5000 (17.1875) lr 1.1564e+01 eta 0:01:55\n",
      "epoch [92/200] batch [1/2] time 0.806 (0.806) data 0.545 (0.545) loss 3.3003 (3.3003) acc 12.5000 (12.5000) lr 1.1564e+01 eta 0:02:54\n",
      "epoch [92/200] batch [2/2] time 0.264 (0.535) data 0.001 (0.273) loss 3.2210 (3.2607) acc 18.7500 (15.6250) lr 1.1409e+01 eta 0:01:55\n",
      "epoch [93/200] batch [1/2] time 1.084 (1.084) data 0.821 (0.821) loss 3.2100 (3.2100) acc 9.3750 (9.3750) lr 1.1409e+01 eta 0:03:52\n",
      "epoch [93/200] batch [2/2] time 0.270 (0.677) data 0.002 (0.412) loss 3.1208 (3.1654) acc 15.6250 (12.5000) lr 1.1253e+01 eta 0:02:24\n",
      "epoch [94/200] batch [1/2] time 1.149 (1.149) data 0.887 (0.887) loss 3.0341 (3.0341) acc 28.1250 (28.1250) lr 1.1253e+01 eta 0:04:04\n",
      "epoch [94/200] batch [2/2] time 0.270 (0.709) data 0.001 (0.444) loss 3.2473 (3.1407) acc 9.3750 (18.7500) lr 1.1097e+01 eta 0:02:30\n",
      "epoch [95/200] batch [1/2] time 1.033 (1.033) data 0.768 (0.768) loss 2.9085 (2.9085) acc 18.7500 (18.7500) lr 1.1097e+01 eta 0:03:37\n",
      "epoch [95/200] batch [2/2] time 0.265 (0.649) data 0.001 (0.384) loss 2.8225 (2.8655) acc 21.8750 (20.3125) lr 1.0941e+01 eta 0:02:16\n",
      "epoch [96/200] batch [1/2] time 0.834 (0.834) data 0.572 (0.572) loss 2.6536 (2.6536) acc 43.7500 (43.7500) lr 1.0941e+01 eta 0:02:54\n",
      "epoch [96/200] batch [2/2] time 0.268 (0.551) data 0.001 (0.286) loss 2.8421 (2.7479) acc 18.7500 (31.2500) lr 1.0785e+01 eta 0:01:54\n",
      "epoch [97/200] batch [1/2] time 0.824 (0.824) data 0.556 (0.556) loss 2.4164 (2.4164) acc 31.2500 (31.2500) lr 1.0785e+01 eta 0:02:50\n",
      "epoch [97/200] batch [2/2] time 0.263 (0.543) data 0.001 (0.278) loss 3.1240 (2.7702) acc 21.8750 (26.5625) lr 1.0628e+01 eta 0:01:51\n",
      "epoch [98/200] batch [1/2] time 0.834 (0.834) data 0.570 (0.570) loss 3.2058 (3.2058) acc 6.2500 (6.2500) lr 1.0628e+01 eta 0:02:51\n",
      "epoch [98/200] batch [2/2] time 0.266 (0.550) data 0.001 (0.285) loss 2.7658 (2.9858) acc 34.3750 (20.3125) lr 1.0471e+01 eta 0:01:52\n",
      "epoch [99/200] batch [1/2] time 0.807 (0.807) data 0.544 (0.544) loss 2.8881 (2.8881) acc 9.3750 (9.3750) lr 1.0471e+01 eta 0:02:43\n",
      "epoch [99/200] batch [2/2] time 0.265 (0.536) data 0.001 (0.272) loss 2.8042 (2.8462) acc 12.5000 (10.9375) lr 1.0314e+01 eta 0:01:48\n",
      "epoch [100/200] batch [1/2] time 0.804 (0.804) data 0.541 (0.541) loss 2.4694 (2.4694) acc 37.5000 (37.5000) lr 1.0314e+01 eta 0:02:41\n",
      "epoch [100/200] batch [2/2] time 0.265 (0.535) data 0.001 (0.271) loss 2.8193 (2.6443) acc 21.8750 (29.6875) lr 1.0157e+01 eta 0:01:46\n",
      "epoch [101/200] batch [1/2] time 0.824 (0.824) data 0.560 (0.560) loss 2.3520 (2.3520) acc 46.8750 (46.8750) lr 1.0157e+01 eta 0:02:43\n",
      "epoch [101/200] batch [2/2] time 0.272 (0.548) data 0.001 (0.280) loss 2.3813 (2.3666) acc 34.3750 (40.6250) lr 1.0000e+01 eta 0:01:48\n",
      "epoch [102/200] batch [1/2] time 0.832 (0.832) data 0.570 (0.570) loss 2.0975 (2.0975) acc 43.7500 (43.7500) lr 1.0000e+01 eta 0:02:43\n",
      "epoch [102/200] batch [2/2] time 0.265 (0.549) data 0.001 (0.285) loss 2.1112 (2.1044) acc 53.1250 (48.4375) lr 9.8429e+00 eta 0:01:47\n",
      "epoch [103/200] batch [1/2] time 0.983 (0.983) data 0.719 (0.719) loss 1.7713 (1.7713) acc 59.3750 (59.3750) lr 9.8429e+00 eta 0:03:11\n",
      "epoch [103/200] batch [2/2] time 0.269 (0.626) data 0.001 (0.360) loss 1.8898 (1.8305) acc 46.8750 (53.1250) lr 9.6859e+00 eta 0:02:01\n",
      "epoch [104/200] batch [1/2] time 1.213 (1.213) data 0.951 (0.951) loss 1.8983 (1.8983) acc 50.0000 (50.0000) lr 9.6859e+00 eta 0:03:54\n",
      "epoch [104/200] batch [2/2] time 0.270 (0.742) data 0.001 (0.476) loss 1.8279 (1.8631) acc 56.2500 (53.1250) lr 9.5289e+00 eta 0:02:22\n",
      "epoch [105/200] batch [1/2] time 1.197 (1.197) data 0.921 (0.921) loss 1.2657 (1.2657) acc 71.8750 (71.8750) lr 9.5289e+00 eta 0:03:48\n",
      "epoch [105/200] batch [2/2] time 0.266 (0.731) data 0.001 (0.461) loss 2.3759 (1.8208) acc 53.1250 (62.5000) lr 9.3721e+00 eta 0:02:18\n",
      "epoch [106/200] batch [1/2] time 0.874 (0.874) data 0.609 (0.609) loss 1.6569 (1.6569) acc 59.3750 (59.3750) lr 9.3721e+00 eta 0:02:45\n",
      "epoch [106/200] batch [2/2] time 0.266 (0.570) data 0.001 (0.305) loss 1.7348 (1.6959) acc 53.1250 (56.2500) lr 9.2154e+00 eta 0:01:47\n",
      "epoch [107/200] batch [1/2] time 0.788 (0.788) data 0.525 (0.525) loss 1.9412 (1.9412) acc 56.2500 (56.2500) lr 9.2154e+00 eta 0:02:27\n",
      "epoch [107/200] batch [2/2] time 0.266 (0.527) data 0.001 (0.263) loss 2.1938 (2.0675) acc 53.1250 (54.6875) lr 9.0589e+00 eta 0:01:37\n",
      "epoch [108/200] batch [1/2] time 0.791 (0.791) data 0.528 (0.528) loss 1.7472 (1.7472) acc 68.7500 (68.7500) lr 9.0589e+00 eta 0:02:26\n",
      "epoch [108/200] batch [2/2] time 0.267 (0.529) data 0.001 (0.264) loss 1.8650 (1.8061) acc 56.2500 (62.5000) lr 8.9027e+00 eta 0:01:37\n",
      "epoch [109/200] batch [1/2] time 0.796 (0.796) data 0.534 (0.534) loss 1.7831 (1.7831) acc 62.5000 (62.5000) lr 8.9027e+00 eta 0:02:25\n",
      "epoch [109/200] batch [2/2] time 0.265 (0.530) data 0.000 (0.267) loss 1.8498 (1.8165) acc 59.3750 (60.9375) lr 8.7467e+00 eta 0:01:36\n",
      "epoch [110/200] batch [1/2] time 0.800 (0.800) data 0.538 (0.538) loss 1.3215 (1.3215) acc 81.2500 (81.2500) lr 8.7467e+00 eta 0:02:24\n",
      "epoch [110/200] batch [2/2] time 0.264 (0.532) data 0.001 (0.269) loss 2.0650 (1.6932) acc 43.7500 (62.5000) lr 8.5910e+00 eta 0:01:35\n",
      "epoch [111/200] batch [1/2] time 0.794 (0.794) data 0.531 (0.531) loss 1.7474 (1.7474) acc 65.6250 (65.6250) lr 8.5910e+00 eta 0:02:22\n",
      "epoch [111/200] batch [2/2] time 0.264 (0.529) data 0.000 (0.266) loss 1.3384 (1.5429) acc 78.1250 (71.8750) lr 8.4357e+00 eta 0:01:34\n",
      "epoch [112/200] batch [1/2] time 0.783 (0.783) data 0.520 (0.520) loss 1.5016 (1.5016) acc 68.7500 (68.7500) lr 8.4357e+00 eta 0:02:18\n",
      "epoch [112/200] batch [2/2] time 0.265 (0.524) data 0.001 (0.260) loss 1.8581 (1.6798) acc 43.7500 (56.2500) lr 8.2807e+00 eta 0:01:32\n",
      "epoch [113/200] batch [1/2] time 0.776 (0.776) data 0.512 (0.512) loss 1.2015 (1.2015) acc 84.3750 (84.3750) lr 8.2807e+00 eta 0:02:15\n",
      "epoch [113/200] batch [2/2] time 0.267 (0.521) data 0.000 (0.256) loss 1.7896 (1.4955) acc 56.2500 (70.3125) lr 8.1262e+00 eta 0:01:30\n",
      "epoch [114/200] batch [1/2] time 0.888 (0.888) data 0.623 (0.623) loss 1.4429 (1.4429) acc 68.7500 (68.7500) lr 8.1262e+00 eta 0:02:33\n",
      "epoch [114/200] batch [2/2] time 0.267 (0.577) data 0.001 (0.312) loss 1.4524 (1.4477) acc 71.8750 (70.3125) lr 7.9721e+00 eta 0:01:39\n",
      "epoch [115/200] batch [1/2] time 1.108 (1.108) data 0.839 (0.839) loss 1.2336 (1.2336) acc 71.8750 (71.8750) lr 7.9721e+00 eta 0:03:09\n",
      "epoch [115/200] batch [2/2] time 0.267 (0.688) data 0.001 (0.420) loss 1.4279 (1.3308) acc 62.5000 (67.1875) lr 7.8186e+00 eta 0:01:56\n",
      "epoch [116/200] batch [1/2] time 1.192 (1.192) data 0.921 (0.921) loss 1.0675 (1.0675) acc 75.0000 (75.0000) lr 7.8186e+00 eta 0:03:21\n",
      "epoch [116/200] batch [2/2] time 0.273 (0.733) data 0.001 (0.461) loss 1.2513 (1.1594) acc 71.8750 (73.4375) lr 7.6655e+00 eta 0:02:03\n",
      "epoch [117/200] batch [1/2] time 0.874 (0.874) data 0.614 (0.614) loss 1.2625 (1.2625) acc 71.8750 (71.8750) lr 7.6655e+00 eta 0:02:25\n",
      "epoch [117/200] batch [2/2] time 0.265 (0.569) data 0.001 (0.307) loss 1.0844 (1.1734) acc 84.3750 (78.1250) lr 7.5131e+00 eta 0:01:34\n",
      "epoch [118/200] batch [1/2] time 0.774 (0.774) data 0.510 (0.510) loss 1.1558 (1.1558) acc 75.0000 (75.0000) lr 7.5131e+00 eta 0:02:07\n",
      "epoch [118/200] batch [2/2] time 0.265 (0.519) data 0.001 (0.255) loss 1.1743 (1.1651) acc 68.7500 (71.8750) lr 7.3613e+00 eta 0:01:25\n",
      "epoch [119/200] batch [1/2] time 0.799 (0.799) data 0.538 (0.538) loss 0.9029 (0.9029) acc 78.1250 (78.1250) lr 7.3613e+00 eta 0:02:10\n",
      "epoch [119/200] batch [2/2] time 0.266 (0.533) data 0.000 (0.269) loss 1.2285 (1.0657) acc 68.7500 (73.4375) lr 7.2101e+00 eta 0:01:26\n",
      "epoch [120/200] batch [1/2] time 0.790 (0.790) data 0.525 (0.525) loss 1.1501 (1.1501) acc 75.0000 (75.0000) lr 7.2101e+00 eta 0:02:07\n",
      "epoch [120/200] batch [2/2] time 0.265 (0.528) data 0.001 (0.263) loss 0.8599 (1.0050) acc 84.3750 (79.6875) lr 7.0596e+00 eta 0:01:24\n",
      "epoch [121/200] batch [1/2] time 0.792 (0.792) data 0.531 (0.531) loss 0.9036 (0.9036) acc 81.2500 (81.2500) lr 7.0596e+00 eta 0:02:05\n",
      "epoch [121/200] batch [2/2] time 0.267 (0.529) data 0.000 (0.266) loss 1.0952 (0.9994) acc 78.1250 (79.6875) lr 6.9098e+00 eta 0:01:23\n",
      "epoch [122/200] batch [1/2] time 0.789 (0.789) data 0.524 (0.524) loss 1.5291 (1.5291) acc 71.8750 (71.8750) lr 6.9098e+00 eta 0:02:03\n",
      "epoch [122/200] batch [2/2] time 0.269 (0.529) data 0.001 (0.262) loss 1.6004 (1.5648) acc 65.6250 (68.7500) lr 6.7608e+00 eta 0:01:22\n",
      "epoch [123/200] batch [1/2] time 0.802 (0.802) data 0.537 (0.537) loss 1.1249 (1.1249) acc 71.8750 (71.8750) lr 6.7608e+00 eta 0:02:04\n",
      "epoch [123/200] batch [2/2] time 0.267 (0.535) data 0.000 (0.269) loss 1.1118 (1.1183) acc 84.3750 (78.1250) lr 6.6126e+00 eta 0:01:22\n",
      "epoch [124/200] batch [1/2] time 0.800 (0.800) data 0.539 (0.539) loss 0.9095 (0.9095) acc 93.7500 (93.7500) lr 6.6126e+00 eta 0:02:02\n",
      "epoch [124/200] batch [2/2] time 0.269 (0.535) data 0.001 (0.270) loss 1.3304 (1.1199) acc 68.7500 (81.2500) lr 6.4653e+00 eta 0:01:21\n",
      "epoch [125/200] batch [1/2] time 0.936 (0.936) data 0.670 (0.670) loss 1.0096 (1.0096) acc 90.6250 (90.6250) lr 6.4653e+00 eta 0:02:21\n",
      "epoch [125/200] batch [2/2] time 0.267 (0.602) data 0.001 (0.335) loss 1.4575 (1.2335) acc 71.8750 (81.2500) lr 6.3188e+00 eta 0:01:30\n",
      "epoch [126/200] batch [1/2] time 1.156 (1.156) data 0.892 (0.892) loss 0.8154 (0.8154) acc 84.3750 (84.3750) lr 6.3188e+00 eta 0:02:52\n",
      "epoch [126/200] batch [2/2] time 0.269 (0.713) data 0.001 (0.446) loss 1.0852 (0.9503) acc 71.8750 (78.1250) lr 6.1732e+00 eta 0:01:45\n",
      "epoch [127/200] batch [1/2] time 1.166 (1.166) data 0.892 (0.892) loss 1.0422 (1.0422) acc 87.5000 (87.5000) lr 6.1732e+00 eta 0:02:51\n",
      "epoch [127/200] batch [2/2] time 0.270 (0.718) data 0.001 (0.446) loss 0.8482 (0.9452) acc 84.3750 (85.9375) lr 6.0285e+00 eta 0:01:44\n",
      "epoch [128/200] batch [1/2] time 0.844 (0.844) data 0.579 (0.579) loss 1.4100 (1.4100) acc 68.7500 (68.7500) lr 6.0285e+00 eta 0:02:02\n",
      "epoch [128/200] batch [2/2] time 0.267 (0.556) data 0.001 (0.290) loss 1.6090 (1.5095) acc 59.3750 (64.0625) lr 5.8849e+00 eta 0:01:20\n",
      "epoch [129/200] batch [1/2] time 0.819 (0.819) data 0.554 (0.554) loss 1.2967 (1.2967) acc 71.8750 (71.8750) lr 5.8849e+00 eta 0:01:57\n",
      "epoch [129/200] batch [2/2] time 0.267 (0.543) data 0.000 (0.277) loss 1.5223 (1.4095) acc 56.2500 (64.0625) lr 5.7422e+00 eta 0:01:17\n",
      "epoch [130/200] batch [1/2] time 0.783 (0.783) data 0.518 (0.518) loss 1.7998 (1.7998) acc 62.5000 (62.5000) lr 5.7422e+00 eta 0:01:50\n",
      "epoch [130/200] batch [2/2] time 0.268 (0.525) data 0.000 (0.259) loss 1.5351 (1.6675) acc 71.8750 (67.1875) lr 5.6006e+00 eta 0:01:13\n",
      "epoch [131/200] batch [1/2] time 0.803 (0.803) data 0.542 (0.542) loss 1.4453 (1.4453) acc 68.7500 (68.7500) lr 5.6006e+00 eta 0:01:51\n",
      "epoch [131/200] batch [2/2] time 0.270 (0.537) data 0.000 (0.271) loss 1.2878 (1.3665) acc 84.3750 (76.5625) lr 5.4601e+00 eta 0:01:14\n",
      "epoch [132/200] batch [1/2] time 0.800 (0.800) data 0.556 (0.556) loss 1.0255 (1.0255) acc 87.5000 (87.5000) lr 5.4601e+00 eta 0:01:49\n",
      "epoch [132/200] batch [2/2] time 0.267 (0.533) data 0.000 (0.278) loss 1.2508 (1.1382) acc 71.8750 (79.6875) lr 5.3207e+00 eta 0:01:12\n",
      "epoch [133/200] batch [1/2] time 0.820 (0.820) data 0.555 (0.555) loss 0.9921 (0.9921) acc 81.2500 (81.2500) lr 5.3207e+00 eta 0:01:50\n",
      "epoch [133/200] batch [2/2] time 0.268 (0.544) data 0.001 (0.278) loss 1.0751 (1.0336) acc 71.8750 (76.5625) lr 5.1825e+00 eta 0:01:12\n",
      "epoch [134/200] batch [1/2] time 0.787 (0.787) data 0.518 (0.518) loss 1.0594 (1.0594) acc 81.2500 (81.2500) lr 5.1825e+00 eta 0:01:44\n",
      "epoch [134/200] batch [2/2] time 0.268 (0.528) data 0.000 (0.259) loss 1.4330 (1.2462) acc 71.8750 (76.5625) lr 5.0454e+00 eta 0:01:09\n",
      "epoch [135/200] batch [1/2] time 0.779 (0.779) data 0.512 (0.512) loss 1.1070 (1.1070) acc 75.0000 (75.0000) lr 5.0454e+00 eta 0:01:42\n",
      "epoch [135/200] batch [2/2] time 0.268 (0.523) data 0.001 (0.257) loss 1.0444 (1.0757) acc 87.5000 (81.2500) lr 4.9096e+00 eta 0:01:08\n",
      "epoch [136/200] batch [1/2] time 1.026 (1.026) data 0.760 (0.760) loss 1.1635 (1.1635) acc 78.1250 (78.1250) lr 4.9096e+00 eta 0:02:12\n",
      "epoch [136/200] batch [2/2] time 0.278 (0.652) data 0.001 (0.380) loss 0.9872 (1.0754) acc 81.2500 (79.6875) lr 4.7750e+00 eta 0:01:23\n",
      "epoch [137/200] batch [1/2] time 1.140 (1.140) data 0.867 (0.867) loss 1.1189 (1.1189) acc 84.3750 (84.3750) lr 4.7750e+00 eta 0:02:24\n",
      "epoch [137/200] batch [2/2] time 0.270 (0.705) data 0.001 (0.434) loss 0.6593 (0.8891) acc 93.7500 (89.0625) lr 4.6417e+00 eta 0:01:28\n",
      "epoch [138/200] batch [1/2] time 1.006 (1.006) data 0.741 (0.741) loss 1.1279 (1.1279) acc 75.0000 (75.0000) lr 4.6417e+00 eta 0:02:05\n",
      "epoch [138/200] batch [2/2] time 0.267 (0.636) data 0.000 (0.371) loss 0.9802 (1.0540) acc 78.1250 (76.5625) lr 4.5098e+00 eta 0:01:18\n",
      "epoch [139/200] batch [1/2] time 0.801 (0.801) data 0.536 (0.536) loss 0.9659 (0.9659) acc 81.2500 (81.2500) lr 4.5098e+00 eta 0:01:38\n",
      "epoch [139/200] batch [2/2] time 0.270 (0.535) data 0.001 (0.268) loss 1.0372 (1.0015) acc 87.5000 (84.3750) lr 4.3792e+00 eta 0:01:05\n",
      "epoch [140/200] batch [1/2] time 0.801 (0.801) data 0.538 (0.538) loss 0.9417 (0.9417) acc 87.5000 (87.5000) lr 4.3792e+00 eta 0:01:36\n",
      "epoch [140/200] batch [2/2] time 0.269 (0.535) data 0.001 (0.269) loss 0.8929 (0.9173) acc 81.2500 (84.3750) lr 4.2499e+00 eta 0:01:04\n",
      "epoch [141/200] batch [1/2] time 0.772 (0.772) data 0.505 (0.505) loss 0.8684 (0.8684) acc 90.6250 (90.6250) lr 4.2499e+00 eta 0:01:31\n",
      "epoch [141/200] batch [2/2] time 0.268 (0.520) data 0.001 (0.253) loss 0.9493 (0.9088) acc 81.2500 (85.9375) lr 4.1221e+00 eta 0:01:01\n",
      "epoch [142/200] batch [1/2] time 0.777 (0.777) data 0.511 (0.511) loss 1.2117 (1.2117) acc 78.1250 (78.1250) lr 4.1221e+00 eta 0:01:30\n",
      "epoch [142/200] batch [2/2] time 0.270 (0.524) data 0.001 (0.256) loss 0.8665 (1.0391) acc 81.2500 (79.6875) lr 3.9958e+00 eta 0:01:00\n",
      "epoch [143/200] batch [1/2] time 0.782 (0.782) data 0.518 (0.518) loss 0.9242 (0.9242) acc 87.5000 (87.5000) lr 3.9958e+00 eta 0:01:29\n",
      "epoch [143/200] batch [2/2] time 0.269 (0.526) data 0.000 (0.259) loss 0.8829 (0.9036) acc 87.5000 (87.5000) lr 3.8709e+00 eta 0:00:59\n",
      "epoch [144/200] batch [1/2] time 0.805 (0.805) data 0.539 (0.539) loss 0.9392 (0.9392) acc 81.2500 (81.2500) lr 3.8709e+00 eta 0:01:30\n",
      "epoch [144/200] batch [2/2] time 0.269 (0.537) data 0.001 (0.270) loss 0.6879 (0.8135) acc 96.8750 (89.0625) lr 3.7476e+00 eta 0:01:00\n",
      "epoch [145/200] batch [1/2] time 0.811 (0.811) data 0.545 (0.545) loss 0.8929 (0.8929) acc 87.5000 (87.5000) lr 3.7476e+00 eta 0:01:30\n",
      "epoch [145/200] batch [2/2] time 0.271 (0.541) data 0.001 (0.273) loss 0.7967 (0.8448) acc 87.5000 (87.5000) lr 3.6258e+00 eta 0:00:59\n",
      "epoch [146/200] batch [1/2] time 0.791 (0.791) data 0.514 (0.514) loss 0.6731 (0.6731) acc 90.6250 (90.6250) lr 3.6258e+00 eta 0:01:26\n",
      "epoch [146/200] batch [2/2] time 0.272 (0.531) data 0.001 (0.257) loss 0.9168 (0.7949) acc 84.3750 (87.5000) lr 3.5055e+00 eta 0:00:57\n",
      "epoch [147/200] batch [1/2] time 1.120 (1.120) data 0.847 (0.847) loss 0.5335 (0.5335) acc 96.8750 (96.8750) lr 3.5055e+00 eta 0:01:59\n",
      "epoch [147/200] batch [2/2] time 0.270 (0.695) data 0.001 (0.424) loss 0.7160 (0.6248) acc 90.6250 (93.7500) lr 3.3869e+00 eta 0:01:13\n",
      "epoch [148/200] batch [1/2] time 1.187 (1.187) data 0.901 (0.901) loss 1.0950 (1.0950) acc 90.6250 (90.6250) lr 3.3869e+00 eta 0:02:04\n",
      "epoch [148/200] batch [2/2] time 0.273 (0.730) data 0.001 (0.451) loss 0.7498 (0.9224) acc 90.6250 (90.6250) lr 3.2699e+00 eta 0:01:15\n",
      "epoch [149/200] batch [1/2] time 1.034 (1.034) data 0.770 (0.770) loss 0.5002 (0.5002) acc 93.7500 (93.7500) lr 3.2699e+00 eta 0:01:46\n",
      "epoch [149/200] batch [2/2] time 0.271 (0.653) data 0.000 (0.385) loss 0.9687 (0.7345) acc 81.2500 (87.5000) lr 3.1545e+00 eta 0:01:06\n",
      "epoch [150/200] batch [1/2] time 0.781 (0.781) data 0.512 (0.512) loss 0.6174 (0.6174) acc 93.7500 (93.7500) lr 3.1545e+00 eta 0:01:18\n",
      "epoch [150/200] batch [2/2] time 0.271 (0.526) data 0.000 (0.256) loss 1.2735 (0.9455) acc 81.2500 (87.5000) lr 3.0409e+00 eta 0:00:52\n",
      "epoch [151/200] batch [1/2] time 0.765 (0.765) data 0.495 (0.495) loss 0.6944 (0.6944) acc 87.5000 (87.5000) lr 3.0409e+00 eta 0:01:15\n",
      "epoch [151/200] batch [2/2] time 0.271 (0.518) data 0.001 (0.248) loss 0.6756 (0.6850) acc 87.5000 (87.5000) lr 2.9289e+00 eta 0:00:50\n",
      "epoch [152/200] batch [1/2] time 0.800 (0.800) data 0.531 (0.531) loss 0.8718 (0.8718) acc 84.3750 (84.3750) lr 2.9289e+00 eta 0:01:17\n",
      "epoch [152/200] batch [2/2] time 0.269 (0.535) data 0.000 (0.266) loss 0.6442 (0.7580) acc 87.5000 (85.9375) lr 2.8187e+00 eta 0:00:51\n",
      "epoch [153/200] batch [1/2] time 0.794 (0.794) data 0.527 (0.527) loss 0.4479 (0.4479) acc 96.8750 (96.8750) lr 2.8187e+00 eta 0:01:15\n",
      "epoch [153/200] batch [2/2] time 0.270 (0.532) data 0.001 (0.264) loss 0.7667 (0.6073) acc 81.2500 (89.0625) lr 2.7103e+00 eta 0:00:50\n",
      "epoch [154/200] batch [1/2] time 0.801 (0.801) data 0.535 (0.535) loss 0.4882 (0.4882) acc 93.7500 (93.7500) lr 2.7103e+00 eta 0:01:14\n",
      "epoch [154/200] batch [2/2] time 0.272 (0.536) data 0.000 (0.268) loss 0.6518 (0.5700) acc 87.5000 (90.6250) lr 2.6037e+00 eta 0:00:49\n",
      "epoch [155/200] batch [1/2] time 0.804 (0.804) data 0.535 (0.535) loss 0.4754 (0.4754) acc 90.6250 (90.6250) lr 2.6037e+00 eta 0:01:13\n",
      "epoch [155/200] batch [2/2] time 0.271 (0.537) data 0.000 (0.268) loss 0.7631 (0.6193) acc 87.5000 (89.0625) lr 2.4989e+00 eta 0:00:48\n",
      "epoch [156/200] batch [1/2] time 0.793 (0.793) data 0.526 (0.526) loss 0.5094 (0.5094) acc 96.8750 (96.8750) lr 2.4989e+00 eta 0:01:10\n",
      "epoch [156/200] batch [2/2] time 0.271 (0.532) data 0.000 (0.263) loss 0.6872 (0.5983) acc 90.6250 (93.7500) lr 2.3959e+00 eta 0:00:46\n",
      "epoch [157/200] batch [1/2] time 1.092 (1.092) data 0.823 (0.823) loss 0.8610 (0.8610) acc 81.2500 (81.2500) lr 2.3959e+00 eta 0:01:35\n",
      "epoch [157/200] batch [2/2] time 0.275 (0.683) data 0.001 (0.412) loss 0.6640 (0.7625) acc 81.2500 (81.2500) lr 2.2949e+00 eta 0:00:58\n",
      "epoch [158/200] batch [1/2] time 1.161 (1.161) data 0.893 (0.893) loss 0.4608 (0.4608) acc 96.8750 (96.8750) lr 2.2949e+00 eta 0:01:38\n",
      "epoch [158/200] batch [2/2] time 0.273 (0.717) data 0.001 (0.447) loss 0.9846 (0.7227) acc 81.2500 (89.0625) lr 2.1957e+00 eta 0:01:00\n",
      "epoch [159/200] batch [1/2] time 0.970 (0.970) data 0.701 (0.701) loss 0.5922 (0.5922) acc 93.7500 (93.7500) lr 2.1957e+00 eta 0:01:20\n",
      "epoch [159/200] batch [2/2] time 0.272 (0.621) data 0.000 (0.351) loss 0.8662 (0.7292) acc 78.1250 (85.9375) lr 2.0984e+00 eta 0:00:50\n",
      "epoch [160/200] batch [1/2] time 0.808 (0.808) data 0.543 (0.543) loss 0.7958 (0.7958) acc 81.2500 (81.2500) lr 2.0984e+00 eta 0:01:05\n",
      "epoch [160/200] batch [2/2] time 0.269 (0.538) data 0.000 (0.272) loss 0.8269 (0.8114) acc 84.3750 (82.8125) lr 2.0032e+00 eta 0:00:43\n",
      "epoch [161/200] batch [1/2] time 0.790 (0.790) data 0.526 (0.526) loss 0.4465 (0.4465) acc 96.8750 (96.8750) lr 2.0032e+00 eta 0:01:02\n",
      "epoch [161/200] batch [2/2] time 0.270 (0.530) data 0.000 (0.263) loss 0.6870 (0.5667) acc 87.5000 (92.1875) lr 1.9098e+00 eta 0:00:41\n",
      "epoch [162/200] batch [1/2] time 0.779 (0.779) data 0.511 (0.511) loss 0.5455 (0.5455) acc 93.7500 (93.7500) lr 1.9098e+00 eta 0:01:00\n",
      "epoch [162/200] batch [2/2] time 0.272 (0.526) data 0.001 (0.256) loss 0.4419 (0.4937) acc 93.7500 (93.7500) lr 1.8185e+00 eta 0:00:39\n",
      "epoch [163/200] batch [1/2] time 0.781 (0.781) data 0.514 (0.514) loss 0.6866 (0.6866) acc 90.6250 (90.6250) lr 1.8185e+00 eta 0:00:58\n",
      "epoch [163/200] batch [2/2] time 0.272 (0.526) data 0.001 (0.257) loss 0.5582 (0.6224) acc 90.6250 (90.6250) lr 1.7292e+00 eta 0:00:38\n",
      "epoch [164/200] batch [1/2] time 0.799 (0.799) data 0.531 (0.531) loss 0.5504 (0.5504) acc 90.6250 (90.6250) lr 1.7292e+00 eta 0:00:58\n",
      "epoch [164/200] batch [2/2] time 0.269 (0.534) data 0.001 (0.266) loss 0.7777 (0.6640) acc 84.3750 (87.5000) lr 1.6419e+00 eta 0:00:38\n",
      "epoch [165/200] batch [1/2] time 0.805 (0.805) data 0.539 (0.539) loss 0.6712 (0.6712) acc 90.6250 (90.6250) lr 1.6419e+00 eta 0:00:57\n",
      "epoch [165/200] batch [2/2] time 0.273 (0.539) data 0.000 (0.270) loss 1.0727 (0.8720) acc 78.1250 (84.3750) lr 1.5567e+00 eta 0:00:37\n",
      "epoch [166/200] batch [1/2] time 0.811 (0.811) data 0.546 (0.546) loss 0.6370 (0.6370) acc 87.5000 (87.5000) lr 1.5567e+00 eta 0:00:55\n",
      "epoch [166/200] batch [2/2] time 0.267 (0.539) data 0.000 (0.273) loss 0.6496 (0.6433) acc 87.5000 (87.5000) lr 1.4736e+00 eta 0:00:36\n",
      "epoch [167/200] batch [1/2] time 0.876 (0.876) data 0.606 (0.606) loss 0.6999 (0.6999) acc 81.2500 (81.2500) lr 1.4736e+00 eta 0:00:58\n",
      "epoch [167/200] batch [2/2] time 0.273 (0.574) data 0.001 (0.303) loss 0.4557 (0.5778) acc 96.8750 (89.0625) lr 1.3926e+00 eta 0:00:37\n",
      "epoch [168/200] batch [1/2] time 1.135 (1.135) data 0.867 (0.867) loss 0.5914 (0.5914) acc 87.5000 (87.5000) lr 1.3926e+00 eta 0:01:13\n",
      "epoch [168/200] batch [2/2] time 0.273 (0.704) data 0.001 (0.434) loss 0.8383 (0.7149) acc 87.5000 (87.5000) lr 1.3137e+00 eta 0:00:45\n",
      "epoch [169/200] batch [1/2] time 1.187 (1.187) data 0.919 (0.919) loss 0.7062 (0.7062) acc 90.6250 (90.6250) lr 1.3137e+00 eta 0:01:14\n",
      "epoch [169/200] batch [2/2] time 0.275 (0.731) data 0.001 (0.460) loss 0.5001 (0.6031) acc 93.7500 (92.1875) lr 1.2369e+00 eta 0:00:45\n",
      "epoch [170/200] batch [1/2] time 0.875 (0.875) data 0.608 (0.608) loss 0.7476 (0.7476) acc 87.5000 (87.5000) lr 1.2369e+00 eta 0:00:53\n",
      "epoch [170/200] batch [2/2] time 0.270 (0.572) data 0.001 (0.304) loss 0.4538 (0.6007) acc 90.6250 (89.0625) lr 1.1623e+00 eta 0:00:34\n",
      "epoch [171/200] batch [1/2] time 0.811 (0.811) data 0.544 (0.544) loss 0.5586 (0.5586) acc 93.7500 (93.7500) lr 1.1623e+00 eta 0:00:47\n",
      "epoch [171/200] batch [2/2] time 0.273 (0.542) data 0.000 (0.272) loss 0.5864 (0.5725) acc 90.6250 (92.1875) lr 1.0899e+00 eta 0:00:31\n",
      "epoch [172/200] batch [1/2] time 0.881 (0.881) data 0.612 (0.612) loss 0.7062 (0.7062) acc 84.3750 (84.3750) lr 1.0899e+00 eta 0:00:50\n",
      "epoch [172/200] batch [2/2] time 0.272 (0.576) data 0.000 (0.306) loss 0.3740 (0.5401) acc 96.8750 (90.6250) lr 1.0197e+00 eta 0:00:32\n",
      "epoch [173/200] batch [1/2] time 0.807 (0.807) data 0.543 (0.543) loss 0.4455 (0.4455) acc 90.6250 (90.6250) lr 1.0197e+00 eta 0:00:44\n",
      "epoch [173/200] batch [2/2] time 0.272 (0.540) data 0.001 (0.272) loss 0.5470 (0.4963) acc 90.6250 (90.6250) lr 9.5173e-01 eta 0:00:29\n",
      "epoch [174/200] batch [1/2] time 0.812 (0.812) data 0.545 (0.545) loss 0.5472 (0.5472) acc 84.3750 (84.3750) lr 9.5173e-01 eta 0:00:43\n",
      "epoch [174/200] batch [2/2] time 0.272 (0.542) data 0.001 (0.273) loss 0.4219 (0.4845) acc 100.0000 (92.1875) lr 8.8597e-01 eta 0:00:28\n",
      "epoch [175/200] batch [1/2] time 0.814 (0.814) data 0.545 (0.545) loss 0.4513 (0.4513) acc 93.7500 (93.7500) lr 8.8597e-01 eta 0:00:41\n",
      "epoch [175/200] batch [2/2] time 0.270 (0.542) data 0.000 (0.273) loss 0.5251 (0.4882) acc 90.6250 (92.1875) lr 8.2245e-01 eta 0:00:27\n",
      "epoch [176/200] batch [1/2] time 0.793 (0.793) data 0.523 (0.523) loss 0.4470 (0.4470) acc 90.6250 (90.6250) lr 8.2245e-01 eta 0:00:38\n",
      "epoch [176/200] batch [2/2] time 0.271 (0.532) data 0.000 (0.262) loss 0.4082 (0.4276) acc 93.7500 (92.1875) lr 7.6120e-01 eta 0:00:25\n",
      "epoch [177/200] batch [1/2] time 0.799 (0.799) data 0.530 (0.530) loss 0.3276 (0.3276) acc 100.0000 (100.0000) lr 7.6120e-01 eta 0:00:37\n",
      "epoch [177/200] batch [2/2] time 0.272 (0.535) data 0.001 (0.265) loss 0.4803 (0.4040) acc 93.7500 (96.8750) lr 7.0224e-01 eta 0:00:24\n",
      "epoch [178/200] batch [1/2] time 1.034 (1.034) data 0.763 (0.763) loss 0.3274 (0.3274) acc 100.0000 (100.0000) lr 7.0224e-01 eta 0:00:46\n",
      "epoch [178/200] batch [2/2] time 0.272 (0.653) data 0.001 (0.382) loss 0.4145 (0.3710) acc 93.7500 (96.8750) lr 6.4556e-01 eta 0:00:28\n",
      "epoch [179/200] batch [1/2] time 1.151 (1.151) data 0.882 (0.882) loss 0.4580 (0.4580) acc 93.7500 (93.7500) lr 6.4556e-01 eta 0:00:49\n",
      "epoch [179/200] batch [2/2] time 0.276 (0.713) data 0.001 (0.441) loss 0.4336 (0.4458) acc 93.7500 (93.7500) lr 5.9119e-01 eta 0:00:29\n",
      "epoch [180/200] batch [1/2] time 1.108 (1.108) data 0.837 (0.837) loss 0.3879 (0.3879) acc 93.7500 (93.7500) lr 5.9119e-01 eta 0:00:45\n",
      "epoch [180/200] batch [2/2] time 0.280 (0.694) data 0.001 (0.419) loss 0.4867 (0.4373) acc 90.6250 (92.1875) lr 5.3915e-01 eta 0:00:27\n",
      "epoch [181/200] batch [1/2] time 0.870 (0.870) data 0.599 (0.599) loss 0.3942 (0.3942) acc 90.6250 (90.6250) lr 5.3915e-01 eta 0:00:33\n",
      "epoch [181/200] batch [2/2] time 0.272 (0.571) data 0.001 (0.300) loss 0.3323 (0.3632) acc 100.0000 (95.3125) lr 4.8943e-01 eta 0:00:21\n",
      "epoch [182/200] batch [1/2] time 0.803 (0.803) data 0.537 (0.537) loss 0.2990 (0.2990) acc 100.0000 (100.0000) lr 4.8943e-01 eta 0:00:29\n",
      "epoch [182/200] batch [2/2] time 0.272 (0.538) data 0.000 (0.269) loss 0.3109 (0.3049) acc 100.0000 (100.0000) lr 4.4207e-01 eta 0:00:19\n",
      "epoch [183/200] batch [1/2] time 0.794 (0.794) data 0.521 (0.521) loss 0.3680 (0.3680) acc 96.8750 (96.8750) lr 4.4207e-01 eta 0:00:27\n",
      "epoch [183/200] batch [2/2] time 0.271 (0.533) data 0.000 (0.261) loss 0.3483 (0.3582) acc 96.8750 (96.8750) lr 3.9706e-01 eta 0:00:18\n",
      "epoch [184/200] batch [1/2] time 0.824 (0.824) data 0.555 (0.555) loss 0.3083 (0.3083) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:27\n",
      "epoch [184/200] batch [2/2] time 0.274 (0.549) data 0.001 (0.278) loss 0.2927 (0.3005) acc 100.0000 (100.0000) lr 3.5443e-01 eta 0:00:17\n",
      "epoch [185/200] batch [1/2] time 0.800 (0.800) data 0.534 (0.534) loss 0.3260 (0.3260) acc 96.8750 (96.8750) lr 3.5443e-01 eta 0:00:24\n",
      "epoch [185/200] batch [2/2] time 0.273 (0.537) data 0.001 (0.267) loss 0.5302 (0.4281) acc 90.6250 (93.7500) lr 3.1417e-01 eta 0:00:16\n",
      "epoch [186/200] batch [1/2] time 0.803 (0.803) data 0.538 (0.538) loss 0.4241 (0.4241) acc 96.8750 (96.8750) lr 3.1417e-01 eta 0:00:23\n",
      "epoch [186/200] batch [2/2] time 0.273 (0.538) data 0.001 (0.269) loss 0.2806 (0.3524) acc 100.0000 (98.4375) lr 2.7630e-01 eta 0:00:15\n",
      "epoch [187/200] batch [1/2] time 0.797 (0.797) data 0.529 (0.529) loss 0.4526 (0.4526) acc 96.8750 (96.8750) lr 2.7630e-01 eta 0:00:21\n",
      "epoch [187/200] batch [2/2] time 0.274 (0.535) data 0.000 (0.264) loss 0.4817 (0.4671) acc 93.7500 (95.3125) lr 2.4083e-01 eta 0:00:13\n",
      "epoch [188/200] batch [1/2] time 0.795 (0.795) data 0.526 (0.526) loss 0.4115 (0.4115) acc 96.8750 (96.8750) lr 2.4083e-01 eta 0:00:19\n",
      "epoch [188/200] batch [2/2] time 0.272 (0.534) data 0.000 (0.263) loss 0.3495 (0.3805) acc 96.8750 (96.8750) lr 2.0777e-01 eta 0:00:12\n",
      "epoch [189/200] batch [1/2] time 0.933 (0.933) data 0.664 (0.664) loss 0.3205 (0.3205) acc 96.8750 (96.8750) lr 2.0777e-01 eta 0:00:21\n",
      "epoch [189/200] batch [2/2] time 0.271 (0.602) data 0.001 (0.333) loss 0.3023 (0.3114) acc 100.0000 (98.4375) lr 1.7713e-01 eta 0:00:13\n",
      "epoch [190/200] batch [1/2] time 1.152 (1.152) data 0.882 (0.882) loss 0.3926 (0.3926) acc 96.8750 (96.8750) lr 1.7713e-01 eta 0:00:24\n",
      "epoch [190/200] batch [2/2] time 0.275 (0.713) data 0.001 (0.441) loss 0.3105 (0.3515) acc 96.8750 (96.8750) lr 1.4891e-01 eta 0:00:14\n",
      "epoch [191/200] batch [1/2] time 1.089 (1.089) data 0.807 (0.807) loss 0.2775 (0.2775) acc 100.0000 (100.0000) lr 1.4891e-01 eta 0:00:20\n",
      "epoch [191/200] batch [2/2] time 0.274 (0.682) data 0.001 (0.404) loss 0.2642 (0.2709) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:12\n",
      "epoch [192/200] batch [1/2] time 0.844 (0.844) data 0.577 (0.577) loss 0.2844 (0.2844) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:14\n",
      "epoch [192/200] batch [2/2] time 0.277 (0.561) data 0.000 (0.289) loss 0.3094 (0.2969) acc 100.0000 (100.0000) lr 9.9763e-02 eta 0:00:08\n",
      "epoch [193/200] batch [1/2] time 0.813 (0.813) data 0.540 (0.540) loss 0.3647 (0.3647) acc 96.8750 (96.8750) lr 9.9763e-02 eta 0:00:12\n",
      "epoch [193/200] batch [2/2] time 0.274 (0.544) data 0.001 (0.270) loss 0.5380 (0.4514) acc 93.7500 (95.3125) lr 7.8853e-02 eta 0:00:07\n",
      "epoch [194/200] batch [1/2] time 0.807 (0.807) data 0.537 (0.537) loss 0.2795 (0.2795) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:10\n",
      "epoch [194/200] batch [2/2] time 0.278 (0.542) data 0.000 (0.268) loss 0.3875 (0.3335) acc 96.8750 (98.4375) lr 6.0390e-02 eta 0:00:06\n",
      "epoch [195/200] batch [1/2] time 0.804 (0.804) data 0.534 (0.534) loss 0.3370 (0.3370) acc 96.8750 (96.8750) lr 6.0390e-02 eta 0:00:08\n",
      "epoch [195/200] batch [2/2] time 0.275 (0.540) data 0.001 (0.267) loss 0.2899 (0.3134) acc 100.0000 (98.4375) lr 4.4380e-02 eta 0:00:05\n",
      "epoch [196/200] batch [1/2] time 0.786 (0.786) data 0.517 (0.517) loss 0.2812 (0.2812) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:07\n",
      "epoch [196/200] batch [2/2] time 0.275 (0.530) data 0.001 (0.259) loss 0.3074 (0.2943) acc 96.8750 (98.4375) lr 3.0827e-02 eta 0:00:04\n",
      "epoch [197/200] batch [1/2] time 0.787 (0.787) data 0.515 (0.515) loss 0.2792 (0.2792) acc 100.0000 (100.0000) lr 3.0827e-02 eta 0:00:05\n",
      "epoch [197/200] batch [2/2] time 0.273 (0.530) data 0.001 (0.258) loss 0.2743 (0.2768) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:03\n",
      "epoch [198/200] batch [1/2] time 0.798 (0.798) data 0.527 (0.527) loss 0.3117 (0.3117) acc 96.8750 (96.8750) lr 1.9733e-02 eta 0:00:03\n",
      "epoch [198/200] batch [2/2] time 0.273 (0.535) data 0.001 (0.264) loss 0.4034 (0.3575) acc 96.8750 (96.8750) lr 1.1101e-02 eta 0:00:02\n",
      "epoch [199/200] batch [1/2] time 0.803 (0.803) data 0.529 (0.529) loss 0.5530 (0.5530) acc 90.6250 (90.6250) lr 1.1101e-02 eta 0:00:02\n",
      "epoch [199/200] batch [2/2] time 0.279 (0.541) data 0.000 (0.265) loss 0.4567 (0.5049) acc 93.7500 (92.1875) lr 4.9344e-03 eta 0:00:01\n",
      "epoch [200/200] batch [1/2] time 1.005 (1.005) data 0.733 (0.733) loss 0.3657 (0.3657) acc 96.8750 (96.8750) lr 4.9344e-03 eta 0:00:01\n",
      "epoch [200/200] batch [2/2] time 0.277 (0.641) data 0.001 (0.367) loss 0.5276 (0.4467) acc 90.6250 (93.7500) lr 1.2337e-03 eta 0:00:00\n",
      "Checkpoint saved to output/eurosat/DAPT/vit_b16_8shots/seed2/prompt_learner/model.pth.tar-200\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:58<00:00,  1.38it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 7,087\n",
      "* accuracy: 87.5%\n",
      "* error: 12.5%\n",
      "* macro_f1: 87.4%\n",
      "Elapsed: 0:05:25\n"
     ]
    }
   ],
   "source": [
    "#eurosat-8shots-seed2\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 2 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "        --output-dir output/eurosat/DAPT/vit_b16_8shots/seed2 \\\n",
    "        DATASET.NUM_SHOTS 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfrH5V-LQCl8",
    "outputId": "a23dba3e-826e-4c8e-868a-8b080dc59a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 16:08:39.809117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 16:08:39.828970: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 16:08:39.834894: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 16:08:39.849265: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 16:08:40.858282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '8']\n",
      "output_dir: output/eurosat/DAPT/vit_b16_8shots/seed3\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 8\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/eurosat/DAPT/vit_b16_8shots/seed3\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_3.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  80\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/eurosat/DAPT/vit_b16_8shots/seed3/tensorboard)\n",
      "epoch [1/200] batch [1/2] time 2.017 (2.017) data 0.629 (0.629) loss 11.7067 (11.7067) acc 9.3750 (9.3750) lr 1.0000e-05 eta 0:13:24\n",
      "epoch [1/200] batch [2/2] time 0.268 (1.142) data 0.001 (0.315) loss 11.8891 (11.7979) acc 15.6250 (12.5000) lr 2.0000e+01 eta 0:07:34\n",
      "epoch [2/200] batch [1/2] time 0.757 (0.757) data 0.489 (0.489) loss 11.9230 (11.9230) acc 6.2500 (6.2500) lr 2.0000e+01 eta 0:05:00\n",
      "epoch [2/200] batch [2/2] time 0.269 (0.513) data 0.000 (0.245) loss 10.0270 (10.9750) acc 6.2500 (6.2500) lr 1.9999e+01 eta 0:03:23\n",
      "epoch [3/200] batch [1/2] time 0.808 (0.808) data 0.541 (0.541) loss 9.4301 (9.4301) acc 9.3750 (9.3750) lr 1.9999e+01 eta 0:05:19\n",
      "epoch [3/200] batch [2/2] time 0.270 (0.539) data 0.000 (0.271) loss 9.5578 (9.4940) acc 3.1250 (6.2500) lr 1.9995e+01 eta 0:03:32\n",
      "epoch [4/200] batch [1/2] time 0.767 (0.767) data 0.501 (0.501) loss 9.1598 (9.1598) acc 15.6250 (15.6250) lr 1.9995e+01 eta 0:05:01\n",
      "epoch [4/200] batch [2/2] time 0.271 (0.519) data 0.001 (0.251) loss 9.1739 (9.1668) acc 15.6250 (15.6250) lr 1.9989e+01 eta 0:03:23\n",
      "epoch [5/200] batch [1/2] time 0.796 (0.796) data 0.527 (0.527) loss 9.1279 (9.1279) acc 18.7500 (18.7500) lr 1.9989e+01 eta 0:05:11\n",
      "epoch [5/200] batch [2/2] time 0.276 (0.536) data 0.001 (0.264) loss 9.0235 (9.0757) acc 9.3750 (14.0625) lr 1.9980e+01 eta 0:03:29\n",
      "epoch [6/200] batch [1/2] time 1.142 (1.142) data 0.874 (0.874) loss 8.9624 (8.9624) acc 18.7500 (18.7500) lr 1.9980e+01 eta 0:07:24\n",
      "epoch [6/200] batch [2/2] time 0.272 (0.707) data 0.001 (0.437) loss 8.9893 (8.9758) acc 6.2500 (12.5000) lr 1.9969e+01 eta 0:04:34\n",
      "epoch [7/200] batch [1/2] time 1.213 (1.213) data 0.936 (0.936) loss 8.8973 (8.8973) acc 15.6250 (15.6250) lr 1.9969e+01 eta 0:07:49\n",
      "epoch [7/200] batch [2/2] time 0.275 (0.744) data 0.001 (0.468) loss 8.8215 (8.8594) acc 12.5000 (14.0625) lr 1.9956e+01 eta 0:04:47\n",
      "epoch [8/200] batch [1/2] time 0.853 (0.853) data 0.585 (0.585) loss 8.8273 (8.8273) acc 18.7500 (18.7500) lr 1.9956e+01 eta 0:05:28\n",
      "epoch [8/200] batch [2/2] time 0.270 (0.561) data 0.001 (0.293) loss 8.6672 (8.7473) acc 21.8750 (20.3125) lr 1.9940e+01 eta 0:03:35\n",
      "epoch [9/200] batch [1/2] time 0.779 (0.779) data 0.507 (0.507) loss 8.6610 (8.6610) acc 18.7500 (18.7500) lr 1.9940e+01 eta 0:04:58\n",
      "epoch [9/200] batch [2/2] time 0.273 (0.526) data 0.001 (0.254) loss 8.5866 (8.6238) acc 18.7500 (18.7500) lr 1.9921e+01 eta 0:03:20\n",
      "epoch [10/200] batch [1/2] time 0.814 (0.814) data 0.546 (0.546) loss 8.4934 (8.4934) acc 28.1250 (28.1250) lr 1.9921e+01 eta 0:05:10\n",
      "epoch [10/200] batch [2/2] time 0.272 (0.543) data 0.001 (0.273) loss 8.5586 (8.5260) acc 18.7500 (23.4375) lr 1.9900e+01 eta 0:03:26\n",
      "epoch [11/200] batch [1/2] time 0.806 (0.806) data 0.538 (0.538) loss 8.4535 (8.4535) acc 25.0000 (25.0000) lr 1.9900e+01 eta 0:05:05\n",
      "epoch [11/200] batch [2/2] time 0.272 (0.539) data 0.000 (0.269) loss 8.3543 (8.4039) acc 25.0000 (25.0000) lr 1.9877e+01 eta 0:03:23\n",
      "epoch [12/200] batch [1/2] time 0.798 (0.798) data 0.527 (0.527) loss 8.2594 (8.2594) acc 25.0000 (25.0000) lr 1.9877e+01 eta 0:05:00\n",
      "epoch [12/200] batch [2/2] time 0.273 (0.535) data 0.000 (0.264) loss 8.1100 (8.1847) acc 37.5000 (31.2500) lr 1.9851e+01 eta 0:03:21\n",
      "epoch [13/200] batch [1/2] time 0.784 (0.784) data 0.512 (0.512) loss 8.1073 (8.1073) acc 28.1250 (28.1250) lr 1.9851e+01 eta 0:04:54\n",
      "epoch [13/200] batch [2/2] time 0.275 (0.530) data 0.001 (0.256) loss 7.5952 (7.8513) acc 43.7500 (35.9375) lr 1.9823e+01 eta 0:03:18\n",
      "epoch [14/200] batch [1/2] time 0.797 (0.797) data 0.528 (0.528) loss 7.6673 (7.6673) acc 25.0000 (25.0000) lr 1.9823e+01 eta 0:04:57\n",
      "epoch [14/200] batch [2/2] time 0.275 (0.536) data 0.000 (0.264) loss 7.4194 (7.5434) acc 31.2500 (28.1250) lr 1.9792e+01 eta 0:03:19\n",
      "epoch [15/200] batch [1/2] time 0.803 (0.803) data 0.533 (0.533) loss 7.1237 (7.1237) acc 43.7500 (43.7500) lr 1.9792e+01 eta 0:04:57\n",
      "epoch [15/200] batch [2/2] time 0.275 (0.539) data 0.000 (0.267) loss 6.7803 (6.9520) acc 43.7500 (43.7500) lr 1.9759e+01 eta 0:03:19\n",
      "epoch [16/200] batch [1/2] time 0.959 (0.959) data 0.683 (0.683) loss 6.2204 (6.2204) acc 50.0000 (50.0000) lr 1.9759e+01 eta 0:05:53\n",
      "epoch [16/200] batch [2/2] time 0.277 (0.618) data 0.001 (0.342) loss 6.4446 (6.3325) acc 43.7500 (46.8750) lr 1.9724e+01 eta 0:03:47\n",
      "epoch [17/200] batch [1/2] time 1.140 (1.140) data 0.870 (0.870) loss 5.8528 (5.8528) acc 25.0000 (25.0000) lr 1.9724e+01 eta 0:06:58\n",
      "epoch [17/200] batch [2/2] time 0.280 (0.710) data 0.001 (0.435) loss 5.1171 (5.4849) acc 28.1250 (26.5625) lr 1.9686e+01 eta 0:04:19\n",
      "epoch [18/200] batch [1/2] time 1.107 (1.107) data 0.835 (0.835) loss 4.7322 (4.7322) acc 21.8750 (21.8750) lr 1.9686e+01 eta 0:06:43\n",
      "epoch [18/200] batch [2/2] time 0.280 (0.693) data 0.001 (0.418) loss 6.1655 (5.4488) acc 31.2500 (26.5625) lr 1.9646e+01 eta 0:04:12\n",
      "epoch [19/200] batch [1/2] time 0.849 (0.849) data 0.576 (0.576) loss 3.7622 (3.7622) acc 43.7500 (43.7500) lr 1.9646e+01 eta 0:05:08\n",
      "epoch [19/200] batch [2/2] time 0.276 (0.563) data 0.001 (0.288) loss 4.4791 (4.1207) acc 28.1250 (35.9375) lr 1.9603e+01 eta 0:03:23\n",
      "epoch [20/200] batch [1/2] time 0.826 (0.826) data 0.558 (0.558) loss 11.4630 (11.4630) acc 12.5000 (12.5000) lr 1.9603e+01 eta 0:04:58\n",
      "epoch [20/200] batch [2/2] time 0.281 (0.553) data 0.000 (0.279) loss 11.0866 (11.2748) acc 18.7500 (15.6250) lr 1.9558e+01 eta 0:03:19\n",
      "epoch [21/200] batch [1/2] time 0.798 (0.798) data 0.526 (0.526) loss 9.6829 (9.6829) acc 18.7500 (18.7500) lr 1.9558e+01 eta 0:04:46\n",
      "epoch [21/200] batch [2/2] time 0.278 (0.538) data 0.000 (0.263) loss 7.8917 (8.7873) acc 28.1250 (23.4375) lr 1.9511e+01 eta 0:03:12\n",
      "epoch [22/200] batch [1/2] time 0.790 (0.790) data 0.519 (0.519) loss 6.6564 (6.6564) acc 40.6250 (40.6250) lr 1.9511e+01 eta 0:04:41\n",
      "epoch [22/200] batch [2/2] time 0.277 (0.533) data 0.000 (0.260) loss 5.4301 (6.0432) acc 56.2500 (48.4375) lr 1.9461e+01 eta 0:03:09\n",
      "epoch [23/200] batch [1/2] time 0.784 (0.784) data 0.511 (0.511) loss 4.8526 (4.8526) acc 37.5000 (37.5000) lr 1.9461e+01 eta 0:04:38\n",
      "epoch [23/200] batch [2/2] time 0.276 (0.530) data 0.001 (0.256) loss 4.3428 (4.5977) acc 56.2500 (46.8750) lr 1.9409e+01 eta 0:03:07\n",
      "epoch [24/200] batch [1/2] time 0.770 (0.770) data 0.498 (0.498) loss 3.8186 (3.8186) acc 62.5000 (62.5000) lr 1.9409e+01 eta 0:04:31\n",
      "epoch [24/200] batch [2/2] time 0.276 (0.523) data 0.001 (0.249) loss 3.4452 (3.6319) acc 75.0000 (68.7500) lr 1.9354e+01 eta 0:03:03\n",
      "epoch [25/200] batch [1/2] time 0.793 (0.793) data 0.519 (0.519) loss 3.1741 (3.1741) acc 65.6250 (65.6250) lr 1.9354e+01 eta 0:04:38\n",
      "epoch [25/200] batch [2/2] time 0.280 (0.536) data 0.001 (0.260) loss 3.4574 (3.3157) acc 50.0000 (57.8125) lr 1.9298e+01 eta 0:03:07\n",
      "epoch [26/200] batch [1/2] time 0.949 (0.949) data 0.675 (0.675) loss 3.0191 (3.0191) acc 65.6250 (65.6250) lr 1.9298e+01 eta 0:05:31\n",
      "epoch [26/200] batch [2/2] time 0.277 (0.613) data 0.000 (0.338) loss 2.9294 (2.9742) acc 56.2500 (60.9375) lr 1.9239e+01 eta 0:03:33\n",
      "epoch [27/200] batch [1/2] time 1.072 (1.072) data 0.796 (0.796) loss 2.7856 (2.7856) acc 59.3750 (59.3750) lr 1.9239e+01 eta 0:06:11\n",
      "epoch [27/200] batch [2/2] time 0.284 (0.678) data 0.001 (0.398) loss 2.7737 (2.7797) acc 56.2500 (57.8125) lr 1.9178e+01 eta 0:03:54\n",
      "epoch [28/200] batch [1/2] time 1.154 (1.154) data 0.878 (0.878) loss 2.7148 (2.7148) acc 62.5000 (62.5000) lr 1.9178e+01 eta 0:06:38\n",
      "epoch [28/200] batch [2/2] time 0.281 (0.717) data 0.001 (0.439) loss 2.5766 (2.6457) acc 68.7500 (65.6250) lr 1.9114e+01 eta 0:04:06\n",
      "epoch [29/200] batch [1/2] time 0.963 (0.963) data 0.692 (0.692) loss 2.3629 (2.3629) acc 78.1250 (78.1250) lr 1.9114e+01 eta 0:05:30\n",
      "epoch [29/200] batch [2/2] time 0.278 (0.621) data 0.000 (0.346) loss 2.1529 (2.2579) acc 75.0000 (76.5625) lr 1.9048e+01 eta 0:03:32\n",
      "epoch [30/200] batch [1/2] time 0.813 (0.813) data 0.539 (0.539) loss 2.0074 (2.0074) acc 71.8750 (71.8750) lr 1.9048e+01 eta 0:04:37\n",
      "epoch [30/200] batch [2/2] time 0.277 (0.545) data 0.001 (0.270) loss 1.9835 (1.9954) acc 71.8750 (71.8750) lr 1.8980e+01 eta 0:03:05\n",
      "epoch [31/200] batch [1/2] time 0.805 (0.805) data 0.533 (0.533) loss 2.1795 (2.1795) acc 68.7500 (68.7500) lr 1.8980e+01 eta 0:04:32\n",
      "epoch [31/200] batch [2/2] time 0.279 (0.542) data 0.001 (0.267) loss 2.3138 (2.2466) acc 59.3750 (64.0625) lr 1.8910e+01 eta 0:03:03\n",
      "epoch [32/200] batch [1/2] time 0.780 (0.780) data 0.508 (0.508) loss 2.9853 (2.9853) acc 59.3750 (59.3750) lr 1.8910e+01 eta 0:04:22\n",
      "epoch [32/200] batch [2/2] time 0.280 (0.530) data 0.000 (0.254) loss 2.1318 (2.5586) acc 62.5000 (60.9375) lr 1.8838e+01 eta 0:02:58\n",
      "epoch [33/200] batch [1/2] time 0.781 (0.781) data 0.508 (0.508) loss 1.8048 (1.8048) acc 71.8750 (71.8750) lr 1.8838e+01 eta 0:04:21\n",
      "epoch [33/200] batch [2/2] time 0.279 (0.530) data 0.000 (0.254) loss 2.0511 (1.9280) acc 62.5000 (67.1875) lr 1.8763e+01 eta 0:02:57\n",
      "epoch [34/200] batch [1/2] time 0.788 (0.788) data 0.512 (0.512) loss 1.9364 (1.9364) acc 59.3750 (59.3750) lr 1.8763e+01 eta 0:04:22\n",
      "epoch [34/200] batch [2/2] time 0.279 (0.533) data 0.001 (0.256) loss 1.9796 (1.9580) acc 53.1250 (56.2500) lr 1.8686e+01 eta 0:02:57\n",
      "epoch [35/200] batch [1/2] time 0.801 (0.801) data 0.523 (0.523) loss 2.7871 (2.7871) acc 21.8750 (21.8750) lr 1.8686e+01 eta 0:04:25\n",
      "epoch [35/200] batch [2/2] time 0.280 (0.540) data 0.000 (0.262) loss 2.2827 (2.5349) acc 50.0000 (35.9375) lr 1.8607e+01 eta 0:02:58\n",
      "epoch [36/200] batch [1/2] time 0.814 (0.814) data 0.537 (0.537) loss 2.9571 (2.9571) acc 40.6250 (40.6250) lr 1.8607e+01 eta 0:04:27\n",
      "epoch [36/200] batch [2/2] time 0.280 (0.547) data 0.000 (0.269) loss 6.1683 (4.5627) acc 18.7500 (29.6875) lr 1.8526e+01 eta 0:02:59\n",
      "epoch [37/200] batch [1/2] time 0.900 (0.900) data 0.625 (0.625) loss 4.8488 (4.8488) acc 25.0000 (25.0000) lr 1.8526e+01 eta 0:04:54\n",
      "epoch [37/200] batch [2/2] time 0.282 (0.591) data 0.001 (0.313) loss 3.6668 (4.2578) acc 31.2500 (28.1250) lr 1.8443e+01 eta 0:03:12\n",
      "epoch [38/200] batch [1/2] time 1.158 (1.158) data 0.883 (0.883) loss 4.1849 (4.1849) acc 34.3750 (34.3750) lr 1.8443e+01 eta 0:06:16\n",
      "epoch [38/200] batch [2/2] time 0.282 (0.720) data 0.001 (0.442) loss 3.9421 (4.0635) acc 9.3750 (21.8750) lr 1.8358e+01 eta 0:03:53\n",
      "epoch [39/200] batch [1/2] time 1.119 (1.119) data 0.841 (0.841) loss 2.8310 (2.8310) acc 31.2500 (31.2500) lr 1.8358e+01 eta 0:06:01\n",
      "epoch [39/200] batch [2/2] time 0.283 (0.701) data 0.001 (0.421) loss 3.0529 (2.9419) acc 31.2500 (31.2500) lr 1.8271e+01 eta 0:03:45\n",
      "epoch [40/200] batch [1/2] time 0.853 (0.853) data 0.578 (0.578) loss 2.4916 (2.4916) acc 62.5000 (62.5000) lr 1.8271e+01 eta 0:04:33\n",
      "epoch [40/200] batch [2/2] time 0.281 (0.567) data 0.001 (0.289) loss 2.3250 (2.4083) acc 53.1250 (57.8125) lr 1.8181e+01 eta 0:03:01\n",
      "epoch [41/200] batch [1/2] time 0.802 (0.802) data 0.524 (0.524) loss 1.9186 (1.9186) acc 62.5000 (62.5000) lr 1.8181e+01 eta 0:04:15\n",
      "epoch [41/200] batch [2/2] time 0.281 (0.541) data 0.001 (0.262) loss 1.9597 (1.9391) acc 56.2500 (59.3750) lr 1.8090e+01 eta 0:02:52\n",
      "epoch [42/200] batch [1/2] time 0.797 (0.797) data 0.521 (0.521) loss 1.7385 (1.7385) acc 68.7500 (68.7500) lr 1.8090e+01 eta 0:04:12\n",
      "epoch [42/200] batch [2/2] time 0.282 (0.539) data 0.001 (0.261) loss 1.3688 (1.5536) acc 78.1250 (73.4375) lr 1.7997e+01 eta 0:02:50\n",
      "epoch [43/200] batch [1/2] time 0.815 (0.815) data 0.539 (0.539) loss 1.4217 (1.4217) acc 75.0000 (75.0000) lr 1.7997e+01 eta 0:04:16\n",
      "epoch [43/200] batch [2/2] time 0.281 (0.548) data 0.000 (0.270) loss 1.4349 (1.4283) acc 68.7500 (71.8750) lr 1.7902e+01 eta 0:02:52\n",
      "epoch [44/200] batch [1/2] time 0.784 (0.784) data 0.506 (0.506) loss 1.2171 (1.2171) acc 71.8750 (71.8750) lr 1.7902e+01 eta 0:04:05\n",
      "epoch [44/200] batch [2/2] time 0.281 (0.533) data 0.000 (0.253) loss 1.4366 (1.3268) acc 62.5000 (67.1875) lr 1.7804e+01 eta 0:02:46\n",
      "epoch [45/200] batch [1/2] time 0.803 (0.803) data 0.526 (0.526) loss 0.9935 (0.9935) acc 84.3750 (84.3750) lr 1.7804e+01 eta 0:04:09\n",
      "epoch [45/200] batch [2/2] time 0.281 (0.542) data 0.000 (0.263) loss 1.1351 (1.0643) acc 71.8750 (78.1250) lr 1.7705e+01 eta 0:02:48\n",
      "epoch [46/200] batch [1/2] time 0.824 (0.824) data 0.551 (0.551) loss 1.5257 (1.5257) acc 62.5000 (62.5000) lr 1.7705e+01 eta 0:04:14\n",
      "epoch [46/200] batch [2/2] time 0.281 (0.552) data 0.000 (0.276) loss 1.1093 (1.3175) acc 71.8750 (67.1875) lr 1.7604e+01 eta 0:02:50\n",
      "epoch [47/200] batch [1/2] time 0.804 (0.804) data 0.526 (0.526) loss 1.1266 (1.1266) acc 75.0000 (75.0000) lr 1.7604e+01 eta 0:04:06\n",
      "epoch [47/200] batch [2/2] time 0.283 (0.543) data 0.001 (0.263) loss 1.3219 (1.2242) acc 78.1250 (76.5625) lr 1.7501e+01 eta 0:02:46\n",
      "epoch [48/200] batch [1/2] time 1.054 (1.054) data 0.739 (0.739) loss 1.0197 (1.0197) acc 81.2500 (81.2500) lr 1.7501e+01 eta 0:05:21\n",
      "epoch [48/200] batch [2/2] time 0.283 (0.669) data 0.001 (0.370) loss 1.0561 (1.0379) acc 78.1250 (79.6875) lr 1.7396e+01 eta 0:03:23\n",
      "epoch [49/200] batch [1/2] time 1.171 (1.171) data 0.896 (0.896) loss 0.7690 (0.7690) acc 87.5000 (87.5000) lr 1.7396e+01 eta 0:05:54\n",
      "epoch [49/200] batch [2/2] time 0.282 (0.727) data 0.001 (0.448) loss 1.5561 (1.1625) acc 56.2500 (71.8750) lr 1.7290e+01 eta 0:03:39\n",
      "epoch [50/200] batch [1/2] time 1.177 (1.177) data 0.893 (0.893) loss 1.1425 (1.1425) acc 75.0000 (75.0000) lr 1.7290e+01 eta 0:05:54\n",
      "epoch [50/200] batch [2/2] time 0.285 (0.731) data 0.001 (0.447) loss 0.8746 (1.0086) acc 90.6250 (82.8125) lr 1.7181e+01 eta 0:03:39\n",
      "epoch [51/200] batch [1/2] time 0.866 (0.866) data 0.592 (0.592) loss 1.7032 (1.7032) acc 71.8750 (71.8750) lr 1.7181e+01 eta 0:04:18\n",
      "epoch [51/200] batch [2/2] time 0.281 (0.573) data 0.001 (0.296) loss 2.4017 (2.0524) acc 56.2500 (64.0625) lr 1.7071e+01 eta 0:02:50\n",
      "epoch [52/200] batch [1/2] time 0.796 (0.796) data 0.521 (0.521) loss 2.2016 (2.2016) acc 56.2500 (56.2500) lr 1.7071e+01 eta 0:03:56\n",
      "epoch [52/200] batch [2/2] time 0.282 (0.539) data 0.001 (0.261) loss 3.1874 (2.6945) acc 31.2500 (43.7500) lr 1.6959e+01 eta 0:02:39\n",
      "epoch [53/200] batch [1/2] time 0.801 (0.801) data 0.526 (0.526) loss 2.5304 (2.5304) acc 53.1250 (53.1250) lr 1.6959e+01 eta 0:03:56\n",
      "epoch [53/200] batch [2/2] time 0.279 (0.540) data 0.001 (0.263) loss 3.1737 (2.8521) acc 37.5000 (45.3125) lr 1.6845e+01 eta 0:02:38\n",
      "epoch [54/200] batch [1/2] time 0.802 (0.802) data 0.524 (0.524) loss 4.1397 (4.1397) acc 31.2500 (31.2500) lr 1.6845e+01 eta 0:03:54\n",
      "epoch [54/200] batch [2/2] time 0.281 (0.542) data 0.001 (0.262) loss 4.6234 (4.3816) acc 34.3750 (32.8125) lr 1.6730e+01 eta 0:02:38\n",
      "epoch [55/200] batch [1/2] time 0.815 (0.815) data 0.536 (0.536) loss 3.7740 (3.7740) acc 31.2500 (31.2500) lr 1.6730e+01 eta 0:03:57\n",
      "epoch [55/200] batch [2/2] time 0.278 (0.546) data 0.000 (0.268) loss 2.3404 (3.0572) acc 50.0000 (40.6250) lr 1.6613e+01 eta 0:02:38\n",
      "epoch [56/200] batch [1/2] time 0.805 (0.805) data 0.529 (0.529) loss 2.1645 (2.1645) acc 68.7500 (68.7500) lr 1.6613e+01 eta 0:03:52\n",
      "epoch [56/200] batch [2/2] time 0.279 (0.542) data 0.001 (0.265) loss 1.9088 (2.0366) acc 68.7500 (68.7500) lr 1.6494e+01 eta 0:02:36\n",
      "epoch [57/200] batch [1/2] time 0.790 (0.790) data 0.516 (0.516) loss 1.6757 (1.6757) acc 75.0000 (75.0000) lr 1.6494e+01 eta 0:03:46\n",
      "epoch [57/200] batch [2/2] time 0.281 (0.536) data 0.000 (0.258) loss 1.5070 (1.5914) acc 71.8750 (73.4375) lr 1.6374e+01 eta 0:02:33\n",
      "epoch [58/200] batch [1/2] time 0.819 (0.819) data 0.545 (0.545) loss 1.5003 (1.5003) acc 78.1250 (78.1250) lr 1.6374e+01 eta 0:03:53\n",
      "epoch [58/200] batch [2/2] time 0.279 (0.549) data 0.001 (0.273) loss 2.5138 (2.0071) acc 40.6250 (59.3750) lr 1.6252e+01 eta 0:02:35\n",
      "epoch [59/200] batch [1/2] time 1.035 (1.035) data 0.761 (0.761) loss 1.7941 (1.7941) acc 71.8750 (71.8750) lr 1.6252e+01 eta 0:04:52\n",
      "epoch [59/200] batch [2/2] time 0.281 (0.658) data 0.001 (0.381) loss 1.6439 (1.7190) acc 78.1250 (75.0000) lr 1.6129e+01 eta 0:03:05\n",
      "epoch [60/200] batch [1/2] time 1.151 (1.151) data 0.870 (0.870) loss 0.9624 (0.9624) acc 87.5000 (87.5000) lr 1.6129e+01 eta 0:05:23\n",
      "epoch [60/200] batch [2/2] time 0.278 (0.715) data 0.001 (0.435) loss 1.8259 (1.3942) acc 59.3750 (73.4375) lr 1.6004e+01 eta 0:03:20\n",
      "epoch [61/200] batch [1/2] time 0.958 (0.958) data 0.684 (0.684) loss 1.4056 (1.4056) acc 75.0000 (75.0000) lr 1.6004e+01 eta 0:04:27\n",
      "epoch [61/200] batch [2/2] time 0.279 (0.618) data 0.000 (0.342) loss 1.0772 (1.2414) acc 84.3750 (79.6875) lr 1.5878e+01 eta 0:02:51\n",
      "epoch [62/200] batch [1/2] time 0.781 (0.781) data 0.505 (0.505) loss 1.7668 (1.7668) acc 65.6250 (65.6250) lr 1.5878e+01 eta 0:03:36\n",
      "epoch [62/200] batch [2/2] time 0.283 (0.532) data 0.001 (0.253) loss 1.3384 (1.5526) acc 78.1250 (71.8750) lr 1.5750e+01 eta 0:02:26\n",
      "epoch [63/200] batch [1/2] time 0.796 (0.796) data 0.523 (0.523) loss 0.9524 (0.9524) acc 81.2500 (81.2500) lr 1.5750e+01 eta 0:03:38\n",
      "epoch [63/200] batch [2/2] time 0.276 (0.536) data 0.001 (0.262) loss 1.6618 (1.3071) acc 78.1250 (79.6875) lr 1.5621e+01 eta 0:02:26\n",
      "epoch [64/200] batch [1/2] time 0.799 (0.799) data 0.528 (0.528) loss 1.2411 (1.2411) acc 81.2500 (81.2500) lr 1.5621e+01 eta 0:03:38\n",
      "epoch [64/200] batch [2/2] time 0.276 (0.538) data 0.000 (0.264) loss 1.1254 (1.1833) acc 75.0000 (78.1250) lr 1.5490e+01 eta 0:02:26\n",
      "epoch [65/200] batch [1/2] time 0.801 (0.801) data 0.527 (0.527) loss 1.2070 (1.2070) acc 75.0000 (75.0000) lr 1.5490e+01 eta 0:03:37\n",
      "epoch [65/200] batch [2/2] time 0.279 (0.540) data 0.001 (0.264) loss 0.8650 (1.0360) acc 81.2500 (78.1250) lr 1.5358e+01 eta 0:02:25\n",
      "epoch [66/200] batch [1/2] time 0.805 (0.805) data 0.533 (0.533) loss 1.0896 (1.0896) acc 71.8750 (71.8750) lr 1.5358e+01 eta 0:03:36\n",
      "epoch [66/200] batch [2/2] time 0.277 (0.541) data 0.001 (0.267) loss 1.1015 (1.0955) acc 81.2500 (76.5625) lr 1.5225e+01 eta 0:02:25\n",
      "epoch [67/200] batch [1/2] time 0.788 (0.788) data 0.515 (0.515) loss 1.2303 (1.2303) acc 78.1250 (78.1250) lr 1.5225e+01 eta 0:03:30\n",
      "epoch [67/200] batch [2/2] time 0.277 (0.533) data 0.001 (0.258) loss 1.5718 (1.4010) acc 71.8750 (75.0000) lr 1.5090e+01 eta 0:02:21\n",
      "epoch [68/200] batch [1/2] time 0.783 (0.783) data 0.511 (0.511) loss 1.1496 (1.1496) acc 81.2500 (81.2500) lr 1.5090e+01 eta 0:03:27\n",
      "epoch [68/200] batch [2/2] time 0.276 (0.529) data 0.001 (0.256) loss 1.5983 (1.3740) acc 68.7500 (75.0000) lr 1.4955e+01 eta 0:02:19\n",
      "epoch [69/200] batch [1/2] time 0.841 (0.841) data 0.567 (0.567) loss 1.5232 (1.5232) acc 62.5000 (62.5000) lr 1.4955e+01 eta 0:03:41\n",
      "epoch [69/200] batch [2/2] time 0.275 (0.558) data 0.000 (0.284) loss 1.0557 (1.2895) acc 68.7500 (65.6250) lr 1.4818e+01 eta 0:02:26\n",
      "epoch [70/200] batch [1/2] time 1.158 (1.158) data 0.886 (0.886) loss 1.3500 (1.3500) acc 68.7500 (68.7500) lr 1.4818e+01 eta 0:05:02\n",
      "epoch [70/200] batch [2/2] time 0.286 (0.722) data 0.001 (0.443) loss 1.4433 (1.3966) acc 75.0000 (71.8750) lr 1.4679e+01 eta 0:03:07\n",
      "epoch [71/200] batch [1/2] time 1.198 (1.198) data 0.925 (0.925) loss 0.8225 (0.8225) acc 90.6250 (90.6250) lr 1.4679e+01 eta 0:05:10\n",
      "epoch [71/200] batch [2/2] time 0.279 (0.739) data 0.001 (0.463) loss 1.2834 (1.0530) acc 68.7500 (79.6875) lr 1.4540e+01 eta 0:03:10\n",
      "epoch [72/200] batch [1/2] time 0.885 (0.885) data 0.612 (0.612) loss 1.4120 (1.4120) acc 65.6250 (65.6250) lr 1.4540e+01 eta 0:03:47\n",
      "epoch [72/200] batch [2/2] time 0.277 (0.581) data 0.001 (0.306) loss 1.2271 (1.3195) acc 71.8750 (68.7500) lr 1.4399e+01 eta 0:02:28\n",
      "epoch [73/200] batch [1/2] time 0.801 (0.801) data 0.526 (0.526) loss 1.8424 (1.8424) acc 68.7500 (68.7500) lr 1.4399e+01 eta 0:03:24\n",
      "epoch [73/200] batch [2/2] time 0.279 (0.540) data 0.001 (0.264) loss 1.1136 (1.4780) acc 75.0000 (71.8750) lr 1.4258e+01 eta 0:02:17\n",
      "epoch [74/200] batch [1/2] time 0.804 (0.804) data 0.531 (0.531) loss 1.2882 (1.2882) acc 78.1250 (78.1250) lr 1.4258e+01 eta 0:03:23\n",
      "epoch [74/200] batch [2/2] time 0.278 (0.541) data 0.001 (0.266) loss 1.6389 (1.4635) acc 59.3750 (68.7500) lr 1.4115e+01 eta 0:02:16\n",
      "epoch [75/200] batch [1/2] time 0.784 (0.784) data 0.515 (0.515) loss 1.1834 (1.1834) acc 78.1250 (78.1250) lr 1.4115e+01 eta 0:03:16\n",
      "epoch [75/200] batch [2/2] time 0.275 (0.530) data 0.001 (0.258) loss 1.4438 (1.3136) acc 62.5000 (70.3125) lr 1.3971e+01 eta 0:02:12\n",
      "epoch [76/200] batch [1/2] time 0.779 (0.779) data 0.509 (0.509) loss 1.1406 (1.1406) acc 84.3750 (84.3750) lr 1.3971e+01 eta 0:03:14\n",
      "epoch [76/200] batch [2/2] time 0.275 (0.527) data 0.001 (0.255) loss 1.5382 (1.3394) acc 65.6250 (75.0000) lr 1.3827e+01 eta 0:02:10\n",
      "epoch [77/200] batch [1/2] time 0.813 (0.813) data 0.542 (0.542) loss 1.6540 (1.6540) acc 71.8750 (71.8750) lr 1.3827e+01 eta 0:03:20\n",
      "epoch [77/200] batch [2/2] time 0.276 (0.544) data 0.001 (0.271) loss 1.3454 (1.4997) acc 75.0000 (73.4375) lr 1.3681e+01 eta 0:02:13\n",
      "epoch [78/200] batch [1/2] time 0.792 (0.792) data 0.520 (0.520) loss 0.8402 (0.8402) acc 84.3750 (84.3750) lr 1.3681e+01 eta 0:03:13\n",
      "epoch [78/200] batch [2/2] time 0.278 (0.535) data 0.000 (0.260) loss 1.1388 (0.9895) acc 87.5000 (85.9375) lr 1.3535e+01 eta 0:02:10\n",
      "epoch [79/200] batch [1/2] time 0.792 (0.792) data 0.517 (0.517) loss 1.0826 (1.0826) acc 75.0000 (75.0000) lr 1.3535e+01 eta 0:03:12\n",
      "epoch [79/200] batch [2/2] time 0.278 (0.535) data 0.000 (0.259) loss 0.9692 (1.0259) acc 90.6250 (82.8125) lr 1.3387e+01 eta 0:02:09\n",
      "epoch [80/200] batch [1/2] time 1.007 (1.007) data 0.734 (0.734) loss 1.2016 (1.2016) acc 75.0000 (75.0000) lr 1.3387e+01 eta 0:04:02\n",
      "epoch [80/200] batch [2/2] time 0.276 (0.642) data 0.001 (0.367) loss 1.3659 (1.2837) acc 71.8750 (73.4375) lr 1.3239e+01 eta 0:02:33\n",
      "epoch [81/200] batch [1/2] time 1.087 (1.087) data 0.813 (0.813) loss 0.7098 (0.7098) acc 96.8750 (96.8750) lr 1.3239e+01 eta 0:04:19\n",
      "epoch [81/200] batch [2/2] time 0.279 (0.683) data 0.001 (0.407) loss 1.3648 (1.0373) acc 75.0000 (85.9375) lr 1.3090e+01 eta 0:02:42\n",
      "epoch [82/200] batch [1/2] time 1.098 (1.098) data 0.823 (0.823) loss 0.8973 (0.8973) acc 84.3750 (84.3750) lr 1.3090e+01 eta 0:04:20\n",
      "epoch [82/200] batch [2/2] time 0.278 (0.688) data 0.001 (0.412) loss 0.9663 (0.9318) acc 78.1250 (81.2500) lr 1.2940e+01 eta 0:02:42\n",
      "epoch [83/200] batch [1/2] time 0.844 (0.844) data 0.569 (0.569) loss 0.9522 (0.9522) acc 84.3750 (84.3750) lr 1.2940e+01 eta 0:03:18\n",
      "epoch [83/200] batch [2/2] time 0.277 (0.561) data 0.000 (0.285) loss 0.9593 (0.9558) acc 84.3750 (84.3750) lr 1.2790e+01 eta 0:02:11\n",
      "epoch [84/200] batch [1/2] time 0.791 (0.791) data 0.519 (0.519) loss 1.0297 (1.0297) acc 78.1250 (78.1250) lr 1.2790e+01 eta 0:03:04\n",
      "epoch [84/200] batch [2/2] time 0.279 (0.535) data 0.001 (0.260) loss 0.5985 (0.8141) acc 100.0000 (89.0625) lr 1.2639e+01 eta 0:02:04\n",
      "epoch [85/200] batch [1/2] time 0.810 (0.810) data 0.538 (0.538) loss 0.7629 (0.7629) acc 87.5000 (87.5000) lr 1.2639e+01 eta 0:03:07\n",
      "epoch [85/200] batch [2/2] time 0.279 (0.544) data 0.001 (0.269) loss 0.9233 (0.8431) acc 84.3750 (85.9375) lr 1.2487e+01 eta 0:02:05\n",
      "epoch [86/200] batch [1/2] time 0.801 (0.801) data 0.527 (0.527) loss 1.2340 (1.2340) acc 75.0000 (75.0000) lr 1.2487e+01 eta 0:03:03\n",
      "epoch [86/200] batch [2/2] time 0.278 (0.539) data 0.001 (0.264) loss 1.2045 (1.2192) acc 75.0000 (75.0000) lr 1.2334e+01 eta 0:02:02\n",
      "epoch [87/200] batch [1/2] time 0.780 (0.780) data 0.505 (0.505) loss 0.8569 (0.8569) acc 84.3750 (84.3750) lr 1.2334e+01 eta 0:02:56\n",
      "epoch [87/200] batch [2/2] time 0.277 (0.528) data 0.000 (0.253) loss 0.8343 (0.8456) acc 87.5000 (85.9375) lr 1.2181e+01 eta 0:01:59\n",
      "epoch [88/200] batch [1/2] time 0.790 (0.790) data 0.517 (0.517) loss 0.6654 (0.6654) acc 93.7500 (93.7500) lr 1.2181e+01 eta 0:02:57\n",
      "epoch [88/200] batch [2/2] time 0.279 (0.534) data 0.001 (0.259) loss 0.8979 (0.7817) acc 78.1250 (85.9375) lr 1.2028e+01 eta 0:01:59\n",
      "epoch [89/200] batch [1/2] time 0.800 (0.800) data 0.522 (0.522) loss 2.0506 (2.0506) acc 50.0000 (50.0000) lr 1.2028e+01 eta 0:02:58\n",
      "epoch [89/200] batch [2/2] time 0.279 (0.539) data 0.000 (0.261) loss 1.1993 (1.6249) acc 78.1250 (64.0625) lr 1.1874e+01 eta 0:01:59\n",
      "epoch [90/200] batch [1/2] time 0.802 (0.802) data 0.529 (0.529) loss 0.9525 (0.9525) acc 78.1250 (78.1250) lr 1.1874e+01 eta 0:02:57\n",
      "epoch [90/200] batch [2/2] time 0.281 (0.542) data 0.000 (0.265) loss 0.9203 (0.9364) acc 81.2500 (79.6875) lr 1.1719e+01 eta 0:01:59\n",
      "epoch [91/200] batch [1/2] time 1.024 (1.024) data 0.748 (0.748) loss 1.3461 (1.3461) acc 71.8750 (71.8750) lr 1.1719e+01 eta 0:03:44\n",
      "epoch [91/200] batch [2/2] time 0.279 (0.652) data 0.001 (0.374) loss 1.2428 (1.2944) acc 68.7500 (70.3125) lr 1.1564e+01 eta 0:02:22\n",
      "epoch [92/200] batch [1/2] time 1.118 (1.118) data 0.845 (0.845) loss 0.7651 (0.7651) acc 93.7500 (93.7500) lr 1.1564e+01 eta 0:04:02\n",
      "epoch [92/200] batch [2/2] time 0.281 (0.700) data 0.001 (0.423) loss 1.5770 (1.1710) acc 65.6250 (79.6875) lr 1.1409e+01 eta 0:02:31\n",
      "epoch [93/200] batch [1/2] time 1.171 (1.171) data 0.896 (0.896) loss 1.4278 (1.4278) acc 68.7500 (68.7500) lr 1.1409e+01 eta 0:04:11\n",
      "epoch [93/200] batch [2/2] time 0.280 (0.725) data 0.000 (0.448) loss 0.9942 (1.2110) acc 78.1250 (73.4375) lr 1.1253e+01 eta 0:02:35\n",
      "epoch [94/200] batch [1/2] time 0.801 (0.801) data 0.524 (0.524) loss 1.2357 (1.2357) acc 75.0000 (75.0000) lr 1.1253e+01 eta 0:02:50\n",
      "epoch [94/200] batch [2/2] time 0.277 (0.539) data 0.000 (0.262) loss 0.9013 (1.0685) acc 84.3750 (79.6875) lr 1.1097e+01 eta 0:01:54\n",
      "epoch [95/200] batch [1/2] time 0.810 (0.810) data 0.534 (0.534) loss 0.9880 (0.9880) acc 78.1250 (78.1250) lr 1.1097e+01 eta 0:02:50\n",
      "epoch [95/200] batch [2/2] time 0.278 (0.544) data 0.001 (0.267) loss 1.1030 (1.0455) acc 75.0000 (76.5625) lr 1.0941e+01 eta 0:01:54\n",
      "epoch [96/200] batch [1/2] time 0.831 (0.831) data 0.554 (0.554) loss 0.6759 (0.6759) acc 93.7500 (93.7500) lr 1.0941e+01 eta 0:02:53\n",
      "epoch [96/200] batch [2/2] time 0.279 (0.555) data 0.000 (0.277) loss 1.7883 (1.2321) acc 59.3750 (76.5625) lr 1.0785e+01 eta 0:01:55\n",
      "epoch [97/200] batch [1/2] time 0.800 (0.800) data 0.527 (0.527) loss 1.5716 (1.5716) acc 75.0000 (75.0000) lr 1.0785e+01 eta 0:02:45\n",
      "epoch [97/200] batch [2/2] time 0.282 (0.541) data 0.000 (0.264) loss 1.1164 (1.3440) acc 75.0000 (75.0000) lr 1.0628e+01 eta 0:01:51\n",
      "epoch [98/200] batch [1/2] time 0.895 (0.895) data 0.623 (0.623) loss 0.8573 (0.8573) acc 81.2500 (81.2500) lr 1.0628e+01 eta 0:03:03\n",
      "epoch [98/200] batch [2/2] time 0.278 (0.587) data 0.000 (0.312) loss 1.3624 (1.1099) acc 75.0000 (78.1250) lr 1.0471e+01 eta 0:01:59\n",
      "epoch [99/200] batch [1/2] time 0.807 (0.807) data 0.534 (0.534) loss 1.0340 (1.0340) acc 78.1250 (78.1250) lr 1.0471e+01 eta 0:02:43\n",
      "epoch [99/200] batch [2/2] time 0.279 (0.543) data 0.000 (0.267) loss 1.2386 (1.1363) acc 75.0000 (76.5625) lr 1.0314e+01 eta 0:01:49\n",
      "epoch [100/200] batch [1/2] time 0.787 (0.787) data 0.510 (0.510) loss 1.0874 (1.0874) acc 71.8750 (71.8750) lr 1.0314e+01 eta 0:02:38\n",
      "epoch [100/200] batch [2/2] time 0.279 (0.533) data 0.000 (0.255) loss 0.8436 (0.9655) acc 78.1250 (75.0000) lr 1.0157e+01 eta 0:01:46\n",
      "epoch [101/200] batch [1/2] time 0.928 (0.928) data 0.657 (0.657) loss 0.6783 (0.6783) acc 87.5000 (87.5000) lr 1.0157e+01 eta 0:03:04\n",
      "epoch [101/200] batch [2/2] time 0.288 (0.608) data 0.001 (0.329) loss 0.8020 (0.7401) acc 81.2500 (84.3750) lr 1.0000e+01 eta 0:02:00\n",
      "epoch [102/200] batch [1/2] time 1.156 (1.156) data 0.882 (0.882) loss 0.6887 (0.6887) acc 90.6250 (90.6250) lr 1.0000e+01 eta 0:03:47\n",
      "epoch [102/200] batch [2/2] time 0.280 (0.718) data 0.001 (0.441) loss 0.6407 (0.6647) acc 90.6250 (90.6250) lr 9.8429e+00 eta 0:02:20\n",
      "epoch [103/200] batch [1/2] time 1.171 (1.171) data 0.874 (0.874) loss 0.6956 (0.6956) acc 87.5000 (87.5000) lr 9.8429e+00 eta 0:03:48\n",
      "epoch [103/200] batch [2/2] time 0.283 (0.727) data 0.001 (0.437) loss 0.5491 (0.6223) acc 93.7500 (90.6250) lr 9.6859e+00 eta 0:02:21\n",
      "epoch [104/200] batch [1/2] time 0.870 (0.870) data 0.596 (0.596) loss 0.5480 (0.5480) acc 90.6250 (90.6250) lr 9.6859e+00 eta 0:02:47\n",
      "epoch [104/200] batch [2/2] time 0.279 (0.574) data 0.001 (0.299) loss 0.7490 (0.6485) acc 87.5000 (89.0625) lr 9.5289e+00 eta 0:01:50\n",
      "epoch [105/200] batch [1/2] time 0.789 (0.789) data 0.512 (0.512) loss 0.5193 (0.5193) acc 93.7500 (93.7500) lr 9.5289e+00 eta 0:02:30\n",
      "epoch [105/200] batch [2/2] time 0.279 (0.534) data 0.001 (0.256) loss 0.5971 (0.5582) acc 90.6250 (92.1875) lr 9.3721e+00 eta 0:01:41\n",
      "epoch [106/200] batch [1/2] time 0.800 (0.800) data 0.523 (0.523) loss 0.8862 (0.8862) acc 87.5000 (87.5000) lr 9.3721e+00 eta 0:02:31\n",
      "epoch [106/200] batch [2/2] time 0.278 (0.539) data 0.001 (0.262) loss 0.6256 (0.7559) acc 90.6250 (89.0625) lr 9.2154e+00 eta 0:01:41\n",
      "epoch [107/200] batch [1/2] time 0.799 (0.799) data 0.523 (0.523) loss 0.6924 (0.6924) acc 84.3750 (84.3750) lr 9.2154e+00 eta 0:02:29\n",
      "epoch [107/200] batch [2/2] time 0.278 (0.538) data 0.001 (0.262) loss 0.7139 (0.7032) acc 87.5000 (85.9375) lr 9.0589e+00 eta 0:01:40\n",
      "epoch [108/200] batch [1/2] time 0.803 (0.803) data 0.529 (0.529) loss 0.7404 (0.7404) acc 84.3750 (84.3750) lr 9.0589e+00 eta 0:02:28\n",
      "epoch [108/200] batch [2/2] time 0.279 (0.541) data 0.001 (0.265) loss 0.9981 (0.8693) acc 71.8750 (78.1250) lr 8.9027e+00 eta 0:01:39\n",
      "epoch [109/200] batch [1/2] time 0.791 (0.791) data 0.514 (0.514) loss 0.5558 (0.5558) acc 93.7500 (93.7500) lr 8.9027e+00 eta 0:02:24\n",
      "epoch [109/200] batch [2/2] time 0.279 (0.535) data 0.001 (0.257) loss 2.0199 (1.2878) acc 59.3750 (76.5625) lr 8.7467e+00 eta 0:01:37\n",
      "epoch [110/200] batch [1/2] time 0.808 (0.808) data 0.530 (0.530) loss 1.0348 (1.0348) acc 81.2500 (81.2500) lr 8.7467e+00 eta 0:02:26\n",
      "epoch [110/200] batch [2/2] time 0.277 (0.542) data 0.001 (0.265) loss 0.8391 (0.9369) acc 78.1250 (79.6875) lr 8.5910e+00 eta 0:01:37\n",
      "epoch [111/200] batch [1/2] time 0.807 (0.807) data 0.533 (0.533) loss 1.0693 (1.0693) acc 84.3750 (84.3750) lr 8.5910e+00 eta 0:02:24\n",
      "epoch [111/200] batch [2/2] time 0.281 (0.544) data 0.001 (0.267) loss 1.3897 (1.2295) acc 68.7500 (76.5625) lr 8.4357e+00 eta 0:01:36\n",
      "epoch [112/200] batch [1/2] time 1.014 (1.014) data 0.741 (0.741) loss 0.9089 (0.9089) acc 90.6250 (90.6250) lr 8.4357e+00 eta 0:02:59\n",
      "epoch [112/200] batch [2/2] time 0.282 (0.648) data 0.001 (0.371) loss 1.2271 (1.0680) acc 87.5000 (89.0625) lr 8.2807e+00 eta 0:01:54\n",
      "epoch [113/200] batch [1/2] time 1.138 (1.138) data 0.861 (0.861) loss 1.0700 (1.0700) acc 78.1250 (78.1250) lr 8.2807e+00 eta 0:03:19\n",
      "epoch [113/200] batch [2/2] time 0.282 (0.710) data 0.001 (0.431) loss 0.8695 (0.9697) acc 81.2500 (79.6875) lr 8.1262e+00 eta 0:02:03\n",
      "epoch [114/200] batch [1/2] time 1.010 (1.010) data 0.736 (0.736) loss 0.9300 (0.9300) acc 78.1250 (78.1250) lr 8.1262e+00 eta 0:02:54\n",
      "epoch [114/200] batch [2/2] time 0.281 (0.645) data 0.001 (0.368) loss 1.2125 (1.0712) acc 81.2500 (79.6875) lr 7.9721e+00 eta 0:01:51\n",
      "epoch [115/200] batch [1/2] time 0.821 (0.821) data 0.549 (0.549) loss 0.8504 (0.8504) acc 90.6250 (90.6250) lr 7.9721e+00 eta 0:02:20\n",
      "epoch [115/200] batch [2/2] time 0.287 (0.554) data 0.001 (0.275) loss 0.9170 (0.8837) acc 84.3750 (87.5000) lr 7.8186e+00 eta 0:01:34\n",
      "epoch [116/200] batch [1/2] time 0.797 (0.797) data 0.523 (0.523) loss 0.7984 (0.7984) acc 81.2500 (81.2500) lr 7.8186e+00 eta 0:02:14\n",
      "epoch [116/200] batch [2/2] time 0.276 (0.536) data 0.001 (0.262) loss 0.8746 (0.8365) acc 84.3750 (82.8125) lr 7.6655e+00 eta 0:01:30\n",
      "epoch [117/200] batch [1/2] time 0.786 (0.786) data 0.512 (0.512) loss 0.8747 (0.8747) acc 90.6250 (90.6250) lr 7.6655e+00 eta 0:02:11\n",
      "epoch [117/200] batch [2/2] time 0.280 (0.533) data 0.001 (0.257) loss 0.7122 (0.7935) acc 84.3750 (87.5000) lr 7.5131e+00 eta 0:01:28\n",
      "epoch [118/200] batch [1/2] time 0.810 (0.810) data 0.534 (0.534) loss 0.6537 (0.6537) acc 87.5000 (87.5000) lr 7.5131e+00 eta 0:02:13\n",
      "epoch [118/200] batch [2/2] time 0.279 (0.544) data 0.001 (0.267) loss 0.5238 (0.5887) acc 93.7500 (90.6250) lr 7.3613e+00 eta 0:01:29\n",
      "epoch [119/200] batch [1/2] time 0.792 (0.792) data 0.516 (0.516) loss 0.6519 (0.6519) acc 90.6250 (90.6250) lr 7.3613e+00 eta 0:02:09\n",
      "epoch [119/200] batch [2/2] time 0.280 (0.536) data 0.000 (0.258) loss 0.5705 (0.6112) acc 90.6250 (90.6250) lr 7.2101e+00 eta 0:01:26\n",
      "epoch [120/200] batch [1/2] time 0.816 (0.816) data 0.546 (0.546) loss 0.7662 (0.7662) acc 84.3750 (84.3750) lr 7.2101e+00 eta 0:02:11\n",
      "epoch [120/200] batch [2/2] time 0.280 (0.548) data 0.000 (0.273) loss 0.7415 (0.7538) acc 84.3750 (84.3750) lr 7.0596e+00 eta 0:01:27\n",
      "epoch [121/200] batch [1/2] time 0.809 (0.809) data 0.536 (0.536) loss 1.0472 (1.0472) acc 84.3750 (84.3750) lr 7.0596e+00 eta 0:02:08\n",
      "epoch [121/200] batch [2/2] time 0.277 (0.543) data 0.000 (0.268) loss 0.7729 (0.9101) acc 90.6250 (87.5000) lr 6.9098e+00 eta 0:01:25\n",
      "epoch [122/200] batch [1/2] time 0.928 (0.928) data 0.650 (0.650) loss 0.7510 (0.7510) acc 84.3750 (84.3750) lr 6.9098e+00 eta 0:02:25\n",
      "epoch [122/200] batch [2/2] time 0.278 (0.603) data 0.001 (0.325) loss 0.7331 (0.7420) acc 81.2500 (82.8125) lr 6.7608e+00 eta 0:01:34\n",
      "epoch [123/200] batch [1/2] time 1.158 (1.158) data 0.883 (0.883) loss 0.6672 (0.6672) acc 90.6250 (90.6250) lr 6.7608e+00 eta 0:02:59\n",
      "epoch [123/200] batch [2/2] time 0.281 (0.719) data 0.001 (0.442) loss 0.4373 (0.5522) acc 96.8750 (93.7500) lr 6.6126e+00 eta 0:01:50\n",
      "epoch [124/200] batch [1/2] time 1.136 (1.136) data 0.862 (0.862) loss 0.7535 (0.7535) acc 84.3750 (84.3750) lr 6.6126e+00 eta 0:02:53\n",
      "epoch [124/200] batch [2/2] time 0.280 (0.708) data 0.001 (0.431) loss 0.5331 (0.6433) acc 93.7500 (89.0625) lr 6.4653e+00 eta 0:01:47\n",
      "epoch [125/200] batch [1/2] time 0.899 (0.899) data 0.625 (0.625) loss 0.5488 (0.5488) acc 93.7500 (93.7500) lr 6.4653e+00 eta 0:02:15\n",
      "epoch [125/200] batch [2/2] time 0.281 (0.590) data 0.001 (0.313) loss 0.6673 (0.6080) acc 84.3750 (89.0625) lr 6.3188e+00 eta 0:01:28\n",
      "epoch [126/200] batch [1/2] time 0.795 (0.795) data 0.522 (0.522) loss 0.7222 (0.7222) acc 87.5000 (87.5000) lr 6.3188e+00 eta 0:01:58\n",
      "epoch [126/200] batch [2/2] time 0.274 (0.535) data 0.000 (0.261) loss 0.4426 (0.5824) acc 96.8750 (92.1875) lr 6.1732e+00 eta 0:01:19\n",
      "epoch [127/200] batch [1/2] time 0.781 (0.781) data 0.508 (0.508) loss 1.1254 (1.1254) acc 75.0000 (75.0000) lr 6.1732e+00 eta 0:01:54\n",
      "epoch [127/200] batch [2/2] time 0.279 (0.530) data 0.000 (0.254) loss 0.8019 (0.9637) acc 81.2500 (78.1250) lr 6.0285e+00 eta 0:01:17\n",
      "epoch [128/200] batch [1/2] time 0.815 (0.815) data 0.542 (0.542) loss 1.0263 (1.0263) acc 78.1250 (78.1250) lr 6.0285e+00 eta 0:01:58\n",
      "epoch [128/200] batch [2/2] time 0.279 (0.547) data 0.000 (0.271) loss 0.6174 (0.8218) acc 87.5000 (82.8125) lr 5.8849e+00 eta 0:01:18\n",
      "epoch [129/200] batch [1/2] time 0.803 (0.803) data 0.531 (0.531) loss 0.5942 (0.5942) acc 96.8750 (96.8750) lr 5.8849e+00 eta 0:01:54\n",
      "epoch [129/200] batch [2/2] time 0.276 (0.539) data 0.001 (0.266) loss 0.7973 (0.6958) acc 81.2500 (89.0625) lr 5.7422e+00 eta 0:01:16\n",
      "epoch [130/200] batch [1/2] time 0.800 (0.800) data 0.530 (0.530) loss 0.7431 (0.7431) acc 90.6250 (90.6250) lr 5.7422e+00 eta 0:01:52\n",
      "epoch [130/200] batch [2/2] time 0.277 (0.539) data 0.000 (0.265) loss 0.9848 (0.8640) acc 81.2500 (85.9375) lr 5.6006e+00 eta 0:01:15\n",
      "epoch [131/200] batch [1/2] time 0.795 (0.795) data 0.522 (0.522) loss 0.5463 (0.5463) acc 96.8750 (96.8750) lr 5.6006e+00 eta 0:01:50\n",
      "epoch [131/200] batch [2/2] time 0.280 (0.537) data 0.000 (0.261) loss 0.5510 (0.5486) acc 93.7500 (95.3125) lr 5.4601e+00 eta 0:01:14\n",
      "epoch [132/200] batch [1/2] time 0.782 (0.782) data 0.510 (0.510) loss 0.8912 (0.8912) acc 81.2500 (81.2500) lr 5.4601e+00 eta 0:01:47\n",
      "epoch [132/200] batch [2/2] time 0.279 (0.530) data 0.001 (0.255) loss 0.7786 (0.8349) acc 84.3750 (82.8125) lr 5.3207e+00 eta 0:01:12\n",
      "epoch [133/200] batch [1/2] time 0.987 (0.987) data 0.716 (0.716) loss 0.5715 (0.5715) acc 96.8750 (96.8750) lr 5.3207e+00 eta 0:02:13\n",
      "epoch [133/200] batch [2/2] time 0.284 (0.636) data 0.001 (0.359) loss 0.7340 (0.6528) acc 84.3750 (90.6250) lr 5.1825e+00 eta 0:01:25\n",
      "epoch [134/200] batch [1/2] time 1.136 (1.136) data 0.864 (0.864) loss 0.5952 (0.5952) acc 87.5000 (87.5000) lr 5.1825e+00 eta 0:02:31\n",
      "epoch [134/200] batch [2/2] time 0.277 (0.706) data 0.001 (0.433) loss 0.6069 (0.6011) acc 93.7500 (90.6250) lr 5.0454e+00 eta 0:01:33\n",
      "epoch [135/200] batch [1/2] time 1.034 (1.034) data 0.733 (0.733) loss 0.6977 (0.6977) acc 90.6250 (90.6250) lr 5.0454e+00 eta 0:02:15\n",
      "epoch [135/200] batch [2/2] time 0.279 (0.657) data 0.001 (0.367) loss 0.7326 (0.7152) acc 87.5000 (89.0625) lr 4.9096e+00 eta 0:01:25\n",
      "epoch [136/200] batch [1/2] time 0.850 (0.850) data 0.577 (0.577) loss 0.7095 (0.7095) acc 90.6250 (90.6250) lr 4.9096e+00 eta 0:01:49\n",
      "epoch [136/200] batch [2/2] time 0.279 (0.565) data 0.000 (0.289) loss 0.7659 (0.7377) acc 90.6250 (90.6250) lr 4.7750e+00 eta 0:01:12\n",
      "epoch [137/200] batch [1/2] time 0.794 (0.794) data 0.521 (0.521) loss 0.5672 (0.5672) acc 96.8750 (96.8750) lr 4.7750e+00 eta 0:01:40\n",
      "epoch [137/200] batch [2/2] time 0.278 (0.536) data 0.001 (0.261) loss 0.5941 (0.5807) acc 90.6250 (93.7500) lr 4.6417e+00 eta 0:01:07\n",
      "epoch [138/200] batch [1/2] time 0.855 (0.855) data 0.582 (0.582) loss 0.5299 (0.5299) acc 90.6250 (90.6250) lr 4.6417e+00 eta 0:01:46\n",
      "epoch [138/200] batch [2/2] time 0.276 (0.566) data 0.001 (0.291) loss 0.6181 (0.5740) acc 90.6250 (90.6250) lr 4.5098e+00 eta 0:01:10\n",
      "epoch [139/200] batch [1/2] time 0.796 (0.796) data 0.521 (0.521) loss 0.8036 (0.8036) acc 78.1250 (78.1250) lr 4.5098e+00 eta 0:01:37\n",
      "epoch [139/200] batch [2/2] time 0.279 (0.537) data 0.000 (0.261) loss 0.9365 (0.8700) acc 90.6250 (84.3750) lr 4.3792e+00 eta 0:01:05\n",
      "epoch [140/200] batch [1/2] time 0.811 (0.811) data 0.543 (0.543) loss 0.3759 (0.3759) acc 96.8750 (96.8750) lr 4.3792e+00 eta 0:01:38\n",
      "epoch [140/200] batch [2/2] time 0.280 (0.545) data 0.000 (0.272) loss 0.7752 (0.5756) acc 81.2500 (89.0625) lr 4.2499e+00 eta 0:01:05\n",
      "epoch [141/200] batch [1/2] time 0.836 (0.836) data 0.565 (0.565) loss 0.4014 (0.4014) acc 96.8750 (96.8750) lr 4.2499e+00 eta 0:01:39\n",
      "epoch [141/200] batch [2/2] time 0.279 (0.558) data 0.000 (0.283) loss 0.8724 (0.6369) acc 84.3750 (90.6250) lr 4.1221e+00 eta 0:01:05\n",
      "epoch [142/200] batch [1/2] time 0.784 (0.784) data 0.513 (0.513) loss 0.6866 (0.6866) acc 90.6250 (90.6250) lr 4.1221e+00 eta 0:01:31\n",
      "epoch [142/200] batch [2/2] time 0.278 (0.531) data 0.001 (0.257) loss 0.5028 (0.5947) acc 87.5000 (89.0625) lr 3.9958e+00 eta 0:01:01\n",
      "epoch [143/200] batch [1/2] time 0.811 (0.811) data 0.534 (0.534) loss 0.7767 (0.7767) acc 84.3750 (84.3750) lr 3.9958e+00 eta 0:01:33\n",
      "epoch [143/200] batch [2/2] time 0.277 (0.544) data 0.001 (0.267) loss 0.4880 (0.6324) acc 96.8750 (90.6250) lr 3.8709e+00 eta 0:01:02\n",
      "epoch [144/200] batch [1/2] time 1.112 (1.112) data 0.839 (0.839) loss 0.5820 (0.5820) acc 87.5000 (87.5000) lr 3.8709e+00 eta 0:02:05\n",
      "epoch [144/200] batch [2/2] time 0.282 (0.697) data 0.001 (0.420) loss 0.4556 (0.5188) acc 93.7500 (90.6250) lr 3.7476e+00 eta 0:01:18\n",
      "epoch [145/200] batch [1/2] time 1.163 (1.163) data 0.887 (0.887) loss 0.4421 (0.4421) acc 90.6250 (90.6250) lr 3.7476e+00 eta 0:02:09\n",
      "epoch [145/200] batch [2/2] time 0.280 (0.722) data 0.001 (0.444) loss 0.5500 (0.4960) acc 90.6250 (90.6250) lr 3.6258e+00 eta 0:01:19\n",
      "epoch [146/200] batch [1/2] time 0.946 (0.946) data 0.672 (0.672) loss 0.6271 (0.6271) acc 93.7500 (93.7500) lr 3.6258e+00 eta 0:01:43\n",
      "epoch [146/200] batch [2/2] time 0.277 (0.612) data 0.000 (0.336) loss 0.4968 (0.5619) acc 93.7500 (93.7500) lr 3.5055e+00 eta 0:01:06\n",
      "epoch [147/200] batch [1/2] time 0.773 (0.773) data 0.501 (0.501) loss 0.6006 (0.6006) acc 90.6250 (90.6250) lr 3.5055e+00 eta 0:01:22\n",
      "epoch [147/200] batch [2/2] time 0.279 (0.526) data 0.000 (0.251) loss 0.4344 (0.5175) acc 90.6250 (90.6250) lr 3.3869e+00 eta 0:00:55\n",
      "epoch [148/200] batch [1/2] time 0.787 (0.787) data 0.515 (0.515) loss 0.4780 (0.4780) acc 93.7500 (93.7500) lr 3.3869e+00 eta 0:01:22\n",
      "epoch [148/200] batch [2/2] time 0.279 (0.533) data 0.000 (0.258) loss 0.5148 (0.4964) acc 90.6250 (92.1875) lr 3.2699e+00 eta 0:00:55\n",
      "epoch [149/200] batch [1/2] time 0.816 (0.816) data 0.545 (0.545) loss 0.4387 (0.4387) acc 100.0000 (100.0000) lr 3.2699e+00 eta 0:01:24\n",
      "epoch [149/200] batch [2/2] time 0.276 (0.546) data 0.000 (0.273) loss 0.6722 (0.5555) acc 84.3750 (92.1875) lr 3.1545e+00 eta 0:00:55\n",
      "epoch [150/200] batch [1/2] time 0.783 (0.783) data 0.513 (0.513) loss 0.3435 (0.3435) acc 93.7500 (93.7500) lr 3.1545e+00 eta 0:01:19\n",
      "epoch [150/200] batch [2/2] time 0.274 (0.528) data 0.000 (0.257) loss 0.6556 (0.4995) acc 87.5000 (90.6250) lr 3.0409e+00 eta 0:00:52\n",
      "epoch [151/200] batch [1/2] time 0.789 (0.789) data 0.515 (0.515) loss 0.4046 (0.4046) acc 96.8750 (96.8750) lr 3.0409e+00 eta 0:01:18\n",
      "epoch [151/200] batch [2/2] time 0.276 (0.533) data 0.000 (0.258) loss 0.4426 (0.4236) acc 96.8750 (96.8750) lr 2.9289e+00 eta 0:00:52\n",
      "epoch [152/200] batch [1/2] time 0.804 (0.804) data 0.532 (0.532) loss 0.4054 (0.4054) acc 100.0000 (100.0000) lr 2.9289e+00 eta 0:01:18\n",
      "epoch [152/200] batch [2/2] time 0.280 (0.542) data 0.000 (0.266) loss 0.4121 (0.4088) acc 96.8750 (98.4375) lr 2.8187e+00 eta 0:00:52\n",
      "epoch [153/200] batch [1/2] time 0.798 (0.798) data 0.526 (0.526) loss 0.3836 (0.3836) acc 96.8750 (96.8750) lr 2.8187e+00 eta 0:01:15\n",
      "epoch [153/200] batch [2/2] time 0.278 (0.538) data 0.001 (0.263) loss 0.6627 (0.5231) acc 90.6250 (93.7500) lr 2.7103e+00 eta 0:00:50\n",
      "epoch [154/200] batch [1/2] time 0.901 (0.901) data 0.626 (0.626) loss 0.3920 (0.3920) acc 93.7500 (93.7500) lr 2.7103e+00 eta 0:01:23\n",
      "epoch [154/200] batch [2/2] time 0.277 (0.589) data 0.001 (0.314) loss 0.3930 (0.3925) acc 96.8750 (95.3125) lr 2.6037e+00 eta 0:00:54\n",
      "epoch [155/200] batch [1/2] time 1.129 (1.129) data 0.855 (0.855) loss 0.5455 (0.5455) acc 90.6250 (90.6250) lr 2.6037e+00 eta 0:01:42\n",
      "epoch [155/200] batch [2/2] time 0.278 (0.703) data 0.000 (0.428) loss 0.2968 (0.4212) acc 100.0000 (95.3125) lr 2.4989e+00 eta 0:01:03\n",
      "epoch [156/200] batch [1/2] time 1.157 (1.157) data 0.885 (0.885) loss 0.3380 (0.3380) acc 96.8750 (96.8750) lr 2.4989e+00 eta 0:01:42\n",
      "epoch [156/200] batch [2/2] time 0.278 (0.718) data 0.001 (0.443) loss 0.3530 (0.3455) acc 96.8750 (96.8750) lr 2.3959e+00 eta 0:01:03\n",
      "epoch [157/200] batch [1/2] time 0.839 (0.839) data 0.567 (0.567) loss 0.6056 (0.6056) acc 87.5000 (87.5000) lr 2.3959e+00 eta 0:01:13\n",
      "epoch [157/200] batch [2/2] time 0.278 (0.559) data 0.000 (0.284) loss 0.4525 (0.5291) acc 93.7500 (90.6250) lr 2.2949e+00 eta 0:00:48\n",
      "epoch [158/200] batch [1/2] time 0.788 (0.788) data 0.514 (0.514) loss 0.4299 (0.4299) acc 93.7500 (93.7500) lr 2.2949e+00 eta 0:01:07\n",
      "epoch [158/200] batch [2/2] time 0.279 (0.534) data 0.001 (0.257) loss 0.5694 (0.4997) acc 90.6250 (92.1875) lr 2.1957e+00 eta 0:00:44\n",
      "epoch [159/200] batch [1/2] time 0.801 (0.801) data 0.525 (0.525) loss 0.3581 (0.3581) acc 100.0000 (100.0000) lr 2.1957e+00 eta 0:01:06\n",
      "epoch [159/200] batch [2/2] time 0.278 (0.539) data 0.001 (0.263) loss 0.3924 (0.3753) acc 93.7500 (96.8750) lr 2.0984e+00 eta 0:00:44\n",
      "epoch [160/200] batch [1/2] time 0.821 (0.821) data 0.552 (0.552) loss 0.3422 (0.3422) acc 96.8750 (96.8750) lr 2.0984e+00 eta 0:01:06\n",
      "epoch [160/200] batch [2/2] time 0.278 (0.549) data 0.000 (0.276) loss 0.6494 (0.4958) acc 90.6250 (93.7500) lr 2.0032e+00 eta 0:00:43\n",
      "epoch [161/200] batch [1/2] time 0.966 (0.966) data 0.694 (0.694) loss 0.4617 (0.4617) acc 96.8750 (96.8750) lr 2.0032e+00 eta 0:01:16\n",
      "epoch [161/200] batch [2/2] time 0.280 (0.623) data 0.001 (0.347) loss 0.4782 (0.4700) acc 96.8750 (96.8750) lr 1.9098e+00 eta 0:00:48\n",
      "epoch [162/200] batch [1/2] time 0.783 (0.783) data 0.510 (0.510) loss 0.2909 (0.2909) acc 100.0000 (100.0000) lr 1.9098e+00 eta 0:01:00\n",
      "epoch [162/200] batch [2/2] time 0.280 (0.531) data 0.001 (0.255) loss 0.4912 (0.3910) acc 93.7500 (96.8750) lr 1.8185e+00 eta 0:00:40\n",
      "epoch [163/200] batch [1/2] time 0.801 (0.801) data 0.528 (0.528) loss 0.4676 (0.4676) acc 93.7500 (93.7500) lr 1.8185e+00 eta 0:01:00\n",
      "epoch [163/200] batch [2/2] time 0.277 (0.539) data 0.001 (0.264) loss 0.3381 (0.4028) acc 100.0000 (96.8750) lr 1.7292e+00 eta 0:00:39\n",
      "epoch [164/200] batch [1/2] time 0.837 (0.837) data 0.566 (0.566) loss 0.5247 (0.5247) acc 90.6250 (90.6250) lr 1.7292e+00 eta 0:01:01\n",
      "epoch [164/200] batch [2/2] time 0.277 (0.557) data 0.000 (0.283) loss 0.5244 (0.5245) acc 93.7500 (92.1875) lr 1.6419e+00 eta 0:00:40\n",
      "epoch [165/200] batch [1/2] time 1.076 (1.076) data 0.805 (0.805) loss 0.5714 (0.5714) acc 87.5000 (87.5000) lr 1.6419e+00 eta 0:01:16\n",
      "epoch [165/200] batch [2/2] time 0.278 (0.677) data 0.001 (0.403) loss 0.3609 (0.4662) acc 96.8750 (92.1875) lr 1.5567e+00 eta 0:00:47\n",
      "epoch [166/200] batch [1/2] time 1.158 (1.158) data 0.883 (0.883) loss 0.4441 (0.4441) acc 87.5000 (87.5000) lr 1.5567e+00 eta 0:01:19\n",
      "epoch [166/200] batch [2/2] time 0.281 (0.720) data 0.001 (0.442) loss 0.3271 (0.3856) acc 96.8750 (92.1875) lr 1.4736e+00 eta 0:00:48\n",
      "epoch [167/200] batch [1/2] time 0.950 (0.950) data 0.679 (0.679) loss 0.3527 (0.3527) acc 96.8750 (96.8750) lr 1.4736e+00 eta 0:01:03\n",
      "epoch [167/200] batch [2/2] time 0.278 (0.614) data 0.000 (0.340) loss 0.4637 (0.4082) acc 90.6250 (93.7500) lr 1.3926e+00 eta 0:00:40\n",
      "epoch [168/200] batch [1/2] time 0.785 (0.785) data 0.513 (0.513) loss 0.4336 (0.4336) acc 93.7500 (93.7500) lr 1.3926e+00 eta 0:00:51\n",
      "epoch [168/200] batch [2/2] time 0.274 (0.530) data 0.000 (0.257) loss 0.3453 (0.3895) acc 100.0000 (96.8750) lr 1.3137e+00 eta 0:00:33\n",
      "epoch [169/200] batch [1/2] time 0.806 (0.806) data 0.533 (0.533) loss 0.5838 (0.5838) acc 93.7500 (93.7500) lr 1.3137e+00 eta 0:00:50\n",
      "epoch [169/200] batch [2/2] time 0.275 (0.541) data 0.001 (0.267) loss 0.3068 (0.4453) acc 100.0000 (96.8750) lr 1.2369e+00 eta 0:00:33\n",
      "epoch [170/200] batch [1/2] time 0.819 (0.819) data 0.545 (0.545) loss 0.5205 (0.5205) acc 87.5000 (87.5000) lr 1.2369e+00 eta 0:00:49\n",
      "epoch [170/200] batch [2/2] time 0.276 (0.548) data 0.000 (0.273) loss 0.3195 (0.4200) acc 100.0000 (93.7500) lr 1.1623e+00 eta 0:00:32\n",
      "epoch [171/200] batch [1/2] time 0.793 (0.793) data 0.521 (0.521) loss 0.3842 (0.3842) acc 93.7500 (93.7500) lr 1.1623e+00 eta 0:00:46\n",
      "epoch [171/200] batch [2/2] time 0.276 (0.534) data 0.001 (0.261) loss 0.6996 (0.5419) acc 87.5000 (90.6250) lr 1.0899e+00 eta 0:00:30\n",
      "epoch [172/200] batch [1/2] time 0.820 (0.820) data 0.548 (0.548) loss 0.3121 (0.3121) acc 100.0000 (100.0000) lr 1.0899e+00 eta 0:00:46\n",
      "epoch [172/200] batch [2/2] time 0.275 (0.547) data 0.000 (0.274) loss 0.4037 (0.3579) acc 96.8750 (98.4375) lr 1.0197e+00 eta 0:00:30\n",
      "epoch [173/200] batch [1/2] time 0.795 (0.795) data 0.525 (0.525) loss 0.3353 (0.3353) acc 96.8750 (96.8750) lr 1.0197e+00 eta 0:00:43\n",
      "epoch [173/200] batch [2/2] time 0.277 (0.536) data 0.001 (0.263) loss 0.3142 (0.3248) acc 96.8750 (96.8750) lr 9.5173e-01 eta 0:00:28\n",
      "epoch [174/200] batch [1/2] time 0.809 (0.809) data 0.536 (0.536) loss 0.3356 (0.3356) acc 96.8750 (96.8750) lr 9.5173e-01 eta 0:00:42\n",
      "epoch [174/200] batch [2/2] time 0.277 (0.543) data 0.001 (0.268) loss 0.3816 (0.3586) acc 96.8750 (96.8750) lr 8.8597e-01 eta 0:00:28\n",
      "epoch [175/200] batch [1/2] time 0.967 (0.967) data 0.690 (0.690) loss 0.2919 (0.2919) acc 100.0000 (100.0000) lr 8.8597e-01 eta 0:00:49\n",
      "epoch [175/200] batch [2/2] time 0.277 (0.622) data 0.001 (0.345) loss 0.3292 (0.3105) acc 93.7500 (96.8750) lr 8.2245e-01 eta 0:00:31\n",
      "epoch [176/200] batch [1/2] time 1.105 (1.105) data 0.832 (0.832) loss 0.3247 (0.3247) acc 96.8750 (96.8750) lr 8.2245e-01 eta 0:00:54\n",
      "epoch [176/200] batch [2/2] time 0.279 (0.692) data 0.000 (0.416) loss 0.5981 (0.4614) acc 87.5000 (92.1875) lr 7.6120e-01 eta 0:00:33\n",
      "epoch [177/200] batch [1/2] time 1.114 (1.114) data 0.824 (0.824) loss 0.3529 (0.3529) acc 96.8750 (96.8750) lr 7.6120e-01 eta 0:00:52\n",
      "epoch [177/200] batch [2/2] time 0.278 (0.696) data 0.001 (0.412) loss 0.3147 (0.3338) acc 96.8750 (96.8750) lr 7.0224e-01 eta 0:00:32\n",
      "epoch [178/200] batch [1/2] time 0.874 (0.874) data 0.602 (0.602) loss 0.3281 (0.3281) acc 93.7500 (93.7500) lr 7.0224e-01 eta 0:00:39\n",
      "epoch [178/200] batch [2/2] time 0.274 (0.574) data 0.000 (0.301) loss 0.3626 (0.3454) acc 96.8750 (95.3125) lr 6.4556e-01 eta 0:00:25\n",
      "epoch [179/200] batch [1/2] time 0.783 (0.783) data 0.509 (0.509) loss 0.3650 (0.3650) acc 96.8750 (96.8750) lr 6.4556e-01 eta 0:00:33\n",
      "epoch [179/200] batch [2/2] time 0.275 (0.529) data 0.000 (0.255) loss 0.2503 (0.3077) acc 100.0000 (98.4375) lr 5.9119e-01 eta 0:00:22\n",
      "epoch [180/200] batch [1/2] time 0.810 (0.810) data 0.540 (0.540) loss 0.2421 (0.2421) acc 100.0000 (100.0000) lr 5.9119e-01 eta 0:00:33\n",
      "epoch [180/200] batch [2/2] time 0.273 (0.542) data 0.000 (0.270) loss 0.3220 (0.2820) acc 96.8750 (98.4375) lr 5.3915e-01 eta 0:00:21\n",
      "epoch [181/200] batch [1/2] time 0.807 (0.807) data 0.535 (0.535) loss 0.2974 (0.2974) acc 100.0000 (100.0000) lr 5.3915e-01 eta 0:00:31\n",
      "epoch [181/200] batch [2/2] time 0.276 (0.541) data 0.001 (0.268) loss 0.4507 (0.3741) acc 87.5000 (93.7500) lr 4.8943e-01 eta 0:00:20\n",
      "epoch [182/200] batch [1/2] time 0.780 (0.780) data 0.508 (0.508) loss 0.3066 (0.3066) acc 96.8750 (96.8750) lr 4.8943e-01 eta 0:00:28\n",
      "epoch [182/200] batch [2/2] time 0.275 (0.527) data 0.000 (0.254) loss 0.2445 (0.2756) acc 100.0000 (98.4375) lr 4.4207e-01 eta 0:00:18\n",
      "epoch [183/200] batch [1/2] time 0.782 (0.782) data 0.511 (0.511) loss 0.2717 (0.2717) acc 100.0000 (100.0000) lr 4.4207e-01 eta 0:00:27\n",
      "epoch [183/200] batch [2/2] time 0.275 (0.529) data 0.001 (0.256) loss 0.2806 (0.2762) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:17\n",
      "epoch [184/200] batch [1/2] time 0.809 (0.809) data 0.536 (0.536) loss 0.3867 (0.3867) acc 96.8750 (96.8750) lr 3.9706e-01 eta 0:00:26\n",
      "epoch [184/200] batch [2/2] time 0.274 (0.542) data 0.000 (0.268) loss 0.2949 (0.3408) acc 96.8750 (96.8750) lr 3.5443e-01 eta 0:00:17\n",
      "epoch [185/200] batch [1/2] time 0.812 (0.812) data 0.542 (0.542) loss 0.3469 (0.3469) acc 96.8750 (96.8750) lr 3.5443e-01 eta 0:00:25\n",
      "epoch [185/200] batch [2/2] time 0.276 (0.544) data 0.001 (0.271) loss 0.2815 (0.3142) acc 96.8750 (96.8750) lr 3.1417e-01 eta 0:00:16\n",
      "epoch [186/200] batch [1/2] time 1.038 (1.038) data 0.764 (0.764) loss 0.2869 (0.2869) acc 96.8750 (96.8750) lr 3.1417e-01 eta 0:00:30\n",
      "epoch [186/200] batch [2/2] time 0.278 (0.658) data 0.000 (0.382) loss 0.3481 (0.3175) acc 96.8750 (96.8750) lr 2.7630e-01 eta 0:00:18\n",
      "epoch [187/200] batch [1/2] time 1.167 (1.167) data 0.893 (0.893) loss 0.3263 (0.3263) acc 93.7500 (93.7500) lr 2.7630e-01 eta 0:00:31\n",
      "epoch [187/200] batch [2/2] time 0.280 (0.723) data 0.001 (0.447) loss 0.4521 (0.3892) acc 93.7500 (93.7500) lr 2.4083e-01 eta 0:00:18\n",
      "epoch [188/200] batch [1/2] time 0.987 (0.987) data 0.717 (0.717) loss 0.2784 (0.2784) acc 96.8750 (96.8750) lr 2.4083e-01 eta 0:00:24\n",
      "epoch [188/200] batch [2/2] time 0.277 (0.632) data 0.001 (0.359) loss 0.2567 (0.2676) acc 96.8750 (96.8750) lr 2.0777e-01 eta 0:00:15\n",
      "epoch [189/200] batch [1/2] time 0.791 (0.791) data 0.517 (0.517) loss 0.2625 (0.2625) acc 96.8750 (96.8750) lr 2.0777e-01 eta 0:00:18\n",
      "epoch [189/200] batch [2/2] time 0.276 (0.534) data 0.001 (0.259) loss 0.4655 (0.3640) acc 93.7500 (95.3125) lr 1.7713e-01 eta 0:00:11\n",
      "epoch [190/200] batch [1/2] time 0.814 (0.814) data 0.543 (0.543) loss 0.2516 (0.2516) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:17\n",
      "epoch [190/200] batch [2/2] time 0.275 (0.544) data 0.000 (0.272) loss 0.3159 (0.2838) acc 93.7500 (96.8750) lr 1.4891e-01 eta 0:00:10\n",
      "epoch [191/200] batch [1/2] time 0.801 (0.801) data 0.530 (0.530) loss 0.3333 (0.3333) acc 96.8750 (96.8750) lr 1.4891e-01 eta 0:00:15\n",
      "epoch [191/200] batch [2/2] time 0.274 (0.537) data 0.001 (0.265) loss 0.3268 (0.3300) acc 93.7500 (95.3125) lr 1.2312e-01 eta 0:00:09\n",
      "epoch [192/200] batch [1/2] time 0.808 (0.808) data 0.534 (0.534) loss 0.2772 (0.2772) acc 96.8750 (96.8750) lr 1.2312e-01 eta 0:00:13\n",
      "epoch [192/200] batch [2/2] time 0.272 (0.540) data 0.001 (0.267) loss 0.3793 (0.3282) acc 93.7500 (95.3125) lr 9.9763e-02 eta 0:00:08\n",
      "epoch [193/200] batch [1/2] time 0.809 (0.809) data 0.538 (0.538) loss 0.2368 (0.2368) acc 100.0000 (100.0000) lr 9.9763e-02 eta 0:00:12\n",
      "epoch [193/200] batch [2/2] time 0.278 (0.543) data 0.000 (0.269) loss 0.2424 (0.2396) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:07\n",
      "epoch [194/200] batch [1/2] time 0.794 (0.794) data 0.514 (0.514) loss 0.2645 (0.2645) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:10\n",
      "epoch [194/200] batch [2/2] time 0.277 (0.536) data 0.000 (0.257) loss 0.2855 (0.2750) acc 100.0000 (100.0000) lr 6.0390e-02 eta 0:00:06\n",
      "epoch [195/200] batch [1/2] time 0.777 (0.777) data 0.506 (0.506) loss 0.2734 (0.2734) acc 100.0000 (100.0000) lr 6.0390e-02 eta 0:00:08\n",
      "epoch [195/200] batch [2/2] time 0.275 (0.526) data 0.001 (0.253) loss 0.2614 (0.2674) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:05\n",
      "epoch [196/200] batch [1/2] time 0.854 (0.854) data 0.584 (0.584) loss 0.2729 (0.2729) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:07\n",
      "epoch [196/200] batch [2/2] time 0.277 (0.565) data 0.001 (0.292) loss 0.2493 (0.2611) acc 100.0000 (100.0000) lr 3.0827e-02 eta 0:00:04\n",
      "epoch [197/200] batch [1/2] time 1.144 (1.144) data 0.869 (0.869) loss 0.2197 (0.2197) acc 100.0000 (100.0000) lr 3.0827e-02 eta 0:00:08\n",
      "epoch [197/200] batch [2/2] time 0.278 (0.711) data 0.001 (0.435) loss 0.4492 (0.3344) acc 96.8750 (98.4375) lr 1.9733e-02 eta 0:00:04\n",
      "epoch [198/200] batch [1/2] time 1.169 (1.169) data 0.897 (0.897) loss 0.2576 (0.2576) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:05\n",
      "epoch [198/200] batch [2/2] time 0.277 (0.723) data 0.001 (0.449) loss 0.2839 (0.2707) acc 96.8750 (98.4375) lr 1.1101e-02 eta 0:00:02\n",
      "epoch [199/200] batch [1/2] time 0.831 (0.831) data 0.559 (0.559) loss 0.3368 (0.3368) acc 90.6250 (90.6250) lr 1.1101e-02 eta 0:00:02\n",
      "epoch [199/200] batch [2/2] time 0.275 (0.553) data 0.000 (0.280) loss 0.4895 (0.4131) acc 93.7500 (92.1875) lr 4.9344e-03 eta 0:00:01\n",
      "epoch [200/200] batch [1/2] time 0.802 (0.802) data 0.531 (0.531) loss 0.2548 (0.2548) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:00\n",
      "epoch [200/200] batch [2/2] time 0.274 (0.538) data 0.000 (0.266) loss 0.2758 (0.2653) acc 96.8750 (98.4375) lr 1.2337e-03 eta 0:00:00\n",
      "Checkpoint saved to output/eurosat/DAPT/vit_b16_8shots/seed3/prompt_learner/model.pth.tar-200\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:56<00:00,  1.44it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 7,388\n",
      "* accuracy: 91.2%\n",
      "* error: 8.8%\n",
      "* macro_f1: 91.0%\n",
      "Elapsed: 0:05:27\n"
     ]
    }
   ],
   "source": [
    "#eurosat-8shots-seed3\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 3 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "        --output-dir output/eurosat/DAPT/vit_b16_8shots/seed3 \\\n",
    "        DATASET.NUM_SHOTS 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn_EbwoAv1nw",
    "outputId": "4cc0b57f-74fa-4560-b46f-17648d95f08f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:48:35.242183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:48:35.262429: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:48:35.269111: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:48:35.283610: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:48:36.319297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '4']\n",
      "output_dir: output/eurosat/DAPT/vit_b16_ep100_4shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 4\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/eurosat/DAPT/vit_b16_ep100_4shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  40\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/eurosat/DAPT/vit_b16_ep100_4shots/seed1/tensorboard)\n",
      "epoch [1/100] batch [1/1] time 1.867 (1.867) data 0.496 (0.496) loss 11.6797 (11.6797) acc 12.5000 (12.5000) lr 2.0000e+01 eta 0:03:04\n",
      "epoch [2/100] batch [1/1] time 0.672 (0.672) data 0.405 (0.405) loss 11.5801 (11.5801) acc 12.5000 (12.5000) lr 1.9995e+01 eta 0:01:05\n",
      "epoch [3/100] batch [1/1] time 0.687 (0.687) data 0.421 (0.421) loss 10.9065 (10.9065) acc 0.0000 (0.0000) lr 1.9980e+01 eta 0:01:06\n",
      "epoch [4/100] batch [1/1] time 0.666 (0.666) data 0.400 (0.400) loss 10.5046 (10.5046) acc 0.0000 (0.0000) lr 1.9956e+01 eta 0:01:03\n",
      "epoch [5/100] batch [1/1] time 0.696 (0.696) data 0.432 (0.432) loss 10.4517 (10.4517) acc 0.0000 (0.0000) lr 1.9921e+01 eta 0:01:06\n",
      "epoch [6/100] batch [1/1] time 0.847 (0.847) data 0.580 (0.580) loss 10.2706 (10.2706) acc 6.2500 (6.2500) lr 1.9877e+01 eta 0:01:19\n",
      "epoch [7/100] batch [1/1] time 0.865 (0.865) data 0.598 (0.598) loss 10.1539 (10.1539) acc 3.1250 (3.1250) lr 1.9823e+01 eta 0:01:20\n",
      "epoch [8/100] batch [1/1] time 0.857 (0.857) data 0.589 (0.589) loss 10.0327 (10.0327) acc 9.3750 (9.3750) lr 1.9759e+01 eta 0:01:18\n",
      "epoch [9/100] batch [1/1] time 0.809 (0.809) data 0.540 (0.540) loss 10.0822 (10.0822) acc 0.0000 (0.0000) lr 1.9686e+01 eta 0:01:13\n",
      "epoch [10/100] batch [1/1] time 0.686 (0.686) data 0.420 (0.420) loss 9.9182 (9.9182) acc 9.3750 (9.3750) lr 1.9603e+01 eta 0:01:01\n",
      "epoch [11/100] batch [1/1] time 0.665 (0.665) data 0.400 (0.400) loss 9.9055 (9.9055) acc 15.6250 (15.6250) lr 1.9511e+01 eta 0:00:59\n",
      "epoch [12/100] batch [1/1] time 0.693 (0.693) data 0.426 (0.426) loss 9.8684 (9.8684) acc 9.3750 (9.3750) lr 1.9409e+01 eta 0:01:00\n",
      "epoch [13/100] batch [1/1] time 0.663 (0.663) data 0.395 (0.395) loss 9.7860 (9.7860) acc 9.3750 (9.3750) lr 1.9298e+01 eta 0:00:57\n",
      "epoch [14/100] batch [1/1] time 0.702 (0.702) data 0.436 (0.436) loss 9.7307 (9.7307) acc 21.8750 (21.8750) lr 1.9178e+01 eta 0:01:00\n",
      "epoch [15/100] batch [1/1] time 0.679 (0.679) data 0.410 (0.410) loss 9.6540 (9.6540) acc 12.5000 (12.5000) lr 1.9048e+01 eta 0:00:57\n",
      "epoch [16/100] batch [1/1] time 0.705 (0.705) data 0.433 (0.433) loss 9.5962 (9.5962) acc 21.8750 (21.8750) lr 1.8910e+01 eta 0:00:59\n",
      "epoch [17/100] batch [1/1] time 0.684 (0.684) data 0.417 (0.417) loss 9.5187 (9.5187) acc 21.8750 (21.8750) lr 1.8763e+01 eta 0:00:56\n",
      "epoch [18/100] batch [1/1] time 0.671 (0.671) data 0.403 (0.403) loss 9.4297 (9.4297) acc 15.6250 (15.6250) lr 1.8607e+01 eta 0:00:55\n",
      "epoch [19/100] batch [1/1] time 0.687 (0.687) data 0.419 (0.419) loss 9.3183 (9.3183) acc 25.0000 (25.0000) lr 1.8443e+01 eta 0:00:55\n",
      "epoch [20/100] batch [1/1] time 0.669 (0.669) data 0.405 (0.405) loss 9.2347 (9.2347) acc 18.7500 (18.7500) lr 1.8271e+01 eta 0:00:53\n",
      "epoch [21/100] batch [1/1] time 0.727 (0.727) data 0.458 (0.458) loss 9.1608 (9.1608) acc 21.8750 (21.8750) lr 1.8090e+01 eta 0:00:57\n",
      "epoch [22/100] batch [1/1] time 0.824 (0.824) data 0.547 (0.547) loss 8.9818 (8.9818) acc 34.3750 (34.3750) lr 1.7902e+01 eta 0:01:04\n",
      "epoch [23/100] batch [1/1] time 0.800 (0.800) data 0.529 (0.529) loss 8.8701 (8.8701) acc 31.2500 (31.2500) lr 1.7705e+01 eta 0:01:01\n",
      "epoch [24/100] batch [1/1] time 0.845 (0.845) data 0.559 (0.559) loss 8.6363 (8.6363) acc 25.0000 (25.0000) lr 1.7501e+01 eta 0:01:04\n",
      "epoch [25/100] batch [1/1] time 0.720 (0.720) data 0.452 (0.452) loss 8.4928 (8.4928) acc 25.0000 (25.0000) lr 1.7290e+01 eta 0:00:54\n",
      "epoch [26/100] batch [1/1] time 0.704 (0.704) data 0.433 (0.433) loss 8.2062 (8.2062) acc 28.1250 (28.1250) lr 1.7071e+01 eta 0:00:52\n",
      "epoch [27/100] batch [1/1] time 0.683 (0.683) data 0.413 (0.413) loss 7.9151 (7.9151) acc 28.1250 (28.1250) lr 1.6845e+01 eta 0:00:49\n",
      "epoch [28/100] batch [1/1] time 0.704 (0.704) data 0.436 (0.436) loss 7.6651 (7.6651) acc 21.8750 (21.8750) lr 1.6613e+01 eta 0:00:50\n",
      "epoch [29/100] batch [1/1] time 0.684 (0.684) data 0.413 (0.413) loss 7.0594 (7.0594) acc 34.3750 (34.3750) lr 1.6374e+01 eta 0:00:48\n",
      "epoch [30/100] batch [1/1] time 0.695 (0.695) data 0.427 (0.427) loss 6.6229 (6.6229) acc 56.2500 (56.2500) lr 1.6129e+01 eta 0:00:48\n",
      "epoch [31/100] batch [1/1] time 0.687 (0.687) data 0.415 (0.415) loss 6.1647 (6.1647) acc 46.8750 (46.8750) lr 1.5878e+01 eta 0:00:47\n",
      "epoch [32/100] batch [1/1] time 0.668 (0.668) data 0.395 (0.395) loss 7.0428 (7.0428) acc 15.6250 (15.6250) lr 1.5621e+01 eta 0:00:45\n",
      "epoch [33/100] batch [1/1] time 0.676 (0.676) data 0.408 (0.408) loss 5.8274 (5.8274) acc 25.0000 (25.0000) lr 1.5358e+01 eta 0:00:45\n",
      "epoch [34/100] batch [1/1] time 0.680 (0.680) data 0.410 (0.410) loss 5.3106 (5.3106) acc 21.8750 (21.8750) lr 1.5090e+01 eta 0:00:44\n",
      "epoch [35/100] batch [1/1] time 0.715 (0.715) data 0.442 (0.442) loss 5.6124 (5.6124) acc 6.2500 (6.2500) lr 1.4818e+01 eta 0:00:46\n",
      "epoch [36/100] batch [1/1] time 0.670 (0.670) data 0.398 (0.398) loss 4.6777 (4.6777) acc 25.0000 (25.0000) lr 1.4540e+01 eta 0:00:42\n",
      "epoch [37/100] batch [1/1] time 0.779 (0.779) data 0.506 (0.506) loss 4.6009 (4.6009) acc 15.6250 (15.6250) lr 1.4258e+01 eta 0:00:49\n",
      "epoch [38/100] batch [1/1] time 0.848 (0.848) data 0.576 (0.576) loss 4.0665 (4.0665) acc 18.7500 (18.7500) lr 1.3971e+01 eta 0:00:52\n",
      "epoch [39/100] batch [1/1] time 0.791 (0.791) data 0.527 (0.527) loss 3.7186 (3.7186) acc 15.6250 (15.6250) lr 1.3681e+01 eta 0:00:48\n",
      "epoch [40/100] batch [1/1] time 0.865 (0.865) data 0.587 (0.587) loss 3.7593 (3.7593) acc 25.0000 (25.0000) lr 1.3387e+01 eta 0:00:51\n",
      "epoch [41/100] batch [1/1] time 0.688 (0.688) data 0.415 (0.415) loss 3.4784 (3.4784) acc 18.7500 (18.7500) lr 1.3090e+01 eta 0:00:40\n",
      "epoch [42/100] batch [1/1] time 0.686 (0.686) data 0.414 (0.414) loss 3.7461 (3.7461) acc 21.8750 (21.8750) lr 1.2790e+01 eta 0:00:39\n",
      "epoch [43/100] batch [1/1] time 0.680 (0.680) data 0.411 (0.411) loss 6.6052 (6.6052) acc 12.5000 (12.5000) lr 1.2487e+01 eta 0:00:38\n",
      "epoch [44/100] batch [1/1] time 0.700 (0.700) data 0.426 (0.426) loss 11.6973 (11.6973) acc 9.3750 (9.3750) lr 1.2181e+01 eta 0:00:39\n",
      "epoch [45/100] batch [1/1] time 0.681 (0.681) data 0.410 (0.410) loss 8.7781 (8.7781) acc 6.2500 (6.2500) lr 1.1874e+01 eta 0:00:37\n",
      "epoch [46/100] batch [1/1] time 0.678 (0.678) data 0.408 (0.408) loss 6.9543 (6.9543) acc 9.3750 (9.3750) lr 1.1564e+01 eta 0:00:36\n",
      "epoch [47/100] batch [1/1] time 0.692 (0.692) data 0.419 (0.419) loss 4.8935 (4.8935) acc 9.3750 (9.3750) lr 1.1253e+01 eta 0:00:36\n",
      "epoch [48/100] batch [1/1] time 0.693 (0.693) data 0.423 (0.423) loss 4.2000 (4.2000) acc 3.1250 (3.1250) lr 1.0941e+01 eta 0:00:36\n",
      "epoch [49/100] batch [1/1] time 0.691 (0.691) data 0.420 (0.420) loss 3.6439 (3.6439) acc 3.1250 (3.1250) lr 1.0628e+01 eta 0:00:35\n",
      "epoch [50/100] batch [1/1] time 0.668 (0.668) data 0.394 (0.394) loss 3.4610 (3.4610) acc 9.3750 (9.3750) lr 1.0314e+01 eta 0:00:33\n",
      "epoch [51/100] batch [1/1] time 0.705 (0.705) data 0.432 (0.432) loss 3.3424 (3.3424) acc 12.5000 (12.5000) lr 1.0000e+01 eta 0:00:34\n",
      "epoch [52/100] batch [1/1] time 0.686 (0.686) data 0.408 (0.408) loss 3.2036 (3.2036) acc 6.2500 (6.2500) lr 9.6859e+00 eta 0:00:32\n",
      "epoch [53/100] batch [1/1] time 0.843 (0.843) data 0.565 (0.565) loss 3.1183 (3.1183) acc 12.5000 (12.5000) lr 9.3721e+00 eta 0:00:39\n",
      "epoch [54/100] batch [1/1] time 0.837 (0.837) data 0.564 (0.564) loss 3.0987 (3.0987) acc 9.3750 (9.3750) lr 9.0589e+00 eta 0:00:38\n",
      "epoch [55/100] batch [1/1] time 0.852 (0.852) data 0.574 (0.574) loss 3.0384 (3.0384) acc 18.7500 (18.7500) lr 8.7467e+00 eta 0:00:38\n",
      "epoch [56/100] batch [1/1] time 0.797 (0.797) data 0.526 (0.526) loss 2.9705 (2.9705) acc 21.8750 (21.8750) lr 8.4357e+00 eta 0:00:35\n",
      "epoch [57/100] batch [1/1] time 0.676 (0.676) data 0.403 (0.403) loss 2.8121 (2.8121) acc 46.8750 (46.8750) lr 8.1262e+00 eta 0:00:29\n",
      "epoch [58/100] batch [1/1] time 0.716 (0.716) data 0.440 (0.440) loss 2.8052 (2.8052) acc 43.7500 (43.7500) lr 7.8186e+00 eta 0:00:30\n",
      "epoch [59/100] batch [1/1] time 0.684 (0.684) data 0.409 (0.409) loss 2.7406 (2.7406) acc 37.5000 (37.5000) lr 7.5131e+00 eta 0:00:28\n",
      "epoch [60/100] batch [1/1] time 0.690 (0.690) data 0.415 (0.415) loss 2.5627 (2.5627) acc 40.6250 (40.6250) lr 7.2101e+00 eta 0:00:27\n",
      "epoch [61/100] batch [1/1] time 0.673 (0.673) data 0.398 (0.398) loss 2.5651 (2.5651) acc 40.6250 (40.6250) lr 6.9098e+00 eta 0:00:26\n",
      "epoch [62/100] batch [1/1] time 0.696 (0.696) data 0.421 (0.421) loss 2.4893 (2.4893) acc 34.3750 (34.3750) lr 6.6126e+00 eta 0:00:26\n",
      "epoch [63/100] batch [1/1] time 0.688 (0.688) data 0.415 (0.415) loss 2.2444 (2.2444) acc 50.0000 (50.0000) lr 6.3188e+00 eta 0:00:25\n",
      "epoch [64/100] batch [1/1] time 0.684 (0.684) data 0.410 (0.410) loss 1.9987 (1.9987) acc 50.0000 (50.0000) lr 6.0285e+00 eta 0:00:24\n",
      "epoch [65/100] batch [1/1] time 0.701 (0.701) data 0.427 (0.427) loss 1.7687 (1.7687) acc 71.8750 (71.8750) lr 5.7422e+00 eta 0:00:24\n",
      "epoch [66/100] batch [1/1] time 0.699 (0.699) data 0.422 (0.422) loss 1.7636 (1.7636) acc 65.6250 (65.6250) lr 5.4601e+00 eta 0:00:23\n",
      "epoch [67/100] batch [1/1] time 0.705 (0.705) data 0.430 (0.430) loss 1.6532 (1.6532) acc 71.8750 (71.8750) lr 5.1825e+00 eta 0:00:23\n",
      "epoch [68/100] batch [1/1] time 0.773 (0.773) data 0.497 (0.497) loss 1.5359 (1.5359) acc 78.1250 (78.1250) lr 4.9096e+00 eta 0:00:24\n",
      "epoch [69/100] batch [1/1] time 0.857 (0.857) data 0.579 (0.579) loss 1.7508 (1.7508) acc 62.5000 (62.5000) lr 4.6417e+00 eta 0:00:26\n",
      "epoch [70/100] batch [1/1] time 0.841 (0.841) data 0.564 (0.564) loss 1.6509 (1.6509) acc 68.7500 (68.7500) lr 4.3792e+00 eta 0:00:25\n",
      "epoch [71/100] batch [1/1] time 0.826 (0.826) data 0.551 (0.551) loss 1.5658 (1.5658) acc 75.0000 (75.0000) lr 4.1221e+00 eta 0:00:23\n",
      "epoch [72/100] batch [1/1] time 0.683 (0.683) data 0.410 (0.410) loss 1.4315 (1.4315) acc 68.7500 (68.7500) lr 3.8709e+00 eta 0:00:19\n",
      "epoch [73/100] batch [1/1] time 0.693 (0.693) data 0.417 (0.417) loss 1.3325 (1.3325) acc 81.2500 (81.2500) lr 3.6258e+00 eta 0:00:18\n",
      "epoch [74/100] batch [1/1] time 0.702 (0.702) data 0.427 (0.427) loss 1.3710 (1.3710) acc 81.2500 (81.2500) lr 3.3869e+00 eta 0:00:18\n",
      "epoch [75/100] batch [1/1] time 0.672 (0.672) data 0.399 (0.399) loss 1.6954 (1.6954) acc 81.2500 (81.2500) lr 3.1545e+00 eta 0:00:16\n",
      "epoch [76/100] batch [1/1] time 0.702 (0.702) data 0.427 (0.427) loss 1.5021 (1.5021) acc 68.7500 (68.7500) lr 2.9289e+00 eta 0:00:16\n",
      "epoch [77/100] batch [1/1] time 0.683 (0.683) data 0.411 (0.411) loss 1.2387 (1.2387) acc 84.3750 (84.3750) lr 2.7103e+00 eta 0:00:15\n",
      "epoch [78/100] batch [1/1] time 0.718 (0.718) data 0.442 (0.442) loss 1.4289 (1.4289) acc 78.1250 (78.1250) lr 2.4989e+00 eta 0:00:15\n",
      "epoch [79/100] batch [1/1] time 0.687 (0.687) data 0.411 (0.411) loss 1.4084 (1.4084) acc 75.0000 (75.0000) lr 2.2949e+00 eta 0:00:14\n",
      "epoch [80/100] batch [1/1] time 0.679 (0.679) data 0.404 (0.404) loss 1.2288 (1.2288) acc 78.1250 (78.1250) lr 2.0984e+00 eta 0:00:13\n",
      "epoch [81/100] batch [1/1] time 0.698 (0.698) data 0.421 (0.421) loss 1.3401 (1.3401) acc 78.1250 (78.1250) lr 1.9098e+00 eta 0:00:13\n",
      "epoch [82/100] batch [1/1] time 0.683 (0.683) data 0.409 (0.409) loss 1.1673 (1.1673) acc 78.1250 (78.1250) lr 1.7292e+00 eta 0:00:12\n",
      "epoch [83/100] batch [1/1] time 0.764 (0.764) data 0.488 (0.488) loss 1.1556 (1.1556) acc 81.2500 (81.2500) lr 1.5567e+00 eta 0:00:12\n",
      "epoch [84/100] batch [1/1] time 0.907 (0.907) data 0.629 (0.629) loss 1.1630 (1.1630) acc 84.3750 (84.3750) lr 1.3926e+00 eta 0:00:14\n",
      "epoch [85/100] batch [1/1] time 0.844 (0.844) data 0.568 (0.568) loss 1.0597 (1.0597) acc 93.7500 (93.7500) lr 1.2369e+00 eta 0:00:12\n",
      "epoch [86/100] batch [1/1] time 0.830 (0.830) data 0.556 (0.556) loss 1.0369 (1.0369) acc 93.7500 (93.7500) lr 1.0899e+00 eta 0:00:11\n",
      "epoch [87/100] batch [1/1] time 0.774 (0.774) data 0.501 (0.501) loss 0.9911 (0.9911) acc 90.6250 (90.6250) lr 9.5173e-01 eta 0:00:10\n",
      "epoch [88/100] batch [1/1] time 0.710 (0.710) data 0.439 (0.439) loss 1.1086 (1.1086) acc 87.5000 (87.5000) lr 8.2245e-01 eta 0:00:08\n",
      "epoch [89/100] batch [1/1] time 0.687 (0.687) data 0.413 (0.413) loss 1.0200 (1.0200) acc 87.5000 (87.5000) lr 7.0224e-01 eta 0:00:07\n",
      "epoch [90/100] batch [1/1] time 0.699 (0.699) data 0.424 (0.424) loss 0.9491 (0.9491) acc 90.6250 (90.6250) lr 5.9119e-01 eta 0:00:06\n",
      "epoch [91/100] batch [1/1] time 0.675 (0.675) data 0.403 (0.403) loss 0.9312 (0.9312) acc 90.6250 (90.6250) lr 4.8943e-01 eta 0:00:06\n",
      "epoch [92/100] batch [1/1] time 0.704 (0.704) data 0.434 (0.434) loss 0.8635 (0.8635) acc 96.8750 (96.8750) lr 3.9706e-01 eta 0:00:05\n",
      "epoch [93/100] batch [1/1] time 0.717 (0.717) data 0.444 (0.444) loss 0.9221 (0.9221) acc 90.6250 (90.6250) lr 3.1417e-01 eta 0:00:05\n",
      "epoch [94/100] batch [1/1] time 0.680 (0.680) data 0.407 (0.407) loss 0.9147 (0.9147) acc 93.7500 (93.7500) lr 2.4083e-01 eta 0:00:04\n",
      "epoch [95/100] batch [1/1] time 0.689 (0.689) data 0.420 (0.420) loss 0.9288 (0.9288) acc 93.7500 (93.7500) lr 1.7713e-01 eta 0:00:03\n",
      "epoch [96/100] batch [1/1] time 0.676 (0.676) data 0.402 (0.402) loss 0.8480 (0.8480) acc 96.8750 (96.8750) lr 1.2312e-01 eta 0:00:02\n",
      "epoch [97/100] batch [1/1] time 0.686 (0.686) data 0.414 (0.414) loss 0.8336 (0.8336) acc 93.7500 (93.7500) lr 7.8853e-02 eta 0:00:02\n",
      "epoch [98/100] batch [1/1] time 0.680 (0.680) data 0.408 (0.408) loss 0.9297 (0.9297) acc 90.6250 (90.6250) lr 4.4380e-02 eta 0:00:01\n",
      "epoch [99/100] batch [1/1] time 0.795 (0.795) data 0.522 (0.522) loss 1.0467 (1.0467) acc 84.3750 (84.3750) lr 1.9733e-02 eta 0:00:00\n",
      "epoch [100/100] batch [1/1] time 0.851 (0.851) data 0.573 (0.573) loss 0.9953 (0.9953) acc 87.5000 (87.5000) lr 4.9344e-03 eta 0:00:00\n",
      "Checkpoint saved to output/eurosat/DAPT/vit_b16_ep100_4shots/seed1/prompt_learner/model.pth.tar-100\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:57<00:00,  1.42it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 4,589\n",
      "* accuracy: 56.7%\n",
      "* error: 43.3%\n",
      "* macro_f1: 55.8%\n",
      "Elapsed: 0:02:28\n"
     ]
    }
   ],
   "source": [
    "#eurosat-4shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/eurosat/DAPT/vit_b16_ep100_4shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "slxfKB7OKD73",
    "outputId": "3151e161-a32e-4975-c0de-7bf0348dec8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:34:42.950945: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:34:42.983469: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:34:42.993555: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:34:43.016350: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:34:44.491149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '4']\n",
      "output_dir: output/eurosat/DAPT/vit_b16_ep100_4shots/seed2\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 4\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/eurosat/DAPT/vit_b16_ep100_4shots/seed2\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_2.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  40\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/eurosat/DAPT/vit_b16_ep100_4shots/seed2/tensorboard)\n",
      "epoch [1/100] batch [1/1] time 1.874 (1.874) data 0.507 (0.507) loss 11.3887 (11.3887) acc 18.7500 (18.7500) lr 2.0000e+01 eta 0:03:05\n",
      "epoch [2/100] batch [1/1] time 0.662 (0.662) data 0.412 (0.412) loss 11.3909 (11.3909) acc 15.6250 (15.6250) lr 1.9995e+01 eta 0:01:04\n",
      "epoch [3/100] batch [1/1] time 0.689 (0.689) data 0.443 (0.443) loss 11.2989 (11.2989) acc 9.3750 (9.3750) lr 1.9980e+01 eta 0:01:06\n",
      "epoch [4/100] batch [1/1] time 0.678 (0.678) data 0.430 (0.430) loss 10.2671 (10.2671) acc 6.2500 (6.2500) lr 1.9956e+01 eta 0:01:05\n",
      "epoch [5/100] batch [1/1] time 0.691 (0.691) data 0.444 (0.444) loss 10.1818 (10.1818) acc 9.3750 (9.3750) lr 1.9921e+01 eta 0:01:05\n",
      "epoch [6/100] batch [1/1] time 0.673 (0.673) data 0.424 (0.424) loss 9.9606 (9.9606) acc 15.6250 (15.6250) lr 1.9877e+01 eta 0:01:03\n",
      "epoch [7/100] batch [1/1] time 0.659 (0.659) data 0.412 (0.412) loss 9.8928 (9.8928) acc 12.5000 (12.5000) lr 1.9823e+01 eta 0:01:01\n",
      "epoch [8/100] batch [1/1] time 0.716 (0.716) data 0.457 (0.457) loss 9.7341 (9.7341) acc 15.6250 (15.6250) lr 1.9759e+01 eta 0:01:05\n",
      "epoch [9/100] batch [1/1] time 0.813 (0.813) data 0.562 (0.562) loss 9.6735 (9.6735) acc 15.6250 (15.6250) lr 1.9686e+01 eta 0:01:14\n",
      "epoch [10/100] batch [1/1] time 0.859 (0.859) data 0.607 (0.607) loss 9.5692 (9.5692) acc 9.3750 (9.3750) lr 1.9603e+01 eta 0:01:17\n",
      "epoch [11/100] batch [1/1] time 0.925 (0.925) data 0.655 (0.655) loss 9.4918 (9.4918) acc 12.5000 (12.5000) lr 1.9511e+01 eta 0:01:22\n",
      "epoch [12/100] batch [1/1] time 0.801 (0.801) data 0.553 (0.553) loss 9.2947 (9.2947) acc 12.5000 (12.5000) lr 1.9409e+01 eta 0:01:10\n",
      "epoch [13/100] batch [1/1] time 0.658 (0.658) data 0.406 (0.406) loss 9.2471 (9.2471) acc 9.3750 (9.3750) lr 1.9298e+01 eta 0:00:57\n",
      "epoch [14/100] batch [1/1] time 0.688 (0.688) data 0.438 (0.438) loss 9.0758 (9.0758) acc 15.6250 (15.6250) lr 1.9178e+01 eta 0:00:59\n",
      "epoch [15/100] batch [1/1] time 0.684 (0.684) data 0.435 (0.435) loss 8.9744 (8.9744) acc 34.3750 (34.3750) lr 1.9048e+01 eta 0:00:58\n",
      "epoch [16/100] batch [1/1] time 0.683 (0.683) data 0.432 (0.432) loss 8.8313 (8.8313) acc 34.3750 (34.3750) lr 1.8910e+01 eta 0:00:57\n",
      "epoch [17/100] batch [1/1] time 0.696 (0.696) data 0.444 (0.444) loss 8.6891 (8.6891) acc 34.3750 (34.3750) lr 1.8763e+01 eta 0:00:57\n",
      "epoch [18/100] batch [1/1] time 0.669 (0.669) data 0.419 (0.419) loss 8.5033 (8.5033) acc 28.1250 (28.1250) lr 1.8607e+01 eta 0:00:54\n",
      "epoch [19/100] batch [1/1] time 0.692 (0.692) data 0.440 (0.440) loss 8.2109 (8.2109) acc 40.6250 (40.6250) lr 1.8443e+01 eta 0:00:56\n",
      "epoch [20/100] batch [1/1] time 0.692 (0.692) data 0.442 (0.442) loss 8.0355 (8.0355) acc 40.6250 (40.6250) lr 1.8271e+01 eta 0:00:55\n",
      "epoch [21/100] batch [1/1] time 0.696 (0.696) data 0.445 (0.445) loss 7.8495 (7.8495) acc 43.7500 (43.7500) lr 1.8090e+01 eta 0:00:55\n",
      "epoch [22/100] batch [1/1] time 0.673 (0.673) data 0.422 (0.422) loss 7.5520 (7.5520) acc 43.7500 (43.7500) lr 1.7902e+01 eta 0:00:52\n",
      "epoch [23/100] batch [1/1] time 0.676 (0.676) data 0.425 (0.425) loss 7.1770 (7.1770) acc 56.2500 (56.2500) lr 1.7705e+01 eta 0:00:52\n",
      "epoch [24/100] batch [1/1] time 0.700 (0.700) data 0.444 (0.444) loss 7.3712 (7.3712) acc 34.3750 (34.3750) lr 1.7501e+01 eta 0:00:53\n",
      "epoch [25/100] batch [1/1] time 0.860 (0.860) data 0.603 (0.603) loss 7.2065 (7.2065) acc 34.3750 (34.3750) lr 1.7290e+01 eta 0:01:04\n",
      "epoch [26/100] batch [1/1] time 0.845 (0.845) data 0.591 (0.591) loss 7.1815 (7.1815) acc 25.0000 (25.0000) lr 1.7071e+01 eta 0:01:02\n",
      "epoch [27/100] batch [1/1] time 0.807 (0.807) data 0.546 (0.546) loss 6.7523 (6.7523) acc 34.3750 (34.3750) lr 1.6845e+01 eta 0:00:58\n",
      "epoch [28/100] batch [1/1] time 0.751 (0.751) data 0.499 (0.499) loss 6.5594 (6.5594) acc 43.7500 (43.7500) lr 1.6613e+01 eta 0:00:54\n",
      "epoch [29/100] batch [1/1] time 0.676 (0.676) data 0.425 (0.425) loss 6.2986 (6.2986) acc 34.3750 (34.3750) lr 1.6374e+01 eta 0:00:48\n",
      "epoch [30/100] batch [1/1] time 0.702 (0.702) data 0.451 (0.451) loss 6.0250 (6.0250) acc 43.7500 (43.7500) lr 1.6129e+01 eta 0:00:49\n",
      "epoch [31/100] batch [1/1] time 0.697 (0.697) data 0.445 (0.445) loss 5.5131 (5.5131) acc 59.3750 (59.3750) lr 1.5878e+01 eta 0:00:48\n",
      "epoch [32/100] batch [1/1] time 0.689 (0.689) data 0.431 (0.431) loss 5.3939 (5.3939) acc 46.8750 (46.8750) lr 1.5621e+01 eta 0:00:46\n",
      "epoch [33/100] batch [1/1] time 0.672 (0.672) data 0.436 (0.436) loss 5.2589 (5.2589) acc 46.8750 (46.8750) lr 1.5358e+01 eta 0:00:45\n",
      "epoch [34/100] batch [1/1] time 0.674 (0.674) data 0.422 (0.422) loss 5.4429 (5.4429) acc 34.3750 (34.3750) lr 1.5090e+01 eta 0:00:44\n",
      "epoch [35/100] batch [1/1] time 0.694 (0.694) data 0.443 (0.443) loss 4.8664 (4.8664) acc 37.5000 (37.5000) lr 1.4818e+01 eta 0:00:45\n",
      "epoch [36/100] batch [1/1] time 0.672 (0.672) data 0.421 (0.421) loss 4.4775 (4.4775) acc 43.7500 (43.7500) lr 1.4540e+01 eta 0:00:43\n",
      "epoch [37/100] batch [1/1] time 0.727 (0.727) data 0.461 (0.461) loss 3.4826 (3.4826) acc 56.2500 (56.2500) lr 1.4258e+01 eta 0:00:45\n",
      "epoch [38/100] batch [1/1] time 0.671 (0.671) data 0.419 (0.419) loss 3.1665 (3.1665) acc 56.2500 (56.2500) lr 1.3971e+01 eta 0:00:41\n",
      "epoch [39/100] batch [1/1] time 0.672 (0.672) data 0.421 (0.421) loss 3.0589 (3.0589) acc 53.1250 (53.1250) lr 1.3681e+01 eta 0:00:41\n",
      "epoch [40/100] batch [1/1] time 0.892 (0.892) data 0.637 (0.637) loss 3.6409 (3.6409) acc 28.1250 (28.1250) lr 1.3387e+01 eta 0:00:53\n",
      "epoch [41/100] batch [1/1] time 0.851 (0.851) data 0.596 (0.596) loss 3.3036 (3.3036) acc 34.3750 (34.3750) lr 1.3090e+01 eta 0:00:50\n",
      "epoch [42/100] batch [1/1] time 0.897 (0.897) data 0.640 (0.640) loss 5.1283 (5.1283) acc 15.6250 (15.6250) lr 1.2790e+01 eta 0:00:52\n",
      "epoch [43/100] batch [1/1] time 0.799 (0.799) data 0.546 (0.546) loss 6.3049 (6.3049) acc 15.6250 (15.6250) lr 1.2487e+01 eta 0:00:45\n",
      "epoch [44/100] batch [1/1] time 0.701 (0.701) data 0.450 (0.450) loss 8.9571 (8.9571) acc 15.6250 (15.6250) lr 1.2181e+01 eta 0:00:39\n",
      "epoch [45/100] batch [1/1] time 0.700 (0.700) data 0.446 (0.446) loss 6.2291 (6.2291) acc 12.5000 (12.5000) lr 1.1874e+01 eta 0:00:38\n",
      "epoch [46/100] batch [1/1] time 0.722 (0.722) data 0.470 (0.470) loss 4.8191 (4.8191) acc 25.0000 (25.0000) lr 1.1564e+01 eta 0:00:38\n",
      "epoch [47/100] batch [1/1] time 0.680 (0.680) data 0.430 (0.430) loss 4.3537 (4.3537) acc 28.1250 (28.1250) lr 1.1253e+01 eta 0:00:36\n",
      "epoch [48/100] batch [1/1] time 0.676 (0.676) data 0.425 (0.425) loss 4.0004 (4.0004) acc 12.5000 (12.5000) lr 1.0941e+01 eta 0:00:35\n",
      "epoch [49/100] batch [1/1] time 0.700 (0.700) data 0.445 (0.445) loss 3.7574 (3.7574) acc 28.1250 (28.1250) lr 1.0628e+01 eta 0:00:35\n",
      "epoch [50/100] batch [1/1] time 0.682 (0.682) data 0.429 (0.429) loss 3.5038 (3.5038) acc 37.5000 (37.5000) lr 1.0314e+01 eta 0:00:34\n",
      "epoch [51/100] batch [1/1] time 0.725 (0.725) data 0.472 (0.472) loss 3.2423 (3.2423) acc 34.3750 (34.3750) lr 1.0000e+01 eta 0:00:35\n",
      "epoch [52/100] batch [1/1] time 0.686 (0.686) data 0.433 (0.433) loss 3.1612 (3.1612) acc 34.3750 (34.3750) lr 9.6859e+00 eta 0:00:32\n",
      "epoch [53/100] batch [1/1] time 0.700 (0.700) data 0.446 (0.446) loss 3.1136 (3.1136) acc 37.5000 (37.5000) lr 9.3721e+00 eta 0:00:32\n",
      "epoch [54/100] batch [1/1] time 0.676 (0.676) data 0.424 (0.424) loss 2.6811 (2.6811) acc 59.3750 (59.3750) lr 9.0589e+00 eta 0:00:31\n",
      "epoch [55/100] batch [1/1] time 0.847 (0.847) data 0.593 (0.593) loss 2.6568 (2.6568) acc 56.2500 (56.2500) lr 8.7467e+00 eta 0:00:38\n",
      "epoch [56/100] batch [1/1] time 0.875 (0.875) data 0.616 (0.616) loss 2.4678 (2.4678) acc 56.2500 (56.2500) lr 8.4357e+00 eta 0:00:38\n",
      "epoch [57/100] batch [1/1] time 0.870 (0.870) data 0.607 (0.607) loss 2.1931 (2.1931) acc 62.5000 (62.5000) lr 8.1262e+00 eta 0:00:37\n",
      "epoch [58/100] batch [1/1] time 0.847 (0.847) data 0.594 (0.594) loss 2.2140 (2.2140) acc 71.8750 (71.8750) lr 7.8186e+00 eta 0:00:35\n",
      "epoch [59/100] batch [1/1] time 0.683 (0.683) data 0.429 (0.429) loss 1.9126 (1.9126) acc 68.7500 (68.7500) lr 7.5131e+00 eta 0:00:28\n",
      "epoch [60/100] batch [1/1] time 0.709 (0.709) data 0.452 (0.452) loss 1.9298 (1.9298) acc 62.5000 (62.5000) lr 7.2101e+00 eta 0:00:28\n",
      "epoch [61/100] batch [1/1] time 0.701 (0.701) data 0.448 (0.448) loss 1.8592 (1.8592) acc 65.6250 (65.6250) lr 6.9098e+00 eta 0:00:27\n",
      "epoch [62/100] batch [1/1] time 0.690 (0.690) data 0.437 (0.437) loss 1.7703 (1.7703) acc 71.8750 (71.8750) lr 6.6126e+00 eta 0:00:26\n",
      "epoch [63/100] batch [1/1] time 0.697 (0.697) data 0.448 (0.448) loss 1.6863 (1.6863) acc 65.6250 (65.6250) lr 6.3188e+00 eta 0:00:25\n",
      "epoch [64/100] batch [1/1] time 0.682 (0.682) data 0.423 (0.423) loss 1.8872 (1.8872) acc 65.6250 (65.6250) lr 6.0285e+00 eta 0:00:24\n",
      "epoch [65/100] batch [1/1] time 0.704 (0.704) data 0.448 (0.448) loss 1.4317 (1.4317) acc 87.5000 (87.5000) lr 5.7422e+00 eta 0:00:24\n",
      "epoch [66/100] batch [1/1] time 0.687 (0.687) data 0.431 (0.431) loss 1.6810 (1.6810) acc 65.6250 (65.6250) lr 5.4601e+00 eta 0:00:23\n",
      "epoch [67/100] batch [1/1] time 0.722 (0.722) data 0.460 (0.460) loss 1.3931 (1.3931) acc 78.1250 (78.1250) lr 5.1825e+00 eta 0:00:23\n",
      "epoch [68/100] batch [1/1] time 0.686 (0.686) data 0.432 (0.432) loss 1.5458 (1.5458) acc 65.6250 (65.6250) lr 4.9096e+00 eta 0:00:21\n",
      "epoch [69/100] batch [1/1] time 0.713 (0.713) data 0.457 (0.457) loss 1.6178 (1.6178) acc 68.7500 (68.7500) lr 4.6417e+00 eta 0:00:22\n",
      "epoch [70/100] batch [1/1] time 0.869 (0.869) data 0.611 (0.611) loss 1.3648 (1.3648) acc 78.1250 (78.1250) lr 4.3792e+00 eta 0:00:26\n",
      "epoch [71/100] batch [1/1] time 0.865 (0.865) data 0.608 (0.608) loss 1.3821 (1.3821) acc 81.2500 (81.2500) lr 4.1221e+00 eta 0:00:25\n",
      "epoch [72/100] batch [1/1] time 0.886 (0.886) data 0.628 (0.628) loss 1.2011 (1.2011) acc 93.7500 (93.7500) lr 3.8709e+00 eta 0:00:24\n",
      "epoch [73/100] batch [1/1] time 0.843 (0.843) data 0.587 (0.587) loss 1.2992 (1.2992) acc 81.2500 (81.2500) lr 3.6258e+00 eta 0:00:22\n",
      "epoch [74/100] batch [1/1] time 0.720 (0.720) data 0.465 (0.465) loss 1.2567 (1.2567) acc 78.1250 (78.1250) lr 3.3869e+00 eta 0:00:18\n",
      "epoch [75/100] batch [1/1] time 0.690 (0.690) data 0.435 (0.435) loss 1.1981 (1.1981) acc 81.2500 (81.2500) lr 3.1545e+00 eta 0:00:17\n",
      "epoch [76/100] batch [1/1] time 0.710 (0.710) data 0.455 (0.455) loss 1.1247 (1.1247) acc 81.2500 (81.2500) lr 2.9289e+00 eta 0:00:17\n",
      "epoch [77/100] batch [1/1] time 0.681 (0.681) data 0.425 (0.425) loss 1.2571 (1.2571) acc 81.2500 (81.2500) lr 2.7103e+00 eta 0:00:15\n",
      "epoch [78/100] batch [1/1] time 0.744 (0.744) data 0.489 (0.489) loss 1.1573 (1.1573) acc 87.5000 (87.5000) lr 2.4989e+00 eta 0:00:16\n",
      "epoch [79/100] batch [1/1] time 0.699 (0.699) data 0.444 (0.444) loss 1.3012 (1.3012) acc 75.0000 (75.0000) lr 2.2949e+00 eta 0:00:14\n",
      "epoch [80/100] batch [1/1] time 0.696 (0.696) data 0.442 (0.442) loss 0.8866 (0.8866) acc 93.7500 (93.7500) lr 2.0984e+00 eta 0:00:13\n",
      "epoch [81/100] batch [1/1] time 0.691 (0.691) data 0.436 (0.436) loss 1.2719 (1.2719) acc 71.8750 (71.8750) lr 1.9098e+00 eta 0:00:13\n",
      "epoch [82/100] batch [1/1] time 0.678 (0.678) data 0.425 (0.425) loss 1.1128 (1.1128) acc 87.5000 (87.5000) lr 1.7292e+00 eta 0:00:12\n",
      "epoch [83/100] batch [1/1] time 0.711 (0.711) data 0.455 (0.455) loss 0.9927 (0.9927) acc 90.6250 (90.6250) lr 1.5567e+00 eta 0:00:12\n",
      "epoch [84/100] batch [1/1] time 0.698 (0.698) data 0.442 (0.442) loss 1.2229 (1.2229) acc 84.3750 (84.3750) lr 1.3926e+00 eta 0:00:11\n",
      "epoch [85/100] batch [1/1] time 0.895 (0.895) data 0.639 (0.639) loss 0.9804 (0.9804) acc 87.5000 (87.5000) lr 1.2369e+00 eta 0:00:13\n",
      "epoch [86/100] batch [1/1] time 0.897 (0.897) data 0.638 (0.638) loss 0.9427 (0.9427) acc 90.6250 (90.6250) lr 1.0899e+00 eta 0:00:12\n",
      "epoch [87/100] batch [1/1] time 0.858 (0.858) data 0.598 (0.598) loss 0.9384 (0.9384) acc 93.7500 (93.7500) lr 9.5173e-01 eta 0:00:11\n",
      "epoch [88/100] batch [1/1] time 0.844 (0.844) data 0.590 (0.590) loss 0.9414 (0.9414) acc 90.6250 (90.6250) lr 8.2245e-01 eta 0:00:10\n",
      "epoch [89/100] batch [1/1] time 0.703 (0.703) data 0.448 (0.448) loss 0.8389 (0.8389) acc 96.8750 (96.8750) lr 7.0224e-01 eta 0:00:07\n",
      "epoch [90/100] batch [1/1] time 0.699 (0.699) data 0.444 (0.444) loss 0.9547 (0.9547) acc 90.6250 (90.6250) lr 5.9119e-01 eta 0:00:06\n",
      "epoch [91/100] batch [1/1] time 0.715 (0.715) data 0.454 (0.454) loss 0.8686 (0.8686) acc 93.7500 (93.7500) lr 4.8943e-01 eta 0:00:06\n",
      "epoch [92/100] batch [1/1] time 0.700 (0.700) data 0.444 (0.444) loss 0.8844 (0.8844) acc 90.6250 (90.6250) lr 3.9706e-01 eta 0:00:05\n",
      "epoch [93/100] batch [1/1] time 0.683 (0.683) data 0.428 (0.428) loss 0.8235 (0.8235) acc 90.6250 (90.6250) lr 3.1417e-01 eta 0:00:04\n",
      "epoch [94/100] batch [1/1] time 0.711 (0.711) data 0.455 (0.455) loss 0.7697 (0.7697) acc 93.7500 (93.7500) lr 2.4083e-01 eta 0:00:04\n",
      "epoch [95/100] batch [1/1] time 0.711 (0.711) data 0.455 (0.455) loss 0.8110 (0.8110) acc 96.8750 (96.8750) lr 1.7713e-01 eta 0:00:03\n",
      "epoch [96/100] batch [1/1] time 0.689 (0.689) data 0.433 (0.433) loss 0.9286 (0.9286) acc 84.3750 (84.3750) lr 1.2312e-01 eta 0:00:02\n",
      "epoch [97/100] batch [1/1] time 0.717 (0.717) data 0.457 (0.457) loss 0.9149 (0.9149) acc 90.6250 (90.6250) lr 7.8853e-02 eta 0:00:02\n",
      "epoch [98/100] batch [1/1] time 0.679 (0.679) data 0.423 (0.423) loss 0.9002 (0.9002) acc 90.6250 (90.6250) lr 4.4380e-02 eta 0:00:01\n",
      "epoch [99/100] batch [1/1] time 0.691 (0.691) data 0.437 (0.437) loss 0.7731 (0.7731) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:00\n",
      "epoch [100/100] batch [1/1] time 0.864 (0.864) data 0.605 (0.605) loss 0.8416 (0.8416) acc 96.8750 (96.8750) lr 4.9344e-03 eta 0:00:00\n",
      "Checkpoint saved to output/eurosat/DAPT/vit_b16_ep100_4shots/seed2/prompt_learner/model.pth.tar-100\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:56<00:00,  1.42it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 4,488\n",
      "* accuracy: 55.4%\n",
      "* error: 44.6%\n",
      "* macro_f1: 54.5%\n",
      "Elapsed: 0:02:29\n"
     ]
    }
   ],
   "source": [
    "#eurosat-4shots-seed2\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 2 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/eurosat/DAPT/vit_b16_ep100_4shots/seed2 \\\n",
    "        DATASET.NUM_SHOTS 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzeUz5WBKI2i",
    "outputId": "e1d6a09c-76d4-4507-fabb-9fc732ff3a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:37:34.913450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:37:34.945563: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:37:34.955623: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:37:34.977894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:37:36.185811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '4']\n",
      "output_dir: output/eurosat/DAPT/vit_b16_ep100_4shots/seed3\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 4\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/eurosat/DAPT/vit_b16_ep100_4shots/seed3\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_3.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  40\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/eurosat/DAPT/vit_b16_ep100_4shots/seed3/tensorboard)\n",
      "epoch [1/100] batch [1/1] time 1.844 (1.844) data 0.512 (0.512) loss 11.9411 (11.9411) acc 12.5000 (12.5000) lr 2.0000e+01 eta 0:03:02\n",
      "epoch [2/100] batch [1/1] time 0.678 (0.678) data 0.421 (0.421) loss 12.0042 (12.0042) acc 9.3750 (9.3750) lr 1.9995e+01 eta 0:01:06\n",
      "epoch [3/100] batch [1/1] time 0.715 (0.715) data 0.456 (0.456) loss 9.0347 (9.0347) acc 6.2500 (6.2500) lr 1.9980e+01 eta 0:01:09\n",
      "epoch [4/100] batch [1/1] time 0.694 (0.694) data 0.436 (0.436) loss 8.2932 (8.2932) acc 12.5000 (12.5000) lr 1.9956e+01 eta 0:01:06\n",
      "epoch [5/100] batch [1/1] time 0.712 (0.712) data 0.453 (0.453) loss 8.4553 (8.4553) acc 12.5000 (12.5000) lr 1.9921e+01 eta 0:01:07\n",
      "epoch [6/100] batch [1/1] time 0.693 (0.693) data 0.433 (0.433) loss 7.8520 (7.8520) acc 12.5000 (12.5000) lr 1.9877e+01 eta 0:01:05\n",
      "epoch [7/100] batch [1/1] time 0.693 (0.693) data 0.434 (0.434) loss 7.7786 (7.7786) acc 6.2500 (6.2500) lr 1.9823e+01 eta 0:01:04\n",
      "epoch [8/100] batch [1/1] time 0.717 (0.717) data 0.453 (0.453) loss 7.6303 (7.6303) acc 12.5000 (12.5000) lr 1.9759e+01 eta 0:01:05\n",
      "epoch [9/100] batch [1/1] time 0.886 (0.886) data 0.616 (0.616) loss 7.4326 (7.4326) acc 12.5000 (12.5000) lr 1.9686e+01 eta 0:01:20\n",
      "epoch [10/100] batch [1/1] time 0.880 (0.880) data 0.617 (0.617) loss 7.4209 (7.4209) acc 12.5000 (12.5000) lr 1.9603e+01 eta 0:01:19\n",
      "epoch [11/100] batch [1/1] time 0.846 (0.846) data 0.574 (0.574) loss 7.1636 (7.1636) acc 18.7500 (18.7500) lr 1.9511e+01 eta 0:01:15\n",
      "epoch [12/100] batch [1/1] time 0.812 (0.812) data 0.552 (0.552) loss 7.1740 (7.1740) acc 18.7500 (18.7500) lr 1.9409e+01 eta 0:01:11\n",
      "epoch [13/100] batch [1/1] time 0.697 (0.697) data 0.439 (0.439) loss 7.0894 (7.0894) acc 21.8750 (21.8750) lr 1.9298e+01 eta 0:01:00\n",
      "epoch [14/100] batch [1/1] time 0.706 (0.706) data 0.447 (0.447) loss 6.9334 (6.9334) acc 25.0000 (25.0000) lr 1.9178e+01 eta 0:01:00\n",
      "epoch [15/100] batch [1/1] time 0.721 (0.721) data 0.465 (0.465) loss 6.7559 (6.7559) acc 31.2500 (31.2500) lr 1.9048e+01 eta 0:01:01\n",
      "epoch [16/100] batch [1/1] time 0.688 (0.688) data 0.430 (0.430) loss 6.6817 (6.6817) acc 31.2500 (31.2500) lr 1.8910e+01 eta 0:00:57\n",
      "epoch [17/100] batch [1/1] time 0.707 (0.707) data 0.446 (0.446) loss 6.6136 (6.6136) acc 28.1250 (28.1250) lr 1.8763e+01 eta 0:00:58\n",
      "epoch [18/100] batch [1/1] time 0.682 (0.682) data 0.423 (0.423) loss 6.6054 (6.6054) acc 21.8750 (21.8750) lr 1.8607e+01 eta 0:00:55\n",
      "epoch [19/100] batch [1/1] time 0.693 (0.693) data 0.434 (0.434) loss 6.4097 (6.4097) acc 31.2500 (31.2500) lr 1.8443e+01 eta 0:00:56\n",
      "epoch [20/100] batch [1/1] time 0.722 (0.722) data 0.461 (0.461) loss 6.2080 (6.2080) acc 37.5000 (37.5000) lr 1.8271e+01 eta 0:00:57\n",
      "epoch [21/100] batch [1/1] time 0.686 (0.686) data 0.426 (0.426) loss 6.2060 (6.2060) acc 28.1250 (28.1250) lr 1.8090e+01 eta 0:00:54\n",
      "epoch [22/100] batch [1/1] time 0.705 (0.705) data 0.445 (0.445) loss 5.8982 (5.8982) acc 34.3750 (34.3750) lr 1.7902e+01 eta 0:00:55\n",
      "epoch [23/100] batch [1/1] time 0.690 (0.690) data 0.426 (0.426) loss 5.7727 (5.7727) acc 31.2500 (31.2500) lr 1.7705e+01 eta 0:00:53\n",
      "epoch [24/100] batch [1/1] time 0.872 (0.872) data 0.609 (0.609) loss 5.6509 (5.6509) acc 31.2500 (31.2500) lr 1.7501e+01 eta 0:01:06\n",
      "epoch [25/100] batch [1/1] time 0.851 (0.851) data 0.589 (0.589) loss 5.4816 (5.4816) acc 40.6250 (40.6250) lr 1.7290e+01 eta 0:01:03\n",
      "epoch [26/100] batch [1/1] time 0.874 (0.874) data 0.611 (0.611) loss 5.8002 (5.8002) acc 21.8750 (21.8750) lr 1.7071e+01 eta 0:01:04\n",
      "epoch [27/100] batch [1/1] time 0.841 (0.841) data 0.576 (0.576) loss 5.2617 (5.2617) acc 31.2500 (31.2500) lr 1.6845e+01 eta 0:01:01\n",
      "epoch [28/100] batch [1/1] time 0.689 (0.689) data 0.429 (0.429) loss 5.0656 (5.0656) acc 31.2500 (31.2500) lr 1.6613e+01 eta 0:00:49\n",
      "epoch [29/100] batch [1/1] time 0.700 (0.700) data 0.439 (0.439) loss 4.8315 (4.8315) acc 25.0000 (25.0000) lr 1.6374e+01 eta 0:00:49\n",
      "epoch [30/100] batch [1/1] time 0.695 (0.695) data 0.436 (0.436) loss 4.5226 (4.5226) acc 56.2500 (56.2500) lr 1.6129e+01 eta 0:00:48\n",
      "epoch [31/100] batch [1/1] time 0.711 (0.711) data 0.451 (0.451) loss 4.3224 (4.3224) acc 40.6250 (40.6250) lr 1.5878e+01 eta 0:00:49\n",
      "epoch [32/100] batch [1/1] time 0.694 (0.694) data 0.432 (0.432) loss 4.2764 (4.2764) acc 31.2500 (31.2500) lr 1.5621e+01 eta 0:00:47\n",
      "epoch [33/100] batch [1/1] time 0.700 (0.700) data 0.436 (0.436) loss 4.7990 (4.7990) acc 18.7500 (18.7500) lr 1.5358e+01 eta 0:00:46\n",
      "epoch [34/100] batch [1/1] time 0.710 (0.710) data 0.450 (0.450) loss 5.5107 (5.5107) acc 18.7500 (18.7500) lr 1.5090e+01 eta 0:00:46\n",
      "epoch [35/100] batch [1/1] time 0.682 (0.682) data 0.420 (0.420) loss 4.8500 (4.8500) acc 18.7500 (18.7500) lr 1.4818e+01 eta 0:00:44\n",
      "epoch [36/100] batch [1/1] time 0.725 (0.725) data 0.463 (0.463) loss 3.7715 (3.7715) acc 40.6250 (40.6250) lr 1.4540e+01 eta 0:00:46\n",
      "epoch [37/100] batch [1/1] time 0.983 (0.983) data 0.718 (0.718) loss 4.7261 (4.7261) acc 18.7500 (18.7500) lr 1.4258e+01 eta 0:01:01\n",
      "epoch [38/100] batch [1/1] time 0.730 (0.730) data 0.465 (0.465) loss 3.9746 (3.9746) acc 21.8750 (21.8750) lr 1.3971e+01 eta 0:00:45\n",
      "epoch [39/100] batch [1/1] time 0.872 (0.872) data 0.607 (0.607) loss 4.0704 (4.0704) acc 12.5000 (12.5000) lr 1.3681e+01 eta 0:00:53\n",
      "epoch [40/100] batch [1/1] time 0.850 (0.850) data 0.580 (0.580) loss 4.1156 (4.1156) acc 21.8750 (21.8750) lr 1.3387e+01 eta 0:00:51\n",
      "epoch [41/100] batch [1/1] time 0.917 (0.917) data 0.651 (0.651) loss 3.4820 (3.4820) acc 40.6250 (40.6250) lr 1.3090e+01 eta 0:00:54\n",
      "epoch [42/100] batch [1/1] time 0.748 (0.748) data 0.485 (0.485) loss 4.5660 (4.5660) acc 12.5000 (12.5000) lr 1.2790e+01 eta 0:00:43\n",
      "epoch [43/100] batch [1/1] time 0.714 (0.714) data 0.451 (0.451) loss 6.4899 (6.4899) acc 12.5000 (12.5000) lr 1.2487e+01 eta 0:00:40\n",
      "epoch [44/100] batch [1/1] time 0.695 (0.695) data 0.434 (0.434) loss 11.1911 (11.1911) acc 9.3750 (9.3750) lr 1.2181e+01 eta 0:00:38\n",
      "epoch [45/100] batch [1/1] time 0.717 (0.717) data 0.455 (0.455) loss 9.0441 (9.0441) acc 12.5000 (12.5000) lr 1.1874e+01 eta 0:00:39\n",
      "epoch [46/100] batch [1/1] time 0.696 (0.696) data 0.435 (0.435) loss 6.6619 (6.6619) acc 12.5000 (12.5000) lr 1.1564e+01 eta 0:00:37\n",
      "epoch [47/100] batch [1/1] time 0.704 (0.704) data 0.442 (0.442) loss 5.1308 (5.1308) acc 21.8750 (21.8750) lr 1.1253e+01 eta 0:00:37\n",
      "epoch [48/100] batch [1/1] time 0.727 (0.727) data 0.463 (0.463) loss 4.2003 (4.2003) acc 15.6250 (15.6250) lr 1.0941e+01 eta 0:00:37\n",
      "epoch [49/100] batch [1/1] time 0.697 (0.697) data 0.433 (0.433) loss 3.6015 (3.6015) acc 21.8750 (21.8750) lr 1.0628e+01 eta 0:00:35\n",
      "epoch [50/100] batch [1/1] time 0.715 (0.715) data 0.453 (0.453) loss 3.1540 (3.1540) acc 31.2500 (31.2500) lr 1.0314e+01 eta 0:00:35\n",
      "epoch [51/100] batch [1/1] time 0.702 (0.702) data 0.433 (0.433) loss 2.9539 (2.9539) acc 34.3750 (34.3750) lr 1.0000e+01 eta 0:00:34\n",
      "epoch [52/100] batch [1/1] time 0.711 (0.711) data 0.447 (0.447) loss 2.7588 (2.7588) acc 40.6250 (40.6250) lr 9.6859e+00 eta 0:00:34\n",
      "epoch [53/100] batch [1/1] time 0.723 (0.723) data 0.455 (0.455) loss 2.5471 (2.5471) acc 37.5000 (37.5000) lr 9.3721e+00 eta 0:00:33\n",
      "epoch [54/100] batch [1/1] time 0.881 (0.881) data 0.613 (0.613) loss 2.5133 (2.5133) acc 37.5000 (37.5000) lr 9.0589e+00 eta 0:00:40\n",
      "epoch [55/100] batch [1/1] time 0.883 (0.883) data 0.616 (0.616) loss 2.2955 (2.2955) acc 53.1250 (53.1250) lr 8.7467e+00 eta 0:00:39\n",
      "epoch [56/100] batch [1/1] time 0.908 (0.908) data 0.626 (0.626) loss 1.9730 (1.9730) acc 56.2500 (56.2500) lr 8.4357e+00 eta 0:00:39\n",
      "epoch [57/100] batch [1/1] time 0.812 (0.812) data 0.549 (0.549) loss 1.9032 (1.9032) acc 59.3750 (59.3750) lr 8.1262e+00 eta 0:00:34\n",
      "epoch [58/100] batch [1/1] time 0.689 (0.689) data 0.425 (0.425) loss 1.8538 (1.8538) acc 59.3750 (59.3750) lr 7.8186e+00 eta 0:00:28\n",
      "epoch [59/100] batch [1/1] time 0.720 (0.720) data 0.458 (0.458) loss 2.0283 (2.0283) acc 62.5000 (62.5000) lr 7.5131e+00 eta 0:00:29\n",
      "epoch [60/100] batch [1/1] time 0.696 (0.696) data 0.432 (0.432) loss 1.5329 (1.5329) acc 68.7500 (68.7500) lr 7.2101e+00 eta 0:00:27\n",
      "epoch [61/100] batch [1/1] time 0.723 (0.723) data 0.462 (0.462) loss 1.3937 (1.3937) acc 87.5000 (87.5000) lr 6.9098e+00 eta 0:00:28\n",
      "epoch [62/100] batch [1/1] time 0.723 (0.723) data 0.459 (0.459) loss 1.5982 (1.5982) acc 71.8750 (71.8750) lr 6.6126e+00 eta 0:00:27\n",
      "epoch [63/100] batch [1/1] time 0.689 (0.689) data 0.425 (0.425) loss 1.6744 (1.6744) acc 56.2500 (56.2500) lr 6.3188e+00 eta 0:00:25\n",
      "epoch [64/100] batch [1/1] time 0.709 (0.709) data 0.445 (0.445) loss 1.4540 (1.4540) acc 75.0000 (75.0000) lr 6.0285e+00 eta 0:00:25\n",
      "epoch [65/100] batch [1/1] time 0.697 (0.697) data 0.435 (0.435) loss 1.6407 (1.6407) acc 68.7500 (68.7500) lr 5.7422e+00 eta 0:00:24\n",
      "epoch [66/100] batch [1/1] time 0.704 (0.704) data 0.441 (0.441) loss 1.7979 (1.7979) acc 59.3750 (59.3750) lr 5.4601e+00 eta 0:00:23\n",
      "epoch [67/100] batch [1/1] time 0.700 (0.700) data 0.436 (0.436) loss 1.2299 (1.2299) acc 84.3750 (84.3750) lr 5.1825e+00 eta 0:00:23\n",
      "epoch [68/100] batch [1/1] time 0.727 (0.727) data 0.463 (0.463) loss 1.7035 (1.7035) acc 59.3750 (59.3750) lr 4.9096e+00 eta 0:00:23\n",
      "epoch [69/100] batch [1/1] time 0.906 (0.906) data 0.639 (0.639) loss 1.6109 (1.6109) acc 68.7500 (68.7500) lr 4.6417e+00 eta 0:00:28\n",
      "epoch [70/100] batch [1/1] time 0.886 (0.886) data 0.616 (0.616) loss 1.4509 (1.4509) acc 68.7500 (68.7500) lr 4.3792e+00 eta 0:00:26\n",
      "epoch [71/100] batch [1/1] time 0.896 (0.896) data 0.631 (0.631) loss 1.3007 (1.3007) acc 81.2500 (81.2500) lr 4.1221e+00 eta 0:00:25\n",
      "epoch [72/100] batch [1/1] time 0.821 (0.821) data 0.564 (0.564) loss 1.3866 (1.3866) acc 75.0000 (75.0000) lr 3.8709e+00 eta 0:00:22\n",
      "epoch [73/100] batch [1/1] time 0.722 (0.722) data 0.459 (0.459) loss 1.0702 (1.0702) acc 90.6250 (90.6250) lr 3.6258e+00 eta 0:00:19\n",
      "epoch [74/100] batch [1/1] time 0.706 (0.706) data 0.441 (0.441) loss 1.1797 (1.1797) acc 81.2500 (81.2500) lr 3.3869e+00 eta 0:00:18\n",
      "epoch [75/100] batch [1/1] time 0.697 (0.697) data 0.433 (0.433) loss 1.1781 (1.1781) acc 78.1250 (78.1250) lr 3.1545e+00 eta 0:00:17\n",
      "epoch [76/100] batch [1/1] time 0.712 (0.712) data 0.448 (0.448) loss 0.9729 (0.9729) acc 87.5000 (87.5000) lr 2.9289e+00 eta 0:00:17\n",
      "epoch [77/100] batch [1/1] time 0.700 (0.700) data 0.435 (0.435) loss 1.1092 (1.1092) acc 87.5000 (87.5000) lr 2.7103e+00 eta 0:00:16\n",
      "epoch [78/100] batch [1/1] time 0.708 (0.708) data 0.443 (0.443) loss 0.9470 (0.9470) acc 90.6250 (90.6250) lr 2.4989e+00 eta 0:00:15\n",
      "epoch [79/100] batch [1/1] time 0.690 (0.690) data 0.429 (0.429) loss 1.1255 (1.1255) acc 84.3750 (84.3750) lr 2.2949e+00 eta 0:00:14\n",
      "epoch [80/100] batch [1/1] time 0.705 (0.705) data 0.439 (0.439) loss 1.2223 (1.2223) acc 78.1250 (78.1250) lr 2.0984e+00 eta 0:00:14\n",
      "epoch [81/100] batch [1/1] time 0.688 (0.688) data 0.424 (0.424) loss 0.8892 (0.8892) acc 87.5000 (87.5000) lr 1.9098e+00 eta 0:00:13\n",
      "epoch [82/100] batch [1/1] time 0.710 (0.710) data 0.448 (0.448) loss 1.0619 (1.0619) acc 87.5000 (87.5000) lr 1.7292e+00 eta 0:00:12\n",
      "epoch [83/100] batch [1/1] time 0.710 (0.710) data 0.448 (0.448) loss 0.9463 (0.9463) acc 90.6250 (90.6250) lr 1.5567e+00 eta 0:00:12\n",
      "epoch [84/100] batch [1/1] time 0.904 (0.904) data 0.639 (0.639) loss 1.0861 (1.0861) acc 84.3750 (84.3750) lr 1.3926e+00 eta 0:00:14\n",
      "epoch [85/100] batch [1/1] time 0.900 (0.900) data 0.634 (0.634) loss 0.8346 (0.8346) acc 96.8750 (96.8750) lr 1.2369e+00 eta 0:00:13\n",
      "epoch [86/100] batch [1/1] time 0.897 (0.897) data 0.629 (0.629) loss 1.1233 (1.1233) acc 78.1250 (78.1250) lr 1.0899e+00 eta 0:00:12\n",
      "epoch [87/100] batch [1/1] time 0.838 (0.838) data 0.575 (0.575) loss 0.7553 (0.7553) acc 93.7500 (93.7500) lr 9.5173e-01 eta 0:00:10\n",
      "epoch [88/100] batch [1/1] time 0.699 (0.699) data 0.435 (0.435) loss 1.0247 (1.0247) acc 84.3750 (84.3750) lr 8.2245e-01 eta 0:00:08\n",
      "epoch [89/100] batch [1/1] time 0.710 (0.710) data 0.446 (0.446) loss 0.9707 (0.9707) acc 84.3750 (84.3750) lr 7.0224e-01 eta 0:00:07\n",
      "epoch [90/100] batch [1/1] time 0.718 (0.718) data 0.453 (0.453) loss 0.8210 (0.8210) acc 87.5000 (87.5000) lr 5.9119e-01 eta 0:00:07\n",
      "epoch [91/100] batch [1/1] time 0.684 (0.684) data 0.420 (0.420) loss 0.8100 (0.8100) acc 93.7500 (93.7500) lr 4.8943e-01 eta 0:00:06\n",
      "epoch [92/100] batch [1/1] time 0.711 (0.711) data 0.449 (0.449) loss 0.7870 (0.7870) acc 96.8750 (96.8750) lr 3.9706e-01 eta 0:00:05\n",
      "epoch [93/100] batch [1/1] time 0.693 (0.693) data 0.428 (0.428) loss 0.8403 (0.8403) acc 93.7500 (93.7500) lr 3.1417e-01 eta 0:00:04\n",
      "epoch [94/100] batch [1/1] time 0.707 (0.707) data 0.444 (0.444) loss 1.1553 (1.1553) acc 78.1250 (78.1250) lr 2.4083e-01 eta 0:00:04\n",
      "epoch [95/100] batch [1/1] time 0.702 (0.702) data 0.435 (0.435) loss 0.9168 (0.9168) acc 93.7500 (93.7500) lr 1.7713e-01 eta 0:00:03\n",
      "epoch [96/100] batch [1/1] time 0.720 (0.720) data 0.456 (0.456) loss 0.7668 (0.7668) acc 93.7500 (93.7500) lr 1.2312e-01 eta 0:00:02\n",
      "epoch [97/100] batch [1/1] time 0.724 (0.724) data 0.461 (0.461) loss 0.9047 (0.9047) acc 93.7500 (93.7500) lr 7.8853e-02 eta 0:00:02\n",
      "epoch [98/100] batch [1/1] time 0.704 (0.704) data 0.437 (0.437) loss 0.9395 (0.9395) acc 87.5000 (87.5000) lr 4.4380e-02 eta 0:00:01\n",
      "epoch [99/100] batch [1/1] time 0.901 (0.901) data 0.625 (0.625) loss 0.7786 (0.7786) acc 93.7500 (93.7500) lr 1.9733e-02 eta 0:00:00\n",
      "epoch [100/100] batch [1/1] time 0.874 (0.874) data 0.607 (0.607) loss 0.7329 (0.7329) acc 96.8750 (96.8750) lr 4.9344e-03 eta 0:00:00\n",
      "Checkpoint saved to output/eurosat/DAPT/vit_b16_ep100_4shots/seed3/prompt_learner/model.pth.tar-100\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:57<00:00,  1.42it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 5,030\n",
      "* accuracy: 62.1%\n",
      "* error: 37.9%\n",
      "* macro_f1: 62.4%\n",
      "Elapsed: 0:02:32\n"
     ]
    }
   ],
   "source": [
    "#eurosat-4shots-seed3\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 3 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/eurosat/DAPT/vit_b16_ep100_4shots/seed3 \\\n",
    "        DATASET.NUM_SHOTS 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RgW5NPS-v9KQ",
    "outputId": "28ddb74f-cc28-41cc-cce0-5313e4e809e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:51:23.431818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:51:23.451413: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:51:23.457268: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:51:23.471408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:51:24.476419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2']\n",
      "output_dir: output/eurosat/DAPT/vit_b16_ep100_2shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/eurosat/DAPT/vit_b16_ep100_2shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  20\n",
      "# val      20\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/eurosat/DAPT/vit_b16_ep100_2shots/seed1/tensorboard)\n",
      "epoch [1/100] batch [1/1] time 1.554 (1.554) data 0.387 (0.387) loss 11.5547 (11.5547) acc 10.0000 (10.0000) lr 2.0000e+01 eta 0:02:33\n",
      "epoch [2/100] batch [1/1] time 0.492 (0.492) data 0.317 (0.317) loss 11.5602 (11.5602) acc 10.0000 (10.0000) lr 1.9995e+01 eta 0:00:48\n",
      "epoch [3/100] batch [1/1] time 0.521 (0.521) data 0.349 (0.349) loss 10.5728 (10.5728) acc 15.0000 (15.0000) lr 1.9980e+01 eta 0:00:50\n",
      "epoch [4/100] batch [1/1] time 0.507 (0.507) data 0.332 (0.332) loss 10.3400 (10.3400) acc 5.0000 (5.0000) lr 1.9956e+01 eta 0:00:48\n",
      "epoch [5/100] batch [1/1] time 0.492 (0.492) data 0.318 (0.318) loss 10.1112 (10.1112) acc 0.0000 (0.0000) lr 1.9921e+01 eta 0:00:46\n",
      "epoch [6/100] batch [1/1] time 0.519 (0.519) data 0.347 (0.347) loss 10.0639 (10.0639) acc 10.0000 (10.0000) lr 1.9877e+01 eta 0:00:48\n",
      "epoch [7/100] batch [1/1] time 0.515 (0.515) data 0.336 (0.336) loss 9.9343 (9.9343) acc 10.0000 (10.0000) lr 1.9823e+01 eta 0:00:47\n",
      "epoch [8/100] batch [1/1] time 0.504 (0.504) data 0.330 (0.330) loss 9.9144 (9.9144) acc 5.0000 (5.0000) lr 1.9759e+01 eta 0:00:46\n",
      "epoch [9/100] batch [1/1] time 0.523 (0.523) data 0.345 (0.345) loss 9.6467 (9.6467) acc 30.0000 (30.0000) lr 1.9686e+01 eta 0:00:47\n",
      "epoch [10/100] batch [1/1] time 0.653 (0.653) data 0.474 (0.474) loss 9.6542 (9.6542) acc 10.0000 (10.0000) lr 1.9603e+01 eta 0:00:58\n",
      "epoch [11/100] batch [1/1] time 0.665 (0.665) data 0.481 (0.481) loss 9.5058 (9.5058) acc 20.0000 (20.0000) lr 1.9511e+01 eta 0:00:59\n",
      "epoch [12/100] batch [1/1] time 0.652 (0.652) data 0.477 (0.477) loss 9.4347 (9.4347) acc 30.0000 (30.0000) lr 1.9409e+01 eta 0:00:57\n",
      "epoch [13/100] batch [1/1] time 0.633 (0.633) data 0.449 (0.449) loss 9.3213 (9.3213) acc 35.0000 (35.0000) lr 1.9298e+01 eta 0:00:55\n",
      "epoch [14/100] batch [1/1] time 0.525 (0.525) data 0.349 (0.349) loss 9.1918 (9.1918) acc 40.0000 (40.0000) lr 1.9178e+01 eta 0:00:45\n",
      "epoch [15/100] batch [1/1] time 0.500 (0.500) data 0.325 (0.325) loss 9.1402 (9.1402) acc 30.0000 (30.0000) lr 1.9048e+01 eta 0:00:42\n",
      "epoch [16/100] batch [1/1] time 0.502 (0.502) data 0.327 (0.327) loss 8.8939 (8.8939) acc 40.0000 (40.0000) lr 1.8910e+01 eta 0:00:42\n",
      "epoch [17/100] batch [1/1] time 0.514 (0.514) data 0.338 (0.338) loss 9.1575 (9.1575) acc 15.0000 (15.0000) lr 1.8763e+01 eta 0:00:42\n",
      "epoch [18/100] batch [1/1] time 0.500 (0.500) data 0.326 (0.326) loss 9.5731 (9.5731) acc 25.0000 (25.0000) lr 1.8607e+01 eta 0:00:41\n",
      "epoch [19/100] batch [1/1] time 0.528 (0.528) data 0.354 (0.354) loss 9.1348 (9.1348) acc 35.0000 (35.0000) lr 1.8443e+01 eta 0:00:42\n",
      "epoch [20/100] batch [1/1] time 0.502 (0.502) data 0.330 (0.330) loss 8.9968 (8.9968) acc 25.0000 (25.0000) lr 1.8271e+01 eta 0:00:40\n",
      "epoch [21/100] batch [1/1] time 0.496 (0.496) data 0.321 (0.321) loss 8.9158 (8.9158) acc 15.0000 (15.0000) lr 1.8090e+01 eta 0:00:39\n",
      "epoch [22/100] batch [1/1] time 0.520 (0.520) data 0.347 (0.347) loss 8.7121 (8.7121) acc 15.0000 (15.0000) lr 1.7902e+01 eta 0:00:40\n",
      "epoch [23/100] batch [1/1] time 0.513 (0.513) data 0.338 (0.338) loss 8.4422 (8.4422) acc 35.0000 (35.0000) lr 1.7705e+01 eta 0:00:39\n",
      "epoch [24/100] batch [1/1] time 0.509 (0.509) data 0.332 (0.332) loss 8.1612 (8.1612) acc 25.0000 (25.0000) lr 1.7501e+01 eta 0:00:38\n",
      "epoch [25/100] batch [1/1] time 0.527 (0.527) data 0.353 (0.353) loss 7.9184 (7.9184) acc 30.0000 (30.0000) lr 1.7290e+01 eta 0:00:39\n",
      "epoch [26/100] batch [1/1] time 0.511 (0.511) data 0.333 (0.333) loss 7.6485 (7.6485) acc 35.0000 (35.0000) lr 1.7071e+01 eta 0:00:37\n",
      "epoch [27/100] batch [1/1] time 0.500 (0.500) data 0.325 (0.325) loss 7.2010 (7.2010) acc 35.0000 (35.0000) lr 1.6845e+01 eta 0:00:36\n",
      "epoch [28/100] batch [1/1] time 0.522 (0.522) data 0.345 (0.345) loss 6.8317 (6.8317) acc 45.0000 (45.0000) lr 1.6613e+01 eta 0:00:37\n",
      "epoch [29/100] batch [1/1] time 0.549 (0.549) data 0.370 (0.370) loss 6.5209 (6.5209) acc 55.0000 (55.0000) lr 1.6374e+01 eta 0:00:39\n",
      "epoch [30/100] batch [1/1] time 0.636 (0.636) data 0.457 (0.457) loss 6.9555 (6.9555) acc 40.0000 (40.0000) lr 1.6129e+01 eta 0:00:44\n",
      "epoch [31/100] batch [1/1] time 0.645 (0.645) data 0.469 (0.469) loss 6.1661 (6.1661) acc 50.0000 (50.0000) lr 1.5878e+01 eta 0:00:44\n",
      "epoch [32/100] batch [1/1] time 0.641 (0.641) data 0.462 (0.462) loss 7.4973 (7.4973) acc 10.0000 (10.0000) lr 1.5621e+01 eta 0:00:43\n",
      "epoch [33/100] batch [1/1] time 0.663 (0.663) data 0.487 (0.487) loss 6.0925 (6.0925) acc 25.0000 (25.0000) lr 1.5358e+01 eta 0:00:44\n",
      "epoch [34/100] batch [1/1] time 0.508 (0.508) data 0.332 (0.332) loss 6.2818 (6.2818) acc 15.0000 (15.0000) lr 1.5090e+01 eta 0:00:33\n",
      "epoch [35/100] batch [1/1] time 0.495 (0.495) data 0.319 (0.319) loss 5.3239 (5.3239) acc 30.0000 (30.0000) lr 1.4818e+01 eta 0:00:32\n",
      "epoch [36/100] batch [1/1] time 0.522 (0.522) data 0.347 (0.347) loss 5.1337 (5.1337) acc 35.0000 (35.0000) lr 1.4540e+01 eta 0:00:33\n",
      "epoch [37/100] batch [1/1] time 0.499 (0.499) data 0.322 (0.322) loss 4.4870 (4.4870) acc 25.0000 (25.0000) lr 1.4258e+01 eta 0:00:31\n",
      "epoch [38/100] batch [1/1] time 0.505 (0.505) data 0.328 (0.328) loss 4.2485 (4.2485) acc 30.0000 (30.0000) lr 1.3971e+01 eta 0:00:31\n",
      "epoch [39/100] batch [1/1] time 0.537 (0.537) data 0.363 (0.363) loss 4.0659 (4.0659) acc 30.0000 (30.0000) lr 1.3681e+01 eta 0:00:32\n",
      "epoch [40/100] batch [1/1] time 0.511 (0.511) data 0.335 (0.335) loss 4.0909 (4.0909) acc 30.0000 (30.0000) lr 1.3387e+01 eta 0:00:30\n",
      "epoch [41/100] batch [1/1] time 0.504 (0.504) data 0.328 (0.328) loss 4.6942 (4.6942) acc 35.0000 (35.0000) lr 1.3090e+01 eta 0:00:29\n",
      "epoch [42/100] batch [1/1] time 0.514 (0.514) data 0.339 (0.339) loss 3.9539 (3.9539) acc 25.0000 (25.0000) lr 1.2790e+01 eta 0:00:29\n",
      "epoch [43/100] batch [1/1] time 0.491 (0.491) data 0.315 (0.315) loss 6.3667 (6.3667) acc 20.0000 (20.0000) lr 1.2487e+01 eta 0:00:28\n",
      "epoch [44/100] batch [1/1] time 0.508 (0.508) data 0.332 (0.332) loss 11.7325 (11.7325) acc 10.0000 (10.0000) lr 1.2181e+01 eta 0:00:28\n",
      "epoch [45/100] batch [1/1] time 0.512 (0.512) data 0.336 (0.336) loss 8.3638 (8.3638) acc 20.0000 (20.0000) lr 1.1874e+01 eta 0:00:28\n",
      "epoch [46/100] batch [1/1] time 0.507 (0.507) data 0.333 (0.333) loss 6.0327 (6.0327) acc 15.0000 (15.0000) lr 1.1564e+01 eta 0:00:27\n",
      "epoch [47/100] batch [1/1] time 0.522 (0.522) data 0.348 (0.348) loss 4.1172 (4.1172) acc 15.0000 (15.0000) lr 1.1253e+01 eta 0:00:27\n",
      "epoch [48/100] batch [1/1] time 0.501 (0.501) data 0.324 (0.324) loss 3.5921 (3.5921) acc 30.0000 (30.0000) lr 1.0941e+01 eta 0:00:26\n",
      "epoch [49/100] batch [1/1] time 0.645 (0.645) data 0.465 (0.465) loss 3.2996 (3.2996) acc 25.0000 (25.0000) lr 1.0628e+01 eta 0:00:32\n",
      "epoch [50/100] batch [1/1] time 0.657 (0.657) data 0.479 (0.479) loss 3.2379 (3.2379) acc 15.0000 (15.0000) lr 1.0314e+01 eta 0:00:32\n",
      "epoch [51/100] batch [1/1] time 0.692 (0.692) data 0.513 (0.513) loss 3.0723 (3.0723) acc 20.0000 (20.0000) lr 1.0000e+01 eta 0:00:33\n",
      "epoch [52/100] batch [1/1] time 0.614 (0.614) data 0.432 (0.432) loss 2.9853 (2.9853) acc 15.0000 (15.0000) lr 9.6859e+00 eta 0:00:29\n",
      "epoch [53/100] batch [1/1] time 0.523 (0.523) data 0.345 (0.345) loss 2.6339 (2.6339) acc 40.0000 (40.0000) lr 9.3721e+00 eta 0:00:24\n",
      "epoch [54/100] batch [1/1] time 0.512 (0.512) data 0.335 (0.335) loss 2.5816 (2.5816) acc 45.0000 (45.0000) lr 9.0589e+00 eta 0:00:23\n",
      "epoch [55/100] batch [1/1] time 0.508 (0.508) data 0.332 (0.332) loss 2.0334 (2.0334) acc 70.0000 (70.0000) lr 8.7467e+00 eta 0:00:22\n",
      "epoch [56/100] batch [1/1] time 0.511 (0.511) data 0.335 (0.335) loss 2.0721 (2.0721) acc 55.0000 (55.0000) lr 8.4357e+00 eta 0:00:22\n",
      "epoch [57/100] batch [1/1] time 0.518 (0.518) data 0.342 (0.342) loss 1.7189 (1.7189) acc 75.0000 (75.0000) lr 8.1262e+00 eta 0:00:22\n",
      "epoch [58/100] batch [1/1] time 0.523 (0.523) data 0.346 (0.346) loss 1.8759 (1.8759) acc 65.0000 (65.0000) lr 7.8186e+00 eta 0:00:21\n",
      "epoch [59/100] batch [1/1] time 0.509 (0.509) data 0.332 (0.332) loss 1.6082 (1.6082) acc 60.0000 (60.0000) lr 7.5131e+00 eta 0:00:20\n",
      "epoch [60/100] batch [1/1] time 0.541 (0.541) data 0.365 (0.365) loss 1.3403 (1.3403) acc 85.0000 (85.0000) lr 7.2101e+00 eta 0:00:21\n",
      "epoch [61/100] batch [1/1] time 0.530 (0.530) data 0.334 (0.334) loss 1.3238 (1.3238) acc 75.0000 (75.0000) lr 6.9098e+00 eta 0:00:20\n",
      "epoch [62/100] batch [1/1] time 0.502 (0.502) data 0.325 (0.325) loss 1.3450 (1.3450) acc 80.0000 (80.0000) lr 6.6126e+00 eta 0:00:19\n",
      "epoch [63/100] batch [1/1] time 0.521 (0.521) data 0.345 (0.345) loss 0.9740 (0.9740) acc 95.0000 (95.0000) lr 6.3188e+00 eta 0:00:19\n",
      "epoch [64/100] batch [1/1] time 0.491 (0.491) data 0.326 (0.326) loss 1.2352 (1.2352) acc 80.0000 (80.0000) lr 6.0285e+00 eta 0:00:17\n",
      "epoch [65/100] batch [1/1] time 0.524 (0.524) data 0.347 (0.347) loss 0.9542 (0.9542) acc 95.0000 (95.0000) lr 5.7422e+00 eta 0:00:18\n",
      "epoch [66/100] batch [1/1] time 0.511 (0.511) data 0.335 (0.335) loss 0.9734 (0.9734) acc 90.0000 (90.0000) lr 5.4601e+00 eta 0:00:17\n",
      "epoch [67/100] batch [1/1] time 0.515 (0.515) data 0.339 (0.339) loss 0.9317 (0.9317) acc 95.0000 (95.0000) lr 5.1825e+00 eta 0:00:17\n",
      "epoch [68/100] batch [1/1] time 0.726 (0.726) data 0.538 (0.538) loss 0.8166 (0.8166) acc 100.0000 (100.0000) lr 4.9096e+00 eta 0:00:23\n",
      "epoch [69/100] batch [1/1] time 0.708 (0.708) data 0.528 (0.528) loss 0.8407 (0.8407) acc 100.0000 (100.0000) lr 4.6417e+00 eta 0:00:21\n",
      "epoch [70/100] batch [1/1] time 0.656 (0.656) data 0.471 (0.471) loss 1.0049 (1.0049) acc 95.0000 (95.0000) lr 4.3792e+00 eta 0:00:19\n",
      "epoch [71/100] batch [1/1] time 0.609 (0.609) data 0.427 (0.427) loss 0.9390 (0.9390) acc 90.0000 (90.0000) lr 4.1221e+00 eta 0:00:17\n",
      "epoch [72/100] batch [1/1] time 0.648 (0.648) data 0.472 (0.472) loss 0.8026 (0.8026) acc 95.0000 (95.0000) lr 3.8709e+00 eta 0:00:18\n",
      "epoch [73/100] batch [1/1] time 0.502 (0.502) data 0.323 (0.323) loss 0.8224 (0.8224) acc 90.0000 (90.0000) lr 3.6258e+00 eta 0:00:13\n",
      "epoch [74/100] batch [1/1] time 0.522 (0.522) data 0.344 (0.344) loss 0.7043 (0.7043) acc 100.0000 (100.0000) lr 3.3869e+00 eta 0:00:13\n",
      "epoch [75/100] batch [1/1] time 0.502 (0.502) data 0.324 (0.324) loss 0.9023 (0.9023) acc 90.0000 (90.0000) lr 3.1545e+00 eta 0:00:12\n",
      "epoch [76/100] batch [1/1] time 0.520 (0.520) data 0.342 (0.342) loss 0.8184 (0.8184) acc 90.0000 (90.0000) lr 2.9289e+00 eta 0:00:12\n",
      "epoch [77/100] batch [1/1] time 0.513 (0.513) data 0.336 (0.336) loss 0.9231 (0.9231) acc 85.0000 (85.0000) lr 2.7103e+00 eta 0:00:11\n",
      "epoch [78/100] batch [1/1] time 0.520 (0.520) data 0.344 (0.344) loss 0.6653 (0.6653) acc 100.0000 (100.0000) lr 2.4989e+00 eta 0:00:11\n",
      "epoch [79/100] batch [1/1] time 0.503 (0.503) data 0.325 (0.325) loss 0.7753 (0.7753) acc 95.0000 (95.0000) lr 2.2949e+00 eta 0:00:10\n",
      "epoch [80/100] batch [1/1] time 0.511 (0.511) data 0.334 (0.334) loss 0.7943 (0.7943) acc 90.0000 (90.0000) lr 2.0984e+00 eta 0:00:10\n",
      "epoch [81/100] batch [1/1] time 0.591 (0.591) data 0.414 (0.414) loss 0.6809 (0.6809) acc 100.0000 (100.0000) lr 1.9098e+00 eta 0:00:11\n",
      "epoch [82/100] batch [1/1] time 0.557 (0.557) data 0.358 (0.358) loss 0.6871 (0.6871) acc 100.0000 (100.0000) lr 1.7292e+00 eta 0:00:10\n",
      "epoch [83/100] batch [1/1] time 0.548 (0.548) data 0.372 (0.372) loss 0.8864 (0.8864) acc 90.0000 (90.0000) lr 1.5567e+00 eta 0:00:09\n",
      "epoch [84/100] batch [1/1] time 0.507 (0.507) data 0.328 (0.328) loss 0.9322 (0.9322) acc 90.0000 (90.0000) lr 1.3926e+00 eta 0:00:08\n",
      "epoch [85/100] batch [1/1] time 0.517 (0.517) data 0.336 (0.336) loss 0.6894 (0.6894) acc 95.0000 (95.0000) lr 1.2369e+00 eta 0:00:07\n",
      "epoch [86/100] batch [1/1] time 0.501 (0.501) data 0.325 (0.325) loss 0.6885 (0.6885) acc 95.0000 (95.0000) lr 1.0899e+00 eta 0:00:07\n",
      "epoch [87/100] batch [1/1] time 0.535 (0.535) data 0.354 (0.354) loss 0.6755 (0.6755) acc 95.0000 (95.0000) lr 9.5173e-01 eta 0:00:06\n",
      "epoch [88/100] batch [1/1] time 0.678 (0.678) data 0.497 (0.497) loss 0.7489 (0.7489) acc 95.0000 (95.0000) lr 8.2245e-01 eta 0:00:08\n",
      "epoch [89/100] batch [1/1] time 0.634 (0.634) data 0.454 (0.454) loss 0.8320 (0.8320) acc 95.0000 (95.0000) lr 7.0224e-01 eta 0:00:06\n",
      "epoch [90/100] batch [1/1] time 0.667 (0.667) data 0.486 (0.486) loss 0.6618 (0.6618) acc 100.0000 (100.0000) lr 5.9119e-01 eta 0:00:06\n",
      "epoch [91/100] batch [1/1] time 0.673 (0.673) data 0.495 (0.495) loss 0.7995 (0.7995) acc 90.0000 (90.0000) lr 4.8943e-01 eta 0:00:06\n",
      "epoch [92/100] batch [1/1] time 0.529 (0.529) data 0.352 (0.352) loss 1.0216 (1.0216) acc 90.0000 (90.0000) lr 3.9706e-01 eta 0:00:04\n",
      "epoch [93/100] batch [1/1] time 0.513 (0.513) data 0.335 (0.335) loss 0.6586 (0.6586) acc 100.0000 (100.0000) lr 3.1417e-01 eta 0:00:03\n",
      "epoch [94/100] batch [1/1] time 0.513 (0.513) data 0.335 (0.335) loss 0.6729 (0.6729) acc 90.0000 (90.0000) lr 2.4083e-01 eta 0:00:03\n",
      "epoch [95/100] batch [1/1] time 0.517 (0.517) data 0.341 (0.341) loss 0.6457 (0.6457) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:02\n",
      "epoch [96/100] batch [1/1] time 0.565 (0.565) data 0.386 (0.386) loss 0.6655 (0.6655) acc 95.0000 (95.0000) lr 1.2312e-01 eta 0:00:02\n",
      "epoch [97/100] batch [1/1] time 0.528 (0.528) data 0.347 (0.347) loss 0.6620 (0.6620) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:01\n",
      "epoch [98/100] batch [1/1] time 0.510 (0.510) data 0.331 (0.331) loss 0.6798 (0.6798) acc 95.0000 (95.0000) lr 4.4380e-02 eta 0:00:01\n",
      "epoch [99/100] batch [1/1] time 0.528 (0.528) data 0.351 (0.351) loss 0.6556 (0.6556) acc 95.0000 (95.0000) lr 1.9733e-02 eta 0:00:00\n",
      "epoch [100/100] batch [1/1] time 0.508 (0.508) data 0.332 (0.332) loss 0.5755 (0.5755) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:00\n",
      "Checkpoint saved to output/eurosat/DAPT/vit_b16_ep100_2shots/seed1/prompt_learner/model.pth.tar-100\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:59<00:00,  1.36it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 5,458\n",
      "* accuracy: 67.4%\n",
      "* error: 32.6%\n",
      "* macro_f1: 64.7%\n",
      "Elapsed: 0:02:12\n"
     ]
    }
   ],
   "source": [
    "#eurosat-2shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/eurosat/DAPT/vit_b16_ep100_2shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2wTiCjlLXd7",
    "outputId": "d0149f0d-1c7a-4ec6-e092-5e800634f0a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:41:23.660401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:41:23.679508: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:41:23.685312: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:41:23.699281: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:41:24.711782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2']\n",
      "output_dir: output/eurosat/DAPT/vit_b16_ep100_2shots/seed2\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/eurosat/DAPT/vit_b16_ep100_2shots/seed2\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_2.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  20\n",
      "# val      20\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/eurosat/DAPT/vit_b16_ep100_2shots/seed2/tensorboard)\n",
      "epoch [1/100] batch [1/1] time 1.584 (1.584) data 0.395 (0.395) loss 11.4453 (11.4453) acc 15.0000 (15.0000) lr 2.0000e+01 eta 0:02:36\n",
      "epoch [2/100] batch [1/1] time 0.515 (0.515) data 0.344 (0.344) loss 11.3769 (11.3769) acc 20.0000 (20.0000) lr 1.9995e+01 eta 0:00:50\n",
      "epoch [3/100] batch [1/1] time 0.523 (0.523) data 0.352 (0.352) loss 10.7150 (10.7150) acc 10.0000 (10.0000) lr 1.9980e+01 eta 0:00:50\n",
      "epoch [4/100] batch [1/1] time 0.530 (0.530) data 0.360 (0.360) loss 9.5569 (9.5569) acc 10.0000 (10.0000) lr 1.9956e+01 eta 0:00:50\n",
      "epoch [5/100] batch [1/1] time 0.521 (0.521) data 0.350 (0.350) loss 9.2181 (9.2181) acc 10.0000 (10.0000) lr 1.9921e+01 eta 0:00:49\n",
      "epoch [6/100] batch [1/1] time 0.539 (0.539) data 0.370 (0.370) loss 9.0796 (9.0796) acc 10.0000 (10.0000) lr 1.9877e+01 eta 0:00:50\n",
      "epoch [7/100] batch [1/1] time 0.526 (0.526) data 0.356 (0.356) loss 8.8093 (8.8093) acc 10.0000 (10.0000) lr 1.9823e+01 eta 0:00:48\n",
      "epoch [8/100] batch [1/1] time 0.529 (0.529) data 0.359 (0.359) loss 8.6678 (8.6678) acc 15.0000 (15.0000) lr 1.9759e+01 eta 0:00:48\n",
      "epoch [9/100] batch [1/1] time 0.776 (0.776) data 0.586 (0.586) loss 8.8407 (8.8407) acc 10.0000 (10.0000) lr 1.9686e+01 eta 0:01:10\n",
      "epoch [10/100] batch [1/1] time 0.716 (0.716) data 0.540 (0.540) loss 8.6815 (8.6815) acc 25.0000 (25.0000) lr 1.9603e+01 eta 0:01:04\n",
      "epoch [11/100] batch [1/1] time 0.664 (0.664) data 0.489 (0.489) loss 8.4627 (8.4627) acc 15.0000 (15.0000) lr 1.9511e+01 eta 0:00:59\n",
      "epoch [12/100] batch [1/1] time 0.687 (0.687) data 0.515 (0.515) loss 8.2539 (8.2539) acc 25.0000 (25.0000) lr 1.9409e+01 eta 0:01:00\n",
      "epoch [13/100] batch [1/1] time 0.656 (0.656) data 0.483 (0.483) loss 8.1041 (8.1041) acc 15.0000 (15.0000) lr 1.9298e+01 eta 0:00:57\n",
      "epoch [14/100] batch [1/1] time 0.518 (0.518) data 0.351 (0.351) loss 7.9181 (7.9181) acc 25.0000 (25.0000) lr 1.9178e+01 eta 0:00:44\n",
      "epoch [15/100] batch [1/1] time 0.539 (0.539) data 0.369 (0.369) loss 7.7865 (7.7865) acc 35.0000 (35.0000) lr 1.9048e+01 eta 0:00:45\n",
      "epoch [16/100] batch [1/1] time 0.508 (0.508) data 0.339 (0.339) loss 7.6683 (7.6683) acc 25.0000 (25.0000) lr 1.8910e+01 eta 0:00:42\n",
      "epoch [17/100] batch [1/1] time 0.539 (0.539) data 0.369 (0.369) loss 7.4276 (7.4276) acc 50.0000 (50.0000) lr 1.8763e+01 eta 0:00:44\n",
      "epoch [18/100] batch [1/1] time 0.525 (0.525) data 0.355 (0.355) loss 7.3048 (7.3048) acc 30.0000 (30.0000) lr 1.8607e+01 eta 0:00:43\n",
      "epoch [19/100] batch [1/1] time 0.537 (0.537) data 0.366 (0.366) loss 7.0818 (7.0818) acc 50.0000 (50.0000) lr 1.8443e+01 eta 0:00:43\n",
      "epoch [20/100] batch [1/1] time 0.538 (0.538) data 0.368 (0.368) loss 6.9205 (6.9205) acc 45.0000 (45.0000) lr 1.8271e+01 eta 0:00:43\n",
      "epoch [21/100] batch [1/1] time 0.535 (0.535) data 0.364 (0.364) loss 6.7695 (6.7695) acc 50.0000 (50.0000) lr 1.8090e+01 eta 0:00:42\n",
      "epoch [22/100] batch [1/1] time 0.533 (0.533) data 0.362 (0.362) loss 6.5474 (6.5474) acc 55.0000 (55.0000) lr 1.7902e+01 eta 0:00:41\n",
      "epoch [23/100] batch [1/1] time 0.529 (0.529) data 0.359 (0.359) loss 6.3181 (6.3181) acc 55.0000 (55.0000) lr 1.7705e+01 eta 0:00:40\n",
      "epoch [24/100] batch [1/1] time 0.544 (0.544) data 0.375 (0.375) loss 6.1110 (6.1110) acc 70.0000 (70.0000) lr 1.7501e+01 eta 0:00:41\n",
      "epoch [25/100] batch [1/1] time 0.519 (0.519) data 0.348 (0.348) loss 6.3571 (6.3571) acc 50.0000 (50.0000) lr 1.7290e+01 eta 0:00:38\n",
      "epoch [26/100] batch [1/1] time 0.555 (0.555) data 0.383 (0.383) loss 6.5699 (6.5699) acc 55.0000 (55.0000) lr 1.7071e+01 eta 0:00:41\n",
      "epoch [27/100] batch [1/1] time 0.555 (0.555) data 0.383 (0.383) loss 6.1967 (6.1967) acc 50.0000 (50.0000) lr 1.6845e+01 eta 0:00:40\n",
      "epoch [28/100] batch [1/1] time 0.693 (0.693) data 0.516 (0.516) loss 5.7585 (5.7585) acc 60.0000 (60.0000) lr 1.6613e+01 eta 0:00:49\n",
      "epoch [29/100] batch [1/1] time 0.657 (0.657) data 0.484 (0.484) loss 5.5451 (5.5451) acc 50.0000 (50.0000) lr 1.6374e+01 eta 0:00:46\n",
      "epoch [30/100] batch [1/1] time 0.699 (0.699) data 0.522 (0.522) loss 5.6546 (5.6546) acc 60.0000 (60.0000) lr 1.6129e+01 eta 0:00:48\n",
      "epoch [31/100] batch [1/1] time 0.625 (0.625) data 0.442 (0.442) loss 5.2242 (5.2242) acc 45.0000 (45.0000) lr 1.5878e+01 eta 0:00:43\n",
      "epoch [32/100] batch [1/1] time 0.633 (0.633) data 0.463 (0.463) loss 4.8448 (4.8448) acc 75.0000 (75.0000) lr 1.5621e+01 eta 0:00:43\n",
      "epoch [33/100] batch [1/1] time 0.548 (0.548) data 0.376 (0.376) loss 4.4639 (4.4639) acc 70.0000 (70.0000) lr 1.5358e+01 eta 0:00:36\n",
      "epoch [34/100] batch [1/1] time 0.520 (0.520) data 0.350 (0.350) loss 4.2087 (4.2087) acc 65.0000 (65.0000) lr 1.5090e+01 eta 0:00:34\n",
      "epoch [35/100] batch [1/1] time 0.540 (0.540) data 0.368 (0.368) loss 4.2007 (4.2007) acc 55.0000 (55.0000) lr 1.4818e+01 eta 0:00:35\n",
      "epoch [36/100] batch [1/1] time 0.526 (0.526) data 0.355 (0.355) loss 3.5327 (3.5327) acc 80.0000 (80.0000) lr 1.4540e+01 eta 0:00:33\n",
      "epoch [37/100] batch [1/1] time 0.528 (0.528) data 0.356 (0.356) loss 3.4159 (3.4159) acc 55.0000 (55.0000) lr 1.4258e+01 eta 0:00:33\n",
      "epoch [38/100] batch [1/1] time 0.525 (0.525) data 0.353 (0.353) loss 3.8809 (3.8809) acc 45.0000 (45.0000) lr 1.3971e+01 eta 0:00:32\n",
      "epoch [39/100] batch [1/1] time 0.524 (0.524) data 0.353 (0.353) loss 3.0491 (3.0491) acc 45.0000 (45.0000) lr 1.3681e+01 eta 0:00:31\n",
      "epoch [40/100] batch [1/1] time 0.523 (0.523) data 0.351 (0.351) loss 3.5134 (3.5134) acc 30.0000 (30.0000) lr 1.3387e+01 eta 0:00:31\n",
      "epoch [41/100] batch [1/1] time 0.529 (0.529) data 0.356 (0.356) loss 4.2490 (4.2490) acc 25.0000 (25.0000) lr 1.3090e+01 eta 0:00:31\n",
      "epoch [42/100] batch [1/1] time 0.540 (0.540) data 0.369 (0.369) loss 5.4792 (5.4792) acc 15.0000 (15.0000) lr 1.2790e+01 eta 0:00:31\n",
      "epoch [43/100] batch [1/1] time 0.538 (0.538) data 0.367 (0.367) loss 6.7557 (6.7557) acc 10.0000 (10.0000) lr 1.2487e+01 eta 0:00:30\n",
      "epoch [44/100] batch [1/1] time 0.537 (0.537) data 0.364 (0.364) loss 9.1655 (9.1655) acc 0.0000 (0.0000) lr 1.2181e+01 eta 0:00:30\n",
      "epoch [45/100] batch [1/1] time 0.612 (0.612) data 0.411 (0.411) loss 6.4514 (6.4514) acc 10.0000 (10.0000) lr 1.1874e+01 eta 0:00:33\n",
      "epoch [46/100] batch [1/1] time 0.538 (0.538) data 0.367 (0.367) loss 5.1306 (5.1306) acc 10.0000 (10.0000) lr 1.1564e+01 eta 0:00:29\n",
      "epoch [47/100] batch [1/1] time 0.682 (0.682) data 0.506 (0.506) loss 4.4352 (4.4352) acc 15.0000 (15.0000) lr 1.1253e+01 eta 0:00:36\n",
      "epoch [48/100] batch [1/1] time 0.679 (0.679) data 0.506 (0.506) loss 4.1645 (4.1645) acc 20.0000 (20.0000) lr 1.0941e+01 eta 0:00:35\n",
      "epoch [49/100] batch [1/1] time 0.764 (0.764) data 0.570 (0.570) loss 3.9784 (3.9784) acc 20.0000 (20.0000) lr 1.0628e+01 eta 0:00:38\n",
      "epoch [50/100] batch [1/1] time 0.641 (0.641) data 0.464 (0.464) loss 3.7275 (3.7275) acc 30.0000 (30.0000) lr 1.0314e+01 eta 0:00:32\n",
      "epoch [51/100] batch [1/1] time 0.628 (0.628) data 0.456 (0.456) loss 3.5403 (3.5403) acc 40.0000 (40.0000) lr 1.0000e+01 eta 0:00:30\n",
      "epoch [52/100] batch [1/1] time 0.537 (0.537) data 0.364 (0.364) loss 3.4267 (3.4267) acc 30.0000 (30.0000) lr 9.6859e+00 eta 0:00:25\n",
      "epoch [53/100] batch [1/1] time 0.510 (0.510) data 0.337 (0.337) loss 3.2619 (3.2619) acc 30.0000 (30.0000) lr 9.3721e+00 eta 0:00:23\n",
      "epoch [54/100] batch [1/1] time 0.535 (0.535) data 0.361 (0.361) loss 3.1350 (3.1350) acc 35.0000 (35.0000) lr 9.0589e+00 eta 0:00:24\n",
      "epoch [55/100] batch [1/1] time 0.527 (0.527) data 0.356 (0.356) loss 2.9764 (2.9764) acc 45.0000 (45.0000) lr 8.7467e+00 eta 0:00:23\n",
      "epoch [56/100] batch [1/1] time 0.514 (0.514) data 0.340 (0.340) loss 2.9659 (2.9659) acc 35.0000 (35.0000) lr 8.4357e+00 eta 0:00:22\n",
      "epoch [57/100] batch [1/1] time 0.545 (0.545) data 0.373 (0.373) loss 2.7511 (2.7511) acc 35.0000 (35.0000) lr 8.1262e+00 eta 0:00:23\n",
      "epoch [58/100] batch [1/1] time 0.519 (0.519) data 0.348 (0.348) loss 2.7430 (2.7430) acc 50.0000 (50.0000) lr 7.8186e+00 eta 0:00:21\n",
      "epoch [59/100] batch [1/1] time 0.521 (0.521) data 0.347 (0.347) loss 2.7232 (2.7232) acc 50.0000 (50.0000) lr 7.5131e+00 eta 0:00:21\n",
      "epoch [60/100] batch [1/1] time 0.550 (0.550) data 0.381 (0.381) loss 2.5565 (2.5565) acc 55.0000 (55.0000) lr 7.2101e+00 eta 0:00:22\n",
      "epoch [61/100] batch [1/1] time 0.526 (0.526) data 0.353 (0.353) loss 2.5469 (2.5469) acc 60.0000 (60.0000) lr 6.9098e+00 eta 0:00:20\n",
      "epoch [62/100] batch [1/1] time 0.530 (0.530) data 0.359 (0.359) loss 2.4469 (2.4469) acc 55.0000 (55.0000) lr 6.6126e+00 eta 0:00:20\n",
      "epoch [63/100] batch [1/1] time 0.536 (0.536) data 0.362 (0.362) loss 2.1999 (2.1999) acc 75.0000 (75.0000) lr 6.3188e+00 eta 0:00:19\n",
      "epoch [64/100] batch [1/1] time 0.549 (0.549) data 0.377 (0.377) loss 2.0537 (2.0537) acc 60.0000 (60.0000) lr 6.0285e+00 eta 0:00:19\n",
      "epoch [65/100] batch [1/1] time 0.536 (0.536) data 0.363 (0.363) loss 1.9220 (1.9220) acc 60.0000 (60.0000) lr 5.7422e+00 eta 0:00:18\n",
      "epoch [66/100] batch [1/1] time 0.685 (0.685) data 0.511 (0.511) loss 1.9067 (1.9067) acc 75.0000 (75.0000) lr 5.4601e+00 eta 0:00:23\n",
      "epoch [67/100] batch [1/1] time 0.688 (0.688) data 0.514 (0.514) loss 1.6842 (1.6842) acc 80.0000 (80.0000) lr 5.1825e+00 eta 0:00:22\n",
      "epoch [68/100] batch [1/1] time 0.702 (0.702) data 0.528 (0.528) loss 1.7318 (1.7318) acc 85.0000 (85.0000) lr 4.9096e+00 eta 0:00:22\n",
      "epoch [69/100] batch [1/1] time 0.587 (0.587) data 0.412 (0.412) loss 1.5939 (1.5939) acc 80.0000 (80.0000) lr 4.6417e+00 eta 0:00:18\n",
      "epoch [70/100] batch [1/1] time 0.642 (0.642) data 0.470 (0.470) loss 1.6382 (1.6382) acc 70.0000 (70.0000) lr 4.3792e+00 eta 0:00:19\n",
      "epoch [71/100] batch [1/1] time 0.524 (0.524) data 0.349 (0.349) loss 1.3532 (1.3532) acc 90.0000 (90.0000) lr 4.1221e+00 eta 0:00:15\n",
      "epoch [72/100] batch [1/1] time 0.525 (0.525) data 0.355 (0.355) loss 1.6304 (1.6304) acc 70.0000 (70.0000) lr 3.8709e+00 eta 0:00:14\n",
      "epoch [73/100] batch [1/1] time 0.534 (0.534) data 0.362 (0.362) loss 1.4163 (1.4163) acc 90.0000 (90.0000) lr 3.6258e+00 eta 0:00:14\n",
      "epoch [74/100] batch [1/1] time 0.540 (0.540) data 0.366 (0.366) loss 1.3916 (1.3916) acc 80.0000 (80.0000) lr 3.3869e+00 eta 0:00:14\n",
      "epoch [75/100] batch [1/1] time 0.545 (0.545) data 0.372 (0.372) loss 1.4635 (1.4635) acc 70.0000 (70.0000) lr 3.1545e+00 eta 0:00:13\n",
      "epoch [76/100] batch [1/1] time 0.532 (0.532) data 0.358 (0.358) loss 1.3180 (1.3180) acc 90.0000 (90.0000) lr 2.9289e+00 eta 0:00:12\n",
      "epoch [77/100] batch [1/1] time 0.538 (0.538) data 0.365 (0.365) loss 1.4049 (1.4049) acc 80.0000 (80.0000) lr 2.7103e+00 eta 0:00:12\n",
      "epoch [78/100] batch [1/1] time 0.545 (0.545) data 0.374 (0.374) loss 1.2359 (1.2359) acc 90.0000 (90.0000) lr 2.4989e+00 eta 0:00:11\n",
      "epoch [79/100] batch [1/1] time 0.524 (0.524) data 0.353 (0.353) loss 1.2087 (1.2087) acc 90.0000 (90.0000) lr 2.2949e+00 eta 0:00:11\n",
      "epoch [80/100] batch [1/1] time 0.519 (0.519) data 0.345 (0.345) loss 1.1176 (1.1176) acc 95.0000 (95.0000) lr 2.0984e+00 eta 0:00:10\n",
      "epoch [81/100] batch [1/1] time 0.546 (0.546) data 0.372 (0.372) loss 1.3616 (1.3616) acc 80.0000 (80.0000) lr 1.9098e+00 eta 0:00:10\n",
      "epoch [82/100] batch [1/1] time 0.520 (0.520) data 0.346 (0.346) loss 1.2960 (1.2960) acc 90.0000 (90.0000) lr 1.7292e+00 eta 0:00:09\n",
      "epoch [83/100] batch [1/1] time 0.537 (0.537) data 0.365 (0.365) loss 1.0428 (1.0428) acc 90.0000 (90.0000) lr 1.5567e+00 eta 0:00:09\n",
      "epoch [84/100] batch [1/1] time 0.525 (0.525) data 0.353 (0.353) loss 1.0168 (1.0168) acc 95.0000 (95.0000) lr 1.3926e+00 eta 0:00:08\n",
      "epoch [85/100] batch [1/1] time 0.687 (0.687) data 0.512 (0.512) loss 0.9955 (0.9955) acc 95.0000 (95.0000) lr 1.2369e+00 eta 0:00:10\n",
      "epoch [86/100] batch [1/1] time 0.722 (0.722) data 0.534 (0.534) loss 1.0882 (1.0882) acc 90.0000 (90.0000) lr 1.0899e+00 eta 0:00:10\n",
      "epoch [87/100] batch [1/1] time 0.689 (0.689) data 0.509 (0.509) loss 0.9507 (0.9507) acc 95.0000 (95.0000) lr 9.5173e-01 eta 0:00:08\n",
      "epoch [88/100] batch [1/1] time 0.682 (0.682) data 0.510 (0.510) loss 0.9376 (0.9376) acc 90.0000 (90.0000) lr 8.2245e-01 eta 0:00:08\n",
      "epoch [89/100] batch [1/1] time 0.521 (0.521) data 0.347 (0.347) loss 1.2589 (1.2589) acc 80.0000 (80.0000) lr 7.0224e-01 eta 0:00:05\n",
      "epoch [90/100] batch [1/1] time 0.539 (0.539) data 0.368 (0.368) loss 0.9877 (0.9877) acc 90.0000 (90.0000) lr 5.9119e-01 eta 0:00:05\n",
      "epoch [91/100] batch [1/1] time 0.539 (0.539) data 0.365 (0.365) loss 1.0377 (1.0377) acc 90.0000 (90.0000) lr 4.8943e-01 eta 0:00:04\n",
      "epoch [92/100] batch [1/1] time 0.525 (0.525) data 0.354 (0.354) loss 1.0354 (1.0354) acc 90.0000 (90.0000) lr 3.9706e-01 eta 0:00:04\n",
      "epoch [93/100] batch [1/1] time 0.523 (0.523) data 0.348 (0.348) loss 0.8810 (0.8810) acc 95.0000 (95.0000) lr 3.1417e-01 eta 0:00:03\n",
      "epoch [94/100] batch [1/1] time 0.535 (0.535) data 0.362 (0.362) loss 0.9696 (0.9696) acc 95.0000 (95.0000) lr 2.4083e-01 eta 0:00:03\n",
      "epoch [95/100] batch [1/1] time 0.518 (0.518) data 0.346 (0.346) loss 1.1186 (1.1186) acc 90.0000 (90.0000) lr 1.7713e-01 eta 0:00:02\n",
      "epoch [96/100] batch [1/1] time 0.544 (0.544) data 0.372 (0.372) loss 1.0082 (1.0082) acc 90.0000 (90.0000) lr 1.2312e-01 eta 0:00:02\n",
      "epoch [97/100] batch [1/1] time 0.512 (0.512) data 0.340 (0.340) loss 0.8555 (0.8555) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:01\n",
      "epoch [98/100] batch [1/1] time 0.515 (0.515) data 0.342 (0.342) loss 0.9729 (0.9729) acc 90.0000 (90.0000) lr 4.4380e-02 eta 0:00:01\n",
      "epoch [99/100] batch [1/1] time 0.547 (0.547) data 0.375 (0.375) loss 0.7794 (0.7794) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:00\n",
      "epoch [100/100] batch [1/1] time 0.512 (0.512) data 0.341 (0.341) loss 0.9703 (0.9703) acc 90.0000 (90.0000) lr 4.9344e-03 eta 0:00:00\n",
      "Checkpoint saved to output/eurosat/DAPT/vit_b16_ep100_2shots/seed2/prompt_learner/model.pth.tar-100\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:58<00:00,  1.38it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 4,487\n",
      "* accuracy: 55.4%\n",
      "* error: 44.6%\n",
      "* macro_f1: 54.2%\n",
      "Elapsed: 0:02:14\n"
     ]
    }
   ],
   "source": [
    "#eurosat-2shots-seed2\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 2 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/eurosat/DAPT/vit_b16_ep100_2shots/seed2 \\\n",
    "        DATASET.NUM_SHOTS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwkB-R1dLbYs",
    "outputId": "36234222-e2b7-451a-8207-872bf193d78b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:43:57.787728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:43:57.807467: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:43:57.813506: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:43:57.827477: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:43:58.842198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2']\n",
      "output_dir: output/eurosat/DAPT/vit_b16_ep100_2shots/seed3\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/eurosat/DAPT/vit_b16_ep100_2shots/seed3\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_3.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  20\n",
      "# val      20\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/eurosat/DAPT/vit_b16_ep100_2shots/seed3/tensorboard)\n",
      "epoch [1/100] batch [1/1] time 2.125 (2.125) data 0.564 (0.564) loss 11.8786 (11.8786) acc 10.0000 (10.0000) lr 2.0000e+01 eta 0:03:30\n",
      "epoch [2/100] batch [1/1] time 0.617 (0.617) data 0.442 (0.442) loss 11.8625 (11.8625) acc 10.0000 (10.0000) lr 1.9995e+01 eta 0:01:00\n",
      "epoch [3/100] batch [1/1] time 0.524 (0.524) data 0.350 (0.350) loss 8.9880 (8.9880) acc 5.0000 (5.0000) lr 1.9980e+01 eta 0:00:50\n",
      "epoch [4/100] batch [1/1] time 0.631 (0.631) data 0.457 (0.457) loss 8.2268 (8.2268) acc 10.0000 (10.0000) lr 1.9956e+01 eta 0:01:00\n",
      "epoch [5/100] batch [1/1] time 0.563 (0.563) data 0.392 (0.392) loss 8.6322 (8.6322) acc 10.0000 (10.0000) lr 1.9921e+01 eta 0:00:53\n",
      "epoch [6/100] batch [1/1] time 0.521 (0.521) data 0.346 (0.346) loss 8.3251 (8.3251) acc 10.0000 (10.0000) lr 1.9877e+01 eta 0:00:49\n",
      "epoch [7/100] batch [1/1] time 0.541 (0.541) data 0.367 (0.367) loss 7.9784 (7.9784) acc 20.0000 (20.0000) lr 1.9823e+01 eta 0:00:50\n",
      "epoch [8/100] batch [1/1] time 0.562 (0.562) data 0.389 (0.389) loss 7.7561 (7.7561) acc 20.0000 (20.0000) lr 1.9759e+01 eta 0:00:51\n",
      "epoch [9/100] batch [1/1] time 0.518 (0.518) data 0.344 (0.344) loss 7.6653 (7.6653) acc 15.0000 (15.0000) lr 1.9686e+01 eta 0:00:47\n",
      "epoch [10/100] batch [1/1] time 0.523 (0.523) data 0.349 (0.349) loss 7.5136 (7.5136) acc 15.0000 (15.0000) lr 1.9603e+01 eta 0:00:47\n",
      "epoch [11/100] batch [1/1] time 0.544 (0.544) data 0.370 (0.370) loss 7.5137 (7.5137) acc 30.0000 (30.0000) lr 1.9511e+01 eta 0:00:48\n",
      "epoch [12/100] batch [1/1] time 0.527 (0.527) data 0.350 (0.350) loss 7.3455 (7.3455) acc 25.0000 (25.0000) lr 1.9409e+01 eta 0:00:46\n",
      "epoch [13/100] batch [1/1] time 0.535 (0.535) data 0.360 (0.360) loss 7.1329 (7.1329) acc 35.0000 (35.0000) lr 1.9298e+01 eta 0:00:46\n",
      "epoch [14/100] batch [1/1] time 0.535 (0.535) data 0.361 (0.361) loss 7.1130 (7.1130) acc 20.0000 (20.0000) lr 1.9178e+01 eta 0:00:45\n",
      "epoch [15/100] batch [1/1] time 0.530 (0.530) data 0.354 (0.354) loss 7.0678 (7.0678) acc 35.0000 (35.0000) lr 1.9048e+01 eta 0:00:45\n",
      "epoch [16/100] batch [1/1] time 0.546 (0.546) data 0.365 (0.365) loss 6.9944 (6.9944) acc 20.0000 (20.0000) lr 1.8910e+01 eta 0:00:45\n",
      "epoch [17/100] batch [1/1] time 0.692 (0.692) data 0.515 (0.515) loss 6.8703 (6.8703) acc 30.0000 (30.0000) lr 1.8763e+01 eta 0:00:57\n",
      "epoch [18/100] batch [1/1] time 0.701 (0.701) data 0.523 (0.523) loss 6.7241 (6.7241) acc 40.0000 (40.0000) lr 1.8607e+01 eta 0:00:57\n",
      "epoch [19/100] batch [1/1] time 0.703 (0.703) data 0.524 (0.524) loss 6.6394 (6.6394) acc 35.0000 (35.0000) lr 1.8443e+01 eta 0:00:56\n",
      "epoch [20/100] batch [1/1] time 0.682 (0.682) data 0.504 (0.504) loss 6.5124 (6.5124) acc 25.0000 (25.0000) lr 1.8271e+01 eta 0:00:54\n",
      "epoch [21/100] batch [1/1] time 0.557 (0.557) data 0.380 (0.380) loss 6.4434 (6.4434) acc 35.0000 (35.0000) lr 1.8090e+01 eta 0:00:44\n",
      "epoch [22/100] batch [1/1] time 0.531 (0.531) data 0.356 (0.356) loss 6.2169 (6.2169) acc 50.0000 (50.0000) lr 1.7902e+01 eta 0:00:41\n",
      "epoch [23/100] batch [1/1] time 0.570 (0.570) data 0.391 (0.391) loss 6.0735 (6.0735) acc 35.0000 (35.0000) lr 1.7705e+01 eta 0:00:43\n",
      "epoch [24/100] batch [1/1] time 0.544 (0.544) data 0.372 (0.372) loss 5.7356 (5.7356) acc 55.0000 (55.0000) lr 1.7501e+01 eta 0:00:41\n",
      "epoch [25/100] batch [1/1] time 0.532 (0.532) data 0.359 (0.359) loss 5.4946 (5.4946) acc 55.0000 (55.0000) lr 1.7290e+01 eta 0:00:39\n",
      "epoch [26/100] batch [1/1] time 0.555 (0.555) data 0.381 (0.381) loss 5.3189 (5.3189) acc 55.0000 (55.0000) lr 1.7071e+01 eta 0:00:41\n",
      "epoch [27/100] batch [1/1] time 0.520 (0.520) data 0.347 (0.347) loss 5.2177 (5.2177) acc 55.0000 (55.0000) lr 1.6845e+01 eta 0:00:37\n",
      "epoch [28/100] batch [1/1] time 0.541 (0.541) data 0.365 (0.365) loss 5.8212 (5.8212) acc 30.0000 (30.0000) lr 1.6613e+01 eta 0:00:38\n",
      "epoch [29/100] batch [1/1] time 0.545 (0.545) data 0.369 (0.369) loss 4.8024 (4.8024) acc 55.0000 (55.0000) lr 1.6374e+01 eta 0:00:38\n",
      "epoch [30/100] batch [1/1] time 0.534 (0.534) data 0.359 (0.359) loss 5.5046 (5.5046) acc 35.0000 (35.0000) lr 1.6129e+01 eta 0:00:37\n",
      "epoch [31/100] batch [1/1] time 0.546 (0.546) data 0.372 (0.372) loss 4.7904 (4.7904) acc 45.0000 (45.0000) lr 1.5878e+01 eta 0:00:37\n",
      "epoch [32/100] batch [1/1] time 0.552 (0.552) data 0.377 (0.377) loss 4.5655 (4.5655) acc 45.0000 (45.0000) lr 1.5621e+01 eta 0:00:37\n",
      "epoch [33/100] batch [1/1] time 0.526 (0.526) data 0.349 (0.349) loss 3.8830 (3.8830) acc 60.0000 (60.0000) lr 1.5358e+01 eta 0:00:35\n",
      "epoch [34/100] batch [1/1] time 0.550 (0.550) data 0.375 (0.375) loss 3.7306 (3.7306) acc 65.0000 (65.0000) lr 1.5090e+01 eta 0:00:36\n",
      "epoch [35/100] batch [1/1] time 0.726 (0.726) data 0.550 (0.550) loss 3.9161 (3.9161) acc 45.0000 (45.0000) lr 1.4818e+01 eta 0:00:47\n",
      "epoch [36/100] batch [1/1] time 0.703 (0.703) data 0.527 (0.527) loss 3.6242 (3.6242) acc 70.0000 (70.0000) lr 1.4540e+01 eta 0:00:45\n",
      "epoch [37/100] batch [1/1] time 0.717 (0.717) data 0.534 (0.534) loss 3.5192 (3.5192) acc 50.0000 (50.0000) lr 1.4258e+01 eta 0:00:45\n",
      "epoch [38/100] batch [1/1] time 0.675 (0.675) data 0.497 (0.497) loss 3.2484 (3.2484) acc 65.0000 (65.0000) lr 1.3971e+01 eta 0:00:41\n",
      "epoch [39/100] batch [1/1] time 0.544 (0.544) data 0.367 (0.367) loss 2.7084 (2.7084) acc 65.0000 (65.0000) lr 1.3681e+01 eta 0:00:33\n",
      "epoch [40/100] batch [1/1] time 0.549 (0.549) data 0.372 (0.372) loss 3.2237 (3.2237) acc 35.0000 (35.0000) lr 1.3387e+01 eta 0:00:32\n",
      "epoch [41/100] batch [1/1] time 0.595 (0.595) data 0.417 (0.417) loss 3.5292 (3.5292) acc 55.0000 (55.0000) lr 1.3090e+01 eta 0:00:35\n",
      "epoch [42/100] batch [1/1] time 0.542 (0.542) data 0.375 (0.375) loss 3.4129 (3.4129) acc 55.0000 (55.0000) lr 1.2790e+01 eta 0:00:31\n",
      "epoch [43/100] batch [1/1] time 0.515 (0.515) data 0.339 (0.339) loss 2.8898 (2.8898) acc 50.0000 (50.0000) lr 1.2487e+01 eta 0:00:29\n",
      "epoch [44/100] batch [1/1] time 0.773 (0.773) data 0.517 (0.517) loss 9.9477 (9.9477) acc 25.0000 (25.0000) lr 1.2181e+01 eta 0:00:43\n",
      "epoch [45/100] batch [1/1] time 0.543 (0.543) data 0.366 (0.366) loss 7.8752 (7.8752) acc 30.0000 (30.0000) lr 1.1874e+01 eta 0:00:29\n",
      "epoch [46/100] batch [1/1] time 0.530 (0.530) data 0.356 (0.356) loss 4.6392 (4.6392) acc 35.0000 (35.0000) lr 1.1564e+01 eta 0:00:28\n",
      "epoch [47/100] batch [1/1] time 0.530 (0.530) data 0.355 (0.355) loss 3.2308 (3.2308) acc 55.0000 (55.0000) lr 1.1253e+01 eta 0:00:28\n",
      "epoch [48/100] batch [1/1] time 0.549 (0.549) data 0.375 (0.375) loss 2.7031 (2.7031) acc 55.0000 (55.0000) lr 1.0941e+01 eta 0:00:28\n",
      "epoch [49/100] batch [1/1] time 0.548 (0.548) data 0.373 (0.373) loss 2.1537 (2.1537) acc 75.0000 (75.0000) lr 1.0628e+01 eta 0:00:27\n",
      "epoch [50/100] batch [1/1] time 0.532 (0.532) data 0.358 (0.358) loss 1.8999 (1.8999) acc 70.0000 (70.0000) lr 1.0314e+01 eta 0:00:26\n",
      "epoch [51/100] batch [1/1] time 0.537 (0.537) data 0.362 (0.362) loss 1.9326 (1.9326) acc 75.0000 (75.0000) lr 1.0000e+01 eta 0:00:26\n",
      "epoch [52/100] batch [1/1] time 0.587 (0.587) data 0.402 (0.402) loss 1.7463 (1.7463) acc 75.0000 (75.0000) lr 9.6859e+00 eta 0:00:28\n",
      "epoch [53/100] batch [1/1] time 0.703 (0.703) data 0.522 (0.522) loss 1.7778 (1.7778) acc 75.0000 (75.0000) lr 9.3721e+00 eta 0:00:33\n",
      "epoch [54/100] batch [1/1] time 0.727 (0.727) data 0.549 (0.549) loss 1.4093 (1.4093) acc 90.0000 (90.0000) lr 9.0589e+00 eta 0:00:33\n",
      "epoch [55/100] batch [1/1] time 0.722 (0.722) data 0.542 (0.542) loss 1.6889 (1.6889) acc 75.0000 (75.0000) lr 8.7467e+00 eta 0:00:32\n",
      "epoch [56/100] batch [1/1] time 0.655 (0.655) data 0.478 (0.478) loss 1.2354 (1.2354) acc 90.0000 (90.0000) lr 8.4357e+00 eta 0:00:28\n",
      "epoch [57/100] batch [1/1] time 0.557 (0.557) data 0.381 (0.381) loss 1.1918 (1.1918) acc 90.0000 (90.0000) lr 8.1262e+00 eta 0:00:23\n",
      "epoch [58/100] batch [1/1] time 0.534 (0.534) data 0.359 (0.359) loss 1.0643 (1.0643) acc 95.0000 (95.0000) lr 7.8186e+00 eta 0:00:22\n",
      "epoch [59/100] batch [1/1] time 0.552 (0.552) data 0.376 (0.376) loss 1.0292 (1.0292) acc 95.0000 (95.0000) lr 7.5131e+00 eta 0:00:22\n",
      "epoch [60/100] batch [1/1] time 0.561 (0.561) data 0.383 (0.383) loss 1.0600 (1.0600) acc 90.0000 (90.0000) lr 7.2101e+00 eta 0:00:22\n",
      "epoch [61/100] batch [1/1] time 0.527 (0.527) data 0.351 (0.351) loss 0.8975 (0.8975) acc 100.0000 (100.0000) lr 6.9098e+00 eta 0:00:20\n",
      "epoch [62/100] batch [1/1] time 0.535 (0.535) data 0.361 (0.361) loss 0.9667 (0.9667) acc 95.0000 (95.0000) lr 6.6126e+00 eta 0:00:20\n",
      "epoch [63/100] batch [1/1] time 0.548 (0.548) data 0.375 (0.375) loss 0.8335 (0.8335) acc 100.0000 (100.0000) lr 6.3188e+00 eta 0:00:20\n",
      "epoch [64/100] batch [1/1] time 0.540 (0.540) data 0.365 (0.365) loss 0.9258 (0.9258) acc 95.0000 (95.0000) lr 6.0285e+00 eta 0:00:19\n",
      "epoch [65/100] batch [1/1] time 0.528 (0.528) data 0.354 (0.354) loss 0.9266 (0.9266) acc 90.0000 (90.0000) lr 5.7422e+00 eta 0:00:18\n",
      "epoch [66/100] batch [1/1] time 0.548 (0.548) data 0.372 (0.372) loss 0.9133 (0.9133) acc 95.0000 (95.0000) lr 5.4601e+00 eta 0:00:18\n",
      "epoch [67/100] batch [1/1] time 0.542 (0.542) data 0.363 (0.363) loss 0.7115 (0.7115) acc 100.0000 (100.0000) lr 5.1825e+00 eta 0:00:17\n",
      "epoch [68/100] batch [1/1] time 0.537 (0.537) data 0.364 (0.364) loss 0.7611 (0.7611) acc 95.0000 (95.0000) lr 4.9096e+00 eta 0:00:17\n",
      "epoch [69/100] batch [1/1] time 0.543 (0.543) data 0.367 (0.367) loss 0.7848 (0.7848) acc 95.0000 (95.0000) lr 4.6417e+00 eta 0:00:16\n",
      "epoch [70/100] batch [1/1] time 0.554 (0.554) data 0.368 (0.368) loss 0.9607 (0.9607) acc 90.0000 (90.0000) lr 4.3792e+00 eta 0:00:16\n",
      "epoch [71/100] batch [1/1] time 0.691 (0.691) data 0.510 (0.510) loss 0.7504 (0.7504) acc 95.0000 (95.0000) lr 4.1221e+00 eta 0:00:20\n",
      "epoch [72/100] batch [1/1] time 0.707 (0.707) data 0.529 (0.529) loss 0.7107 (0.7107) acc 100.0000 (100.0000) lr 3.8709e+00 eta 0:00:19\n",
      "epoch [73/100] batch [1/1] time 0.693 (0.693) data 0.513 (0.513) loss 0.6708 (0.6708) acc 100.0000 (100.0000) lr 3.6258e+00 eta 0:00:18\n",
      "epoch [74/100] batch [1/1] time 0.674 (0.674) data 0.501 (0.501) loss 0.6726 (0.6726) acc 100.0000 (100.0000) lr 3.3869e+00 eta 0:00:17\n",
      "epoch [75/100] batch [1/1] time 0.549 (0.549) data 0.374 (0.374) loss 0.6392 (0.6392) acc 100.0000 (100.0000) lr 3.1545e+00 eta 0:00:13\n",
      "epoch [76/100] batch [1/1] time 0.534 (0.534) data 0.360 (0.360) loss 0.7484 (0.7484) acc 95.0000 (95.0000) lr 2.9289e+00 eta 0:00:12\n",
      "epoch [77/100] batch [1/1] time 0.558 (0.558) data 0.383 (0.383) loss 0.9142 (0.9142) acc 95.0000 (95.0000) lr 2.7103e+00 eta 0:00:12\n",
      "epoch [78/100] batch [1/1] time 0.544 (0.544) data 0.370 (0.370) loss 0.7812 (0.7812) acc 95.0000 (95.0000) lr 2.4989e+00 eta 0:00:11\n",
      "epoch [79/100] batch [1/1] time 0.539 (0.539) data 0.363 (0.363) loss 0.6886 (0.6886) acc 100.0000 (100.0000) lr 2.2949e+00 eta 0:00:11\n",
      "epoch [80/100] batch [1/1] time 0.533 (0.533) data 0.356 (0.356) loss 0.8549 (0.8549) acc 85.0000 (85.0000) lr 2.0984e+00 eta 0:00:10\n",
      "epoch [81/100] batch [1/1] time 0.546 (0.546) data 0.369 (0.369) loss 0.6550 (0.6550) acc 100.0000 (100.0000) lr 1.9098e+00 eta 0:00:10\n",
      "epoch [82/100] batch [1/1] time 0.552 (0.552) data 0.375 (0.375) loss 0.6167 (0.6167) acc 100.0000 (100.0000) lr 1.7292e+00 eta 0:00:09\n",
      "epoch [83/100] batch [1/1] time 0.550 (0.550) data 0.371 (0.371) loss 0.6819 (0.6819) acc 95.0000 (95.0000) lr 1.5567e+00 eta 0:00:09\n",
      "epoch [84/100] batch [1/1] time 0.547 (0.547) data 0.370 (0.370) loss 0.6278 (0.6278) acc 100.0000 (100.0000) lr 1.3926e+00 eta 0:00:08\n",
      "epoch [85/100] batch [1/1] time 0.554 (0.554) data 0.379 (0.379) loss 0.6057 (0.6057) acc 100.0000 (100.0000) lr 1.2369e+00 eta 0:00:08\n",
      "epoch [86/100] batch [1/1] time 0.528 (0.528) data 0.354 (0.354) loss 0.6087 (0.6087) acc 100.0000 (100.0000) lr 1.0899e+00 eta 0:00:07\n",
      "epoch [87/100] batch [1/1] time 0.543 (0.543) data 0.367 (0.367) loss 0.5903 (0.5903) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:07\n",
      "epoch [88/100] batch [1/1] time 0.541 (0.541) data 0.364 (0.364) loss 0.6191 (0.6191) acc 100.0000 (100.0000) lr 8.2245e-01 eta 0:00:06\n",
      "epoch [89/100] batch [1/1] time 0.691 (0.691) data 0.512 (0.512) loss 0.7988 (0.7988) acc 95.0000 (95.0000) lr 7.0224e-01 eta 0:00:07\n",
      "epoch [90/100] batch [1/1] time 0.709 (0.709) data 0.531 (0.531) loss 0.6399 (0.6399) acc 95.0000 (95.0000) lr 5.9119e-01 eta 0:00:07\n",
      "epoch [91/100] batch [1/1] time 0.713 (0.713) data 0.532 (0.532) loss 0.5753 (0.5753) acc 100.0000 (100.0000) lr 4.8943e-01 eta 0:00:06\n",
      "epoch [92/100] batch [1/1] time 0.675 (0.675) data 0.498 (0.498) loss 0.6370 (0.6370) acc 95.0000 (95.0000) lr 3.9706e-01 eta 0:00:05\n",
      "epoch [93/100] batch [1/1] time 0.537 (0.537) data 0.360 (0.360) loss 0.6384 (0.6384) acc 95.0000 (95.0000) lr 3.1417e-01 eta 0:00:03\n",
      "epoch [94/100] batch [1/1] time 0.529 (0.529) data 0.356 (0.356) loss 0.9727 (0.9727) acc 85.0000 (85.0000) lr 2.4083e-01 eta 0:00:03\n",
      "epoch [95/100] batch [1/1] time 0.548 (0.548) data 0.374 (0.374) loss 0.5633 (0.5633) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:02\n",
      "epoch [96/100] batch [1/1] time 0.544 (0.544) data 0.368 (0.368) loss 0.5693 (0.5693) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:02\n",
      "epoch [97/100] batch [1/1] time 0.541 (0.541) data 0.366 (0.366) loss 0.5656 (0.5656) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:01\n",
      "epoch [98/100] batch [1/1] time 0.535 (0.535) data 0.359 (0.359) loss 0.5796 (0.5796) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:01\n",
      "epoch [99/100] batch [1/1] time 0.530 (0.530) data 0.354 (0.354) loss 0.5931 (0.5931) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:00\n",
      "epoch [100/100] batch [1/1] time 0.557 (0.557) data 0.378 (0.378) loss 0.5891 (0.5891) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:00\n",
      "Checkpoint saved to output/eurosat/DAPT/vit_b16_ep100_2shots/seed3/prompt_learner/model.pth.tar-100\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:58<00:00,  1.38it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 6,464\n",
      "* accuracy: 79.8%\n",
      "* error: 20.2%\n",
      "* macro_f1: 79.4%\n",
      "Elapsed: 0:02:16\n"
     ]
    }
   ],
   "source": [
    "#eurosat-2shots-seed3\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 3 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/eurosat/DAPT/vit_b16_ep100_2shots/seed3 \\\n",
    "        DATASET.NUM_SHOTS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQrOy8iiwCY6",
    "outputId": "48b46023-ba65-4b4e-e8c7-ca7324926814"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:53:55.524058: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:53:55.544158: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:53:55.550198: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:53:55.566198: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:53:56.588595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '1']\n",
      "output_dir: output/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 1\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 50\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_1-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  10\n",
      "# val      10\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/eurosat/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
      "epoch [1/50] batch [1/1] time 1.628 (1.628) data 0.317 (0.317) loss 11.6641 (11.6641) acc 10.0000 (10.0000) lr 2.0000e+01 eta 0:01:19\n",
      "epoch [2/50] batch [1/1] time 0.550 (0.550) data 0.421 (0.421) loss 11.6230 (11.6230) acc 10.0000 (10.0000) lr 1.9980e+01 eta 0:00:26\n",
      "epoch [3/50] batch [1/1] time 0.575 (0.575) data 0.451 (0.451) loss 10.8498 (10.8498) acc 0.0000 (0.0000) lr 1.9921e+01 eta 0:00:27\n",
      "epoch [4/50] batch [1/1] time 0.431 (0.431) data 0.324 (0.324) loss 10.5265 (10.5265) acc 10.0000 (10.0000) lr 1.9823e+01 eta 0:00:19\n",
      "epoch [5/50] batch [1/1] time 0.499 (0.499) data 0.399 (0.399) loss 10.3568 (10.3568) acc 10.0000 (10.0000) lr 1.9686e+01 eta 0:00:22\n",
      "epoch [6/50] batch [1/1] time 0.359 (0.359) data 0.261 (0.261) loss 10.2511 (10.2511) acc 0.0000 (0.0000) lr 1.9511e+01 eta 0:00:15\n",
      "epoch [7/50] batch [1/1] time 0.366 (0.366) data 0.267 (0.267) loss 10.1339 (10.1339) acc 0.0000 (0.0000) lr 1.9298e+01 eta 0:00:15\n",
      "epoch [8/50] batch [1/1] time 0.364 (0.364) data 0.266 (0.266) loss 10.0390 (10.0390) acc 0.0000 (0.0000) lr 1.9048e+01 eta 0:00:15\n",
      "epoch [9/50] batch [1/1] time 0.365 (0.365) data 0.268 (0.268) loss 10.0222 (10.0222) acc 0.0000 (0.0000) lr 1.8763e+01 eta 0:00:14\n",
      "epoch [10/50] batch [1/1] time 0.359 (0.359) data 0.263 (0.263) loss 9.9208 (9.9208) acc 0.0000 (0.0000) lr 1.8443e+01 eta 0:00:14\n",
      "epoch [11/50] batch [1/1] time 0.372 (0.372) data 0.273 (0.273) loss 9.8446 (9.8446) acc 0.0000 (0.0000) lr 1.8090e+01 eta 0:00:14\n",
      "epoch [12/50] batch [1/1] time 0.363 (0.363) data 0.266 (0.266) loss 9.7838 (9.7838) acc 20.0000 (20.0000) lr 1.7705e+01 eta 0:00:13\n",
      "epoch [13/50] batch [1/1] time 0.381 (0.381) data 0.281 (0.281) loss 9.7694 (9.7694) acc 10.0000 (10.0000) lr 1.7290e+01 eta 0:00:14\n",
      "epoch [14/50] batch [1/1] time 0.362 (0.362) data 0.263 (0.263) loss 9.6756 (9.6756) acc 10.0000 (10.0000) lr 1.6845e+01 eta 0:00:13\n",
      "epoch [15/50] batch [1/1] time 0.372 (0.372) data 0.273 (0.273) loss 9.6145 (9.6145) acc 10.0000 (10.0000) lr 1.6374e+01 eta 0:00:13\n",
      "epoch [16/50] batch [1/1] time 0.368 (0.368) data 0.266 (0.266) loss 9.5292 (9.5292) acc 20.0000 (20.0000) lr 1.5878e+01 eta 0:00:12\n",
      "epoch [17/50] batch [1/1] time 0.373 (0.373) data 0.274 (0.274) loss 9.4470 (9.4470) acc 20.0000 (20.0000) lr 1.5358e+01 eta 0:00:12\n",
      "epoch [18/50] batch [1/1] time 0.390 (0.390) data 0.287 (0.287) loss 9.3718 (9.3718) acc 20.0000 (20.0000) lr 1.4818e+01 eta 0:00:12\n",
      "epoch [19/50] batch [1/1] time 0.372 (0.372) data 0.274 (0.274) loss 9.3458 (9.3458) acc 20.0000 (20.0000) lr 1.4258e+01 eta 0:00:11\n",
      "epoch [20/50] batch [1/1] time 0.373 (0.373) data 0.274 (0.274) loss 9.2606 (9.2606) acc 20.0000 (20.0000) lr 1.3681e+01 eta 0:00:11\n",
      "epoch [21/50] batch [1/1] time 0.369 (0.369) data 0.271 (0.271) loss 9.1274 (9.1274) acc 40.0000 (40.0000) lr 1.3090e+01 eta 0:00:10\n",
      "epoch [22/50] batch [1/1] time 0.381 (0.381) data 0.282 (0.282) loss 9.0514 (9.0514) acc 50.0000 (50.0000) lr 1.2487e+01 eta 0:00:10\n",
      "epoch [23/50] batch [1/1] time 0.377 (0.377) data 0.279 (0.279) loss 8.9617 (8.9617) acc 40.0000 (40.0000) lr 1.1874e+01 eta 0:00:10\n",
      "epoch [24/50] batch [1/1] time 0.373 (0.373) data 0.272 (0.272) loss 8.7204 (8.7204) acc 50.0000 (50.0000) lr 1.1253e+01 eta 0:00:09\n",
      "epoch [25/50] batch [1/1] time 0.493 (0.493) data 0.389 (0.389) loss 8.5441 (8.5441) acc 40.0000 (40.0000) lr 1.0628e+01 eta 0:00:12\n",
      "epoch [26/50] batch [1/1] time 0.546 (0.546) data 0.433 (0.433) loss 8.4203 (8.4203) acc 50.0000 (50.0000) lr 1.0000e+01 eta 0:00:13\n",
      "epoch [27/50] batch [1/1] time 0.510 (0.510) data 0.393 (0.393) loss 8.2791 (8.2791) acc 50.0000 (50.0000) lr 9.3721e+00 eta 0:00:11\n",
      "epoch [28/50] batch [1/1] time 0.532 (0.532) data 0.417 (0.417) loss 8.1444 (8.1444) acc 50.0000 (50.0000) lr 8.7467e+00 eta 0:00:11\n",
      "epoch [29/50] batch [1/1] time 0.502 (0.502) data 0.400 (0.400) loss 8.0063 (8.0063) acc 50.0000 (50.0000) lr 8.1262e+00 eta 0:00:10\n",
      "epoch [30/50] batch [1/1] time 0.382 (0.382) data 0.284 (0.284) loss 7.6437 (7.6437) acc 60.0000 (60.0000) lr 7.5131e+00 eta 0:00:07\n",
      "epoch [31/50] batch [1/1] time 0.367 (0.367) data 0.266 (0.266) loss 7.5516 (7.5516) acc 70.0000 (70.0000) lr 6.9098e+00 eta 0:00:06\n",
      "epoch [32/50] batch [1/1] time 0.376 (0.376) data 0.279 (0.279) loss 7.2874 (7.2874) acc 70.0000 (70.0000) lr 6.3188e+00 eta 0:00:06\n",
      "epoch [33/50] batch [1/1] time 0.362 (0.362) data 0.264 (0.264) loss 7.2574 (7.2574) acc 60.0000 (60.0000) lr 5.7422e+00 eta 0:00:06\n",
      "epoch [34/50] batch [1/1] time 0.376 (0.376) data 0.279 (0.279) loss 6.8026 (6.8026) acc 90.0000 (90.0000) lr 5.1825e+00 eta 0:00:06\n",
      "epoch [35/50] batch [1/1] time 0.360 (0.360) data 0.260 (0.260) loss 6.6006 (6.6006) acc 90.0000 (90.0000) lr 4.6417e+00 eta 0:00:05\n",
      "epoch [36/50] batch [1/1] time 0.374 (0.374) data 0.275 (0.275) loss 6.6406 (6.6406) acc 80.0000 (80.0000) lr 4.1221e+00 eta 0:00:05\n",
      "epoch [37/50] batch [1/1] time 0.361 (0.361) data 0.263 (0.263) loss 6.3328 (6.3328) acc 80.0000 (80.0000) lr 3.6258e+00 eta 0:00:04\n",
      "epoch [38/50] batch [1/1] time 0.373 (0.373) data 0.277 (0.277) loss 6.1476 (6.1476) acc 80.0000 (80.0000) lr 3.1545e+00 eta 0:00:04\n",
      "epoch [39/50] batch [1/1] time 0.397 (0.397) data 0.294 (0.294) loss 6.5989 (6.5989) acc 60.0000 (60.0000) lr 2.7103e+00 eta 0:00:04\n",
      "epoch [40/50] batch [1/1] time 0.372 (0.372) data 0.273 (0.273) loss 5.9704 (5.9704) acc 90.0000 (90.0000) lr 2.2949e+00 eta 0:00:03\n",
      "epoch [41/50] batch [1/1] time 0.359 (0.359) data 0.258 (0.258) loss 5.8529 (5.8529) acc 90.0000 (90.0000) lr 1.9098e+00 eta 0:00:03\n",
      "epoch [42/50] batch [1/1] time 0.370 (0.370) data 0.271 (0.271) loss 5.6535 (5.6535) acc 100.0000 (100.0000) lr 1.5567e+00 eta 0:00:02\n",
      "epoch [43/50] batch [1/1] time 0.360 (0.360) data 0.260 (0.260) loss 5.8264 (5.8264) acc 80.0000 (80.0000) lr 1.2369e+00 eta 0:00:02\n",
      "epoch [44/50] batch [1/1] time 0.397 (0.397) data 0.297 (0.297) loss 5.6009 (5.6009) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:02\n",
      "epoch [45/50] batch [1/1] time 0.361 (0.361) data 0.263 (0.263) loss 5.6003 (5.6003) acc 90.0000 (90.0000) lr 7.0224e-01 eta 0:00:01\n",
      "epoch [46/50] batch [1/1] time 0.378 (0.378) data 0.279 (0.279) loss 5.6519 (5.6519) acc 90.0000 (90.0000) lr 4.8943e-01 eta 0:00:01\n",
      "epoch [47/50] batch [1/1] time 0.367 (0.367) data 0.269 (0.269) loss 5.6265 (5.6265) acc 90.0000 (90.0000) lr 3.1417e-01 eta 0:00:01\n",
      "epoch [48/50] batch [1/1] time 0.375 (0.375) data 0.278 (0.278) loss 5.4535 (5.4535) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:00\n",
      "epoch [49/50] batch [1/1] time 0.488 (0.488) data 0.384 (0.384) loss 5.6008 (5.6008) acc 90.0000 (90.0000) lr 7.8853e-02 eta 0:00:00\n",
      "epoch [50/50] batch [1/1] time 0.517 (0.517) data 0.403 (0.403) loss 5.4201 (5.4201) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:00\n",
      "Checkpoint saved to output/eurosat/DAPT/vit_b16_ep50_1shots/seed1/prompt_learner/model.pth.tar-50\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:59<00:00,  1.36it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 3,149\n",
      "* accuracy: 38.9%\n",
      "* error: 61.1%\n",
      "* macro_f1: 34.8%\n",
      "Elapsed: 0:01:30\n"
     ]
    }
   ],
   "source": [
    "#eurosat-2shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
    "        --output-dir output/eurosat/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv19Qq8PptMF"
   },
   "source": [
    "##Bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfigfSzbpwEN"
   },
   "outputs": [],
   "source": [
    "# 터미널에서 스크립트 파일 생성\n",
    "!mkdir -p /content/drive/MyDrive/DAPT/scripts\n",
    "!nano /content/drive/MyDrive/DAPT/scripts/main.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6DfIJ21pzGw"
   },
   "outputs": [],
   "source": [
    "!chmod +x /content/drive/MyDrive/DAPT/scripts/main.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_byil-Fvr6ec",
    "outputId": "f836cbb3-e05a-4713-9ca7-09a1588f4600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:26:34.439180: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:26:34.458489: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:26:34.464296: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:26:34.478009: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:26:35.483263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
      "dataset_config_file: configs/datasets/caltech101.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '1', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: Caltech101\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 1\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.2\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 50\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.01\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: False\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: N/A\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: Caltech101\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 207, in <module>\n",
      "    main(args)\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 138, in main\n",
      "    trainer = build_trainer(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/build.py\", line 11, in build_trainer\n",
      "    return TRAINER_REGISTRY.get(cfg.TRAINER.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 323, in __init__\n",
      "    self.build_data_loader()\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 346, in build_data_loader\n",
      "    dm = DataManager(self.cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/data_manager.py\", line 61, in __init__\n",
      "    dataset = build_dataset(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/datasets/build.py\", line 11, in build_dataset\n",
      "    return DATASET_REGISTRY.get(cfg.DATASET.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/caltech101.py\", line 35, in __init__\n",
      "    train, val, test = DTD.read_and_split_data(self.image_dir, ignored=IGNORED, new_cnames=NEW_CNAMES)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/dtd.py\", line 62, in read_and_split_data\n",
      "    categories = listdir_nohidden(image_dir)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/utils/tools.py\", line 142, in listdir_nohidden\n",
      "    items = [f for f in os.listdir(path) if not f.startswith(\".\")]\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DAPT/DATA/caltech-101/101_ObjectCategories'\n",
      "2024-12-03 13:26:44.735184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:26:44.754793: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:26:44.760671: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:26:44.774408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:26:45.775741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
      "dataset_config_file: configs/datasets/caltech101.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '1', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: Caltech101\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 1\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.2\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 50\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.01\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: False\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: N/A\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: Caltech101\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 207, in <module>\n",
      "    main(args)\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 138, in main\n",
      "    trainer = build_trainer(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/build.py\", line 11, in build_trainer\n",
      "    return TRAINER_REGISTRY.get(cfg.TRAINER.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 323, in __init__\n",
      "    self.build_data_loader()\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 346, in build_data_loader\n",
      "    dm = DataManager(self.cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/data_manager.py\", line 61, in __init__\n",
      "    dataset = build_dataset(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/datasets/build.py\", line 11, in build_dataset\n",
      "    return DATASET_REGISTRY.get(cfg.DATASET.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/caltech101.py\", line 35, in __init__\n",
      "    train, val, test = DTD.read_and_split_data(self.image_dir, ignored=IGNORED, new_cnames=NEW_CNAMES)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/dtd.py\", line 62, in read_and_split_data\n",
      "    categories = listdir_nohidden(image_dir)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/utils/tools.py\", line 142, in listdir_nohidden\n",
      "    items = [f for f in os.listdir(path) if not f.startswith(\".\")]\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DAPT/DATA/caltech-101/101_ObjectCategories'\n",
      "2024-12-03 13:26:54.915204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:26:54.934210: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:26:54.940109: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:26:54.953748: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:26:55.947047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
      "dataset_config_file: configs/datasets/caltech101.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '1', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: Caltech101\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 1\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.2\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 50\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.01\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: False\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: N/A\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: Caltech101\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 207, in <module>\n",
      "    main(args)\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 138, in main\n",
      "    trainer = build_trainer(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/build.py\", line 11, in build_trainer\n",
      "    return TRAINER_REGISTRY.get(cfg.TRAINER.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 323, in __init__\n",
      "    self.build_data_loader()\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 346, in build_data_loader\n",
      "    dm = DataManager(self.cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/data_manager.py\", line 61, in __init__\n",
      "    dataset = build_dataset(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/datasets/build.py\", line 11, in build_dataset\n",
      "    return DATASET_REGISTRY.get(cfg.DATASET.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/caltech101.py\", line 35, in __init__\n",
      "    train, val, test = DTD.read_and_split_data(self.image_dir, ignored=IGNORED, new_cnames=NEW_CNAMES)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/dtd.py\", line 62, in read_and_split_data\n",
      "    categories = listdir_nohidden(image_dir)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/utils/tools.py\", line 142, in listdir_nohidden\n",
      "    items = [f for f in os.listdir(path) if not f.startswith(\".\")]\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DAPT/DATA/caltech-101/101_ObjectCategories'\n",
      "2024-12-03 13:27:04.585013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:27:04.617478: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:27:04.628072: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:27:04.649806: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:27:06.137888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/caltech101.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: Caltech101\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.2\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.01\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: False\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: N/A\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: Caltech101\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 207, in <module>\n",
      "    main(args)\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 138, in main\n",
      "    trainer = build_trainer(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/build.py\", line 11, in build_trainer\n",
      "    return TRAINER_REGISTRY.get(cfg.TRAINER.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 323, in __init__\n",
      "    self.build_data_loader()\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 346, in build_data_loader\n",
      "    dm = DataManager(self.cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/data_manager.py\", line 61, in __init__\n",
      "    dataset = build_dataset(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/datasets/build.py\", line 11, in build_dataset\n",
      "    return DATASET_REGISTRY.get(cfg.DATASET.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/caltech101.py\", line 35, in __init__\n",
      "    train, val, test = DTD.read_and_split_data(self.image_dir, ignored=IGNORED, new_cnames=NEW_CNAMES)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/dtd.py\", line 62, in read_and_split_data\n",
      "    categories = listdir_nohidden(image_dir)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/utils/tools.py\", line 142, in listdir_nohidden\n",
      "    items = [f for f in os.listdir(path) if not f.startswith(\".\")]\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DAPT/DATA/caltech-101/101_ObjectCategories'\n",
      "2024-12-03 13:27:14.274657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:27:14.294247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:27:14.300622: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:27:14.314069: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:27:15.328850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/caltech101.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: Caltech101\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.2\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.01\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: False\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: N/A\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: Caltech101\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 207, in <module>\n",
      "    main(args)\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 138, in main\n",
      "    trainer = build_trainer(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/build.py\", line 11, in build_trainer\n",
      "    return TRAINER_REGISTRY.get(cfg.TRAINER.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 323, in __init__\n",
      "    self.build_data_loader()\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 346, in build_data_loader\n",
      "    dm = DataManager(self.cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/data_manager.py\", line 61, in __init__\n",
      "    dataset = build_dataset(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/datasets/build.py\", line 11, in build_dataset\n",
      "    return DATASET_REGISTRY.get(cfg.DATASET.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/caltech101.py\", line 35, in __init__\n",
      "    train, val, test = DTD.read_and_split_data(self.image_dir, ignored=IGNORED, new_cnames=NEW_CNAMES)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/dtd.py\", line 62, in read_and_split_data\n",
      "    categories = listdir_nohidden(image_dir)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/utils/tools.py\", line 142, in listdir_nohidden\n",
      "    items = [f for f in os.listdir(path) if not f.startswith(\".\")]\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DAPT/DATA/caltech-101/101_ObjectCategories'\n",
      "2024-12-03 13:27:25.589232: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:27:25.622112: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:27:25.632015: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:27:25.653732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:27:26.750896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/caltech101.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: Caltech101\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.2\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.01\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: False\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: N/A\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: Caltech101\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 207, in <module>\n",
      "    main(args)\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 138, in main\n",
      "    trainer = build_trainer(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/build.py\", line 11, in build_trainer\n",
      "    return TRAINER_REGISTRY.get(cfg.TRAINER.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 323, in __init__\n",
      "    self.build_data_loader()\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 346, in build_data_loader\n",
      "    dm = DataManager(self.cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/data_manager.py\", line 61, in __init__\n",
      "    dataset = build_dataset(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/datasets/build.py\", line 11, in build_dataset\n",
      "    return DATASET_REGISTRY.get(cfg.DATASET.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/caltech101.py\", line 35, in __init__\n",
      "    train, val, test = DTD.read_and_split_data(self.image_dir, ignored=IGNORED, new_cnames=NEW_CNAMES)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/dtd.py\", line 62, in read_and_split_data\n",
      "    categories = listdir_nohidden(image_dir)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/utils/tools.py\", line 142, in listdir_nohidden\n",
      "    items = [f for f in os.listdir(path) if not f.startswith(\".\")]\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DAPT/DATA/caltech-101/101_ObjectCategories'\n",
      "2024-12-03 13:27:35.883382: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:27:35.902649: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:27:35.908417: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:27:35.921942: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:27:36.946400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/caltech101.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: Caltech101\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 4\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.2\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.01\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: False\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: N/A\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: Caltech101\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 207, in <module>\n",
      "    main(args)\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 138, in main\n",
      "    trainer = build_trainer(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/build.py\", line 11, in build_trainer\n",
      "    return TRAINER_REGISTRY.get(cfg.TRAINER.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 323, in __init__\n",
      "    self.build_data_loader()\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 346, in build_data_loader\n",
      "    dm = DataManager(self.cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/data_manager.py\", line 61, in __init__\n",
      "    dataset = build_dataset(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/datasets/build.py\", line 11, in build_dataset\n",
      "    return DATASET_REGISTRY.get(cfg.DATASET.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/caltech101.py\", line 35, in __init__\n",
      "    train, val, test = DTD.read_and_split_data(self.image_dir, ignored=IGNORED, new_cnames=NEW_CNAMES)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/dtd.py\", line 62, in read_and_split_data\n",
      "    categories = listdir_nohidden(image_dir)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/utils/tools.py\", line 142, in listdir_nohidden\n",
      "    items = [f for f in os.listdir(path) if not f.startswith(\".\")]\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DAPT/DATA/caltech-101/101_ObjectCategories'\n",
      "2024-12-03 13:27:45.607078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:27:45.643777: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:27:45.653674: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:27:45.676399: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:27:47.171890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/caltech101.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: Caltech101\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 4\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.2\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.01\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: False\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: N/A\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: Caltech101\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 207, in <module>\n",
      "    main(args)\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 138, in main\n",
      "    trainer = build_trainer(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/build.py\", line 11, in build_trainer\n",
      "    return TRAINER_REGISTRY.get(cfg.TRAINER.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 323, in __init__\n",
      "    self.build_data_loader()\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 346, in build_data_loader\n",
      "    dm = DataManager(self.cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/data_manager.py\", line 61, in __init__\n",
      "    dataset = build_dataset(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/data/datasets/build.py\", line 11, in build_dataset\n",
      "    return DATASET_REGISTRY.get(cfg.DATASET.NAME)(cfg)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/caltech101.py\", line 35, in __init__\n",
      "    train, val, test = DTD.read_and_split_data(self.image_dir, ignored=IGNORED, new_cnames=NEW_CNAMES)\n",
      "  File \"/content/drive/MyDrive/DAPT/datasets/dtd.py\", line 62, in read_and_split_data\n",
      "    categories = listdir_nohidden(image_dir)\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/utils/tools.py\", line 142, in listdir_nohidden\n",
      "    items = [f for f in os.listdir(path) if not f.startswith(\".\")]\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/DAPT/DATA/caltech-101/101_ObjectCategories'\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bash /content/drive/MyDrive//DAPT/scripts/gen_prototype.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ZUvaQd6bsJR6",
    "outputId": "6500f369-7b53-4b5d-f65f-956c8c7e9207"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/DAPT'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxfuftwWp5fg",
    "outputId": "0c41c013-bb6d-4f47-886a-d624de6f82bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/DAPT/scripts/main.sh: line 15: [: -eq: unary operator expected\n",
      "/content/drive/MyDrive/DAPT/scripts/main.sh: line 17: [: -eq: unary operator expected\n",
      "/content/drive/MyDrive/DAPT/scripts/main.sh: line 17: [: -eq: unary operator expected\n",
      "/content/drive/MyDrive/DAPT/scripts/main.sh: line 19: [: -eq: unary operator expected\n",
      "/content/drive/MyDrive/DAPT/scripts/main.sh: line 19: [: -eq: unary operator expected\n",
      "2024-12-03 13:21:41.649116: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:21:41.669107: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:21:41.675252: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:21:41.688992: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 13:21:42.769915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 207, in <module>\n",
      "    main(args)\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 125, in main\n",
      "    cfg = setup_cfg(args)\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 107, in setup_cfg\n",
      "    cfg.merge_from_file(args.dataset_config_file)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/yacs/config.py\", line 211, in merge_from_file\n",
      "    with open(cfg_filename, \"r\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'configs/datasets/0.yaml'\n",
      "/content/drive/MyDrive/DAPT/scripts/main.sh: line 15: [: -eq: unary operator expected\n",
      "/content/drive/MyDrive/DAPT/scripts/main.sh: line 17: [: -eq: unary operator expected\n",
      "/content/drive/MyDrive/DAPT/scripts/main.sh: line 17: [: -eq: unary operator expected\n",
      "/content/drive/MyDrive/DAPT/scripts/main.sh: line 19: [: -eq: unary operator expected\n",
      "/content/drive/MyDrive/DAPT/scripts/main.sh: line 19: [: -eq: unary operator expected\n",
      "2024-12-03 13:21:51.240139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 13:21:51.274202: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 13:21:51.284010: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 13:21:51.305501: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 42, in tf\n",
      "    from tensorboard.compat import notf  # noqa: F401\n",
      "ImportError: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/DAPT/train.py\", line 6, in <module>\n",
      "    from dassl.engine import build_trainer\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/__init__.py\", line 2, in <module>\n",
      "    from .trainer import TrainerX, TrainerXU, TrainerBase, SimpleTrainer, SimpleNet  # isort:skip\n",
      "  File \"/content/drive/MyDrive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 9, in <module>\n",
      "    from torch.utils.tensorboard import SummaryWriter\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py\", line 12, in <module>\n",
      "    from .writer import FileWriter, SummaryWriter  # noqa: F401\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 19, in <module>\n",
      "    from ._embedding import get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/_embedding.py\", line 10, in <module>\n",
      "    _HAS_GFILE_JOIN = hasattr(tf.io.gfile, \"join\")\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 65, in __getattr__\n",
      "    return getattr(load_once(self), attr_name)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 97, in wrapper\n",
      "    cache[arg] = f(arg)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/lazy.py\", line 50, in load_once\n",
      "    module = load_fn()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/compat/__init__.py\", line 45, in tf\n",
      "    import tensorflow\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 47, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 11, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import distribute\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__.distribute import combinations\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.distribute.combinations import env # line: 456\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/combinations.py\", line 33, in <module>\n",
      "    from tensorflow.python.distribute import collective_all_reduce_strategy\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 25, in <module>\n",
      "    from tensorflow.python.distribute import cross_device_ops as cross_device_ops_lib\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cross_device_ops.py\", line 28, in <module>\n",
      "    from tensorflow.python.distribute import cross_device_utils\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cross_device_utils.py\", line 22, in <module>\n",
      "    from tensorflow.python.distribute import values as value_lib\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/values.py\", line 23, in <module>\n",
      "    from tensorflow.python.distribute import distribute_lib\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 205, in <module>\n",
      "    from tensorflow.python.data.ops import dataset_ops\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.data import experimental\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/__init__.py\", line 98, in <module>\n",
      "    from tensorflow.python.data.experimental import service\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/service/__init__.py\", line 419, in <module>\n",
      "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 23, in <module>\n",
      "    from tensorflow.python.data.experimental.ops import compression_ops\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/compression_ops.py\", line 16, in <module>\n",
      "    from tensorflow.python.data.util import structure\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/structure.py\", line 32, in <module>\n",
      "    from tensorflow.python.ops.ragged import ragged_tensor\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ragged/__init__.py\", line 28, in <module>\n",
      "    from tensorflow.python.ops.ragged import ragged_tensor\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ragged/ragged_tensor.py\", line 3149, in <module>\n",
      "    from tensorflow.python.ops.ragged import ragged_ops  # pylint: disable=unused-import, g-bad-import-order, g-import-not-at-top\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ragged/ragged_ops.py\", line 41, in <module>\n",
      "    from tensorflow.python.ops.ragged import ragged_image_ops\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ragged/ragged_image_ops.py\", line 24, in <module>\n",
      "    from tensorflow.python.ops import image_ops\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/image_ops.py\", line 159, in <module>\n",
      "    from tensorflow.python.ops.image_ops_impl import *\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/image_ops_impl.py\", line 40, in <module>\n",
      "    from tensorflow.python.ops import nn_impl\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/nn_impl.py\", line 26, in <module>\n",
      "    from tensorflow.python.ops import ctc_ops  # pylint: disable=unused-import\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ctc_ops.py\", line 32, in <module>\n",
      "    from tensorflow.python.ops import custom_gradient\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/custom_gradient.py\", line 17, in <module>\n",
      "    from tensorflow.python.eager import backprop\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\", line 45, in <module>\n",
      "    from tensorflow.python.ops import gradients_impl  # pylint: disable=unused-import\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 27, in <module>\n",
      "    from tensorflow.python.ops import io_ops  # pylint: disable=unused-import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!!bash /content/drive/MyDrive/DAPT/scripts/main.sh 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-J0_SOKousA"
   },
   "source": [
    "#Oxfordpets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUQduJ4MxAah"
   },
   "source": [
    "##prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c187tYLmozdv",
    "outputId": "e132f1ce-1974-471b-bc77-71ddc1c72669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:26:36.564665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:26:36.597168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:26:36.606898: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:26:36.628995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:26:38.130646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordPets\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.02\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.1\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: OxfordPets\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_16-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  ----------\n",
      "Dataset    OxfordPets\n",
      "# classes  37\n",
      "# train_x  592\n",
      "# val      148\n",
      "# test     3,669\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.vis_ctx', 'prompt_learner.txt_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "OxfordPets (SHOTS: 16)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#oxford_pets prototype-16shots-seed1\n",
    "\n",
    "!python train.py \\\n",
    "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "            --seed 1 \\\n",
    "            --trainer DAPT \\\n",
    "            --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "            --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "            DATASET.NUM_SHOTS 16 \\\n",
    "            TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WU7zM8d1xRJP",
    "outputId": "b39b576c-aa69-43b1-be72-52fe42d7d03c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:31:17.027409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:31:17.047538: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:31:17.053641: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:31:17.067753: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:31:18.082293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '8', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordPets\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 8\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.02\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.1\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: OxfordPets\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_8-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  ----------\n",
      "Dataset    OxfordPets\n",
      "# classes  37\n",
      "# train_x  296\n",
      "# val      148\n",
      "# test     3,669\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.vis_ctx', 'prompt_learner.txt_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "OxfordPets (SHOTS: 8)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#oxford_pets prototype-8shots-seed1\n",
    "\n",
    "!python train.py \\\n",
    "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "            --seed 1 \\\n",
    "            --trainer DAPT \\\n",
    "            --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "            --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "            DATASET.NUM_SHOTS 8 \\\n",
    "            TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8m2C6mtxVaw",
    "outputId": "48c0fe48-4b09-4445-e978-b106a3b6f94d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:33:10.417212: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:33:10.437494: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:33:10.443491: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:33:10.458790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:33:11.474108: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordPets\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 4\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.02\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.1\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: OxfordPets\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_4-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  ----------\n",
      "Dataset    OxfordPets\n",
      "# classes  37\n",
      "# train_x  148\n",
      "# val      148\n",
      "# test     3,669\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.txt_ctx', 'prompt_learner.vis_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "OxfordPets (SHOTS: 4)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#oxford_pets prototype-4shots-seed1\n",
    "\n",
    "!python train.py \\\n",
    "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "            --seed 1 \\\n",
    "            --trainer DAPT \\\n",
    "            --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "            --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "            DATASET.NUM_SHOTS 4 \\\n",
    "            TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7sJnrR6YxYeU",
    "outputId": "6500984a-63a0-4d16-f639-ad034d0f0293"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:34:12.950562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:34:12.972743: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:34:12.978883: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:34:12.995461: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:34:14.471519: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordPets\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.02\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.1\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: OxfordPets\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_2-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  ----------\n",
      "Dataset    OxfordPets\n",
      "# classes  37\n",
      "# train_x  74\n",
      "# val      74\n",
      "# test     3,669\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.txt_ctx', 'prompt_learner.vis_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "OxfordPets (SHOTS: 2)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#oxford_pets prototype-2shots-seed1\n",
    "\n",
    "!python train.py \\\n",
    "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "            --seed 1 \\\n",
    "            --trainer DAPT \\\n",
    "            --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "            --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "            DATASET.NUM_SHOTS 2 \\\n",
    "            TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XD4SC5mVxcb6",
    "outputId": "acf3f2e3-afa0-4275-aaaa-c687708ee2f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:34:47.570184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:34:47.590359: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:34:47.597186: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:34:47.611802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:34:48.621423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
      "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '1', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
      "output_dir: \n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordPets\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 1\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.02\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 50\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ./output\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: True\n",
      "    TXT_BETA: 0.1\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: OxfordPets\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_1-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  ----------\n",
      "Dataset    OxfordPets\n",
      "# classes  37\n",
      "# train_x  37\n",
      "# val      37\n",
      "# test     3,669\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.vis_ctx', 'prompt_learner.txt_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "=================================\n",
      "Prototype generator\n",
      "OxfordPets (SHOTS: 1)\n",
      "=================================\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Making prototype finished!!\n",
      "Loading evaluator: Classification\n"
     ]
    }
   ],
   "source": [
    "#oxford_pets prototype-1shots-seed1\n",
    "\n",
    "!python train.py \\\n",
    "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "            --seed 1 \\\n",
    "            --trainer DAPT \\\n",
    "            --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "            --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
    "            DATASET.NUM_SHOTS 1 \\\n",
    "            TRAINER.DAPT.PROTOTYPE_GEN True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OabBykT5x3jo"
   },
   "source": [
    "##Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMzYnHP0x5EA",
    "outputId": "783af47d-4bb1-4f48-a162-4d7ff963a0dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 14:10:11.808861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 14:10:11.829452: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 14:10:11.835285: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 14:10:11.849178: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 14:10:12.861376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16']\n",
      "output_dir: output/oxford_pets/DAPT/vit_b16_16shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordPets\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.02\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/oxford_pets/DAPT/vit_b16_16shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 0.1\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: OxfordPets\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_16-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  ----------\n",
      "Dataset    OxfordPets\n",
      "# classes  37\n",
      "# train_x  592\n",
      "# val      148\n",
      "# test     3,669\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/oxford_pets/DAPT/vit_b16_16shots/seed1/tensorboard)\n",
      "epoch [1/200] batch [5/18] time 0.305 (1.532) data 0.000 (0.381) loss 2.8620 (2.9731) acc 34.3750 (23.7500) lr 1.0000e-05 eta 1:31:47\n",
      "epoch [1/200] batch [10/18] time 0.304 (0.917) data 0.000 (0.191) loss 3.0340 (2.9353) acc 15.6250 (21.2500) lr 1.0000e-05 eta 0:54:51\n",
      "epoch [1/200] batch [15/18] time 0.304 (0.713) data 0.001 (0.127) loss 3.1777 (2.9373) acc 21.8750 (21.0417) lr 1.0000e-05 eta 0:42:34\n",
      "epoch [2/200] batch [5/18] time 0.323 (0.741) data 0.000 (0.401) loss 1.8097 (1.9294) acc 65.6250 (45.6250) lr 2.0000e-02 eta 0:44:09\n",
      "epoch [2/200] batch [10/18] time 0.309 (0.531) data 0.000 (0.201) loss 1.7733 (1.7148) acc 53.1250 (52.8125) lr 2.0000e-02 eta 0:31:36\n",
      "epoch [2/200] batch [15/18] time 0.304 (0.455) data 0.000 (0.134) loss 1.3006 (1.6257) acc 62.5000 (55.0000) lr 2.0000e-02 eta 0:27:01\n",
      "epoch [3/200] batch [5/18] time 0.327 (0.752) data 0.000 (0.413) loss 0.4480 (1.0014) acc 84.3750 (70.6250) lr 1.9999e-02 eta 0:44:36\n",
      "epoch [3/200] batch [10/18] time 0.328 (0.648) data 0.000 (0.283) loss 1.1800 (1.0343) acc 71.8750 (71.2500) lr 1.9999e-02 eta 0:38:21\n",
      "epoch [3/200] batch [15/18] time 0.304 (0.536) data 0.000 (0.189) loss 0.7122 (0.9323) acc 81.2500 (73.5417) lr 1.9999e-02 eta 0:31:41\n",
      "epoch [4/200] batch [5/18] time 0.326 (0.750) data 0.000 (0.409) loss 0.7202 (0.7009) acc 81.2500 (83.1250) lr 1.9995e-02 eta 0:44:16\n",
      "epoch [4/200] batch [10/18] time 0.311 (0.535) data 0.000 (0.205) loss 0.5890 (0.7068) acc 81.2500 (81.2500) lr 1.9995e-02 eta 0:31:31\n",
      "epoch [4/200] batch [15/18] time 0.304 (0.458) data 0.000 (0.137) loss 1.1209 (0.7078) acc 62.5000 (81.2500) lr 1.9995e-02 eta 0:26:58\n",
      "epoch [5/200] batch [5/18] time 0.341 (0.754) data 0.000 (0.369) loss 0.2886 (0.4972) acc 90.6250 (83.7500) lr 1.9989e-02 eta 0:44:16\n",
      "epoch [5/200] batch [10/18] time 0.353 (0.636) data 0.000 (0.246) loss 0.8876 (0.6506) acc 68.7500 (81.2500) lr 1.9989e-02 eta 0:37:18\n",
      "epoch [5/200] batch [15/18] time 0.312 (0.528) data 0.000 (0.164) loss 0.9401 (0.6885) acc 71.8750 (80.0000) lr 1.9989e-02 eta 0:30:55\n",
      "epoch [6/200] batch [5/18] time 0.327 (0.773) data 0.001 (0.450) loss 0.6909 (0.5757) acc 78.1250 (83.7500) lr 1.9980e-02 eta 0:45:09\n",
      "epoch [6/200] batch [10/18] time 0.313 (0.545) data 0.000 (0.225) loss 0.6439 (0.6772) acc 75.0000 (80.6250) lr 1.9980e-02 eta 0:31:47\n",
      "epoch [6/200] batch [15/18] time 0.312 (0.467) data 0.000 (0.150) loss 0.5329 (0.7010) acc 90.6250 (81.0417) lr 1.9980e-02 eta 0:27:11\n",
      "epoch [7/200] batch [5/18] time 0.357 (0.853) data 0.004 (0.456) loss 0.5811 (0.4777) acc 84.3750 (85.0000) lr 1.9969e-02 eta 0:49:33\n",
      "epoch [7/200] batch [10/18] time 0.332 (0.635) data 0.001 (0.229) loss 0.5454 (0.5603) acc 78.1250 (83.1250) lr 1.9969e-02 eta 0:36:51\n",
      "epoch [7/200] batch [15/18] time 0.317 (0.530) data 0.000 (0.152) loss 0.9330 (0.6140) acc 71.8750 (81.8750) lr 1.9969e-02 eta 0:30:44\n",
      "epoch [8/200] batch [5/18] time 0.332 (0.775) data 0.001 (0.434) loss 0.3069 (0.5922) acc 93.7500 (85.0000) lr 1.9956e-02 eta 0:44:48\n",
      "epoch [8/200] batch [10/18] time 0.314 (0.550) data 0.000 (0.217) loss 0.6446 (0.6255) acc 84.3750 (82.8125) lr 1.9956e-02 eta 0:31:45\n",
      "epoch [8/200] batch [15/18] time 0.315 (0.472) data 0.000 (0.145) loss 0.3892 (0.6171) acc 90.6250 (82.5000) lr 1.9956e-02 eta 0:27:11\n",
      "epoch [9/200] batch [5/18] time 0.347 (0.792) data 0.003 (0.395) loss 0.5569 (0.5991) acc 78.1250 (82.5000) lr 1.9940e-02 eta 0:45:31\n",
      "epoch [9/200] batch [10/18] time 0.342 (0.641) data 0.001 (0.248) loss 0.7360 (0.5554) acc 78.1250 (84.0625) lr 1.9940e-02 eta 0:36:47\n",
      "epoch [9/200] batch [15/18] time 0.317 (0.535) data 0.000 (0.166) loss 0.6184 (0.5412) acc 81.2500 (84.3750) lr 1.9940e-02 eta 0:30:39\n",
      "epoch [10/200] batch [5/18] time 0.344 (0.789) data 0.000 (0.424) loss 0.7119 (0.6102) acc 84.3750 (82.5000) lr 1.9921e-02 eta 0:45:07\n",
      "epoch [10/200] batch [10/18] time 0.344 (0.559) data 0.032 (0.216) loss 0.4533 (0.6062) acc 90.6250 (82.8125) lr 1.9921e-02 eta 0:31:56\n",
      "epoch [10/200] batch [15/18] time 0.321 (0.479) data 0.000 (0.144) loss 0.5306 (0.6067) acc 87.5000 (83.5417) lr 1.9921e-02 eta 0:27:20\n",
      "epoch [11/200] batch [5/18] time 0.354 (0.799) data 0.000 (0.383) loss 0.6413 (0.5788) acc 84.3750 (83.1250) lr 1.9900e-02 eta 0:45:28\n",
      "epoch [11/200] batch [10/18] time 0.671 (0.659) data 0.225 (0.249) loss 0.5831 (0.5534) acc 84.3750 (85.6250) lr 1.9900e-02 eta 0:37:25\n",
      "epoch [11/200] batch [15/18] time 0.315 (0.552) data 0.000 (0.166) loss 0.4347 (0.5484) acc 84.3750 (84.7917) lr 1.9900e-02 eta 0:31:18\n",
      "epoch [12/200] batch [5/18] time 0.331 (0.801) data 0.000 (0.449) loss 0.6875 (0.6472) acc 81.2500 (81.8750) lr 1.9877e-02 eta 0:45:20\n",
      "epoch [12/200] batch [10/18] time 0.332 (0.567) data 0.000 (0.225) loss 0.5679 (0.5829) acc 81.2500 (84.0625) lr 1.9877e-02 eta 0:32:04\n",
      "epoch [12/200] batch [15/18] time 0.321 (0.485) data 0.000 (0.150) loss 1.1601 (0.6345) acc 75.0000 (82.9167) lr 1.9877e-02 eta 0:27:24\n",
      "epoch [13/200] batch [5/18] time 0.421 (0.892) data 0.003 (0.484) loss 1.1590 (0.7027) acc 68.7500 (80.6250) lr 1.9851e-02 eta 0:50:12\n",
      "epoch [13/200] batch [10/18] time 0.363 (0.699) data 0.003 (0.297) loss 0.8782 (0.7670) acc 71.8750 (77.5000) lr 1.9851e-02 eta 0:39:17\n",
      "epoch [13/200] batch [15/18] time 0.324 (0.575) data 0.000 (0.198) loss 0.3567 (0.6535) acc 87.5000 (81.0417) lr 1.9851e-02 eta 0:32:16\n",
      "epoch [14/200] batch [5/18] time 0.322 (0.781) data 0.001 (0.438) loss 0.7545 (0.5802) acc 71.8750 (83.7500) lr 1.9823e-02 eta 0:43:45\n",
      "epoch [14/200] batch [10/18] time 0.336 (0.557) data 0.000 (0.219) loss 0.4554 (0.6181) acc 90.6250 (82.5000) lr 1.9823e-02 eta 0:31:09\n",
      "epoch [14/200] batch [15/18] time 0.321 (0.479) data 0.000 (0.146) loss 0.3497 (0.5906) acc 87.5000 (82.9167) lr 1.9823e-02 eta 0:26:46\n",
      "epoch [15/200] batch [5/18] time 0.332 (0.791) data 0.000 (0.405) loss 0.7134 (0.7447) acc 81.2500 (76.2500) lr 1.9792e-02 eta 0:44:02\n",
      "epoch [15/200] batch [10/18] time 0.342 (0.653) data 0.000 (0.262) loss 0.1665 (0.6347) acc 100.0000 (80.6250) lr 1.9792e-02 eta 0:36:18\n",
      "epoch [15/200] batch [15/18] time 0.326 (0.547) data 0.002 (0.175) loss 0.4843 (0.5981) acc 78.1250 (81.2500) lr 1.9792e-02 eta 0:30:24\n",
      "epoch [16/200] batch [5/18] time 0.341 (0.801) data 0.000 (0.460) loss 0.6145 (0.5206) acc 84.3750 (85.0000) lr 1.9759e-02 eta 0:44:21\n",
      "epoch [16/200] batch [10/18] time 0.326 (0.567) data 0.000 (0.230) loss 0.6613 (0.5031) acc 81.2500 (85.0000) lr 1.9759e-02 eta 0:31:22\n",
      "epoch [16/200] batch [15/18] time 0.325 (0.487) data 0.000 (0.154) loss 0.6343 (0.5329) acc 87.5000 (84.5833) lr 1.9759e-02 eta 0:26:55\n",
      "epoch [17/200] batch [5/18] time 0.473 (0.967) data 0.011 (0.513) loss 0.7314 (0.5685) acc 75.0000 (85.0000) lr 1.9724e-02 eta 0:53:17\n",
      "epoch [17/200] batch [10/18] time 0.349 (0.712) data 0.000 (0.295) loss 0.8807 (0.6113) acc 78.1250 (83.4375) lr 1.9724e-02 eta 0:39:09\n",
      "epoch [17/200] batch [15/18] time 0.330 (0.587) data 0.000 (0.197) loss 0.6717 (0.5914) acc 81.2500 (83.3333) lr 1.9724e-02 eta 0:32:16\n",
      "epoch [18/200] batch [5/18] time 0.332 (0.745) data 0.000 (0.399) loss 0.8466 (0.5104) acc 65.6250 (83.7500) lr 1.9686e-02 eta 0:40:50\n",
      "epoch [18/200] batch [10/18] time 0.336 (0.545) data 0.000 (0.200) loss 0.6654 (0.5271) acc 81.2500 (83.4375) lr 1.9686e-02 eta 0:29:50\n",
      "epoch [18/200] batch [15/18] time 0.329 (0.475) data 0.000 (0.133) loss 0.2973 (0.5128) acc 93.7500 (83.7500) lr 1.9686e-02 eta 0:25:57\n",
      "epoch [19/200] batch [5/18] time 0.389 (0.852) data 0.000 (0.461) loss 0.5772 (0.6186) acc 78.1250 (78.1250) lr 1.9646e-02 eta 0:46:28\n",
      "epoch [19/200] batch [10/18] time 0.396 (0.660) data 0.001 (0.266) loss 0.5738 (0.6198) acc 81.2500 (79.0625) lr 1.9646e-02 eta 0:35:56\n",
      "epoch [19/200] batch [15/18] time 0.332 (0.555) data 0.000 (0.178) loss 0.3373 (0.5988) acc 93.7500 (81.2500) lr 1.9646e-02 eta 0:30:10\n",
      "epoch [20/200] batch [5/18] time 0.342 (0.810) data 0.001 (0.423) loss 0.2373 (0.4168) acc 90.6250 (86.8750) lr 1.9603e-02 eta 0:43:55\n",
      "epoch [20/200] batch [10/18] time 0.357 (0.577) data 0.001 (0.212) loss 0.4816 (0.4197) acc 84.3750 (86.5625) lr 1.9603e-02 eta 0:31:12\n",
      "epoch [20/200] batch [15/18] time 0.328 (0.495) data 0.000 (0.142) loss 0.3201 (0.4615) acc 90.6250 (85.4167) lr 1.9603e-02 eta 0:26:43\n",
      "epoch [21/200] batch [5/18] time 0.376 (0.787) data 0.000 (0.383) loss 0.5526 (0.4328) acc 78.1250 (84.3750) lr 1.9558e-02 eta 0:42:25\n",
      "epoch [21/200] batch [10/18] time 0.551 (0.618) data 0.059 (0.220) loss 0.6579 (0.4775) acc 75.0000 (84.0625) lr 1.9558e-02 eta 0:33:17\n",
      "epoch [21/200] batch [15/18] time 0.339 (0.552) data 0.000 (0.175) loss 0.4941 (0.4927) acc 93.7500 (83.9583) lr 1.9558e-02 eta 0:29:39\n",
      "epoch [22/200] batch [5/18] time 0.341 (0.778) data 0.000 (0.425) loss 0.2460 (0.5707) acc 96.8750 (80.0000) lr 1.9511e-02 eta 0:41:44\n",
      "epoch [22/200] batch [10/18] time 0.332 (0.557) data 0.001 (0.213) loss 0.4195 (0.5362) acc 87.5000 (81.5625) lr 1.9511e-02 eta 0:29:50\n",
      "epoch [22/200] batch [15/18] time 0.330 (0.482) data 0.000 (0.142) loss 0.4375 (0.5744) acc 84.3750 (81.4583) lr 1.9511e-02 eta 0:25:44\n",
      "epoch [23/200] batch [5/18] time 0.351 (0.755) data 0.000 (0.342) loss 0.6174 (0.4226) acc 84.3750 (88.1250) lr 1.9461e-02 eta 0:40:15\n",
      "epoch [23/200] batch [10/18] time 0.773 (0.630) data 0.371 (0.234) loss 0.3567 (0.4054) acc 90.6250 (87.8125) lr 1.9461e-02 eta 0:33:30\n",
      "epoch [23/200] batch [15/18] time 0.336 (0.531) data 0.004 (0.157) loss 0.4959 (0.4411) acc 84.3750 (87.0833) lr 1.9461e-02 eta 0:28:13\n",
      "epoch [24/200] batch [5/18] time 0.324 (0.720) data 0.000 (0.371) loss 0.4649 (0.4695) acc 84.3750 (86.8750) lr 1.9409e-02 eta 0:38:10\n",
      "epoch [24/200] batch [10/18] time 0.323 (0.527) data 0.004 (0.187) loss 0.3079 (0.4388) acc 90.6250 (88.4375) lr 1.9409e-02 eta 0:27:52\n",
      "epoch [24/200] batch [15/18] time 0.330 (0.461) data 0.000 (0.125) loss 0.4972 (0.4765) acc 84.3750 (86.8750) lr 1.9409e-02 eta 0:24:21\n",
      "epoch [25/200] batch [5/18] time 0.379 (0.743) data 0.000 (0.365) loss 0.3665 (0.4619) acc 87.5000 (85.6250) lr 1.9354e-02 eta 0:39:08\n",
      "epoch [25/200] batch [10/18] time 0.488 (0.585) data 0.102 (0.219) loss 0.4160 (0.5560) acc 87.5000 (84.0625) lr 1.9354e-02 eta 0:30:48\n",
      "epoch [25/200] batch [15/18] time 0.327 (0.502) data 0.000 (0.146) loss 0.2736 (0.5157) acc 87.5000 (85.4167) lr 1.9354e-02 eta 0:26:23\n",
      "epoch [26/200] batch [5/18] time 0.343 (0.732) data 0.001 (0.380) loss 0.2394 (0.4395) acc 93.7500 (85.0000) lr 1.9298e-02 eta 0:38:21\n",
      "epoch [26/200] batch [10/18] time 0.336 (0.536) data 0.000 (0.190) loss 0.5896 (0.4765) acc 90.6250 (86.2500) lr 1.9298e-02 eta 0:28:04\n",
      "epoch [26/200] batch [15/18] time 0.329 (0.468) data 0.000 (0.127) loss 0.6033 (0.5074) acc 81.2500 (84.3750) lr 1.9298e-02 eta 0:24:27\n",
      "epoch [27/200] batch [5/18] time 0.339 (0.854) data 0.002 (0.483) loss 0.4076 (0.4888) acc 84.3750 (84.3750) lr 1.9239e-02 eta 0:44:29\n",
      "epoch [27/200] batch [10/18] time 0.348 (0.657) data 0.000 (0.277) loss 0.5644 (0.5201) acc 84.3750 (84.0625) lr 1.9239e-02 eta 0:34:12\n",
      "epoch [27/200] batch [15/18] time 0.331 (0.549) data 0.000 (0.185) loss 0.3729 (0.4787) acc 84.3750 (84.7917) lr 1.9239e-02 eta 0:28:31\n",
      "epoch [28/200] batch [5/18] time 0.340 (0.754) data 0.001 (0.381) loss 0.8480 (0.6365) acc 78.1250 (80.6250) lr 1.9178e-02 eta 0:39:04\n",
      "epoch [28/200] batch [10/18] time 0.334 (0.546) data 0.000 (0.191) loss 0.4707 (0.5516) acc 87.5000 (83.7500) lr 1.9178e-02 eta 0:28:13\n",
      "epoch [28/200] batch [15/18] time 0.331 (0.474) data 0.000 (0.127) loss 0.5207 (0.5405) acc 84.3750 (84.1667) lr 1.9178e-02 eta 0:24:27\n",
      "epoch [29/200] batch [5/18] time 0.331 (0.799) data 0.000 (0.381) loss 0.6211 (0.4585) acc 75.0000 (85.0000) lr 1.9114e-02 eta 0:41:10\n",
      "epoch [29/200] batch [10/18] time 0.370 (0.647) data 0.001 (0.214) loss 0.3535 (0.4862) acc 87.5000 (85.9375) lr 1.9114e-02 eta 0:33:16\n",
      "epoch [29/200] batch [15/18] time 0.329 (0.543) data 0.000 (0.143) loss 0.6427 (0.5318) acc 84.3750 (84.3750) lr 1.9114e-02 eta 0:27:53\n",
      "epoch [30/200] batch [5/18] time 0.352 (0.670) data 0.000 (0.319) loss 0.4890 (0.3222) acc 87.5000 (91.8750) lr 1.9048e-02 eta 0:34:18\n",
      "epoch [30/200] batch [10/18] time 0.324 (0.506) data 0.000 (0.160) loss 0.5330 (0.4083) acc 81.2500 (89.0625) lr 1.9048e-02 eta 0:25:52\n",
      "epoch [30/200] batch [15/18] time 0.333 (0.448) data 0.001 (0.107) loss 0.6739 (0.4313) acc 78.1250 (87.2917) lr 1.9048e-02 eta 0:22:52\n",
      "epoch [31/200] batch [5/18] time 0.328 (0.744) data 0.002 (0.371) loss 0.3623 (0.4590) acc 87.5000 (87.5000) lr 1.8980e-02 eta 0:37:51\n",
      "epoch [31/200] batch [10/18] time 0.362 (0.641) data 0.000 (0.260) loss 0.5166 (0.5057) acc 81.2500 (86.2500) lr 1.8980e-02 eta 0:32:34\n",
      "epoch [31/200] batch [15/18] time 0.328 (0.539) data 0.000 (0.173) loss 0.4125 (0.5637) acc 90.6250 (83.7500) lr 1.8980e-02 eta 0:27:20\n",
      "epoch [32/200] batch [5/18] time 0.335 (0.749) data 0.000 (0.395) loss 0.7210 (0.5207) acc 75.0000 (81.8750) lr 1.8910e-02 eta 0:37:54\n",
      "epoch [32/200] batch [10/18] time 0.341 (0.548) data 0.000 (0.198) loss 0.4302 (0.5241) acc 84.3750 (82.5000) lr 1.8910e-02 eta 0:27:40\n",
      "epoch [32/200] batch [15/18] time 0.330 (0.475) data 0.000 (0.132) loss 0.5616 (0.5050) acc 81.2500 (83.3333) lr 1.8910e-02 eta 0:23:58\n",
      "epoch [33/200] batch [5/18] time 0.396 (0.817) data 0.000 (0.405) loss 0.3884 (0.4177) acc 87.5000 (86.2500) lr 1.8838e-02 eta 0:41:05\n",
      "epoch [33/200] batch [10/18] time 0.362 (0.661) data 0.000 (0.240) loss 0.5269 (0.4583) acc 84.3750 (84.6875) lr 1.8838e-02 eta 0:33:11\n",
      "epoch [33/200] batch [15/18] time 0.331 (0.551) data 0.000 (0.160) loss 0.5779 (0.4604) acc 81.2500 (85.2083) lr 1.8838e-02 eta 0:27:39\n",
      "epoch [34/200] batch [5/18] time 0.340 (0.789) data 0.000 (0.446) loss 0.4312 (0.4663) acc 84.3750 (86.2500) lr 1.8763e-02 eta 0:39:27\n",
      "epoch [34/200] batch [10/18] time 0.331 (0.562) data 0.000 (0.224) loss 0.6573 (0.4804) acc 75.0000 (85.6250) lr 1.8763e-02 eta 0:28:05\n",
      "epoch [34/200] batch [15/18] time 0.332 (0.485) data 0.000 (0.149) loss 0.8496 (0.4963) acc 75.0000 (85.4167) lr 1.8763e-02 eta 0:24:10\n",
      "epoch [35/200] batch [5/18] time 0.357 (0.785) data 0.001 (0.398) loss 0.3737 (0.5041) acc 87.5000 (85.0000) lr 1.8686e-02 eta 0:39:00\n",
      "epoch [35/200] batch [10/18] time 0.463 (0.600) data 0.000 (0.206) loss 0.6167 (0.5031) acc 78.1250 (85.6250) lr 1.8686e-02 eta 0:29:47\n",
      "epoch [35/200] batch [15/18] time 0.341 (0.527) data 0.000 (0.151) loss 0.3753 (0.4617) acc 90.6250 (86.6667) lr 1.8686e-02 eta 0:26:07\n",
      "epoch [36/200] batch [5/18] time 0.348 (0.755) data 0.000 (0.393) loss 0.5457 (0.4730) acc 78.1250 (85.0000) lr 1.8607e-02 eta 0:37:17\n",
      "epoch [36/200] batch [10/18] time 0.336 (0.545) data 0.000 (0.197) loss 0.3998 (0.5032) acc 84.3750 (84.6875) lr 1.8607e-02 eta 0:26:54\n",
      "epoch [36/200] batch [15/18] time 0.332 (0.474) data 0.000 (0.131) loss 0.3468 (0.4973) acc 96.8750 (83.7500) lr 1.8607e-02 eta 0:23:20\n",
      "epoch [37/200] batch [5/18] time 0.346 (0.723) data 0.000 (0.337) loss 0.5025 (0.4275) acc 84.3750 (87.5000) lr 1.8526e-02 eta 0:35:31\n",
      "epoch [37/200] batch [10/18] time 0.564 (0.611) data 0.001 (0.207) loss 0.5272 (0.4870) acc 81.2500 (85.3125) lr 1.8526e-02 eta 0:29:56\n",
      "epoch [37/200] batch [15/18] time 0.324 (0.519) data 0.001 (0.138) loss 0.2198 (0.4888) acc 90.6250 (85.0000) lr 1.8526e-02 eta 0:25:24\n",
      "epoch [38/200] batch [5/18] time 0.336 (0.776) data 0.004 (0.416) loss 0.4427 (0.4706) acc 87.5000 (86.8750) lr 1.8443e-02 eta 0:37:52\n",
      "epoch [38/200] batch [10/18] time 0.327 (0.554) data 0.000 (0.208) loss 0.5361 (0.4489) acc 81.2500 (85.6250) lr 1.8443e-02 eta 0:27:00\n",
      "epoch [38/200] batch [15/18] time 0.331 (0.480) data 0.000 (0.139) loss 0.7436 (0.4550) acc 78.1250 (85.8333) lr 1.8443e-02 eta 0:23:20\n",
      "epoch [39/200] batch [5/18] time 0.418 (0.859) data 0.001 (0.487) loss 0.3994 (0.3925) acc 90.6250 (86.8750) lr 1.8358e-02 eta 0:41:39\n",
      "epoch [39/200] batch [10/18] time 0.335 (0.635) data 0.000 (0.266) loss 0.4450 (0.4611) acc 90.6250 (86.2500) lr 1.8358e-02 eta 0:30:45\n",
      "epoch [39/200] batch [15/18] time 0.329 (0.534) data 0.000 (0.178) loss 0.3258 (0.4492) acc 93.7500 (85.8333) lr 1.8358e-02 eta 0:25:49\n",
      "epoch [40/200] batch [5/18] time 0.344 (0.755) data 0.000 (0.407) loss 0.2789 (0.3792) acc 87.5000 (87.5000) lr 1.8271e-02 eta 0:36:23\n",
      "epoch [40/200] batch [10/18] time 0.337 (0.544) data 0.000 (0.205) loss 0.3424 (0.4115) acc 93.7500 (87.8125) lr 1.8271e-02 eta 0:26:11\n",
      "epoch [40/200] batch [15/18] time 0.330 (0.472) data 0.000 (0.136) loss 0.6429 (0.4580) acc 87.5000 (87.0833) lr 1.8271e-02 eta 0:22:41\n",
      "epoch [41/200] batch [5/18] time 0.333 (0.767) data 0.000 (0.396) loss 0.5001 (0.4267) acc 84.3750 (88.1250) lr 1.8181e-02 eta 0:36:44\n",
      "epoch [41/200] batch [10/18] time 0.346 (0.614) data 0.000 (0.246) loss 0.5079 (0.4776) acc 81.2500 (85.0000) lr 1.8181e-02 eta 0:29:21\n",
      "epoch [41/200] batch [15/18] time 0.328 (0.526) data 0.000 (0.164) loss 0.6609 (0.5148) acc 81.2500 (84.3750) lr 1.8181e-02 eta 0:25:05\n",
      "epoch [42/200] batch [5/18] time 0.358 (0.727) data 0.003 (0.370) loss 0.4269 (0.4323) acc 87.5000 (86.8750) lr 1.8090e-02 eta 0:34:38\n",
      "epoch [42/200] batch [10/18] time 0.337 (0.535) data 0.000 (0.185) loss 0.4668 (0.4279) acc 90.6250 (87.8125) lr 1.8090e-02 eta 0:25:25\n",
      "epoch [42/200] batch [15/18] time 0.325 (0.467) data 0.000 (0.124) loss 0.4597 (0.4590) acc 84.3750 (86.2500) lr 1.8090e-02 eta 0:22:10\n",
      "epoch [43/200] batch [5/18] time 0.327 (0.831) data 0.000 (0.470) loss 0.3249 (0.3440) acc 90.6250 (91.2500) lr 1.7997e-02 eta 0:39:19\n",
      "epoch [43/200] batch [10/18] time 0.343 (0.614) data 0.000 (0.243) loss 0.5002 (0.4105) acc 93.7500 (86.8750) lr 1.7997e-02 eta 0:29:00\n",
      "epoch [43/200] batch [15/18] time 0.335 (0.521) data 0.000 (0.162) loss 0.4603 (0.4110) acc 84.3750 (86.4583) lr 1.7997e-02 eta 0:24:32\n",
      "epoch [44/200] batch [5/18] time 0.348 (0.726) data 0.000 (0.371) loss 0.5724 (0.4268) acc 84.3750 (88.1250) lr 1.7902e-02 eta 0:34:06\n",
      "epoch [44/200] batch [10/18] time 0.353 (0.537) data 0.000 (0.187) loss 0.5103 (0.4258) acc 84.3750 (86.8750) lr 1.7902e-02 eta 0:25:11\n",
      "epoch [44/200] batch [15/18] time 0.319 (0.466) data 0.000 (0.125) loss 0.1561 (0.4278) acc 93.7500 (86.4583) lr 1.7902e-02 eta 0:21:51\n",
      "epoch [45/200] batch [5/18] time 0.401 (0.880) data 0.001 (0.474) loss 0.4070 (0.4496) acc 87.5000 (86.2500) lr 1.7804e-02 eta 0:41:07\n",
      "epoch [45/200] batch [10/18] time 0.337 (0.630) data 0.000 (0.246) loss 0.5705 (0.4650) acc 75.0000 (85.0000) lr 1.7804e-02 eta 0:29:23\n",
      "epoch [45/200] batch [15/18] time 0.334 (0.531) data 0.000 (0.164) loss 0.4894 (0.4470) acc 84.3750 (85.2083) lr 1.7804e-02 eta 0:24:42\n",
      "epoch [46/200] batch [5/18] time 0.344 (0.775) data 0.000 (0.438) loss 0.4537 (0.5911) acc 84.3750 (83.7500) lr 1.7705e-02 eta 0:35:59\n",
      "epoch [46/200] batch [10/18] time 0.326 (0.555) data 0.000 (0.219) loss 0.2983 (0.5044) acc 93.7500 (85.0000) lr 1.7705e-02 eta 0:25:43\n",
      "epoch [46/200] batch [15/18] time 0.330 (0.480) data 0.000 (0.146) loss 0.2710 (0.4842) acc 93.7500 (85.4167) lr 1.7705e-02 eta 0:22:12\n",
      "epoch [47/200] batch [5/18] time 0.415 (0.878) data 0.000 (0.494) loss 0.2754 (0.3907) acc 93.7500 (86.8750) lr 1.7604e-02 eta 0:40:29\n",
      "epoch [47/200] batch [10/18] time 0.326 (0.665) data 0.000 (0.286) loss 0.3443 (0.3780) acc 87.5000 (88.4375) lr 1.7604e-02 eta 0:30:35\n",
      "epoch [47/200] batch [15/18] time 0.331 (0.555) data 0.000 (0.191) loss 0.3684 (0.4303) acc 87.5000 (86.6667) lr 1.7604e-02 eta 0:25:29\n",
      "epoch [48/200] batch [5/18] time 0.338 (0.749) data 0.000 (0.402) loss 0.2837 (0.3379) acc 87.5000 (90.0000) lr 1.7501e-02 eta 0:34:19\n",
      "epoch [48/200] batch [10/18] time 0.323 (0.545) data 0.001 (0.201) loss 0.2276 (0.3825) acc 96.8750 (88.7500) lr 1.7501e-02 eta 0:24:56\n",
      "epoch [48/200] batch [15/18] time 0.331 (0.475) data 0.000 (0.134) loss 0.7607 (0.4478) acc 81.2500 (86.4583) lr 1.7501e-02 eta 0:21:41\n",
      "epoch [49/200] batch [5/18] time 0.364 (0.840) data 0.001 (0.457) loss 0.6347 (0.4449) acc 78.1250 (86.8750) lr 1.7396e-02 eta 0:38:12\n",
      "epoch [49/200] batch [10/18] time 0.333 (0.656) data 0.000 (0.263) loss 0.3194 (0.3988) acc 93.7500 (89.0625) lr 1.7396e-02 eta 0:29:48\n",
      "epoch [49/200] batch [15/18] time 0.331 (0.548) data 0.000 (0.175) loss 0.4862 (0.4008) acc 87.5000 (88.5417) lr 1.7396e-02 eta 0:24:51\n",
      "epoch [50/200] batch [5/18] time 0.376 (0.785) data 0.001 (0.432) loss 0.5227 (0.5780) acc 84.3750 (80.6250) lr 1.7290e-02 eta 0:35:28\n",
      "epoch [50/200] batch [10/18] time 0.324 (0.563) data 0.000 (0.216) loss 0.8689 (0.5395) acc 75.0000 (83.4375) lr 1.7290e-02 eta 0:25:24\n",
      "epoch [50/200] batch [15/18] time 0.335 (0.486) data 0.000 (0.144) loss 0.5463 (0.4811) acc 75.0000 (84.7917) lr 1.7290e-02 eta 0:21:52\n",
      "epoch [51/200] batch [5/18] time 0.355 (0.867) data 0.002 (0.489) loss 0.2451 (0.3338) acc 93.7500 (92.5000) lr 1.7181e-02 eta 0:38:55\n",
      "epoch [51/200] batch [10/18] time 0.348 (0.671) data 0.000 (0.279) loss 0.4076 (0.4011) acc 84.3750 (87.8125) lr 1.7181e-02 eta 0:30:05\n",
      "epoch [51/200] batch [15/18] time 0.328 (0.558) data 0.000 (0.186) loss 0.3224 (0.4140) acc 90.6250 (87.5000) lr 1.7181e-02 eta 0:24:58\n",
      "epoch [52/200] batch [5/18] time 0.363 (0.802) data 0.001 (0.431) loss 0.2737 (0.3680) acc 90.6250 (88.7500) lr 1.7071e-02 eta 0:35:46\n",
      "epoch [52/200] batch [10/18] time 0.332 (0.566) data 0.000 (0.216) loss 0.6755 (0.4131) acc 81.2500 (87.5000) lr 1.7071e-02 eta 0:25:11\n",
      "epoch [52/200] batch [15/18] time 0.332 (0.488) data 0.000 (0.144) loss 0.4340 (0.4038) acc 84.3750 (88.1250) lr 1.7071e-02 eta 0:21:40\n",
      "epoch [53/200] batch [5/18] time 0.368 (0.844) data 0.001 (0.421) loss 0.4761 (0.4681) acc 81.2500 (83.7500) lr 1.6959e-02 eta 0:37:24\n",
      "epoch [53/200] batch [10/18] time 0.339 (0.655) data 0.003 (0.248) loss 0.3115 (0.4900) acc 90.6250 (83.7500) lr 1.6959e-02 eta 0:28:59\n",
      "epoch [53/200] batch [15/18] time 0.338 (0.548) data 0.000 (0.166) loss 0.2722 (0.4545) acc 87.5000 (85.4167) lr 1.6959e-02 eta 0:24:10\n",
      "epoch [54/200] batch [5/18] time 0.337 (0.781) data 0.001 (0.437) loss 0.4317 (0.4256) acc 81.2500 (84.3750) lr 1.6845e-02 eta 0:34:21\n",
      "epoch [54/200] batch [10/18] time 0.343 (0.558) data 0.000 (0.219) loss 0.6655 (0.4412) acc 87.5000 (85.9375) lr 1.6845e-02 eta 0:24:30\n",
      "epoch [54/200] batch [15/18] time 0.324 (0.482) data 0.000 (0.146) loss 0.1872 (0.4099) acc 96.8750 (87.5000) lr 1.6845e-02 eta 0:21:07\n",
      "epoch [55/200] batch [5/18] time 0.335 (0.770) data 0.000 (0.373) loss 0.6359 (0.5024) acc 84.3750 (86.2500) lr 1.6730e-02 eta 0:33:38\n",
      "epoch [55/200] batch [10/18] time 0.825 (0.641) data 0.421 (0.248) loss 0.6982 (0.4779) acc 75.0000 (86.5625) lr 1.6730e-02 eta 0:27:57\n",
      "epoch [55/200] batch [15/18] time 0.337 (0.538) data 0.000 (0.166) loss 0.3316 (0.4717) acc 90.6250 (86.2500) lr 1.6730e-02 eta 0:23:26\n",
      "epoch [56/200] batch [5/18] time 0.320 (0.769) data 0.000 (0.413) loss 0.3576 (0.4332) acc 87.5000 (86.2500) lr 1.6613e-02 eta 0:33:22\n",
      "epoch [56/200] batch [10/18] time 0.340 (0.557) data 0.000 (0.207) loss 0.5104 (0.4427) acc 84.3750 (85.9375) lr 1.6613e-02 eta 0:24:07\n",
      "epoch [56/200] batch [15/18] time 0.328 (0.480) data 0.000 (0.138) loss 0.2995 (0.4425) acc 90.6250 (85.6250) lr 1.6613e-02 eta 0:20:46\n",
      "epoch [57/200] batch [5/18] time 0.436 (0.845) data 0.000 (0.445) loss 0.5589 (0.4941) acc 84.3750 (85.0000) lr 1.6494e-02 eta 0:36:25\n",
      "epoch [57/200] batch [10/18] time 0.352 (0.646) data 0.000 (0.251) loss 0.4656 (0.5439) acc 81.2500 (82.5000) lr 1.6494e-02 eta 0:27:47\n",
      "epoch [57/200] batch [15/18] time 0.335 (0.542) data 0.000 (0.167) loss 0.4128 (0.5025) acc 81.2500 (84.1667) lr 1.6494e-02 eta 0:23:16\n",
      "epoch [58/200] batch [5/18] time 0.433 (0.788) data 0.000 (0.423) loss 0.4781 (0.4873) acc 81.2500 (86.2500) lr 1.6374e-02 eta 0:33:44\n",
      "epoch [58/200] batch [10/18] time 0.323 (0.559) data 0.000 (0.212) loss 0.3607 (0.4414) acc 93.7500 (87.5000) lr 1.6374e-02 eta 0:23:52\n",
      "epoch [58/200] batch [15/18] time 0.333 (0.483) data 0.000 (0.141) loss 0.3494 (0.4124) acc 93.7500 (88.9583) lr 1.6374e-02 eta 0:20:36\n",
      "epoch [59/200] batch [5/18] time 0.422 (0.827) data 0.000 (0.382) loss 0.3672 (0.4509) acc 90.6250 (88.1250) lr 1.6252e-02 eta 0:35:10\n",
      "epoch [59/200] batch [10/18] time 0.603 (0.632) data 0.275 (0.219) loss 0.2451 (0.4239) acc 93.7500 (87.1875) lr 1.6252e-02 eta 0:26:48\n",
      "epoch [59/200] batch [15/18] time 0.328 (0.532) data 0.000 (0.147) loss 0.6515 (0.4288) acc 68.7500 (86.4583) lr 1.6252e-02 eta 0:22:30\n",
      "epoch [60/200] batch [5/18] time 0.331 (0.710) data 0.001 (0.368) loss 0.3947 (0.4229) acc 90.6250 (88.7500) lr 1.6129e-02 eta 0:29:58\n",
      "epoch [60/200] batch [10/18] time 0.330 (0.527) data 0.000 (0.184) loss 0.5122 (0.4702) acc 84.3750 (87.5000) lr 1.6129e-02 eta 0:22:12\n",
      "epoch [60/200] batch [15/18] time 0.325 (0.462) data 0.000 (0.123) loss 0.3416 (0.4716) acc 90.6250 (86.4583) lr 1.6129e-02 eta 0:19:24\n",
      "epoch [61/200] batch [5/18] time 0.353 (0.813) data 0.001 (0.420) loss 0.4537 (0.5121) acc 87.5000 (83.1250) lr 1.6004e-02 eta 0:34:05\n",
      "epoch [61/200] batch [10/18] time 0.442 (0.602) data 0.001 (0.210) loss 0.7282 (0.4595) acc 71.8750 (84.3750) lr 1.6004e-02 eta 0:25:12\n",
      "epoch [61/200] batch [15/18] time 0.323 (0.512) data 0.000 (0.140) loss 0.3842 (0.4591) acc 87.5000 (85.4167) lr 1.6004e-02 eta 0:21:21\n",
      "epoch [62/200] batch [5/18] time 0.370 (0.778) data 0.000 (0.430) loss 0.2285 (0.3479) acc 96.8750 (90.6250) lr 1.5878e-02 eta 0:32:22\n",
      "epoch [62/200] batch [10/18] time 0.333 (0.559) data 0.000 (0.215) loss 0.2094 (0.3827) acc 93.7500 (88.1250) lr 1.5878e-02 eta 0:23:11\n",
      "epoch [62/200] batch [15/18] time 0.328 (0.481) data 0.000 (0.144) loss 0.4104 (0.4093) acc 84.3750 (87.7083) lr 1.5878e-02 eta 0:19:57\n",
      "epoch [63/200] batch [5/18] time 0.443 (0.842) data 0.001 (0.413) loss 0.6636 (0.4642) acc 81.2500 (85.0000) lr 1.5750e-02 eta 0:34:47\n",
      "epoch [63/200] batch [10/18] time 0.330 (0.641) data 0.001 (0.222) loss 0.5884 (0.4384) acc 78.1250 (85.9375) lr 1.5750e-02 eta 0:26:26\n",
      "epoch [63/200] batch [15/18] time 0.336 (0.539) data 0.000 (0.148) loss 0.5896 (0.4543) acc 81.2500 (85.4167) lr 1.5750e-02 eta 0:22:10\n",
      "epoch [64/200] batch [5/18] time 0.342 (0.708) data 0.001 (0.361) loss 0.1823 (0.2910) acc 96.8750 (90.6250) lr 1.5621e-02 eta 0:29:02\n",
      "epoch [64/200] batch [10/18] time 0.326 (0.523) data 0.001 (0.186) loss 0.1626 (0.3343) acc 96.8750 (90.0000) lr 1.5621e-02 eta 0:21:24\n",
      "epoch [64/200] batch [15/18] time 0.328 (0.458) data 0.000 (0.124) loss 0.4732 (0.4055) acc 87.5000 (87.5000) lr 1.5621e-02 eta 0:18:42\n",
      "epoch [65/200] batch [5/18] time 0.363 (0.763) data 0.003 (0.365) loss 0.2290 (0.4104) acc 90.6250 (84.3750) lr 1.5490e-02 eta 0:31:04\n",
      "epoch [65/200] batch [10/18] time 0.520 (0.609) data 0.103 (0.206) loss 0.1550 (0.4067) acc 96.8750 (87.1875) lr 1.5490e-02 eta 0:24:45\n",
      "epoch [65/200] batch [15/18] time 0.323 (0.517) data 0.000 (0.138) loss 0.5012 (0.4127) acc 84.3750 (86.8750) lr 1.5490e-02 eta 0:20:57\n",
      "epoch [66/200] batch [5/18] time 0.369 (0.698) data 0.003 (0.340) loss 0.3839 (0.3003) acc 87.5000 (91.8750) lr 1.5358e-02 eta 0:28:13\n",
      "epoch [66/200] batch [10/18] time 0.331 (0.520) data 0.000 (0.170) loss 0.4522 (0.3900) acc 87.5000 (89.3750) lr 1.5358e-02 eta 0:20:59\n",
      "epoch [66/200] batch [15/18] time 0.334 (0.457) data 0.000 (0.114) loss 0.3386 (0.3830) acc 93.7500 (89.3750) lr 1.5358e-02 eta 0:18:24\n",
      "epoch [67/200] batch [5/18] time 0.377 (0.767) data 0.000 (0.391) loss 0.2463 (0.4629) acc 96.8750 (88.7500) lr 1.5225e-02 eta 0:30:46\n",
      "epoch [67/200] batch [10/18] time 0.372 (0.639) data 0.001 (0.255) loss 0.2069 (0.4250) acc 96.8750 (89.0625) lr 1.5225e-02 eta 0:25:34\n",
      "epoch [67/200] batch [15/18] time 0.328 (0.537) data 0.000 (0.170) loss 0.3456 (0.3959) acc 93.7500 (90.2083) lr 1.5225e-02 eta 0:21:27\n",
      "epoch [68/200] batch [5/18] time 0.336 (0.687) data 0.000 (0.338) loss 0.4575 (0.4189) acc 87.5000 (88.7500) lr 1.5090e-02 eta 0:27:22\n",
      "epoch [68/200] batch [10/18] time 0.338 (0.513) data 0.000 (0.170) loss 0.2011 (0.4152) acc 96.8750 (88.7500) lr 1.5090e-02 eta 0:20:24\n",
      "epoch [68/200] batch [15/18] time 0.327 (0.453) data 0.000 (0.113) loss 0.2338 (0.4028) acc 93.7500 (89.1667) lr 1.5090e-02 eta 0:17:57\n",
      "epoch [69/200] batch [5/18] time 0.337 (0.794) data 0.000 (0.402) loss 0.6271 (0.3872) acc 78.1250 (87.5000) lr 1.4955e-02 eta 0:31:23\n",
      "epoch [69/200] batch [10/18] time 0.328 (0.634) data 0.000 (0.243) loss 0.7427 (0.4735) acc 78.1250 (85.6250) lr 1.4955e-02 eta 0:24:59\n",
      "epoch [69/200] batch [15/18] time 0.337 (0.533) data 0.000 (0.162) loss 0.5647 (0.4998) acc 90.6250 (86.0417) lr 1.4955e-02 eta 0:20:59\n",
      "epoch [70/200] batch [5/18] time 0.372 (0.777) data 0.002 (0.422) loss 0.6002 (0.4365) acc 84.3750 (90.0000) lr 1.4818e-02 eta 0:30:28\n",
      "epoch [70/200] batch [10/18] time 0.336 (0.556) data 0.000 (0.211) loss 0.5523 (0.4078) acc 84.3750 (89.6875) lr 1.4818e-02 eta 0:21:44\n",
      "epoch [70/200] batch [15/18] time 0.336 (0.482) data 0.008 (0.141) loss 0.3012 (0.3723) acc 93.7500 (91.0417) lr 1.4818e-02 eta 0:18:48\n",
      "epoch [71/200] batch [5/18] time 0.436 (0.840) data 0.006 (0.457) loss 0.3004 (0.3903) acc 93.7500 (90.0000) lr 1.4679e-02 eta 0:32:41\n",
      "epoch [71/200] batch [10/18] time 0.330 (0.649) data 0.003 (0.282) loss 0.2846 (0.4350) acc 93.7500 (87.5000) lr 1.4679e-02 eta 0:25:12\n",
      "epoch [71/200] batch [15/18] time 0.335 (0.544) data 0.000 (0.189) loss 0.2286 (0.4008) acc 96.8750 (88.7500) lr 1.4679e-02 eta 0:21:05\n",
      "epoch [72/200] batch [5/18] time 0.333 (0.761) data 0.000 (0.417) loss 0.5203 (0.3457) acc 84.3750 (91.2500) lr 1.4540e-02 eta 0:29:23\n",
      "epoch [72/200] batch [10/18] time 0.329 (0.547) data 0.000 (0.209) loss 0.5459 (0.3753) acc 87.5000 (90.6250) lr 1.4540e-02 eta 0:21:05\n",
      "epoch [72/200] batch [15/18] time 0.332 (0.475) data 0.000 (0.140) loss 0.5735 (0.3677) acc 75.0000 (90.4167) lr 1.4540e-02 eta 0:18:15\n",
      "epoch [73/200] batch [5/18] time 0.356 (0.762) data 0.001 (0.370) loss 0.6397 (0.3781) acc 81.2500 (90.6250) lr 1.4399e-02 eta 0:29:11\n",
      "epoch [73/200] batch [10/18] time 0.346 (0.635) data 0.000 (0.242) loss 0.5396 (0.3492) acc 84.3750 (90.6250) lr 1.4399e-02 eta 0:24:17\n",
      "epoch [73/200] batch [15/18] time 0.337 (0.536) data 0.000 (0.161) loss 0.7243 (0.3927) acc 81.2500 (90.0000) lr 1.4399e-02 eta 0:20:25\n",
      "epoch [74/200] batch [5/18] time 0.343 (0.773) data 0.001 (0.422) loss 0.3190 (0.3090) acc 84.3750 (90.0000) lr 1.4258e-02 eta 0:29:23\n",
      "epoch [74/200] batch [10/18] time 0.352 (0.560) data 0.000 (0.211) loss 0.5244 (0.3474) acc 84.3750 (88.7500) lr 1.4258e-02 eta 0:21:14\n",
      "epoch [74/200] batch [15/18] time 0.328 (0.483) data 0.000 (0.141) loss 0.6020 (0.3834) acc 75.0000 (87.2917) lr 1.4258e-02 eta 0:18:15\n",
      "epoch [75/200] batch [5/18] time 0.424 (0.852) data 0.001 (0.463) loss 0.3131 (0.3490) acc 90.6250 (90.6250) lr 1.4115e-02 eta 0:32:07\n",
      "epoch [75/200] batch [10/18] time 0.339 (0.644) data 0.000 (0.232) loss 0.2178 (0.3451) acc 96.8750 (89.3750) lr 1.4115e-02 eta 0:24:14\n",
      "epoch [75/200] batch [15/18] time 0.330 (0.541) data 0.000 (0.155) loss 0.3082 (0.3640) acc 93.7500 (89.3750) lr 1.4115e-02 eta 0:20:18\n",
      "epoch [76/200] batch [5/18] time 0.342 (0.791) data 0.000 (0.445) loss 0.3459 (0.3795) acc 90.6250 (88.7500) lr 1.3971e-02 eta 0:29:35\n",
      "epoch [76/200] batch [10/18] time 0.327 (0.564) data 0.000 (0.223) loss 0.2496 (0.3684) acc 93.7500 (88.7500) lr 1.3971e-02 eta 0:21:03\n",
      "epoch [76/200] batch [15/18] time 0.327 (0.485) data 0.000 (0.148) loss 0.4633 (0.3565) acc 81.2500 (89.5833) lr 1.3971e-02 eta 0:18:04\n",
      "epoch [77/200] batch [5/18] time 0.378 (0.938) data 0.000 (0.500) loss 0.3560 (0.3832) acc 90.6250 (89.3750) lr 1.3827e-02 eta 0:34:48\n",
      "epoch [77/200] batch [10/18] time 0.326 (0.693) data 0.000 (0.288) loss 0.2577 (0.3842) acc 93.7500 (89.0625) lr 1.3827e-02 eta 0:25:39\n",
      "epoch [77/200] batch [15/18] time 0.338 (0.572) data 0.000 (0.192) loss 0.4533 (0.4025) acc 84.3750 (87.7083) lr 1.3827e-02 eta 0:21:07\n",
      "epoch [78/200] batch [5/18] time 0.350 (0.705) data 0.000 (0.356) loss 0.3634 (0.4374) acc 87.5000 (85.6250) lr 1.3681e-02 eta 0:25:56\n",
      "epoch [78/200] batch [10/18] time 0.336 (0.524) data 0.000 (0.178) loss 0.2292 (0.4039) acc 93.7500 (88.4375) lr 1.3681e-02 eta 0:19:14\n",
      "epoch [78/200] batch [15/18] time 0.330 (0.460) data 0.000 (0.119) loss 0.2970 (0.4195) acc 90.6250 (87.7083) lr 1.3681e-02 eta 0:16:50\n",
      "epoch [79/200] batch [5/18] time 0.352 (0.762) data 0.001 (0.380) loss 0.5453 (0.3592) acc 81.2500 (89.3750) lr 1.3535e-02 eta 0:27:48\n",
      "epoch [79/200] batch [10/18] time 0.494 (0.618) data 0.171 (0.236) loss 0.2720 (0.3622) acc 87.5000 (88.4375) lr 1.3535e-02 eta 0:22:31\n",
      "epoch [79/200] batch [15/18] time 0.329 (0.523) data 0.000 (0.158) loss 0.3191 (0.3454) acc 90.6250 (89.3750) lr 1.3535e-02 eta 0:19:00\n",
      "epoch [80/200] batch [5/18] time 0.340 (0.728) data 0.000 (0.376) loss 0.3987 (0.3170) acc 87.5000 (91.8750) lr 1.3387e-02 eta 0:26:21\n",
      "epoch [80/200] batch [10/18] time 0.331 (0.536) data 0.000 (0.190) loss 0.4341 (0.3902) acc 87.5000 (90.3125) lr 1.3387e-02 eta 0:19:22\n",
      "epoch [80/200] batch [15/18] time 0.331 (0.468) data 0.000 (0.127) loss 0.4151 (0.4145) acc 87.5000 (89.5833) lr 1.3387e-02 eta 0:16:53\n",
      "epoch [81/200] batch [5/18] time 0.411 (0.827) data 0.002 (0.434) loss 0.5515 (0.4312) acc 87.5000 (89.3750) lr 1.3239e-02 eta 0:29:42\n",
      "epoch [81/200] batch [10/18] time 0.342 (0.640) data 0.002 (0.259) loss 0.4335 (0.4330) acc 81.2500 (87.1875) lr 1.3239e-02 eta 0:22:56\n",
      "epoch [81/200] batch [15/18] time 0.335 (0.538) data 0.000 (0.172) loss 0.3475 (0.4360) acc 90.6250 (86.8750) lr 1.3239e-02 eta 0:19:14\n",
      "epoch [82/200] batch [5/18] time 0.331 (0.753) data 0.000 (0.412) loss 0.2852 (0.3537) acc 93.7500 (91.2500) lr 1.3090e-02 eta 0:26:49\n",
      "epoch [82/200] batch [10/18] time 0.331 (0.552) data 0.000 (0.206) loss 0.3903 (0.4007) acc 90.6250 (89.0625) lr 1.3090e-02 eta 0:19:36\n",
      "epoch [82/200] batch [15/18] time 0.331 (0.478) data 0.000 (0.138) loss 0.6579 (0.4143) acc 78.1250 (88.1250) lr 1.3090e-02 eta 0:16:55\n",
      "epoch [83/200] batch [5/18] time 0.446 (0.849) data 0.001 (0.398) loss 0.4059 (0.2982) acc 84.3750 (90.6250) lr 1.2940e-02 eta 0:29:58\n",
      "epoch [83/200] batch [10/18] time 0.731 (0.688) data 0.382 (0.255) loss 0.5855 (0.3877) acc 87.5000 (89.0625) lr 1.2940e-02 eta 0:24:15\n",
      "epoch [83/200] batch [15/18] time 0.335 (0.571) data 0.000 (0.170) loss 0.7334 (0.3828) acc 75.0000 (88.9583) lr 1.2940e-02 eta 0:20:04\n",
      "epoch [84/200] batch [5/18] time 0.331 (0.734) data 0.000 (0.384) loss 0.3095 (0.4294) acc 90.6250 (86.8750) lr 1.2790e-02 eta 0:25:42\n",
      "epoch [84/200] batch [10/18] time 0.330 (0.533) data 0.000 (0.193) loss 0.4006 (0.3699) acc 87.5000 (88.7500) lr 1.2790e-02 eta 0:18:37\n",
      "epoch [84/200] batch [15/18] time 0.329 (0.465) data 0.000 (0.128) loss 0.5721 (0.4177) acc 84.3750 (87.9167) lr 1.2790e-02 eta 0:16:12\n",
      "epoch [85/200] batch [5/18] time 0.332 (0.796) data 0.000 (0.409) loss 0.2579 (0.3363) acc 93.7500 (91.8750) lr 1.2639e-02 eta 0:27:38\n",
      "epoch [85/200] batch [10/18] time 0.336 (0.651) data 0.000 (0.264) loss 0.4192 (0.3797) acc 84.3750 (90.0000) lr 1.2639e-02 eta 0:22:32\n",
      "epoch [85/200] batch [15/18] time 0.324 (0.544) data 0.000 (0.176) loss 0.1928 (0.3585) acc 93.7500 (90.4167) lr 1.2639e-02 eta 0:18:47\n",
      "epoch [86/200] batch [5/18] time 0.334 (0.778) data 0.000 (0.435) loss 0.3489 (0.3991) acc 87.5000 (88.1250) lr 1.2487e-02 eta 0:26:47\n",
      "epoch [86/200] batch [10/18] time 0.332 (0.564) data 0.000 (0.218) loss 0.4493 (0.3710) acc 87.5000 (89.3750) lr 1.2487e-02 eta 0:19:21\n",
      "epoch [86/200] batch [15/18] time 0.331 (0.485) data 0.000 (0.145) loss 0.1952 (0.3574) acc 93.7500 (89.1667) lr 1.2487e-02 eta 0:16:37\n",
      "epoch [87/200] batch [5/18] time 0.562 (0.839) data 0.009 (0.396) loss 0.3198 (0.4234) acc 93.7500 (87.5000) lr 1.2334e-02 eta 0:28:36\n",
      "epoch [87/200] batch [10/18] time 0.350 (0.666) data 0.000 (0.252) loss 0.3859 (0.4153) acc 87.5000 (87.1875) lr 1.2334e-02 eta 0:22:40\n",
      "epoch [87/200] batch [15/18] time 0.338 (0.555) data 0.000 (0.168) loss 0.3358 (0.3888) acc 93.7500 (88.7500) lr 1.2334e-02 eta 0:18:50\n",
      "epoch [88/200] batch [5/18] time 0.351 (0.692) data 0.001 (0.347) loss 0.4100 (0.3116) acc 81.2500 (89.3750) lr 1.2181e-02 eta 0:23:23\n",
      "epoch [88/200] batch [10/18] time 0.335 (0.516) data 0.001 (0.175) loss 0.3250 (0.3373) acc 87.5000 (89.6875) lr 1.2181e-02 eta 0:17:25\n",
      "epoch [88/200] batch [15/18] time 0.328 (0.454) data 0.000 (0.117) loss 0.3409 (0.3474) acc 87.5000 (89.3750) lr 1.2181e-02 eta 0:15:17\n",
      "epoch [89/200] batch [5/18] time 0.368 (0.789) data 0.001 (0.405) loss 0.1682 (0.2509) acc 93.7500 (91.2500) lr 1.2028e-02 eta 0:26:27\n",
      "epoch [89/200] batch [10/18] time 0.626 (0.641) data 0.265 (0.239) loss 0.2236 (0.2715) acc 93.7500 (91.2500) lr 1.2028e-02 eta 0:21:26\n",
      "epoch [89/200] batch [15/18] time 0.329 (0.540) data 0.000 (0.160) loss 0.2454 (0.3325) acc 93.7500 (90.0000) lr 1.2028e-02 eta 0:18:01\n",
      "epoch [90/200] batch [5/18] time 0.347 (0.774) data 0.000 (0.432) loss 0.5101 (0.4564) acc 87.5000 (86.8750) lr 1.1874e-02 eta 0:25:41\n",
      "epoch [90/200] batch [10/18] time 0.322 (0.557) data 0.000 (0.216) loss 0.2126 (0.3601) acc 96.8750 (91.2500) lr 1.1874e-02 eta 0:18:27\n",
      "epoch [90/200] batch [15/18] time 0.333 (0.482) data 0.000 (0.144) loss 0.4500 (0.4013) acc 81.2500 (88.3333) lr 1.1874e-02 eta 0:15:56\n",
      "epoch [91/200] batch [5/18] time 0.422 (0.828) data 0.001 (0.434) loss 0.4933 (0.4616) acc 84.3750 (86.2500) lr 1.1719e-02 eta 0:27:15\n",
      "epoch [91/200] batch [10/18] time 0.621 (0.658) data 0.271 (0.245) loss 0.2705 (0.4242) acc 87.5000 (86.8750) lr 1.1719e-02 eta 0:21:36\n",
      "epoch [91/200] batch [15/18] time 0.325 (0.549) data 0.000 (0.164) loss 0.4204 (0.3837) acc 87.5000 (88.5417) lr 1.1719e-02 eta 0:17:58\n",
      "epoch [92/200] batch [5/18] time 0.329 (0.689) data 0.000 (0.352) loss 0.3228 (0.3171) acc 87.5000 (91.2500) lr 1.1564e-02 eta 0:22:27\n",
      "epoch [92/200] batch [10/18] time 0.332 (0.513) data 0.000 (0.176) loss 0.3694 (0.3337) acc 90.6250 (90.0000) lr 1.1564e-02 eta 0:16:41\n",
      "epoch [92/200] batch [15/18] time 0.325 (0.452) data 0.000 (0.118) loss 0.4813 (0.3345) acc 87.5000 (90.8333) lr 1.1564e-02 eta 0:14:39\n",
      "epoch [93/200] batch [5/18] time 0.342 (0.783) data 0.006 (0.386) loss 0.2667 (0.4039) acc 96.8750 (90.0000) lr 1.1409e-02 eta 0:25:19\n",
      "epoch [93/200] batch [10/18] time 0.348 (0.633) data 0.000 (0.254) loss 0.3612 (0.4074) acc 90.6250 (89.6875) lr 1.1409e-02 eta 0:20:23\n",
      "epoch [93/200] batch [15/18] time 0.331 (0.532) data 0.000 (0.169) loss 0.4337 (0.4075) acc 87.5000 (89.1667) lr 1.1409e-02 eta 0:17:05\n",
      "epoch [94/200] batch [5/18] time 0.355 (0.700) data 0.000 (0.347) loss 0.6289 (0.3823) acc 78.1250 (86.8750) lr 1.1253e-02 eta 0:22:24\n",
      "epoch [94/200] batch [10/18] time 0.321 (0.518) data 0.001 (0.174) loss 0.2876 (0.3950) acc 93.7500 (86.8750) lr 1.1253e-02 eta 0:16:31\n",
      "epoch [94/200] batch [15/18] time 0.331 (0.455) data 0.000 (0.116) loss 0.4230 (0.3921) acc 81.2500 (87.2917) lr 1.1253e-02 eta 0:14:29\n",
      "epoch [95/200] batch [5/18] time 0.408 (0.782) data 0.001 (0.406) loss 0.5541 (0.4552) acc 84.3750 (86.2500) lr 1.1097e-02 eta 0:24:48\n",
      "epoch [95/200] batch [10/18] time 0.325 (0.604) data 0.000 (0.225) loss 0.4914 (0.4651) acc 90.6250 (85.6250) lr 1.1097e-02 eta 0:19:07\n",
      "epoch [95/200] batch [15/18] time 0.340 (0.514) data 0.000 (0.150) loss 0.7843 (0.4596) acc 90.6250 (87.0833) lr 1.1097e-02 eta 0:16:12\n",
      "epoch [96/200] batch [5/18] time 0.336 (0.749) data 0.000 (0.413) loss 0.6312 (0.4405) acc 78.1250 (87.5000) lr 1.0941e-02 eta 0:23:31\n",
      "epoch [96/200] batch [10/18] time 0.331 (0.542) data 0.000 (0.208) loss 0.3179 (0.4199) acc 90.6250 (87.1875) lr 1.0941e-02 eta 0:16:58\n",
      "epoch [96/200] batch [15/18] time 0.328 (0.471) data 0.000 (0.139) loss 0.2749 (0.3817) acc 93.7500 (88.9583) lr 1.0941e-02 eta 0:14:42\n",
      "epoch [97/200] batch [5/18] time 0.404 (0.899) data 0.001 (0.511) loss 0.3340 (0.3630) acc 90.6250 (88.1250) lr 1.0785e-02 eta 0:27:58\n",
      "epoch [97/200] batch [10/18] time 0.351 (0.685) data 0.001 (0.309) loss 0.4783 (0.4187) acc 87.5000 (87.5000) lr 1.0785e-02 eta 0:21:14\n",
      "epoch [97/200] batch [15/18] time 0.329 (0.567) data 0.000 (0.206) loss 0.2172 (0.3947) acc 93.7500 (88.3333) lr 1.0785e-02 eta 0:17:32\n",
      "epoch [98/200] batch [5/18] time 0.323 (0.768) data 0.000 (0.424) loss 0.3698 (0.2778) acc 90.6250 (94.3750) lr 1.0628e-02 eta 0:23:39\n",
      "epoch [98/200] batch [10/18] time 0.344 (0.551) data 0.000 (0.212) loss 0.1824 (0.3199) acc 96.8750 (93.1250) lr 1.0628e-02 eta 0:16:55\n",
      "epoch [98/200] batch [15/18] time 0.325 (0.477) data 0.000 (0.142) loss 0.3918 (0.3622) acc 90.6250 (90.8333) lr 1.0628e-02 eta 0:14:37\n",
      "epoch [99/200] batch [5/18] time 0.340 (0.747) data 0.004 (0.343) loss 0.3354 (0.3594) acc 90.6250 (88.7500) lr 1.0471e-02 eta 0:22:46\n",
      "epoch [99/200] batch [10/18] time 0.379 (0.631) data 0.001 (0.234) loss 0.1561 (0.3727) acc 100.0000 (88.1250) lr 1.0471e-02 eta 0:19:12\n",
      "epoch [99/200] batch [15/18] time 0.333 (0.532) data 0.000 (0.156) loss 0.4962 (0.4080) acc 84.3750 (87.7083) lr 1.0471e-02 eta 0:16:08\n",
      "epoch [100/200] batch [5/18] time 0.335 (0.707) data 0.000 (0.346) loss 0.3625 (0.3706) acc 93.7500 (90.6250) lr 1.0314e-02 eta 0:21:21\n",
      "epoch [100/200] batch [10/18] time 0.340 (0.526) data 0.000 (0.173) loss 0.3084 (0.3600) acc 87.5000 (89.3750) lr 1.0314e-02 eta 0:15:51\n",
      "epoch [100/200] batch [15/18] time 0.323 (0.461) data 0.000 (0.116) loss 0.6679 (0.4024) acc 75.0000 (88.3333) lr 1.0314e-02 eta 0:13:50\n",
      "epoch [101/200] batch [5/18] time 0.369 (0.832) data 0.001 (0.437) loss 0.5773 (0.3977) acc 90.6250 (90.6250) lr 1.0157e-02 eta 0:24:53\n",
      "epoch [101/200] batch [10/18] time 0.347 (0.630) data 0.000 (0.234) loss 0.5305 (0.3986) acc 84.3750 (90.3125) lr 1.0157e-02 eta 0:18:47\n",
      "epoch [101/200] batch [15/18] time 0.333 (0.531) data 0.000 (0.156) loss 0.5910 (0.4311) acc 84.3750 (89.7917) lr 1.0157e-02 eta 0:15:48\n",
      "epoch [102/200] batch [5/18] time 0.346 (0.776) data 0.000 (0.425) loss 0.3652 (0.4044) acc 87.5000 (90.0000) lr 1.0000e-02 eta 0:22:59\n",
      "epoch [102/200] batch [10/18] time 0.327 (0.554) data 0.000 (0.213) loss 0.4714 (0.3647) acc 81.2500 (90.9375) lr 1.0000e-02 eta 0:16:21\n",
      "epoch [102/200] batch [15/18] time 0.331 (0.480) data 0.000 (0.142) loss 0.2971 (0.3579) acc 90.6250 (91.4583) lr 1.0000e-02 eta 0:14:07\n",
      "epoch [103/200] batch [5/18] time 0.439 (0.903) data 0.001 (0.512) loss 0.3476 (0.3471) acc 93.7500 (90.6250) lr 9.8429e-03 eta 0:26:28\n",
      "epoch [103/200] batch [10/18] time 0.374 (0.659) data 0.000 (0.274) loss 0.2783 (0.3429) acc 93.7500 (90.3125) lr 9.8429e-03 eta 0:19:15\n",
      "epoch [103/200] batch [15/18] time 0.327 (0.549) data 0.000 (0.183) loss 0.1972 (0.3355) acc 93.7500 (90.8333) lr 9.8429e-03 eta 0:16:00\n",
      "epoch [104/200] batch [5/18] time 0.330 (0.705) data 0.000 (0.361) loss 0.3133 (0.3821) acc 93.7500 (90.0000) lr 9.6859e-03 eta 0:20:27\n",
      "epoch [104/200] batch [10/18] time 0.322 (0.528) data 0.001 (0.189) loss 0.3728 (0.3361) acc 90.6250 (90.6250) lr 9.6859e-03 eta 0:15:16\n",
      "epoch [104/200] batch [15/18] time 0.337 (0.462) data 0.000 (0.126) loss 0.4215 (0.3581) acc 87.5000 (90.4167) lr 9.6859e-03 eta 0:13:19\n",
      "epoch [105/200] batch [5/18] time 0.368 (0.780) data 0.001 (0.373) loss 0.8309 (0.4083) acc 75.0000 (88.1250) lr 9.5289e-03 eta 0:22:23\n",
      "epoch [105/200] batch [10/18] time 0.456 (0.613) data 0.000 (0.198) loss 0.2492 (0.3642) acc 93.7500 (90.0000) lr 9.5289e-03 eta 0:17:33\n",
      "epoch [105/200] batch [15/18] time 0.332 (0.522) data 0.000 (0.132) loss 0.2553 (0.3820) acc 90.6250 (89.5833) lr 9.5289e-03 eta 0:14:54\n",
      "epoch [106/200] batch [5/18] time 0.368 (0.765) data 0.001 (0.413) loss 0.4775 (0.3518) acc 81.2500 (90.0000) lr 9.3721e-03 eta 0:21:44\n",
      "epoch [106/200] batch [10/18] time 0.326 (0.548) data 0.000 (0.207) loss 0.2921 (0.3992) acc 90.6250 (86.8750) lr 9.3721e-03 eta 0:15:30\n",
      "epoch [106/200] batch [15/18] time 0.324 (0.475) data 0.000 (0.138) loss 0.2158 (0.3729) acc 96.8750 (88.9583) lr 9.3721e-03 eta 0:13:25\n",
      "epoch [107/200] batch [5/18] time 0.392 (0.834) data 0.000 (0.436) loss 0.3395 (0.3472) acc 93.7500 (90.6250) lr 9.2154e-03 eta 0:23:26\n",
      "epoch [107/200] batch [10/18] time 0.333 (0.636) data 0.000 (0.265) loss 0.4394 (0.3744) acc 87.5000 (90.0000) lr 9.2154e-03 eta 0:17:50\n",
      "epoch [107/200] batch [15/18] time 0.328 (0.535) data 0.000 (0.177) loss 0.3826 (0.3648) acc 87.5000 (90.0000) lr 9.2154e-03 eta 0:14:57\n",
      "epoch [108/200] batch [5/18] time 0.347 (0.739) data 0.029 (0.389) loss 0.3431 (0.3738) acc 90.6250 (91.2500) lr 9.0589e-03 eta 0:20:33\n",
      "epoch [108/200] batch [10/18] time 0.335 (0.539) data 0.000 (0.195) loss 0.3995 (0.3663) acc 87.5000 (91.5625) lr 9.0589e-03 eta 0:14:57\n",
      "epoch [108/200] batch [15/18] time 0.331 (0.470) data 0.000 (0.130) loss 0.3577 (0.4260) acc 90.6250 (89.5833) lr 9.0589e-03 eta 0:13:00\n",
      "epoch [109/200] batch [5/18] time 0.367 (0.789) data 0.000 (0.407) loss 0.2808 (0.3544) acc 90.6250 (91.8750) lr 8.9027e-03 eta 0:21:42\n",
      "epoch [109/200] batch [10/18] time 0.391 (0.634) data 0.001 (0.231) loss 0.4963 (0.3248) acc 90.6250 (92.1875) lr 8.9027e-03 eta 0:17:23\n",
      "epoch [109/200] batch [15/18] time 0.326 (0.535) data 0.000 (0.154) loss 0.3165 (0.3190) acc 90.6250 (92.2917) lr 8.9027e-03 eta 0:14:37\n",
      "epoch [110/200] batch [5/18] time 0.336 (0.703) data 0.000 (0.358) loss 0.5413 (0.4127) acc 87.5000 (90.0000) lr 8.7467e-03 eta 0:19:07\n",
      "epoch [110/200] batch [10/18] time 0.320 (0.518) data 0.001 (0.180) loss 0.3392 (0.3924) acc 90.6250 (88.7500) lr 8.7467e-03 eta 0:14:02\n",
      "epoch [110/200] batch [15/18] time 0.329 (0.455) data 0.000 (0.120) loss 0.6459 (0.4084) acc 84.3750 (88.9583) lr 8.7467e-03 eta 0:12:18\n",
      "epoch [111/200] batch [5/18] time 0.382 (0.894) data 0.001 (0.453) loss 0.3962 (0.3509) acc 93.7500 (91.8750) lr 8.5910e-03 eta 0:24:04\n",
      "epoch [111/200] batch [10/18] time 0.337 (0.687) data 0.000 (0.275) loss 0.1985 (0.3524) acc 93.7500 (90.6250) lr 8.5910e-03 eta 0:18:25\n",
      "epoch [111/200] batch [15/18] time 0.330 (0.569) data 0.000 (0.184) loss 0.3810 (0.3733) acc 90.6250 (89.3750) lr 8.5910e-03 eta 0:15:13\n",
      "epoch [112/200] batch [5/18] time 0.340 (0.693) data 0.000 (0.341) loss 0.5013 (0.3865) acc 87.5000 (89.3750) lr 8.4357e-03 eta 0:18:26\n",
      "epoch [112/200] batch [10/18] time 0.326 (0.516) data 0.001 (0.172) loss 0.2410 (0.3790) acc 93.7500 (89.3750) lr 8.4357e-03 eta 0:13:41\n",
      "epoch [112/200] batch [15/18] time 0.328 (0.454) data 0.000 (0.115) loss 0.2234 (0.3595) acc 93.7500 (90.0000) lr 8.4357e-03 eta 0:12:00\n",
      "epoch [113/200] batch [5/18] time 0.352 (0.757) data 0.000 (0.397) loss 0.4155 (0.3420) acc 90.6250 (89.3750) lr 8.2807e-03 eta 0:19:55\n",
      "epoch [113/200] batch [10/18] time 0.329 (0.622) data 0.000 (0.264) loss 0.1822 (0.3241) acc 96.8750 (90.6250) lr 8.2807e-03 eta 0:16:19\n",
      "epoch [113/200] batch [15/18] time 0.337 (0.527) data 0.000 (0.176) loss 0.3585 (0.3540) acc 84.3750 (89.3750) lr 8.2807e-03 eta 0:13:46\n",
      "epoch [114/200] batch [5/18] time 0.351 (0.680) data 0.000 (0.346) loss 0.4469 (0.4019) acc 90.6250 (90.6250) lr 8.1262e-03 eta 0:17:41\n",
      "epoch [114/200] batch [10/18] time 0.330 (0.512) data 0.001 (0.173) loss 0.3656 (0.4019) acc 93.7500 (91.2500) lr 8.1262e-03 eta 0:13:17\n",
      "epoch [114/200] batch [15/18] time 0.329 (0.453) data 0.000 (0.116) loss 0.5660 (0.4119) acc 87.5000 (90.6250) lr 8.1262e-03 eta 0:11:41\n",
      "epoch [115/200] batch [5/18] time 0.421 (0.731) data 0.001 (0.347) loss 0.5567 (0.4252) acc 87.5000 (88.7500) lr 7.9721e-03 eta 0:18:47\n",
      "epoch [115/200] batch [10/18] time 0.541 (0.604) data 0.000 (0.190) loss 0.2451 (0.4324) acc 93.7500 (88.4375) lr 7.9721e-03 eta 0:15:29\n",
      "epoch [115/200] batch [15/18] time 0.331 (0.515) data 0.000 (0.127) loss 0.4992 (0.4094) acc 84.3750 (88.9583) lr 7.9721e-03 eta 0:13:09\n",
      "epoch [116/200] batch [5/18] time 0.334 (0.749) data 0.000 (0.409) loss 0.2553 (0.3646) acc 93.7500 (88.7500) lr 7.8186e-03 eta 0:19:01\n",
      "epoch [116/200] batch [10/18] time 0.328 (0.544) data 0.000 (0.205) loss 0.3262 (0.3626) acc 84.3750 (88.7500) lr 7.8186e-03 eta 0:13:46\n",
      "epoch [116/200] batch [15/18] time 0.330 (0.473) data 0.000 (0.137) loss 0.4715 (0.3882) acc 87.5000 (88.1250) lr 7.8186e-03 eta 0:11:56\n",
      "epoch [117/200] batch [5/18] time 0.347 (0.733) data 0.005 (0.324) loss 0.3609 (0.3300) acc 81.2500 (88.7500) lr 7.6655e-03 eta 0:18:24\n",
      "epoch [117/200] batch [10/18] time 0.467 (0.611) data 0.003 (0.201) loss 0.5193 (0.3544) acc 90.6250 (88.7500) lr 7.6655e-03 eta 0:15:17\n",
      "epoch [117/200] batch [15/18] time 0.326 (0.522) data 0.000 (0.136) loss 0.3824 (0.3571) acc 90.6250 (88.7500) lr 7.6655e-03 eta 0:13:01\n",
      "epoch [118/200] batch [5/18] time 0.339 (0.761) data 0.000 (0.417) loss 0.3633 (0.3401) acc 93.7500 (91.2500) lr 7.5131e-03 eta 0:18:53\n",
      "epoch [118/200] batch [10/18] time 0.329 (0.548) data 0.000 (0.209) loss 0.2369 (0.3611) acc 93.7500 (90.9375) lr 7.5131e-03 eta 0:13:33\n",
      "epoch [118/200] batch [15/18] time 0.330 (0.476) data 0.000 (0.140) loss 0.3922 (0.3553) acc 90.6250 (90.8333) lr 7.5131e-03 eta 0:11:43\n",
      "epoch [119/200] batch [5/18] time 0.362 (0.773) data 0.006 (0.367) loss 0.5046 (0.2554) acc 75.0000 (90.6250) lr 7.3613e-03 eta 0:18:57\n",
      "epoch [119/200] batch [10/18] time 0.325 (0.611) data 0.001 (0.220) loss 0.5154 (0.3092) acc 87.5000 (89.6875) lr 7.3613e-03 eta 0:14:55\n",
      "epoch [119/200] batch [15/18] time 0.335 (0.519) data 0.000 (0.147) loss 0.6218 (0.3426) acc 90.6250 (90.2083) lr 7.3613e-03 eta 0:12:38\n",
      "epoch [120/200] batch [5/18] time 0.339 (0.730) data 0.001 (0.390) loss 0.3886 (0.3615) acc 87.5000 (88.1250) lr 7.2101e-03 eta 0:17:41\n",
      "epoch [120/200] batch [10/18] time 0.323 (0.532) data 0.000 (0.195) loss 0.4106 (0.3685) acc 84.3750 (87.8125) lr 7.2101e-03 eta 0:12:50\n",
      "epoch [120/200] batch [15/18] time 0.332 (0.466) data 0.000 (0.130) loss 0.2154 (0.3643) acc 93.7500 (88.3333) lr 7.2101e-03 eta 0:11:11\n",
      "epoch [121/200] batch [5/18] time 0.384 (0.770) data 0.000 (0.399) loss 0.2833 (0.2768) acc 87.5000 (90.6250) lr 7.0596e-03 eta 0:18:24\n",
      "epoch [121/200] batch [10/18] time 0.335 (0.630) data 0.000 (0.255) loss 0.4964 (0.2966) acc 90.6250 (92.1875) lr 7.0596e-03 eta 0:15:01\n",
      "epoch [121/200] batch [15/18] time 0.329 (0.531) data 0.001 (0.170) loss 0.2226 (0.3121) acc 96.8750 (92.0833) lr 7.0596e-03 eta 0:12:36\n",
      "epoch [122/200] batch [5/18] time 0.338 (0.751) data 0.000 (0.406) loss 0.2832 (0.3107) acc 90.6250 (91.8750) lr 6.9098e-03 eta 0:17:44\n",
      "epoch [122/200] batch [10/18] time 0.330 (0.548) data 0.000 (0.203) loss 0.6062 (0.3822) acc 87.5000 (90.0000) lr 6.9098e-03 eta 0:12:53\n",
      "epoch [122/200] batch [15/18] time 0.333 (0.475) data 0.000 (0.136) loss 0.2878 (0.4224) acc 87.5000 (88.5417) lr 6.9098e-03 eta 0:11:08\n",
      "epoch [123/200] batch [5/18] time 0.406 (0.879) data 0.000 (0.490) loss 0.4626 (0.3809) acc 84.3750 (86.8750) lr 6.7608e-03 eta 0:20:29\n",
      "epoch [123/200] batch [10/18] time 0.338 (0.657) data 0.001 (0.284) loss 0.5509 (0.3371) acc 87.5000 (89.0625) lr 6.7608e-03 eta 0:15:16\n",
      "epoch [123/200] batch [15/18] time 0.332 (0.548) data 0.000 (0.190) loss 0.2394 (0.3355) acc 93.7500 (89.5833) lr 6.7608e-03 eta 0:12:41\n",
      "epoch [124/200] batch [5/18] time 0.343 (0.691) data 0.000 (0.341) loss 0.5435 (0.3847) acc 81.2500 (88.1250) lr 6.6126e-03 eta 0:15:54\n",
      "epoch [124/200] batch [10/18] time 0.344 (0.515) data 0.001 (0.171) loss 0.4898 (0.4362) acc 81.2500 (86.2500) lr 6.6126e-03 eta 0:11:49\n",
      "epoch [124/200] batch [15/18] time 0.332 (0.454) data 0.000 (0.114) loss 0.2856 (0.3982) acc 93.7500 (87.7083) lr 6.6126e-03 eta 0:10:22\n",
      "epoch [125/200] batch [5/18] time 0.349 (0.799) data 0.000 (0.415) loss 0.1876 (0.4043) acc 93.7500 (85.0000) lr 6.4653e-03 eta 0:18:08\n",
      "epoch [125/200] batch [10/18] time 0.413 (0.634) data 0.092 (0.262) loss 0.3531 (0.4330) acc 90.6250 (85.9375) lr 6.4653e-03 eta 0:14:20\n",
      "epoch [125/200] batch [15/18] time 0.325 (0.532) data 0.000 (0.175) loss 0.3763 (0.3803) acc 90.6250 (88.3333) lr 6.4653e-03 eta 0:12:00\n",
      "epoch [126/200] batch [5/18] time 0.333 (0.765) data 0.001 (0.425) loss 0.1740 (0.3906) acc 96.8750 (90.6250) lr 6.3188e-03 eta 0:17:09\n",
      "epoch [126/200] batch [10/18] time 0.336 (0.549) data 0.000 (0.213) loss 0.5451 (0.3800) acc 78.1250 (90.0000) lr 6.3188e-03 eta 0:12:15\n",
      "epoch [126/200] batch [15/18] time 0.330 (0.476) data 0.000 (0.142) loss 0.4312 (0.4058) acc 90.6250 (88.9583) lr 6.3188e-03 eta 0:10:35\n",
      "epoch [127/200] batch [5/18] time 0.336 (0.788) data 0.001 (0.388) loss 0.3828 (0.3491) acc 90.6250 (90.0000) lr 6.1732e-03 eta 0:17:25\n",
      "epoch [127/200] batch [10/18] time 0.344 (0.626) data 0.002 (0.225) loss 0.4378 (0.3321) acc 87.5000 (90.6250) lr 6.1732e-03 eta 0:13:47\n",
      "epoch [127/200] batch [15/18] time 0.338 (0.531) data 0.000 (0.150) loss 0.1030 (0.3918) acc 96.8750 (88.9583) lr 6.1732e-03 eta 0:11:39\n",
      "epoch [128/200] batch [5/18] time 0.342 (0.747) data 0.000 (0.398) loss 0.3638 (0.3646) acc 87.5000 (88.1250) lr 6.0285e-03 eta 0:16:17\n",
      "epoch [128/200] batch [10/18] time 0.330 (0.549) data 0.000 (0.200) loss 0.2810 (0.3751) acc 90.6250 (89.3750) lr 6.0285e-03 eta 0:11:55\n",
      "epoch [128/200] batch [15/18] time 0.328 (0.476) data 0.000 (0.133) loss 0.2972 (0.3779) acc 90.6250 (89.7917) lr 6.0285e-03 eta 0:10:18\n",
      "epoch [129/200] batch [5/18] time 0.407 (0.808) data 0.000 (0.418) loss 0.1558 (0.3322) acc 93.7500 (90.6250) lr 5.8849e-03 eta 0:17:23\n",
      "epoch [129/200] batch [10/18] time 0.346 (0.647) data 0.002 (0.267) loss 0.2528 (0.3338) acc 90.6250 (90.3125) lr 5.8849e-03 eta 0:13:52\n",
      "epoch [129/200] batch [15/18] time 0.336 (0.542) data 0.000 (0.178) loss 0.5032 (0.3551) acc 81.2500 (88.7500) lr 5.8849e-03 eta 0:11:34\n",
      "epoch [130/200] batch [5/18] time 0.333 (0.669) data 0.000 (0.323) loss 0.3146 (0.2810) acc 93.7500 (95.0000) lr 5.7422e-03 eta 0:14:11\n",
      "epoch [130/200] batch [10/18] time 0.326 (0.505) data 0.000 (0.162) loss 0.4207 (0.3191) acc 90.6250 (93.1250) lr 5.7422e-03 eta 0:10:40\n",
      "epoch [130/200] batch [15/18] time 0.335 (0.447) data 0.000 (0.108) loss 0.3692 (0.3148) acc 90.6250 (91.2500) lr 5.7422e-03 eta 0:09:25\n",
      "epoch [131/200] batch [5/18] time 0.344 (0.795) data 0.000 (0.413) loss 0.2524 (0.2503) acc 90.6250 (91.2500) lr 5.6006e-03 eta 0:16:37\n",
      "epoch [131/200] batch [10/18] time 0.343 (0.622) data 0.000 (0.246) loss 0.2009 (0.2989) acc 93.7500 (90.3125) lr 5.6006e-03 eta 0:12:57\n",
      "epoch [131/200] batch [15/18] time 0.332 (0.525) data 0.000 (0.164) loss 0.3177 (0.3128) acc 87.5000 (90.4167) lr 5.6006e-03 eta 0:10:53\n",
      "epoch [132/200] batch [5/18] time 0.329 (0.709) data 0.001 (0.358) loss 0.4558 (0.3657) acc 87.5000 (89.3750) lr 5.4601e-03 eta 0:14:36\n",
      "epoch [132/200] batch [10/18] time 0.351 (0.532) data 0.001 (0.179) loss 0.1947 (0.3695) acc 93.7500 (90.0000) lr 5.4601e-03 eta 0:10:55\n",
      "epoch [132/200] batch [15/18] time 0.322 (0.464) data 0.000 (0.120) loss 0.3008 (0.3554) acc 93.7500 (90.0000) lr 5.4601e-03 eta 0:09:28\n",
      "epoch [133/200] batch [5/18] time 0.334 (0.767) data 0.000 (0.380) loss 0.2324 (0.5112) acc 90.6250 (85.6250) lr 5.3207e-03 eta 0:15:35\n",
      "epoch [133/200] batch [10/18] time 0.345 (0.635) data 0.000 (0.239) loss 0.2534 (0.3777) acc 93.7500 (89.3750) lr 5.3207e-03 eta 0:12:50\n",
      "epoch [133/200] batch [15/18] time 0.330 (0.535) data 0.000 (0.159) loss 0.3004 (0.3548) acc 90.6250 (90.4167) lr 5.3207e-03 eta 0:10:46\n",
      "epoch [134/200] batch [5/18] time 0.340 (0.728) data 0.000 (0.377) loss 0.2032 (0.2360) acc 93.7500 (93.7500) lr 5.1825e-03 eta 0:14:34\n",
      "epoch [134/200] batch [10/18] time 0.328 (0.532) data 0.002 (0.189) loss 0.1563 (0.2808) acc 96.8750 (93.1250) lr 5.1825e-03 eta 0:10:36\n",
      "epoch [134/200] batch [15/18] time 0.332 (0.465) data 0.000 (0.126) loss 0.5240 (0.3038) acc 81.2500 (91.6667) lr 5.1825e-03 eta 0:09:13\n",
      "epoch [135/200] batch [5/18] time 0.369 (0.832) data 0.003 (0.435) loss 0.4861 (0.3119) acc 90.6250 (95.0000) lr 5.0454e-03 eta 0:16:24\n",
      "epoch [135/200] batch [10/18] time 0.330 (0.630) data 0.000 (0.231) loss 0.3852 (0.3420) acc 90.6250 (91.5625) lr 5.0454e-03 eta 0:12:21\n",
      "epoch [135/200] batch [15/18] time 0.346 (0.532) data 0.000 (0.154) loss 0.3095 (0.3654) acc 90.6250 (91.2500) lr 5.0454e-03 eta 0:10:24\n",
      "epoch [136/200] batch [5/18] time 0.344 (0.777) data 0.000 (0.435) loss 0.2397 (0.2001) acc 93.7500 (95.6250) lr 4.9096e-03 eta 0:15:05\n",
      "epoch [136/200] batch [10/18] time 0.331 (0.557) data 0.000 (0.218) loss 0.3440 (0.2580) acc 87.5000 (93.4375) lr 4.9096e-03 eta 0:10:45\n",
      "epoch [136/200] batch [15/18] time 0.330 (0.481) data 0.000 (0.146) loss 0.3200 (0.3191) acc 93.7500 (91.6667) lr 4.9096e-03 eta 0:09:15\n",
      "epoch [137/200] batch [5/18] time 0.446 (0.838) data 0.000 (0.434) loss 0.4610 (0.3178) acc 84.3750 (91.2500) lr 4.7750e-03 eta 0:16:00\n",
      "epoch [137/200] batch [10/18] time 0.410 (0.642) data 0.007 (0.224) loss 0.3032 (0.3169) acc 90.6250 (90.9375) lr 4.7750e-03 eta 0:12:13\n",
      "epoch [137/200] batch [15/18] time 0.329 (0.538) data 0.000 (0.150) loss 0.2056 (0.3525) acc 96.8750 (90.6250) lr 4.7750e-03 eta 0:10:11\n",
      "epoch [138/200] batch [5/18] time 0.334 (0.734) data 0.000 (0.368) loss 0.4661 (0.4419) acc 87.5000 (86.2500) lr 4.6417e-03 eta 0:13:49\n",
      "epoch [138/200] batch [10/18] time 0.326 (0.534) data 0.000 (0.184) loss 0.3450 (0.3709) acc 90.6250 (89.0625) lr 4.6417e-03 eta 0:09:59\n",
      "epoch [138/200] batch [15/18] time 0.330 (0.466) data 0.000 (0.123) loss 0.1546 (0.3451) acc 96.8750 (90.2083) lr 4.6417e-03 eta 0:08:41\n",
      "epoch [139/200] batch [5/18] time 0.471 (0.853) data 0.002 (0.433) loss 0.1049 (0.2356) acc 100.0000 (93.7500) lr 4.5098e-03 eta 0:15:47\n",
      "epoch [139/200] batch [10/18] time 0.361 (0.653) data 0.010 (0.251) loss 0.2807 (0.2984) acc 93.7500 (92.1875) lr 4.5098e-03 eta 0:12:02\n",
      "epoch [139/200] batch [15/18] time 0.330 (0.547) data 0.000 (0.168) loss 0.4401 (0.3060) acc 87.5000 (91.6667) lr 4.5098e-03 eta 0:10:02\n",
      "epoch [140/200] batch [5/18] time 0.330 (0.693) data 0.000 (0.351) loss 0.1728 (0.3299) acc 96.8750 (90.6250) lr 4.3792e-03 eta 0:12:37\n",
      "epoch [140/200] batch [10/18] time 0.339 (0.513) data 0.001 (0.176) loss 0.2207 (0.2969) acc 93.7500 (90.9375) lr 4.3792e-03 eta 0:09:18\n",
      "epoch [140/200] batch [15/18] time 0.328 (0.452) data 0.000 (0.117) loss 0.2858 (0.2735) acc 96.8750 (92.0833) lr 4.3792e-03 eta 0:08:09\n",
      "epoch [141/200] batch [5/18] time 0.367 (0.791) data 0.000 (0.413) loss 0.4613 (0.2994) acc 87.5000 (92.5000) lr 4.2499e-03 eta 0:14:10\n",
      "epoch [141/200] batch [10/18] time 0.336 (0.614) data 0.000 (0.238) loss 0.3157 (0.3747) acc 93.7500 (89.6875) lr 4.2499e-03 eta 0:10:56\n",
      "epoch [141/200] batch [15/18] time 0.330 (0.521) data 0.000 (0.159) loss 0.1945 (0.3300) acc 93.7500 (91.4583) lr 4.2499e-03 eta 0:09:14\n",
      "epoch [142/200] batch [5/18] time 0.339 (0.728) data 0.000 (0.384) loss 0.7313 (0.4733) acc 90.6250 (88.7500) lr 4.1221e-03 eta 0:12:49\n",
      "epoch [142/200] batch [10/18] time 0.324 (0.544) data 0.001 (0.201) loss 0.2746 (0.4198) acc 93.7500 (90.0000) lr 4.1221e-03 eta 0:09:32\n",
      "epoch [142/200] batch [15/18] time 0.343 (0.473) data 0.000 (0.134) loss 0.3198 (0.3996) acc 87.5000 (90.2083) lr 4.1221e-03 eta 0:08:15\n",
      "epoch [143/200] batch [5/18] time 0.423 (0.889) data 0.001 (0.475) loss 0.5015 (0.3035) acc 84.3750 (91.2500) lr 3.9958e-03 eta 0:15:23\n",
      "epoch [143/200] batch [10/18] time 0.353 (0.637) data 0.000 (0.238) loss 0.5616 (0.3168) acc 87.5000 (92.8125) lr 3.9958e-03 eta 0:10:58\n",
      "epoch [143/200] batch [15/18] time 0.332 (0.535) data 0.000 (0.159) loss 0.4077 (0.3105) acc 87.5000 (92.5000) lr 3.9958e-03 eta 0:09:10\n",
      "epoch [144/200] batch [5/18] time 0.335 (0.739) data 0.000 (0.389) loss 0.1406 (0.2628) acc 100.0000 (91.8750) lr 3.8709e-03 eta 0:12:34\n",
      "epoch [144/200] batch [10/18] time 0.338 (0.539) data 0.000 (0.196) loss 0.6324 (0.3377) acc 87.5000 (89.3750) lr 3.8709e-03 eta 0:09:07\n",
      "epoch [144/200] batch [15/18] time 0.327 (0.469) data 0.000 (0.130) loss 0.2385 (0.3519) acc 90.6250 (89.3750) lr 3.8709e-03 eta 0:07:54\n",
      "epoch [145/200] batch [5/18] time 0.357 (0.754) data 0.000 (0.370) loss 0.2739 (0.2907) acc 93.7500 (91.8750) lr 3.7476e-03 eta 0:12:36\n",
      "epoch [145/200] batch [10/18] time 0.383 (0.629) data 0.002 (0.234) loss 0.3512 (0.2879) acc 84.3750 (90.6250) lr 3.7476e-03 eta 0:10:27\n",
      "epoch [145/200] batch [15/18] time 0.329 (0.530) data 0.000 (0.156) loss 0.4444 (0.3216) acc 87.5000 (90.4167) lr 3.7476e-03 eta 0:08:46\n",
      "epoch [146/200] batch [5/18] time 0.336 (0.748) data 0.001 (0.398) loss 0.3910 (0.3740) acc 87.5000 (85.6250) lr 3.6258e-03 eta 0:12:16\n",
      "epoch [146/200] batch [10/18] time 0.329 (0.543) data 0.000 (0.201) loss 0.1614 (0.3530) acc 93.7500 (86.8750) lr 3.6258e-03 eta 0:08:52\n",
      "epoch [146/200] batch [15/18] time 0.335 (0.472) data 0.000 (0.134) loss 0.3532 (0.3369) acc 87.5000 (87.9167) lr 3.6258e-03 eta 0:07:40\n",
      "epoch [147/200] batch [5/18] time 0.389 (0.821) data 0.000 (0.394) loss 0.4363 (0.3459) acc 90.6250 (91.2500) lr 3.5055e-03 eta 0:13:14\n",
      "epoch [147/200] batch [10/18] time 0.324 (0.631) data 0.000 (0.211) loss 0.3807 (0.3299) acc 90.6250 (91.2500) lr 3.5055e-03 eta 0:10:06\n",
      "epoch [147/200] batch [15/18] time 0.337 (0.532) data 0.000 (0.141) loss 0.3228 (0.2857) acc 90.6250 (92.7083) lr 3.5055e-03 eta 0:08:29\n",
      "epoch [148/200] batch [5/18] time 0.326 (0.720) data 0.001 (0.374) loss 0.4423 (0.3018) acc 87.5000 (91.8750) lr 3.3869e-03 eta 0:11:23\n",
      "epoch [148/200] batch [10/18] time 0.338 (0.533) data 0.000 (0.187) loss 0.2179 (0.3101) acc 93.7500 (91.8750) lr 3.3869e-03 eta 0:08:23\n",
      "epoch [148/200] batch [15/18] time 0.326 (0.466) data 0.000 (0.125) loss 0.2929 (0.3327) acc 93.7500 (91.6667) lr 3.3869e-03 eta 0:07:17\n",
      "epoch [149/200] batch [5/18] time 0.456 (0.931) data 0.001 (0.522) loss 0.3095 (0.2654) acc 96.8750 (93.7500) lr 3.2699e-03 eta 0:14:26\n",
      "epoch [149/200] batch [10/18] time 0.339 (0.667) data 0.000 (0.281) loss 0.1569 (0.2741) acc 96.8750 (93.7500) lr 3.2699e-03 eta 0:10:18\n",
      "epoch [149/200] batch [15/18] time 0.339 (0.556) data 0.000 (0.187) loss 0.2527 (0.2909) acc 93.7500 (92.7083) lr 3.2699e-03 eta 0:08:31\n",
      "epoch [150/200] batch [5/18] time 0.346 (0.725) data 0.000 (0.378) loss 0.1994 (0.2636) acc 96.8750 (93.7500) lr 3.1545e-03 eta 0:11:02\n",
      "epoch [150/200] batch [10/18] time 0.324 (0.534) data 0.000 (0.189) loss 0.4850 (0.3054) acc 90.6250 (91.8750) lr 3.1545e-03 eta 0:08:05\n",
      "epoch [150/200] batch [15/18] time 0.328 (0.468) data 0.000 (0.126) loss 0.6796 (0.3370) acc 75.0000 (90.4167) lr 3.1545e-03 eta 0:07:02\n",
      "epoch [151/200] batch [5/18] time 0.418 (0.841) data 0.013 (0.464) loss 0.4357 (0.2537) acc 90.6250 (94.3750) lr 3.0409e-03 eta 0:12:32\n",
      "epoch [151/200] batch [10/18] time 0.390 (0.634) data 0.000 (0.232) loss 0.3216 (0.2988) acc 93.7500 (93.4375) lr 3.0409e-03 eta 0:09:24\n",
      "epoch [151/200] batch [15/18] time 0.331 (0.534) data 0.000 (0.155) loss 0.2901 (0.2821) acc 87.5000 (93.1250) lr 3.0409e-03 eta 0:07:52\n",
      "epoch [152/200] batch [5/18] time 0.327 (0.748) data 0.001 (0.405) loss 0.2391 (0.2837) acc 96.8750 (90.6250) lr 2.9289e-03 eta 0:10:56\n",
      "epoch [152/200] batch [10/18] time 0.337 (0.542) data 0.000 (0.203) loss 0.3465 (0.2980) acc 93.7500 (91.5625) lr 2.9289e-03 eta 0:07:52\n",
      "epoch [152/200] batch [15/18] time 0.324 (0.471) data 0.000 (0.135) loss 0.1100 (0.2736) acc 96.8750 (92.0833) lr 2.9289e-03 eta 0:06:48\n",
      "epoch [153/200] batch [5/18] time 0.333 (0.745) data 0.000 (0.373) loss 0.3064 (0.3548) acc 90.6250 (90.6250) lr 2.8187e-03 eta 0:10:39\n",
      "epoch [153/200] batch [10/18] time 0.340 (0.628) data 0.000 (0.254) loss 0.3067 (0.4072) acc 93.7500 (90.0000) lr 2.8187e-03 eta 0:08:56\n",
      "epoch [153/200] batch [15/18] time 0.331 (0.530) data 0.000 (0.170) loss 0.2769 (0.3821) acc 84.3750 (90.6250) lr 2.8187e-03 eta 0:07:29\n",
      "epoch [154/200] batch [5/18] time 0.428 (0.755) data 0.000 (0.374) loss 0.1102 (0.3518) acc 100.0000 (91.2500) lr 2.7103e-03 eta 0:10:34\n",
      "epoch [154/200] batch [10/18] time 0.321 (0.543) data 0.000 (0.187) loss 0.4966 (0.3061) acc 87.5000 (92.1875) lr 2.7103e-03 eta 0:07:34\n",
      "epoch [154/200] batch [15/18] time 0.333 (0.473) data 0.000 (0.125) loss 0.5768 (0.3115) acc 84.3750 (91.8750) lr 2.7103e-03 eta 0:06:32\n",
      "epoch [155/200] batch [5/18] time 0.440 (0.865) data 0.004 (0.448) loss 0.3010 (0.2719) acc 87.5000 (90.6250) lr 2.6037e-03 eta 0:11:51\n",
      "epoch [155/200] batch [10/18] time 0.330 (0.665) data 0.000 (0.270) loss 0.6269 (0.3541) acc 81.2500 (89.0625) lr 2.6037e-03 eta 0:09:03\n",
      "epoch [155/200] batch [15/18] time 0.333 (0.555) data 0.000 (0.180) loss 0.4540 (0.3934) acc 84.3750 (87.2917) lr 2.6037e-03 eta 0:07:30\n",
      "epoch [156/200] batch [5/18] time 0.329 (0.751) data 0.000 (0.376) loss 0.4243 (0.3683) acc 84.3750 (89.3750) lr 2.4989e-03 eta 0:10:04\n",
      "epoch [156/200] batch [10/18] time 0.340 (0.546) data 0.000 (0.188) loss 0.2153 (0.3561) acc 93.7500 (89.3750) lr 2.4989e-03 eta 0:07:17\n",
      "epoch [156/200] batch [15/18] time 0.326 (0.475) data 0.000 (0.125) loss 0.2619 (0.3119) acc 93.7500 (91.2500) lr 2.4989e-03 eta 0:06:17\n",
      "epoch [157/200] batch [5/18] time 0.327 (0.798) data 0.000 (0.432) loss 0.5844 (0.3205) acc 84.3750 (91.8750) lr 2.3959e-03 eta 0:10:28\n",
      "epoch [157/200] batch [10/18] time 0.383 (0.632) data 0.041 (0.252) loss 0.1804 (0.3397) acc 93.7500 (90.0000) lr 2.3959e-03 eta 0:08:14\n",
      "epoch [157/200] batch [15/18] time 0.329 (0.534) data 0.000 (0.168) loss 0.4134 (0.3634) acc 87.5000 (89.1667) lr 2.3959e-03 eta 0:06:54\n",
      "epoch [158/200] batch [5/18] time 0.343 (0.749) data 0.011 (0.405) loss 0.4151 (0.3873) acc 87.5000 (88.1250) lr 2.2949e-03 eta 0:09:36\n",
      "epoch [158/200] batch [10/18] time 0.353 (0.547) data 0.000 (0.203) loss 0.3087 (0.3928) acc 87.5000 (87.8125) lr 2.2949e-03 eta 0:06:57\n",
      "epoch [158/200] batch [15/18] time 0.325 (0.474) data 0.000 (0.136) loss 0.4021 (0.3813) acc 90.6250 (88.5417) lr 2.2949e-03 eta 0:05:59\n",
      "epoch [159/200] batch [5/18] time 0.387 (0.785) data 0.001 (0.386) loss 0.3459 (0.2704) acc 90.6250 (93.1250) lr 2.1957e-03 eta 0:09:49\n",
      "epoch [159/200] batch [10/18] time 0.483 (0.624) data 0.123 (0.239) loss 0.1799 (0.2975) acc 96.8750 (92.1875) lr 2.1957e-03 eta 0:07:45\n",
      "epoch [159/200] batch [15/18] time 0.328 (0.527) data 0.001 (0.160) loss 0.4783 (0.2909) acc 90.6250 (92.0833) lr 2.1957e-03 eta 0:06:30\n",
      "epoch [160/200] batch [5/18] time 0.325 (0.741) data 0.002 (0.393) loss 0.3113 (0.2940) acc 90.6250 (91.8750) lr 2.0984e-03 eta 0:09:03\n",
      "epoch [160/200] batch [10/18] time 0.352 (0.546) data 0.000 (0.197) loss 0.3267 (0.3635) acc 90.6250 (90.3125) lr 2.0984e-03 eta 0:06:37\n",
      "epoch [160/200] batch [15/18] time 0.324 (0.472) data 0.000 (0.131) loss 0.4285 (0.3379) acc 84.3750 (90.8333) lr 2.0984e-03 eta 0:05:41\n",
      "epoch [161/200] batch [5/18] time 0.384 (0.809) data 0.002 (0.388) loss 0.0843 (0.1912) acc 100.0000 (96.2500) lr 2.0032e-03 eta 0:09:38\n",
      "epoch [161/200] batch [10/18] time 0.345 (0.639) data 0.001 (0.231) loss 0.3172 (0.2154) acc 84.3750 (94.3750) lr 2.0032e-03 eta 0:07:33\n",
      "epoch [161/200] batch [15/18] time 0.332 (0.538) data 0.000 (0.154) loss 0.4065 (0.2409) acc 87.5000 (93.3333) lr 2.0032e-03 eta 0:06:19\n",
      "epoch [162/200] batch [5/18] time 0.330 (0.763) data 0.000 (0.418) loss 0.1503 (0.3457) acc 93.7500 (91.2500) lr 1.9098e-03 eta 0:08:52\n",
      "epoch [162/200] batch [10/18] time 0.340 (0.552) data 0.000 (0.209) loss 0.4817 (0.4331) acc 87.5000 (89.0625) lr 1.9098e-03 eta 0:06:22\n",
      "epoch [162/200] batch [15/18] time 0.330 (0.478) data 0.000 (0.140) loss 0.1536 (0.3833) acc 96.8750 (90.4167) lr 1.9098e-03 eta 0:05:28\n",
      "epoch [163/200] batch [5/18] time 0.481 (0.815) data 0.000 (0.383) loss 0.2460 (0.2163) acc 93.7500 (95.6250) lr 1.8185e-03 eta 0:09:13\n",
      "epoch [163/200] batch [10/18] time 0.710 (0.647) data 0.389 (0.233) loss 0.4885 (0.2924) acc 84.3750 (93.1250) lr 1.8185e-03 eta 0:07:15\n",
      "epoch [163/200] batch [15/18] time 0.327 (0.542) data 0.000 (0.156) loss 0.4609 (0.3220) acc 81.2500 (91.6667) lr 1.8185e-03 eta 0:06:02\n",
      "epoch [164/200] batch [5/18] time 0.330 (0.729) data 0.000 (0.390) loss 0.4729 (0.3217) acc 84.3750 (91.8750) lr 1.7292e-03 eta 0:08:02\n",
      "epoch [164/200] batch [10/18] time 0.331 (0.535) data 0.000 (0.195) loss 0.3869 (0.3727) acc 90.6250 (90.0000) lr 1.7292e-03 eta 0:05:50\n",
      "epoch [164/200] batch [15/18] time 0.330 (0.467) data 0.000 (0.130) loss 0.5705 (0.3478) acc 90.6250 (90.8333) lr 1.7292e-03 eta 0:05:04\n",
      "epoch [165/200] batch [5/18] time 0.360 (0.717) data 0.001 (0.356) loss 0.1527 (0.3365) acc 100.0000 (91.8750) lr 1.6419e-03 eta 0:07:40\n",
      "epoch [165/200] batch [10/18] time 0.590 (0.613) data 0.272 (0.243) loss 0.4089 (0.3225) acc 87.5000 (91.8750) lr 1.6419e-03 eta 0:06:31\n",
      "epoch [165/200] batch [15/18] time 0.333 (0.522) data 0.000 (0.162) loss 0.2856 (0.3201) acc 96.8750 (92.7083) lr 1.6419e-03 eta 0:05:30\n",
      "epoch [166/200] batch [5/18] time 0.423 (0.761) data 0.000 (0.398) loss 0.3418 (0.2969) acc 90.6250 (92.5000) lr 1.5567e-03 eta 0:07:55\n",
      "epoch [166/200] batch [10/18] time 0.335 (0.551) data 0.000 (0.199) loss 0.5788 (0.3127) acc 84.3750 (92.5000) lr 1.5567e-03 eta 0:05:41\n",
      "epoch [166/200] batch [15/18] time 0.327 (0.477) data 0.000 (0.133) loss 0.1144 (0.2982) acc 100.0000 (93.3333) lr 1.5567e-03 eta 0:04:53\n",
      "epoch [167/200] batch [5/18] time 0.325 (0.782) data 0.005 (0.405) loss 0.1342 (0.3650) acc 96.8750 (88.7500) lr 1.4736e-03 eta 0:07:54\n",
      "epoch [167/200] batch [10/18] time 0.337 (0.626) data 0.000 (0.259) loss 0.4528 (0.3852) acc 93.7500 (89.6875) lr 1.4736e-03 eta 0:06:16\n",
      "epoch [167/200] batch [15/18] time 0.329 (0.530) data 0.000 (0.172) loss 0.1154 (0.3529) acc 100.0000 (90.2083) lr 1.4736e-03 eta 0:05:16\n",
      "epoch [168/200] batch [5/18] time 0.337 (0.745) data 0.000 (0.381) loss 0.4360 (0.3448) acc 90.6250 (91.8750) lr 1.3926e-03 eta 0:07:18\n",
      "epoch [168/200] batch [10/18] time 0.333 (0.542) data 0.000 (0.192) loss 0.2404 (0.3173) acc 96.8750 (92.1875) lr 1.3926e-03 eta 0:05:16\n",
      "epoch [168/200] batch [15/18] time 0.324 (0.471) data 0.000 (0.128) loss 0.2410 (0.2789) acc 90.6250 (93.5417) lr 1.3926e-03 eta 0:04:32\n",
      "epoch [169/200] batch [5/18] time 0.328 (0.736) data 0.000 (0.356) loss 0.1781 (0.2908) acc 90.6250 (93.1250) lr 1.3137e-03 eta 0:07:00\n",
      "epoch [169/200] batch [10/18] time 0.671 (0.597) data 0.220 (0.212) loss 0.3803 (0.3225) acc 87.5000 (90.6250) lr 1.3137e-03 eta 0:05:37\n",
      "epoch [169/200] batch [15/18] time 0.334 (0.513) data 0.000 (0.142) loss 0.3730 (0.3369) acc 81.2500 (90.0000) lr 1.3137e-03 eta 0:04:47\n",
      "epoch [170/200] batch [5/18] time 0.353 (0.762) data 0.000 (0.397) loss 0.1952 (0.3959) acc 96.8750 (88.7500) lr 1.2369e-03 eta 0:07:01\n",
      "epoch [170/200] batch [10/18] time 0.331 (0.548) data 0.000 (0.201) loss 0.1387 (0.4233) acc 93.7500 (88.7500) lr 1.2369e-03 eta 0:05:00\n",
      "epoch [170/200] batch [15/18] time 0.328 (0.475) data 0.000 (0.134) loss 0.1878 (0.3697) acc 96.8750 (90.4167) lr 1.2369e-03 eta 0:04:18\n",
      "epoch [171/200] batch [5/18] time 0.372 (0.891) data 0.005 (0.500) loss 0.2788 (0.3800) acc 93.7500 (89.3750) lr 1.1623e-03 eta 0:07:56\n",
      "epoch [171/200] batch [10/18] time 0.347 (0.668) data 0.000 (0.275) loss 0.4265 (0.3296) acc 81.2500 (90.9375) lr 1.1623e-03 eta 0:05:53\n",
      "epoch [171/200] batch [15/18] time 0.332 (0.555) data 0.000 (0.183) loss 0.1917 (0.3451) acc 93.7500 (90.6250) lr 1.1623e-03 eta 0:04:51\n",
      "epoch [172/200] batch [5/18] time 0.331 (0.758) data 0.000 (0.415) loss 0.1922 (0.2923) acc 93.7500 (92.5000) lr 1.0899e-03 eta 0:06:31\n",
      "epoch [172/200] batch [10/18] time 0.337 (0.550) data 0.000 (0.208) loss 0.2817 (0.3078) acc 90.6250 (91.2500) lr 1.0899e-03 eta 0:04:41\n",
      "epoch [172/200] batch [15/18] time 0.328 (0.477) data 0.000 (0.139) loss 0.1982 (0.2891) acc 100.0000 (92.2917) lr 1.0899e-03 eta 0:04:01\n",
      "epoch [173/200] batch [5/18] time 0.402 (0.805) data 0.000 (0.377) loss 0.1724 (0.1779) acc 93.7500 (97.5000) lr 1.0197e-03 eta 0:06:41\n",
      "epoch [173/200] batch [10/18] time 0.387 (0.613) data 0.047 (0.197) loss 0.2961 (0.2857) acc 87.5000 (92.8125) lr 1.0197e-03 eta 0:05:02\n",
      "epoch [173/200] batch [15/18] time 0.332 (0.521) data 0.003 (0.132) loss 0.3300 (0.2941) acc 87.5000 (92.0833) lr 1.0197e-03 eta 0:04:14\n",
      "epoch [174/200] batch [5/18] time 0.344 (0.761) data 0.001 (0.418) loss 0.1643 (0.2618) acc 100.0000 (93.7500) lr 9.5173e-04 eta 0:06:05\n",
      "epoch [174/200] batch [10/18] time 0.333 (0.549) data 0.000 (0.210) loss 0.2917 (0.2611) acc 90.6250 (93.7500) lr 9.5173e-04 eta 0:04:21\n",
      "epoch [174/200] batch [15/18] time 0.328 (0.476) data 0.000 (0.140) loss 0.3298 (0.2969) acc 93.7500 (92.2917) lr 9.5173e-04 eta 0:03:44\n",
      "epoch [175/200] batch [5/18] time 0.342 (0.753) data 0.001 (0.357) loss 0.3454 (0.4105) acc 90.6250 (88.7500) lr 8.8597e-04 eta 0:05:48\n",
      "epoch [175/200] batch [10/18] time 0.569 (0.625) data 0.077 (0.222) loss 0.3117 (0.3300) acc 87.5000 (90.6250) lr 8.8597e-04 eta 0:04:46\n",
      "epoch [175/200] batch [15/18] time 0.326 (0.528) data 0.000 (0.148) loss 0.3640 (0.3357) acc 90.6250 (90.6250) lr 8.8597e-04 eta 0:03:59\n",
      "epoch [176/200] batch [5/18] time 0.334 (0.775) data 0.000 (0.431) loss 0.3830 (0.2821) acc 87.5000 (91.8750) lr 8.2245e-04 eta 0:05:44\n",
      "epoch [176/200] batch [10/18] time 0.349 (0.560) data 0.000 (0.216) loss 0.4348 (0.2837) acc 90.6250 (92.1875) lr 8.2245e-04 eta 0:04:06\n",
      "epoch [176/200] batch [15/18] time 0.321 (0.482) data 0.000 (0.144) loss 0.3335 (0.2848) acc 90.6250 (92.0833) lr 8.2245e-04 eta 0:03:29\n",
      "epoch [177/200] batch [5/18] time 0.366 (0.840) data 0.000 (0.435) loss 0.5881 (0.3896) acc 84.3750 (86.2500) lr 7.6120e-04 eta 0:05:58\n",
      "epoch [177/200] batch [10/18] time 0.356 (0.692) data 0.008 (0.287) loss 0.2478 (0.3370) acc 93.7500 (89.0625) lr 7.6120e-04 eta 0:04:51\n",
      "epoch [177/200] batch [15/18] time 0.332 (0.572) data 0.000 (0.191) loss 0.1938 (0.3158) acc 93.7500 (89.7917) lr 7.6120e-04 eta 0:03:58\n",
      "epoch [178/200] batch [5/18] time 0.340 (0.714) data 0.000 (0.364) loss 0.5876 (0.3524) acc 84.3750 (90.6250) lr 7.0224e-04 eta 0:04:51\n",
      "epoch [178/200] batch [10/18] time 0.322 (0.534) data 0.001 (0.182) loss 0.2482 (0.3249) acc 93.7500 (91.2500) lr 7.0224e-04 eta 0:03:35\n",
      "epoch [178/200] batch [15/18] time 0.329 (0.467) data 0.000 (0.122) loss 0.2380 (0.3219) acc 93.7500 (91.4583) lr 7.0224e-04 eta 0:03:06\n",
      "epoch [179/200] batch [5/18] time 0.451 (0.850) data 0.001 (0.445) loss 0.4333 (0.3732) acc 84.3750 (88.7500) lr 6.4556e-04 eta 0:05:32\n",
      "epoch [179/200] batch [10/18] time 0.336 (0.648) data 0.003 (0.259) loss 0.2534 (0.3436) acc 93.7500 (91.2500) lr 6.4556e-04 eta 0:04:10\n",
      "epoch [179/200] batch [15/18] time 0.332 (0.542) data 0.000 (0.173) loss 0.0910 (0.3495) acc 100.0000 (90.6250) lr 6.4556e-04 eta 0:03:26\n",
      "epoch [180/200] batch [5/18] time 0.353 (0.751) data 0.006 (0.387) loss 0.1651 (0.2775) acc 100.0000 (92.5000) lr 5.9119e-04 eta 0:04:40\n",
      "epoch [180/200] batch [10/18] time 0.324 (0.542) data 0.000 (0.194) loss 0.1815 (0.3031) acc 93.7500 (91.5625) lr 5.9119e-04 eta 0:03:19\n",
      "epoch [180/200] batch [15/18] time 0.331 (0.472) data 0.000 (0.129) loss 0.1470 (0.3198) acc 96.8750 (90.4167) lr 5.9119e-04 eta 0:02:51\n",
      "epoch [181/200] batch [5/18] time 0.411 (0.875) data 0.000 (0.510) loss 0.3076 (0.2344) acc 90.6250 (93.1250) lr 5.3915e-04 eta 0:05:10\n",
      "epoch [181/200] batch [10/18] time 0.360 (0.636) data 0.001 (0.272) loss 0.1365 (0.2647) acc 96.8750 (93.1250) lr 5.3915e-04 eta 0:03:42\n",
      "epoch [181/200] batch [15/18] time 0.327 (0.535) data 0.000 (0.182) loss 0.3644 (0.2926) acc 90.6250 (92.0833) lr 5.3915e-04 eta 0:03:04\n",
      "epoch [182/200] batch [5/18] time 0.331 (0.736) data 0.000 (0.385) loss 0.1675 (0.2777) acc 96.8750 (92.5000) lr 4.8943e-04 eta 0:04:07\n",
      "epoch [182/200] batch [10/18] time 0.343 (0.538) data 0.000 (0.193) loss 0.3356 (0.3604) acc 96.8750 (91.5625) lr 4.8943e-04 eta 0:02:58\n",
      "epoch [182/200] batch [15/18] time 0.330 (0.468) data 0.000 (0.129) loss 0.3879 (0.3636) acc 90.6250 (91.6667) lr 4.8943e-04 eta 0:02:33\n",
      "epoch [183/200] batch [5/18] time 0.400 (0.778) data 0.000 (0.383) loss 0.6950 (0.4019) acc 78.1250 (90.0000) lr 4.4207e-04 eta 0:04:08\n",
      "epoch [183/200] batch [10/18] time 0.349 (0.674) data 0.000 (0.273) loss 0.3361 (0.3635) acc 87.5000 (89.3750) lr 4.4207e-04 eta 0:03:31\n",
      "epoch [183/200] batch [15/18] time 0.334 (0.560) data 0.001 (0.182) loss 0.2773 (0.3145) acc 93.7500 (91.2500) lr 4.4207e-04 eta 0:02:52\n",
      "epoch [184/200] batch [5/18] time 0.356 (0.758) data 0.000 (0.408) loss 0.2248 (0.2508) acc 96.8750 (94.3750) lr 3.9706e-04 eta 0:03:48\n",
      "epoch [184/200] batch [10/18] time 0.328 (0.551) data 0.000 (0.206) loss 0.2131 (0.2339) acc 100.0000 (95.3125) lr 3.9706e-04 eta 0:02:43\n",
      "epoch [184/200] batch [15/18] time 0.334 (0.478) data 0.000 (0.137) loss 0.1567 (0.2607) acc 100.0000 (94.1667) lr 3.9706e-04 eta 0:02:19\n",
      "epoch [185/200] batch [5/18] time 0.424 (0.872) data 0.001 (0.486) loss 0.1982 (0.2829) acc 96.8750 (93.1250) lr 3.5443e-04 eta 0:04:06\n",
      "epoch [185/200] batch [10/18] time 0.340 (0.658) data 0.003 (0.277) loss 0.1824 (0.3274) acc 93.7500 (91.5625) lr 3.5443e-04 eta 0:03:03\n",
      "epoch [185/200] batch [15/18] time 0.333 (0.550) data 0.000 (0.185) loss 0.2077 (0.3015) acc 90.6250 (92.2917) lr 3.5443e-04 eta 0:02:30\n",
      "epoch [186/200] batch [5/18] time 0.346 (0.717) data 0.000 (0.363) loss 0.3188 (0.3569) acc 90.6250 (90.0000) lr 3.1417e-04 eta 0:03:09\n",
      "epoch [186/200] batch [10/18] time 0.323 (0.526) data 0.000 (0.182) loss 0.1332 (0.2691) acc 96.8750 (93.4375) lr 3.1417e-04 eta 0:02:16\n",
      "epoch [186/200] batch [15/18] time 0.329 (0.462) data 0.000 (0.121) loss 0.1459 (0.2542) acc 96.8750 (93.3333) lr 3.1417e-04 eta 0:01:57\n",
      "epoch [187/200] batch [5/18] time 0.332 (0.804) data 0.001 (0.427) loss 0.2668 (0.3902) acc 93.7500 (89.3750) lr 2.7630e-04 eta 0:03:18\n",
      "epoch [187/200] batch [10/18] time 0.337 (0.641) data 0.000 (0.260) loss 0.2900 (0.3261) acc 93.7500 (91.2500) lr 2.7630e-04 eta 0:02:35\n",
      "epoch [187/200] batch [15/18] time 0.337 (0.539) data 0.000 (0.173) loss 0.1119 (0.3349) acc 100.0000 (91.6667) lr 2.7630e-04 eta 0:02:07\n",
      "epoch [188/200] batch [5/18] time 0.328 (0.733) data 0.000 (0.384) loss 0.2033 (0.3860) acc 90.6250 (89.3750) lr 2.4083e-04 eta 0:02:47\n",
      "epoch [188/200] batch [10/18] time 0.327 (0.535) data 0.001 (0.192) loss 0.2716 (0.3535) acc 87.5000 (89.3750) lr 2.4083e-04 eta 0:01:59\n",
      "epoch [188/200] batch [15/18] time 0.326 (0.467) data 0.000 (0.128) loss 0.3278 (0.3648) acc 90.6250 (88.9583) lr 2.4083e-04 eta 0:01:42\n",
      "epoch [189/200] batch [5/18] time 0.363 (0.811) data 0.000 (0.402) loss 0.3239 (0.2722) acc 87.5000 (91.8750) lr 2.0777e-04 eta 0:02:51\n",
      "epoch [189/200] batch [10/18] time 0.354 (0.636) data 0.000 (0.251) loss 0.2096 (0.3053) acc 96.8750 (91.2500) lr 2.0777e-04 eta 0:02:11\n",
      "epoch [189/200] batch [15/18] time 0.329 (0.535) data 0.000 (0.167) loss 0.1205 (0.3038) acc 100.0000 (91.6667) lr 2.0777e-04 eta 0:01:47\n",
      "epoch [190/200] batch [5/18] time 0.335 (0.731) data 0.000 (0.385) loss 0.4215 (0.4165) acc 87.5000 (86.2500) lr 1.7713e-04 eta 0:02:21\n",
      "epoch [190/200] batch [10/18] time 0.343 (0.535) data 0.000 (0.193) loss 0.5241 (0.4123) acc 78.1250 (86.8750) lr 1.7713e-04 eta 0:01:40\n",
      "epoch [190/200] batch [15/18] time 0.331 (0.466) data 0.000 (0.129) loss 0.2804 (0.3785) acc 93.7500 (88.7500) lr 1.7713e-04 eta 0:01:25\n",
      "epoch [191/200] batch [5/18] time 0.364 (0.770) data 0.000 (0.369) loss 0.4267 (0.3215) acc 87.5000 (90.0000) lr 1.4891e-04 eta 0:02:14\n",
      "epoch [191/200] batch [10/18] time 0.488 (0.622) data 0.001 (0.222) loss 0.2431 (0.3129) acc 93.7500 (90.3125) lr 1.4891e-04 eta 0:01:45\n",
      "epoch [191/200] batch [15/18] time 0.332 (0.528) data 0.000 (0.150) loss 0.3239 (0.3240) acc 93.7500 (90.2083) lr 1.4891e-04 eta 0:01:27\n",
      "epoch [192/200] batch [5/18] time 0.335 (0.764) data 0.001 (0.412) loss 0.3984 (0.3382) acc 90.6250 (90.0000) lr 1.2312e-04 eta 0:01:59\n",
      "epoch [192/200] batch [10/18] time 0.336 (0.548) data 0.000 (0.206) loss 0.4589 (0.3462) acc 84.3750 (90.0000) lr 1.2312e-04 eta 0:01:23\n",
      "epoch [192/200] batch [15/18] time 0.327 (0.475) data 0.000 (0.137) loss 0.2010 (0.2986) acc 90.6250 (91.4583) lr 1.2312e-04 eta 0:01:09\n",
      "epoch [193/200] batch [5/18] time 0.336 (0.785) data 0.000 (0.390) loss 0.6027 (0.2849) acc 81.2500 (92.5000) lr 9.9763e-05 eta 0:01:49\n",
      "epoch [193/200] batch [10/18] time 0.522 (0.641) data 0.000 (0.236) loss 0.4696 (0.2877) acc 87.5000 (92.1875) lr 9.9763e-05 eta 0:01:25\n",
      "epoch [193/200] batch [15/18] time 0.321 (0.539) data 0.000 (0.157) loss 0.4220 (0.2852) acc 87.5000 (92.7083) lr 9.9763e-05 eta 0:01:09\n",
      "epoch [194/200] batch [5/18] time 0.344 (0.701) data 0.000 (0.348) loss 0.2447 (0.3788) acc 96.8750 (90.0000) lr 7.8853e-05 eta 0:01:24\n",
      "epoch [194/200] batch [10/18] time 0.325 (0.525) data 0.000 (0.174) loss 0.2790 (0.3729) acc 93.7500 (90.6250) lr 7.8853e-05 eta 0:01:00\n",
      "epoch [194/200] batch [15/18] time 0.331 (0.461) data 0.000 (0.116) loss 0.4328 (0.3558) acc 87.5000 (90.6250) lr 7.8853e-05 eta 0:00:51\n",
      "epoch [195/200] batch [5/18] time 0.333 (0.765) data 0.000 (0.368) loss 0.2856 (0.4165) acc 93.7500 (86.8750) lr 6.0390e-05 eta 0:01:18\n",
      "epoch [195/200] batch [10/18] time 0.348 (0.611) data 0.000 (0.222) loss 0.5593 (0.3762) acc 81.2500 (88.7500) lr 6.0390e-05 eta 0:00:59\n",
      "epoch [195/200] batch [15/18] time 0.339 (0.520) data 0.000 (0.148) loss 0.3381 (0.3609) acc 93.7500 (89.7917) lr 6.0390e-05 eta 0:00:48\n",
      "epoch [196/200] batch [5/18] time 0.343 (0.708) data 0.001 (0.350) loss 0.2109 (0.2996) acc 90.6250 (90.0000) lr 4.4380e-05 eta 0:01:00\n",
      "epoch [196/200] batch [10/18] time 0.331 (0.522) data 0.000 (0.175) loss 0.2541 (0.2996) acc 93.7500 (91.2500) lr 4.4380e-05 eta 0:00:41\n",
      "epoch [196/200] batch [15/18] time 0.331 (0.459) data 0.000 (0.117) loss 0.3674 (0.3236) acc 87.5000 (90.8333) lr 4.4380e-05 eta 0:00:34\n",
      "epoch [197/200] batch [5/18] time 0.331 (0.816) data 0.000 (0.441) loss 0.1634 (0.2781) acc 96.8750 (92.5000) lr 3.0827e-05 eta 0:00:54\n",
      "epoch [197/200] batch [10/18] time 0.356 (0.621) data 0.000 (0.223) loss 0.2101 (0.3068) acc 90.6250 (91.5625) lr 3.0827e-05 eta 0:00:38\n",
      "epoch [197/200] batch [15/18] time 0.328 (0.525) data 0.000 (0.149) loss 0.4230 (0.3367) acc 87.5000 (90.6250) lr 3.0827e-05 eta 0:00:29\n",
      "epoch [198/200] batch [5/18] time 0.342 (0.785) data 0.013 (0.438) loss 0.3561 (0.3369) acc 90.6250 (90.6250) lr 1.9733e-05 eta 0:00:38\n",
      "epoch [198/200] batch [10/18] time 0.331 (0.559) data 0.000 (0.220) loss 0.2738 (0.3100) acc 96.8750 (91.8750) lr 1.9733e-05 eta 0:00:24\n",
      "epoch [198/200] batch [15/18] time 0.329 (0.482) data 0.000 (0.146) loss 0.2524 (0.3015) acc 96.8750 (92.9167) lr 1.9733e-05 eta 0:00:18\n",
      "epoch [199/200] batch [5/18] time 0.353 (0.855) data 0.001 (0.441) loss 0.5383 (0.3065) acc 87.5000 (93.1250) lr 1.1101e-05 eta 0:00:26\n",
      "epoch [199/200] batch [10/18] time 0.335 (0.675) data 0.000 (0.280) loss 0.6257 (0.3321) acc 78.1250 (91.5625) lr 1.1101e-05 eta 0:00:17\n",
      "epoch [199/200] batch [15/18] time 0.336 (0.560) data 0.000 (0.187) loss 0.1729 (0.3221) acc 96.8750 (91.0417) lr 1.1101e-05 eta 0:00:11\n",
      "epoch [200/200] batch [5/18] time 0.343 (0.764) data 0.001 (0.417) loss 0.2651 (0.3025) acc 93.7500 (93.1250) lr 4.9344e-06 eta 0:00:09\n",
      "epoch [200/200] batch [10/18] time 0.341 (0.556) data 0.001 (0.209) loss 0.2088 (0.3169) acc 93.7500 (92.1875) lr 4.9344e-06 eta 0:00:04\n",
      "epoch [200/200] batch [15/18] time 0.326 (0.480) data 0.000 (0.139) loss 0.3092 (0.3159) acc 87.5000 (91.0417) lr 4.9344e-06 eta 0:00:01\n",
      "Checkpoint saved to output/oxford_pets/DAPT/vit_b16_16shots/seed1/prompt_learner/model.pth.tar-200\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 37/37 [02:51<00:00,  4.64s/it]\n",
      "=> result\n",
      "* total: 3,669\n",
      "* correct: 3,369\n",
      "* accuracy: 91.8%\n",
      "* error: 8.2%\n",
      "* macro_f1: 91.7%\n",
      "Elapsed: 0:32:18\n"
     ]
    }
   ],
   "source": [
    "#oxford_pets-16shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "        --output-dir output/oxford_pets/DAPT/vit_b16_16shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OXQwQTInyMf5",
    "outputId": "86b597b4-8e38-4d0f-9d5d-c646e468bab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 14:42:46.183969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 14:42:46.203994: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 14:42:46.210258: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 14:42:46.226347: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 14:42:47.249437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '8']\n",
      "output_dir: output/oxford_pets/DAPT/vit_b16_8shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordPets\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 8\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.02\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/oxford_pets/DAPT/vit_b16_8shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 0.1\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: OxfordPets\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_8-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  ----------\n",
      "Dataset    OxfordPets\n",
      "# classes  37\n",
      "# train_x  296\n",
      "# val      148\n",
      "# test     3,669\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/oxford_pets/DAPT/vit_b16_8shots/seed1/tensorboard)\n",
      "epoch [1/200] batch [5/9] time 0.312 (1.016) data 0.000 (0.386) loss 3.3013 (3.0179) acc 15.6250 (25.6250) lr 1.0000e-05 eta 0:30:23\n",
      "epoch [2/200] batch [5/9] time 0.317 (0.892) data 0.000 (0.563) loss 1.1325 (1.8678) acc 68.7500 (47.5000) lr 2.0000e-02 eta 0:26:33\n",
      "epoch [3/200] batch [5/9] time 0.323 (0.736) data 0.000 (0.409) loss 0.9186 (1.0066) acc 75.0000 (73.1250) lr 1.9999e-02 eta 0:21:47\n",
      "epoch [4/200] batch [5/9] time 0.319 (0.746) data 0.000 (0.417) loss 0.7915 (0.8183) acc 78.1250 (76.2500) lr 1.9995e-02 eta 0:21:59\n",
      "epoch [5/200] batch [5/9] time 0.330 (0.989) data 0.000 (0.623) loss 0.8925 (0.7984) acc 75.0000 (80.0000) lr 1.9989e-02 eta 0:28:58\n",
      "epoch [6/200] batch [5/9] time 0.324 (0.754) data 0.000 (0.425) loss 0.6802 (0.7323) acc 84.3750 (79.3750) lr 1.9980e-02 eta 0:22:00\n",
      "epoch [7/200] batch [5/9] time 0.328 (0.731) data 0.000 (0.396) loss 1.0076 (0.7289) acc 68.7500 (80.0000) lr 1.9969e-02 eta 0:21:11\n",
      "epoch [8/200] batch [5/9] time 0.339 (0.973) data 0.000 (0.613) loss 0.4262 (0.6913) acc 87.5000 (80.0000) lr 1.9956e-02 eta 0:28:05\n",
      "epoch [9/200] batch [5/9] time 0.330 (0.735) data 0.000 (0.399) loss 1.0095 (0.7204) acc 71.8750 (81.2500) lr 1.9940e-02 eta 0:21:06\n",
      "epoch [10/200] batch [5/9] time 0.327 (0.742) data 0.000 (0.407) loss 0.9389 (0.6160) acc 81.2500 (83.7500) lr 1.9921e-02 eta 0:21:12\n",
      "epoch [11/200] batch [5/9] time 0.331 (1.013) data 0.000 (0.644) loss 0.5231 (0.7067) acc 84.3750 (77.5000) lr 1.9900e-02 eta 0:28:46\n",
      "epoch [12/200] batch [5/9] time 0.328 (0.701) data 0.000 (0.362) loss 0.3664 (0.5483) acc 90.6250 (90.0000) lr 1.9877e-02 eta 0:19:49\n",
      "epoch [13/200] batch [5/9] time 0.333 (0.744) data 0.000 (0.405) loss 0.9363 (0.7202) acc 78.1250 (83.7500) lr 1.9851e-02 eta 0:20:55\n",
      "epoch [14/200] batch [5/9] time 0.331 (1.068) data 0.000 (0.722) loss 0.6295 (0.6152) acc 81.2500 (81.2500) lr 1.9823e-02 eta 0:29:51\n",
      "epoch [15/200] batch [5/9] time 0.335 (0.732) data 0.000 (0.390) loss 0.7451 (0.6857) acc 75.0000 (80.0000) lr 1.9792e-02 eta 0:20:22\n",
      "epoch [16/200] batch [5/9] time 0.333 (0.717) data 0.000 (0.377) loss 0.6292 (0.5779) acc 87.5000 (83.1250) lr 1.9759e-02 eta 0:19:49\n",
      "epoch [17/200] batch [5/9] time 0.335 (1.041) data 0.000 (0.701) loss 0.2536 (0.4297) acc 90.6250 (88.1250) lr 1.9724e-02 eta 0:28:39\n",
      "epoch [18/200] batch [5/9] time 0.328 (0.719) data 0.001 (0.382) loss 0.6020 (0.5598) acc 87.5000 (85.6250) lr 1.9686e-02 eta 0:19:41\n",
      "epoch [19/200] batch [5/9] time 0.328 (0.700) data 0.000 (0.359) loss 0.5108 (0.5980) acc 84.3750 (83.7500) lr 1.9646e-02 eta 0:19:03\n",
      "epoch [20/200] batch [5/9] time 0.332 (1.009) data 0.000 (0.668) loss 0.3533 (0.5133) acc 90.6250 (86.2500) lr 1.9603e-02 eta 0:27:18\n",
      "epoch [21/200] batch [5/9] time 0.329 (0.683) data 0.000 (0.345) loss 0.5824 (0.6136) acc 90.6250 (83.1250) lr 1.9558e-02 eta 0:18:23\n",
      "epoch [22/200] batch [5/9] time 0.330 (0.797) data 0.000 (0.461) loss 0.2408 (0.5014) acc 93.7500 (85.0000) lr 1.9511e-02 eta 0:21:20\n",
      "epoch [23/200] batch [5/9] time 0.325 (1.028) data 0.000 (0.689) loss 0.4780 (0.6995) acc 87.5000 (80.6250) lr 1.9461e-02 eta 0:27:21\n",
      "epoch [24/200] batch [5/9] time 0.326 (0.719) data 0.000 (0.385) loss 0.4429 (0.4537) acc 84.3750 (88.7500) lr 1.9409e-02 eta 0:19:02\n",
      "epoch [25/200] batch [5/9] time 0.333 (0.740) data 0.000 (0.401) loss 0.5902 (0.6344) acc 87.5000 (81.8750) lr 1.9354e-02 eta 0:19:28\n",
      "epoch [26/200] batch [5/9] time 0.329 (1.019) data 0.000 (0.680) loss 0.5613 (0.5422) acc 93.7500 (86.8750) lr 1.9298e-02 eta 0:26:40\n",
      "epoch [27/200] batch [5/9] time 0.330 (0.737) data 0.000 (0.398) loss 0.4898 (0.6209) acc 78.1250 (80.0000) lr 1.9239e-02 eta 0:19:10\n",
      "epoch [28/200] batch [5/9] time 0.334 (0.737) data 0.000 (0.399) loss 0.4684 (0.5179) acc 81.2500 (83.1250) lr 1.9178e-02 eta 0:19:04\n",
      "epoch [29/200] batch [5/9] time 0.332 (1.019) data 0.000 (0.656) loss 0.4665 (0.5497) acc 78.1250 (80.6250) lr 1.9114e-02 eta 0:26:12\n",
      "epoch [30/200] batch [5/9] time 0.328 (0.748) data 0.000 (0.413) loss 0.6833 (0.4997) acc 75.0000 (85.6250) lr 1.9048e-02 eta 0:19:08\n",
      "epoch [31/200] batch [5/9] time 0.328 (0.733) data 0.000 (0.398) loss 0.5026 (0.5509) acc 90.6250 (85.6250) lr 1.8980e-02 eta 0:18:37\n",
      "epoch [32/200] batch [5/9] time 0.338 (1.042) data 0.000 (0.655) loss 0.7540 (0.5964) acc 71.8750 (81.8750) lr 1.8910e-02 eta 0:26:18\n",
      "epoch [33/200] batch [5/9] time 0.324 (0.701) data 0.000 (0.362) loss 0.7525 (0.6111) acc 75.0000 (82.5000) lr 1.8838e-02 eta 0:17:35\n",
      "epoch [34/200] batch [5/9] time 0.330 (0.698) data 0.000 (0.358) loss 0.7475 (0.5239) acc 81.2500 (85.0000) lr 1.8763e-02 eta 0:17:25\n",
      "epoch [35/200] batch [5/9] time 0.333 (0.998) data 0.000 (0.657) loss 0.4579 (0.4923) acc 81.2500 (83.1250) lr 1.8686e-02 eta 0:24:46\n",
      "epoch [36/200] batch [5/9] time 0.332 (0.724) data 0.000 (0.383) loss 0.6629 (0.6726) acc 75.0000 (75.6250) lr 1.8607e-02 eta 0:17:51\n",
      "epoch [37/200] batch [5/9] time 0.322 (0.681) data 0.000 (0.342) loss 0.5848 (0.5510) acc 84.3750 (85.6250) lr 1.8526e-02 eta 0:16:41\n",
      "epoch [38/200] batch [5/9] time 0.329 (1.028) data 0.000 (0.689) loss 0.9353 (0.6166) acc 68.7500 (82.5000) lr 1.8443e-02 eta 0:25:02\n",
      "epoch [39/200] batch [5/9] time 0.324 (0.739) data 0.000 (0.408) loss 0.2922 (0.5669) acc 96.8750 (84.3750) lr 1.8358e-02 eta 0:17:54\n",
      "epoch [40/200] batch [5/9] time 0.324 (0.690) data 0.000 (0.353) loss 0.6228 (0.6793) acc 78.1250 (81.2500) lr 1.8271e-02 eta 0:16:36\n",
      "epoch [41/200] batch [5/9] time 0.332 (0.971) data 0.000 (0.614) loss 0.2771 (0.5120) acc 93.7500 (85.6250) lr 1.8181e-02 eta 0:23:13\n",
      "epoch [42/200] batch [5/9] time 0.327 (0.747) data 0.000 (0.410) loss 0.7692 (0.5730) acc 71.8750 (82.5000) lr 1.8090e-02 eta 0:17:44\n",
      "epoch [43/200] batch [5/9] time 0.330 (0.747) data 0.000 (0.410) loss 0.4256 (0.4968) acc 84.3750 (81.8750) lr 1.7997e-02 eta 0:17:38\n",
      "epoch [44/200] batch [5/9] time 0.331 (1.039) data 0.000 (0.695) loss 0.4084 (0.4381) acc 84.3750 (88.1250) lr 1.7902e-02 eta 0:24:22\n",
      "epoch [45/200] batch [5/9] time 0.331 (0.742) data 0.000 (0.409) loss 0.4342 (0.6698) acc 90.6250 (78.1250) lr 1.7804e-02 eta 0:17:17\n",
      "epoch [46/200] batch [5/9] time 0.327 (0.744) data 0.000 (0.411) loss 0.7712 (0.5756) acc 81.2500 (86.2500) lr 1.7705e-02 eta 0:17:14\n",
      "epoch [47/200] batch [5/9] time 0.342 (1.052) data 0.000 (0.698) loss 0.8815 (0.6417) acc 68.7500 (83.1250) lr 1.7604e-02 eta 0:24:12\n",
      "epoch [48/200] batch [5/9] time 0.331 (0.739) data 0.000 (0.407) loss 0.4390 (0.4608) acc 87.5000 (88.1250) lr 1.7501e-02 eta 0:16:54\n",
      "epoch [49/200] batch [5/9] time 0.333 (0.732) data 0.000 (0.391) loss 0.3887 (0.4097) acc 87.5000 (90.0000) lr 1.7396e-02 eta 0:16:38\n",
      "epoch [50/200] batch [5/9] time 0.331 (1.041) data 0.000 (0.678) loss 0.7788 (0.6043) acc 71.8750 (81.8750) lr 1.7290e-02 eta 0:23:29\n",
      "epoch [51/200] batch [5/9] time 0.326 (0.746) data 0.000 (0.415) loss 0.4320 (0.4333) acc 87.5000 (85.6250) lr 1.7181e-02 eta 0:16:44\n",
      "epoch [52/200] batch [5/9] time 0.329 (0.741) data 0.000 (0.406) loss 0.6400 (0.5344) acc 87.5000 (85.0000) lr 1.7071e-02 eta 0:16:30\n",
      "epoch [53/200] batch [5/9] time 0.332 (1.051) data 0.000 (0.703) loss 0.6736 (0.6012) acc 84.3750 (82.5000) lr 1.6959e-02 eta 0:23:14\n",
      "epoch [54/200] batch [5/9] time 0.327 (0.799) data 0.000 (0.463) loss 0.4922 (0.5254) acc 84.3750 (84.3750) lr 1.6845e-02 eta 0:17:33\n",
      "epoch [55/200] batch [5/9] time 0.335 (0.741) data 0.000 (0.402) loss 0.4651 (0.4640) acc 84.3750 (87.5000) lr 1.6730e-02 eta 0:16:09\n",
      "epoch [56/200] batch [5/9] time 0.346 (0.976) data 0.000 (0.589) loss 0.4558 (0.5065) acc 81.2500 (83.1250) lr 1.6613e-02 eta 0:21:08\n",
      "epoch [57/200] batch [5/9] time 0.333 (0.707) data 0.000 (0.358) loss 0.5534 (0.4135) acc 84.3750 (88.7500) lr 1.6494e-02 eta 0:15:12\n",
      "epoch [58/200] batch [5/9] time 0.328 (0.735) data 0.000 (0.400) loss 0.4322 (0.5389) acc 90.6250 (83.7500) lr 1.6374e-02 eta 0:15:41\n",
      "epoch [59/200] batch [5/9] time 0.338 (1.047) data 0.000 (0.690) loss 0.4778 (0.4979) acc 81.2500 (83.1250) lr 1.6252e-02 eta 0:22:13\n",
      "epoch [60/200] batch [5/9] time 0.331 (0.723) data 0.000 (0.389) loss 0.5017 (0.4223) acc 84.3750 (88.7500) lr 1.6129e-02 eta 0:15:13\n",
      "epoch [61/200] batch [5/9] time 0.332 (0.726) data 0.000 (0.394) loss 0.3386 (0.4188) acc 87.5000 (88.7500) lr 1.6004e-02 eta 0:15:11\n",
      "epoch [62/200] batch [5/9] time 0.334 (1.029) data 0.000 (0.677) loss 0.4130 (0.3509) acc 87.5000 (91.2500) lr 1.5878e-02 eta 0:21:22\n",
      "epoch [63/200] batch [5/9] time 0.328 (0.712) data 0.000 (0.375) loss 0.3709 (0.3504) acc 93.7500 (92.5000) lr 1.5750e-02 eta 0:14:40\n",
      "epoch [64/200] batch [5/9] time 0.333 (0.718) data 0.000 (0.380) loss 0.3223 (0.4648) acc 87.5000 (88.7500) lr 1.5621e-02 eta 0:14:41\n",
      "epoch [65/200] batch [5/9] time 0.331 (1.048) data 0.000 (0.711) loss 0.6487 (0.4326) acc 68.7500 (85.0000) lr 1.5490e-02 eta 0:21:18\n",
      "epoch [66/200] batch [5/9] time 0.330 (0.713) data 0.000 (0.370) loss 0.2402 (0.4579) acc 96.8750 (86.2500) lr 1.5358e-02 eta 0:14:22\n",
      "epoch [67/200] batch [5/9] time 0.332 (0.710) data 0.000 (0.371) loss 0.2349 (0.4557) acc 96.8750 (86.2500) lr 1.5225e-02 eta 0:14:13\n",
      "epoch [68/200] batch [5/9] time 0.332 (1.065) data 0.000 (0.722) loss 0.8585 (0.4860) acc 81.2500 (86.8750) lr 1.5090e-02 eta 0:21:08\n",
      "epoch [69/200] batch [5/9] time 0.328 (0.698) data 0.000 (0.353) loss 0.3167 (0.3254) acc 90.6250 (90.6250) lr 1.4955e-02 eta 0:13:45\n",
      "epoch [70/200] batch [5/9] time 0.325 (0.749) data 0.000 (0.413) loss 0.4251 (0.4547) acc 87.5000 (83.7500) lr 1.4818e-02 eta 0:14:39\n",
      "epoch [71/200] batch [5/9] time 0.334 (1.037) data 0.001 (0.692) loss 0.6320 (0.4897) acc 87.5000 (86.8750) lr 1.4679e-02 eta 0:20:08\n",
      "epoch [72/200] batch [5/9] time 0.331 (0.752) data 0.000 (0.419) loss 0.4391 (0.4888) acc 87.5000 (83.7500) lr 1.4540e-02 eta 0:14:29\n",
      "epoch [73/200] batch [5/9] time 0.330 (0.711) data 0.000 (0.369) loss 0.4159 (0.4735) acc 87.5000 (87.5000) lr 1.4399e-02 eta 0:13:35\n",
      "epoch [74/200] batch [5/9] time 0.334 (1.074) data 0.000 (0.732) loss 0.4496 (0.4777) acc 90.6250 (87.5000) lr 1.4258e-02 eta 0:20:22\n",
      "epoch [75/200] batch [5/9] time 0.330 (0.729) data 0.000 (0.393) loss 0.4953 (0.3626) acc 84.3750 (91.2500) lr 1.4115e-02 eta 0:13:43\n",
      "epoch [76/200] batch [5/9] time 0.324 (0.685) data 0.000 (0.351) loss 0.3341 (0.4447) acc 90.6250 (86.2500) lr 1.3971e-02 eta 0:12:46\n",
      "epoch [77/200] batch [5/9] time 0.334 (1.006) data 0.000 (0.665) loss 0.3645 (0.5158) acc 90.6250 (83.7500) lr 1.3827e-02 eta 0:18:37\n",
      "epoch [78/200] batch [5/9] time 0.329 (0.713) data 0.000 (0.368) loss 0.3767 (0.3941) acc 90.6250 (90.6250) lr 1.3681e-02 eta 0:13:06\n",
      "epoch [79/200] batch [5/9] time 0.328 (0.730) data 0.000 (0.393) loss 0.3447 (0.4377) acc 87.5000 (85.0000) lr 1.3535e-02 eta 0:13:17\n",
      "epoch [80/200] batch [5/9] time 0.338 (1.037) data 0.000 (0.651) loss 0.4548 (0.4271) acc 84.3750 (87.5000) lr 1.3387e-02 eta 0:18:43\n",
      "epoch [81/200] batch [5/9] time 0.330 (0.778) data 0.000 (0.446) loss 0.5332 (0.5146) acc 81.2500 (83.1250) lr 1.3239e-02 eta 0:13:56\n",
      "epoch [82/200] batch [5/9] time 0.328 (0.666) data 0.000 (0.328) loss 0.2993 (0.6380) acc 87.5000 (81.8750) lr 1.3090e-02 eta 0:11:49\n",
      "epoch [83/200] batch [5/9] time 0.332 (1.029) data 0.000 (0.677) loss 0.6591 (0.4400) acc 84.3750 (84.3750) lr 1.2940e-02 eta 0:18:08\n",
      "epoch [84/200] batch [5/9] time 0.329 (0.702) data 0.000 (0.369) loss 0.3485 (0.5322) acc 90.6250 (84.3750) lr 1.2790e-02 eta 0:12:16\n",
      "epoch [85/200] batch [5/9] time 0.328 (0.689) data 0.000 (0.345) loss 0.4123 (0.4628) acc 90.6250 (88.7500) lr 1.2639e-02 eta 0:11:56\n",
      "epoch [86/200] batch [5/9] time 0.331 (0.996) data 0.000 (0.660) loss 0.2692 (0.3881) acc 93.7500 (90.6250) lr 1.2487e-02 eta 0:17:06\n",
      "epoch [87/200] batch [5/9] time 0.330 (0.695) data 0.000 (0.350) loss 0.5721 (0.4423) acc 87.5000 (90.0000) lr 1.2334e-02 eta 0:11:49\n",
      "epoch [88/200] batch [5/9] time 0.328 (0.737) data 0.000 (0.404) loss 0.4185 (0.3720) acc 84.3750 (89.3750) lr 1.2181e-02 eta 0:12:25\n",
      "epoch [89/200] batch [5/9] time 0.331 (1.060) data 0.000 (0.719) loss 0.1195 (0.3820) acc 100.0000 (90.0000) lr 1.2028e-02 eta 0:17:43\n",
      "epoch [90/200] batch [5/9] time 0.327 (0.742) data 0.000 (0.411) loss 0.4672 (0.4395) acc 87.5000 (86.2500) lr 1.1874e-02 eta 0:12:17\n",
      "epoch [91/200] batch [5/9] time 0.329 (0.739) data 0.000 (0.404) loss 0.3147 (0.4576) acc 90.6250 (85.6250) lr 1.1719e-02 eta 0:12:07\n",
      "epoch [92/200] batch [5/9] time 0.332 (0.985) data 0.000 (0.643) loss 0.5906 (0.5588) acc 81.2500 (82.5000) lr 1.1564e-02 eta 0:16:01\n",
      "epoch [93/200] batch [5/9] time 0.328 (0.729) data 0.000 (0.395) loss 0.5862 (0.5051) acc 87.5000 (86.2500) lr 1.1409e-02 eta 0:11:44\n",
      "epoch [94/200] batch [5/9] time 0.329 (0.735) data 0.000 (0.398) loss 0.9930 (0.6502) acc 71.8750 (81.2500) lr 1.1253e-02 eta 0:11:43\n",
      "epoch [95/200] batch [5/9] time 0.333 (1.038) data 0.000 (0.696) loss 0.4271 (0.3379) acc 84.3750 (90.6250) lr 1.1097e-02 eta 0:16:24\n",
      "epoch [96/200] batch [5/9] time 0.328 (0.732) data 0.000 (0.396) loss 0.3890 (0.4657) acc 90.6250 (85.0000) lr 1.0941e-02 eta 0:11:28\n",
      "epoch [97/200] batch [5/9] time 0.330 (0.682) data 0.000 (0.347) loss 0.5612 (0.4554) acc 78.1250 (84.3750) lr 1.0785e-02 eta 0:10:34\n",
      "epoch [98/200] batch [5/9] time 0.339 (0.980) data 0.000 (0.622) loss 0.4420 (0.3829) acc 87.5000 (90.6250) lr 1.0628e-02 eta 0:15:03\n",
      "epoch [99/200] batch [5/9] time 0.336 (0.710) data 0.000 (0.361) loss 0.3856 (0.5365) acc 90.6250 (85.0000) lr 1.0471e-02 eta 0:10:48\n",
      "epoch [100/200] batch [5/9] time 0.329 (0.713) data 0.000 (0.375) loss 0.2830 (0.2980) acc 96.8750 (92.5000) lr 1.0314e-02 eta 0:10:44\n",
      "epoch [101/200] batch [5/9] time 0.332 (1.007) data 0.000 (0.672) loss 0.6333 (0.4245) acc 81.2500 (86.8750) lr 1.0157e-02 eta 0:15:01\n",
      "epoch [102/200] batch [5/9] time 0.332 (0.743) data 0.000 (0.405) loss 0.3692 (0.4699) acc 81.2500 (84.3750) lr 1.0000e-02 eta 0:10:58\n",
      "epoch [103/200] batch [5/9] time 0.329 (0.770) data 0.000 (0.436) loss 0.1978 (0.4068) acc 96.8750 (87.5000) lr 9.8429e-03 eta 0:11:14\n",
      "epoch [104/200] batch [5/9] time 0.329 (1.047) data 0.000 (0.710) loss 0.7128 (0.4491) acc 81.2500 (88.1250) lr 9.6859e-03 eta 0:15:08\n",
      "epoch [105/200] batch [5/9] time 0.331 (0.742) data 0.000 (0.407) loss 0.3491 (0.5061) acc 84.3750 (81.2500) lr 9.5289e-03 eta 0:10:37\n",
      "epoch [106/200] batch [5/9] time 0.329 (0.737) data 0.000 (0.401) loss 0.6342 (0.5076) acc 81.2500 (84.3750) lr 9.3721e-03 eta 0:10:26\n",
      "epoch [107/200] batch [5/9] time 0.343 (1.051) data 0.000 (0.661) loss 0.5024 (0.3542) acc 87.5000 (89.3750) lr 9.2154e-03 eta 0:14:43\n",
      "epoch [108/200] batch [5/9] time 0.335 (0.740) data 0.000 (0.404) loss 0.3109 (0.4049) acc 93.7500 (87.5000) lr 9.0589e-03 eta 0:10:15\n",
      "epoch [109/200] batch [5/9] time 0.329 (0.685) data 0.000 (0.344) loss 0.6143 (0.4385) acc 84.3750 (86.2500) lr 8.9027e-03 eta 0:09:23\n",
      "epoch [110/200] batch [5/9] time 0.332 (1.001) data 0.000 (0.663) loss 0.4778 (0.3388) acc 87.5000 (91.8750) lr 8.7467e-03 eta 0:13:34\n",
      "epoch [111/200] batch [5/9] time 0.331 (0.758) data 0.000 (0.424) loss 0.2690 (0.4378) acc 96.8750 (88.1250) lr 8.5910e-03 eta 0:10:09\n",
      "epoch [112/200] batch [5/9] time 0.328 (0.718) data 0.000 (0.384) loss 0.2495 (0.3767) acc 90.6250 (89.3750) lr 8.4357e-03 eta 0:09:31\n",
      "epoch [113/200] batch [5/9] time 0.328 (1.042) data 0.000 (0.700) loss 0.3264 (0.4208) acc 87.5000 (88.1250) lr 8.2807e-03 eta 0:13:40\n",
      "epoch [114/200] batch [5/9] time 0.335 (0.716) data 0.000 (0.375) loss 0.4336 (0.4788) acc 84.3750 (83.7500) lr 8.1262e-03 eta 0:09:17\n",
      "epoch [115/200] batch [5/9] time 0.330 (0.745) data 0.000 (0.409) loss 0.6826 (0.5117) acc 81.2500 (85.6250) lr 7.9721e-03 eta 0:09:32\n",
      "epoch [116/200] batch [5/9] time 0.330 (1.025) data 0.000 (0.681) loss 0.3240 (0.3819) acc 90.6250 (89.3750) lr 7.8186e-03 eta 0:12:59\n",
      "epoch [117/200] batch [5/9] time 0.329 (0.748) data 0.000 (0.414) loss 0.2052 (0.4180) acc 96.8750 (89.3750) lr 7.6655e-03 eta 0:09:21\n",
      "epoch [118/200] batch [5/9] time 0.328 (0.728) data 0.000 (0.390) loss 0.4406 (0.3643) acc 90.6250 (88.7500) lr 7.5131e-03 eta 0:09:00\n",
      "epoch [119/200] batch [5/9] time 0.332 (1.038) data 0.000 (0.660) loss 0.3031 (0.3953) acc 90.6250 (91.2500) lr 7.3613e-03 eta 0:12:41\n",
      "epoch [120/200] batch [5/9] time 0.327 (0.741) data 0.000 (0.408) loss 0.6693 (0.5096) acc 84.3750 (85.6250) lr 7.2101e-03 eta 0:08:56\n",
      "epoch [121/200] batch [5/9] time 0.333 (0.732) data 0.000 (0.394) loss 0.3813 (0.3320) acc 87.5000 (89.3750) lr 7.0596e-03 eta 0:08:43\n",
      "epoch [122/200] batch [5/9] time 0.340 (0.988) data 0.000 (0.611) loss 0.3417 (0.3871) acc 90.6250 (88.7500) lr 6.9098e-03 eta 0:11:37\n",
      "epoch [123/200] batch [5/9] time 0.335 (0.745) data 0.000 (0.410) loss 0.5966 (0.3884) acc 78.1250 (88.7500) lr 6.7608e-03 eta 0:08:39\n",
      "epoch [124/200] batch [5/9] time 0.331 (0.710) data 0.000 (0.369) loss 0.2318 (0.3549) acc 93.7500 (90.0000) lr 6.6126e-03 eta 0:08:08\n",
      "epoch [125/200] batch [5/9] time 0.330 (1.037) data 0.000 (0.697) loss 0.5882 (0.4622) acc 87.5000 (87.5000) lr 6.4653e-03 eta 0:11:44\n",
      "epoch [126/200] batch [5/9] time 0.329 (0.704) data 0.000 (0.364) loss 0.2562 (0.4590) acc 93.7500 (86.2500) lr 6.3188e-03 eta 0:07:51\n",
      "epoch [127/200] batch [5/9] time 0.327 (0.725) data 0.000 (0.392) loss 0.4320 (0.4074) acc 87.5000 (86.8750) lr 6.1732e-03 eta 0:07:59\n",
      "epoch [128/200] batch [5/9] time 0.335 (1.048) data 0.000 (0.709) loss 0.2548 (0.4664) acc 93.7500 (89.3750) lr 6.0285e-03 eta 0:11:23\n",
      "epoch [129/200] batch [5/9] time 0.330 (0.722) data 0.000 (0.382) loss 0.2695 (0.4431) acc 96.8750 (88.1250) lr 5.8849e-03 eta 0:07:44\n",
      "epoch [130/200] batch [5/9] time 0.330 (0.783) data 0.000 (0.450) loss 0.4191 (0.3731) acc 84.3750 (88.7500) lr 5.7422e-03 eta 0:08:16\n",
      "epoch [131/200] batch [5/9] time 0.329 (1.054) data 0.000 (0.690) loss 0.2685 (0.4125) acc 87.5000 (87.5000) lr 5.6006e-03 eta 0:10:58\n",
      "epoch [132/200] batch [5/9] time 0.329 (0.736) data 0.000 (0.401) loss 0.3320 (0.4220) acc 90.6250 (86.8750) lr 5.4601e-03 eta 0:07:33\n",
      "epoch [133/200] batch [5/9] time 0.334 (0.734) data 0.000 (0.400) loss 0.3844 (0.4003) acc 90.6250 (89.3750) lr 5.3207e-03 eta 0:07:25\n",
      "epoch [134/200] batch [5/9] time 0.336 (1.005) data 0.000 (0.637) loss 0.4076 (0.3784) acc 87.5000 (87.5000) lr 5.1825e-03 eta 0:10:00\n",
      "epoch [135/200] batch [5/9] time 0.329 (0.804) data 0.000 (0.470) loss 0.3831 (0.3303) acc 93.7500 (93.7500) lr 5.0454e-03 eta 0:07:53\n",
      "epoch [136/200] batch [5/9] time 0.333 (0.710) data 0.000 (0.373) loss 0.2116 (0.2763) acc 93.7500 (93.1250) lr 4.9096e-03 eta 0:06:51\n",
      "epoch [137/200] batch [5/9] time 0.336 (1.047) data 0.000 (0.704) loss 0.2381 (0.3512) acc 87.5000 (86.2500) lr 4.7750e-03 eta 0:09:57\n",
      "epoch [138/200] batch [5/9] time 0.326 (0.739) data 0.000 (0.402) loss 0.1595 (0.3569) acc 96.8750 (86.8750) lr 4.6417e-03 eta 0:06:55\n",
      "epoch [139/200] batch [5/9] time 0.337 (0.699) data 0.000 (0.364) loss 0.2629 (0.3999) acc 87.5000 (88.1250) lr 4.5098e-03 eta 0:06:26\n",
      "epoch [140/200] batch [5/9] time 0.332 (1.024) data 0.000 (0.684) loss 0.3745 (0.3336) acc 90.6250 (91.2500) lr 4.3792e-03 eta 0:09:17\n",
      "epoch [141/200] batch [5/9] time 0.327 (0.689) data 0.000 (0.351) loss 0.5789 (0.3699) acc 75.0000 (89.3750) lr 4.2499e-03 eta 0:06:08\n",
      "epoch [142/200] batch [5/9] time 0.333 (0.706) data 0.000 (0.366) loss 0.3013 (0.4826) acc 90.6250 (87.5000) lr 4.1221e-03 eta 0:06:11\n",
      "epoch [143/200] batch [5/9] time 0.330 (1.014) data 0.000 (0.675) loss 0.3231 (0.3606) acc 87.5000 (89.3750) lr 3.9958e-03 eta 0:08:44\n",
      "epoch [144/200] batch [5/9] time 0.322 (0.718) data 0.000 (0.381) loss 0.4471 (0.3511) acc 84.3750 (88.7500) lr 3.8709e-03 eta 0:06:04\n",
      "epoch [145/200] batch [5/9] time 0.336 (0.739) data 0.000 (0.401) loss 0.4082 (0.3454) acc 87.5000 (91.8750) lr 3.7476e-03 eta 0:06:08\n",
      "epoch [146/200] batch [5/9] time 0.332 (1.043) data 0.000 (0.706) loss 0.4160 (0.3677) acc 90.6250 (90.6250) lr 3.6258e-03 eta 0:08:30\n",
      "epoch [147/200] batch [5/9] time 0.332 (0.746) data 0.000 (0.409) loss 0.5261 (0.4546) acc 81.2500 (87.5000) lr 3.5055e-03 eta 0:05:58\n",
      "epoch [148/200] batch [5/9] time 0.330 (0.730) data 0.000 (0.393) loss 0.7092 (0.5282) acc 81.2500 (85.0000) lr 3.3869e-03 eta 0:05:44\n",
      "epoch [149/200] batch [5/9] time 0.328 (1.033) data 0.000 (0.691) loss 0.3145 (0.4043) acc 90.6250 (85.6250) lr 3.2699e-03 eta 0:07:58\n",
      "epoch [150/200] batch [5/9] time 0.327 (0.743) data 0.000 (0.410) loss 0.3061 (0.3849) acc 90.6250 (90.6250) lr 3.1545e-03 eta 0:05:37\n",
      "epoch [151/200] batch [5/9] time 0.337 (0.739) data 0.000 (0.399) loss 0.2743 (0.3292) acc 93.7500 (90.6250) lr 3.0409e-03 eta 0:05:28\n",
      "epoch [152/200] batch [5/9] time 0.327 (1.039) data 0.000 (0.700) loss 0.4693 (0.4740) acc 87.5000 (85.6250) lr 2.9289e-03 eta 0:07:33\n",
      "epoch [153/200] batch [5/9] time 0.330 (0.711) data 0.000 (0.363) loss 0.4437 (0.4288) acc 81.2500 (86.8750) lr 2.8187e-03 eta 0:05:03\n",
      "epoch [154/200] batch [5/9] time 0.336 (0.733) data 0.000 (0.396) loss 0.3992 (0.3278) acc 87.5000 (88.7500) lr 2.7103e-03 eta 0:05:06\n",
      "epoch [155/200] batch [5/9] time 0.326 (1.021) data 0.000 (0.670) loss 0.2151 (0.4389) acc 93.7500 (86.8750) lr 2.6037e-03 eta 0:06:57\n",
      "epoch [156/200] batch [5/9] time 0.328 (0.743) data 0.000 (0.412) loss 0.2521 (0.3905) acc 93.7500 (89.3750) lr 2.4989e-03 eta 0:04:57\n",
      "epoch [157/200] batch [5/9] time 0.329 (0.683) data 0.000 (0.341) loss 0.5462 (0.4816) acc 84.3750 (85.0000) lr 2.3959e-03 eta 0:04:27\n",
      "epoch [158/200] batch [5/9] time 0.327 (1.061) data 0.000 (0.722) loss 0.5091 (0.4175) acc 84.3750 (86.8750) lr 2.2949e-03 eta 0:06:45\n",
      "epoch [159/200] batch [5/9] time 0.329 (0.751) data 0.000 (0.415) loss 0.4339 (0.4078) acc 90.6250 (90.6250) lr 2.1957e-03 eta 0:04:40\n",
      "epoch [160/200] batch [5/9] time 0.328 (0.731) data 0.000 (0.398) loss 0.4888 (0.3209) acc 78.1250 (90.0000) lr 2.0984e-03 eta 0:04:26\n",
      "epoch [161/200] batch [5/9] time 0.333 (1.077) data 0.000 (0.737) loss 0.3492 (0.3429) acc 93.7500 (90.0000) lr 2.0032e-03 eta 0:06:22\n",
      "epoch [162/200] batch [5/9] time 0.327 (0.746) data 0.000 (0.403) loss 0.2640 (0.3470) acc 93.7500 (90.6250) lr 1.9098e-03 eta 0:04:18\n",
      "epoch [163/200] batch [5/9] time 0.331 (0.724) data 0.000 (0.384) loss 0.3328 (0.3913) acc 90.6250 (89.3750) lr 1.8185e-03 eta 0:04:03\n",
      "epoch [164/200] batch [5/9] time 0.332 (1.046) data 0.000 (0.704) loss 0.2342 (0.3351) acc 96.8750 (91.2500) lr 1.7292e-03 eta 0:05:42\n",
      "epoch [165/200] batch [5/9] time 0.332 (0.750) data 0.000 (0.418) loss 0.2296 (0.3410) acc 96.8750 (91.2500) lr 1.6419e-03 eta 0:03:59\n",
      "epoch [166/200] batch [5/9] time 0.332 (0.720) data 0.000 (0.378) loss 0.4632 (0.3745) acc 84.3750 (89.3750) lr 1.5567e-03 eta 0:03:43\n",
      "epoch [167/200] batch [5/9] time 0.339 (1.017) data 0.000 (0.636) loss 0.1319 (0.4346) acc 100.0000 (88.1250) lr 1.4736e-03 eta 0:05:05\n",
      "epoch [168/200] batch [5/9] time 0.330 (0.749) data 0.000 (0.416) loss 0.4900 (0.3928) acc 87.5000 (88.1250) lr 1.3926e-03 eta 0:03:38\n",
      "epoch [169/200] batch [5/9] time 0.328 (0.711) data 0.000 (0.365) loss 0.2322 (0.4313) acc 96.8750 (86.8750) lr 1.3137e-03 eta 0:03:21\n",
      "epoch [170/200] batch [5/9] time 0.331 (1.025) data 0.000 (0.685) loss 0.3230 (0.4113) acc 90.6250 (86.8750) lr 1.2369e-03 eta 0:04:40\n",
      "epoch [171/200] batch [5/9] time 0.329 (0.729) data 0.000 (0.393) loss 0.4681 (0.4525) acc 81.2500 (85.0000) lr 1.1623e-03 eta 0:03:13\n",
      "epoch [172/200] batch [5/9] time 0.331 (0.721) data 0.000 (0.376) loss 0.2188 (0.2918) acc 93.7500 (92.5000) lr 1.0899e-03 eta 0:03:04\n",
      "epoch [173/200] batch [5/9] time 0.331 (1.062) data 0.000 (0.725) loss 0.3189 (0.3204) acc 87.5000 (88.1250) lr 1.0197e-03 eta 0:04:22\n",
      "epoch [174/200] batch [5/9] time 0.330 (0.733) data 0.000 (0.399) loss 0.3670 (0.3804) acc 93.7500 (90.6250) lr 9.5173e-04 eta 0:02:54\n",
      "epoch [175/200] batch [5/9] time 0.326 (0.734) data 0.001 (0.400) loss 0.1536 (0.4577) acc 96.8750 (88.7500) lr 8.8597e-04 eta 0:02:48\n",
      "epoch [176/200] batch [5/9] time 0.332 (1.031) data 0.000 (0.667) loss 0.4710 (0.3655) acc 90.6250 (91.2500) lr 8.2245e-04 eta 0:03:46\n",
      "epoch [177/200] batch [5/9] time 0.333 (0.732) data 0.000 (0.397) loss 0.5325 (0.3996) acc 84.3750 (86.2500) lr 7.6120e-04 eta 0:02:34\n",
      "epoch [178/200] batch [5/9] time 0.334 (0.753) data 0.000 (0.416) loss 0.5984 (0.4570) acc 81.2500 (86.8750) lr 7.0224e-04 eta 0:02:32\n",
      "epoch [179/200] batch [5/9] time 0.339 (1.055) data 0.000 (0.705) loss 0.2847 (0.3382) acc 93.7500 (90.6250) lr 6.4556e-04 eta 0:03:23\n",
      "epoch [180/200] batch [5/9] time 0.325 (0.743) data 0.000 (0.407) loss 0.4429 (0.3885) acc 84.3750 (88.1250) lr 5.9119e-04 eta 0:02:16\n",
      "epoch [181/200] batch [5/9] time 0.332 (0.722) data 0.000 (0.388) loss 0.5793 (0.3895) acc 84.3750 (90.0000) lr 5.3915e-04 eta 0:02:06\n",
      "epoch [182/200] batch [5/9] time 0.330 (1.061) data 0.001 (0.722) loss 0.3601 (0.3124) acc 87.5000 (91.2500) lr 4.8943e-04 eta 0:02:56\n",
      "epoch [183/200] batch [5/9] time 0.328 (0.713) data 0.000 (0.380) loss 0.3076 (0.3209) acc 90.6250 (91.8750) lr 4.4207e-04 eta 0:01:51\n",
      "epoch [184/200] batch [5/9] time 0.332 (0.743) data 0.005 (0.407) loss 0.3699 (0.3544) acc 90.6250 (90.6250) lr 3.9706e-04 eta 0:01:50\n",
      "epoch [185/200] batch [5/9] time 0.328 (1.069) data 0.000 (0.728) loss 0.4612 (0.3685) acc 84.3750 (89.3750) lr 3.5443e-04 eta 0:02:28\n",
      "epoch [186/200] batch [5/9] time 0.329 (0.718) data 0.000 (0.373) loss 0.2834 (0.3799) acc 93.7500 (89.3750) lr 3.1417e-04 eta 0:01:33\n",
      "epoch [187/200] batch [5/9] time 0.333 (0.737) data 0.000 (0.403) loss 0.5320 (0.3804) acc 84.3750 (89.3750) lr 2.7630e-04 eta 0:01:29\n",
      "epoch [188/200] batch [5/9] time 0.329 (1.063) data 0.000 (0.725) loss 0.3240 (0.4651) acc 93.7500 (84.3750) lr 2.4083e-04 eta 0:01:59\n",
      "epoch [189/200] batch [5/9] time 0.328 (0.756) data 0.000 (0.423) loss 0.3472 (0.4207) acc 90.6250 (86.8750) lr 2.0777e-04 eta 0:01:17\n",
      "epoch [190/200] batch [5/9] time 0.328 (0.738) data 0.000 (0.405) loss 0.3384 (0.3394) acc 84.3750 (91.2500) lr 1.7713e-04 eta 0:01:09\n",
      "epoch [191/200] batch [5/9] time 0.334 (0.975) data 0.000 (0.617) loss 0.4145 (0.3695) acc 90.6250 (89.3750) lr 1.4891e-04 eta 0:01:22\n",
      "epoch [192/200] batch [5/9] time 0.329 (0.725) data 0.000 (0.389) loss 0.4355 (0.3424) acc 90.6250 (91.8750) lr 1.2312e-04 eta 0:00:55\n",
      "epoch [193/200] batch [5/9] time 0.326 (0.734) data 0.000 (0.399) loss 0.3821 (0.4025) acc 90.6250 (89.3750) lr 9.9763e-05 eta 0:00:49\n",
      "epoch [194/200] batch [5/9] time 0.334 (1.044) data 0.000 (0.693) loss 0.2582 (0.2904) acc 90.6250 (91.2500) lr 7.8853e-05 eta 0:01:00\n",
      "epoch [195/200] batch [5/9] time 0.331 (0.754) data 0.000 (0.420) loss 0.3259 (0.3867) acc 93.7500 (86.8750) lr 6.0390e-05 eta 0:00:36\n",
      "epoch [196/200] batch [5/9] time 0.333 (0.738) data 0.000 (0.400) loss 0.5178 (0.3545) acc 84.3750 (90.6250) lr 4.4380e-05 eta 0:00:29\n",
      "epoch [197/200] batch [5/9] time 0.328 (1.009) data 0.000 (0.659) loss 0.5383 (0.4963) acc 84.3750 (85.0000) lr 3.0827e-05 eta 0:00:31\n",
      "epoch [198/200] batch [5/9] time 0.329 (0.731) data 0.000 (0.398) loss 0.2859 (0.3306) acc 90.6250 (90.0000) lr 1.9733e-05 eta 0:00:16\n",
      "epoch [199/200] batch [5/9] time 0.328 (0.739) data 0.000 (0.404) loss 0.4346 (0.3542) acc 84.3750 (90.6250) lr 1.1101e-05 eta 0:00:09\n",
      "epoch [200/200] batch [5/9] time 0.331 (1.042) data 0.000 (0.703) loss 0.5159 (0.4108) acc 87.5000 (88.7500) lr 4.9344e-06 eta 0:00:04\n",
      "Checkpoint saved to output/oxford_pets/DAPT/vit_b16_8shots/seed1/prompt_learner/model.pth.tar-200\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 37/37 [00:42<00:00,  1.14s/it]\n",
      "=> result\n",
      "* total: 3,669\n",
      "* correct: 3,376\n",
      "* accuracy: 92.0%\n",
      "* error: 8.0%\n",
      "* macro_f1: 92.0%\n",
      "Elapsed: 0:19:44\n"
     ]
    }
   ],
   "source": [
    "#oxford_pets-8shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "        --output-dir output/oxford_pets/DAPT/vit_b16_8shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFIrtg4_yP3b",
    "outputId": "43f5dab6-00f9-4786-f5d9-18412ff0ed7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:02:46.870030: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:02:46.889281: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:02:46.895117: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:02:46.909411: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:02:47.927895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '4']\n",
      "output_dir: output/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordPets\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 4\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.02\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 0.1\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: OxfordPets\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_4-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  ----------\n",
      "Dataset    OxfordPets\n",
      "# classes  37\n",
      "# train_x  148\n",
      "# val      148\n",
      "# test     3,669\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1/tensorboard)\n",
      "epoch [1/100] batch [1/4] time 2.630 (2.630) data 1.201 (1.201) loss 3.4911 (3.4911) acc 9.3750 (9.3750) lr 1.0000e-05 eta 0:17:29\n",
      "epoch [1/100] batch [2/4] time 0.317 (1.474) data 0.001 (0.601) loss 2.7860 (3.1386) acc 31.2500 (20.3125) lr 1.0000e-05 eta 0:09:46\n",
      "epoch [1/100] batch [3/4] time 0.318 (1.088) data 0.000 (0.400) loss 2.9930 (3.0901) acc 21.8750 (20.8333) lr 1.0000e-05 eta 0:07:12\n",
      "epoch [1/100] batch [4/4] time 0.343 (0.902) data 0.000 (0.300) loss 3.0790 (3.0873) acc 15.6250 (19.5312) lr 2.0000e-02 eta 0:05:57\n",
      "epoch [2/100] batch [1/4] time 1.406 (1.406) data 1.091 (1.091) loss 2.9815 (2.9815) acc 18.7500 (18.7500) lr 2.0000e-02 eta 0:09:15\n",
      "epoch [2/100] batch [2/4] time 0.320 (0.863) data 0.001 (0.546) loss 2.8219 (2.9017) acc 31.2500 (25.0000) lr 2.0000e-02 eta 0:05:40\n",
      "epoch [2/100] batch [3/4] time 0.332 (0.686) data 0.000 (0.364) loss 2.5242 (2.7759) acc 37.5000 (29.1667) lr 2.0000e-02 eta 0:04:29\n",
      "epoch [2/100] batch [4/4] time 0.326 (0.596) data 0.000 (0.273) loss 0.7713 (2.2747) acc 78.1250 (41.4062) lr 1.9995e-02 eta 0:03:53\n",
      "epoch [3/100] batch [1/4] time 1.405 (1.405) data 1.088 (1.088) loss 1.2590 (1.2590) acc 68.7500 (68.7500) lr 1.9995e-02 eta 0:09:09\n",
      "epoch [3/100] batch [2/4] time 0.320 (0.862) data 0.001 (0.544) loss 1.3474 (1.3032) acc 65.6250 (67.1875) lr 1.9995e-02 eta 0:05:36\n",
      "epoch [3/100] batch [3/4] time 0.331 (0.685) data 0.000 (0.363) loss 1.0984 (1.2349) acc 62.5000 (65.6250) lr 1.9995e-02 eta 0:04:26\n",
      "epoch [3/100] batch [4/4] time 0.330 (0.596) data 0.000 (0.272) loss 1.0104 (1.1788) acc 78.1250 (68.7500) lr 1.9980e-02 eta 0:03:51\n",
      "epoch [4/100] batch [1/4] time 2.155 (2.155) data 1.799 (1.799) loss 1.5478 (1.5478) acc 62.5000 (62.5000) lr 1.9980e-02 eta 0:13:53\n",
      "epoch [4/100] batch [2/4] time 0.321 (1.238) data 0.001 (0.900) loss 1.0561 (1.3019) acc 68.7500 (65.6250) lr 1.9980e-02 eta 0:07:57\n",
      "epoch [4/100] batch [3/4] time 0.321 (0.932) data 0.000 (0.600) loss 1.0780 (1.2273) acc 71.8750 (67.7083) lr 1.9980e-02 eta 0:05:58\n",
      "epoch [4/100] batch [4/4] time 0.346 (0.786) data 0.000 (0.450) loss 0.8416 (1.1309) acc 71.8750 (68.7500) lr 1.9956e-02 eta 0:05:01\n",
      "epoch [5/100] batch [1/4] time 1.484 (1.484) data 1.166 (1.166) loss 0.6809 (0.6809) acc 78.1250 (78.1250) lr 1.9956e-02 eta 0:09:28\n",
      "epoch [5/100] batch [2/4] time 0.321 (0.902) data 0.000 (0.583) loss 0.8289 (0.7549) acc 68.7500 (73.4375) lr 1.9956e-02 eta 0:05:44\n",
      "epoch [5/100] batch [3/4] time 0.342 (0.716) data 0.000 (0.389) loss 1.0322 (0.8473) acc 71.8750 (72.9167) lr 1.9956e-02 eta 0:04:32\n",
      "epoch [5/100] batch [4/4] time 0.326 (0.618) data 0.000 (0.292) loss 0.6794 (0.8053) acc 84.3750 (75.7812) lr 1.9921e-02 eta 0:03:54\n",
      "epoch [6/100] batch [1/4] time 1.430 (1.430) data 1.112 (1.112) loss 1.0336 (1.0336) acc 65.6250 (65.6250) lr 1.9921e-02 eta 0:09:01\n",
      "epoch [6/100] batch [2/4] time 0.321 (0.875) data 0.000 (0.556) loss 1.1389 (1.0862) acc 71.8750 (68.7500) lr 1.9921e-02 eta 0:05:30\n",
      "epoch [6/100] batch [3/4] time 0.343 (0.698) data 0.000 (0.371) loss 0.8487 (1.0070) acc 78.1250 (71.8750) lr 1.9921e-02 eta 0:04:23\n",
      "epoch [6/100] batch [4/4] time 0.330 (0.606) data 0.000 (0.278) loss 0.6552 (0.9191) acc 87.5000 (75.7812) lr 1.9877e-02 eta 0:03:47\n",
      "epoch [7/100] batch [1/4] time 1.381 (1.381) data 1.060 (1.060) loss 0.9695 (0.9695) acc 75.0000 (75.0000) lr 1.9877e-02 eta 0:08:37\n",
      "epoch [7/100] batch [2/4] time 0.322 (0.851) data 0.001 (0.530) loss 0.4832 (0.7263) acc 84.3750 (79.6875) lr 1.9877e-02 eta 0:05:18\n",
      "epoch [7/100] batch [3/4] time 0.339 (0.681) data 0.000 (0.354) loss 0.7963 (0.7497) acc 78.1250 (79.1667) lr 1.9877e-02 eta 0:04:13\n",
      "epoch [7/100] batch [4/4] time 0.329 (0.593) data 0.000 (0.265) loss 0.8562 (0.7763) acc 75.0000 (78.1250) lr 1.9823e-02 eta 0:03:40\n",
      "epoch [8/100] batch [1/4] time 1.426 (1.426) data 1.107 (1.107) loss 0.6740 (0.6740) acc 78.1250 (78.1250) lr 1.9823e-02 eta 0:08:49\n",
      "epoch [8/100] batch [2/4] time 0.323 (0.875) data 0.000 (0.554) loss 0.8408 (0.7574) acc 75.0000 (76.5625) lr 1.9823e-02 eta 0:05:23\n",
      "epoch [8/100] batch [3/4] time 0.343 (0.697) data 0.000 (0.369) loss 0.7118 (0.7422) acc 81.2500 (78.1250) lr 1.9823e-02 eta 0:04:17\n",
      "epoch [8/100] batch [4/4] time 0.327 (0.605) data 0.000 (0.277) loss 0.8982 (0.7812) acc 71.8750 (76.5625) lr 1.9759e-02 eta 0:03:42\n",
      "epoch [9/100] batch [1/4] time 2.275 (2.275) data 1.917 (1.917) loss 0.3271 (0.3271) acc 90.6250 (90.6250) lr 1.9759e-02 eta 0:13:54\n",
      "epoch [9/100] batch [2/4] time 0.328 (1.301) data 0.006 (0.961) loss 0.7413 (0.5342) acc 78.1250 (84.3750) lr 1.9759e-02 eta 0:07:56\n",
      "epoch [9/100] batch [3/4] time 0.323 (0.975) data 0.000 (0.641) loss 0.6602 (0.5762) acc 81.2500 (83.3333) lr 1.9759e-02 eta 0:05:56\n",
      "epoch [9/100] batch [4/4] time 0.347 (0.818) data 0.000 (0.481) loss 0.9571 (0.6714) acc 84.3750 (83.5938) lr 1.9686e-02 eta 0:04:57\n",
      "epoch [10/100] batch [1/4] time 1.571 (1.571) data 1.252 (1.252) loss 0.4743 (0.4743) acc 87.5000 (87.5000) lr 1.9686e-02 eta 0:09:30\n",
      "epoch [10/100] batch [2/4] time 0.322 (0.946) data 0.001 (0.626) loss 0.7494 (0.6119) acc 71.8750 (79.6875) lr 1.9686e-02 eta 0:05:42\n",
      "epoch [10/100] batch [3/4] time 0.345 (0.746) data 0.000 (0.418) loss 0.8056 (0.6764) acc 78.1250 (79.1667) lr 1.9686e-02 eta 0:04:29\n",
      "epoch [10/100] batch [4/4] time 0.329 (0.642) data 0.000 (0.313) loss 0.5746 (0.6510) acc 78.1250 (78.9062) lr 1.9603e-02 eta 0:03:51\n",
      "epoch [11/100] batch [1/4] time 1.414 (1.414) data 1.093 (1.093) loss 0.8162 (0.8162) acc 71.8750 (71.8750) lr 1.9603e-02 eta 0:08:27\n",
      "epoch [11/100] batch [2/4] time 0.324 (0.869) data 0.000 (0.546) loss 0.5354 (0.6758) acc 84.3750 (78.1250) lr 1.9603e-02 eta 0:05:11\n",
      "epoch [11/100] batch [3/4] time 0.342 (0.693) data 0.000 (0.364) loss 0.5749 (0.6422) acc 87.5000 (81.2500) lr 1.9603e-02 eta 0:04:07\n",
      "epoch [11/100] batch [4/4] time 0.330 (0.602) data 0.000 (0.273) loss 0.4421 (0.5922) acc 84.3750 (82.0312) lr 1.9511e-02 eta 0:03:34\n",
      "epoch [12/100] batch [1/4] time 1.402 (1.402) data 1.085 (1.085) loss 0.6169 (0.6169) acc 78.1250 (78.1250) lr 1.9511e-02 eta 0:08:17\n",
      "epoch [12/100] batch [2/4] time 0.323 (0.862) data 0.000 (0.543) loss 0.7911 (0.7040) acc 75.0000 (76.5625) lr 1.9511e-02 eta 0:05:05\n",
      "epoch [12/100] batch [3/4] time 0.345 (0.690) data 0.000 (0.362) loss 0.6617 (0.6899) acc 81.2500 (78.1250) lr 1.9511e-02 eta 0:04:03\n",
      "epoch [12/100] batch [4/4] time 0.327 (0.599) data 0.000 (0.271) loss 0.6510 (0.6802) acc 81.2500 (78.9062) lr 1.9409e-02 eta 0:03:30\n",
      "epoch [13/100] batch [1/4] time 1.414 (1.414) data 1.098 (1.098) loss 0.7276 (0.7276) acc 75.0000 (75.0000) lr 1.9409e-02 eta 0:08:16\n",
      "epoch [13/100] batch [2/4] time 0.326 (0.870) data 0.001 (0.549) loss 0.6465 (0.6870) acc 78.1250 (76.5625) lr 1.9409e-02 eta 0:05:04\n",
      "epoch [13/100] batch [3/4] time 0.337 (0.692) data 0.000 (0.366) loss 0.7412 (0.7051) acc 78.1250 (77.0833) lr 1.9409e-02 eta 0:04:01\n",
      "epoch [13/100] batch [4/4] time 0.330 (0.602) data 0.000 (0.275) loss 0.7900 (0.7263) acc 71.8750 (75.7812) lr 1.9298e-02 eta 0:03:29\n",
      "epoch [14/100] batch [1/4] time 2.194 (2.194) data 1.833 (1.833) loss 0.3442 (0.3442) acc 90.6250 (90.6250) lr 1.9298e-02 eta 0:12:41\n",
      "epoch [14/100] batch [2/4] time 0.325 (1.259) data 0.001 (0.917) loss 0.4619 (0.4031) acc 81.2500 (85.9375) lr 1.9298e-02 eta 0:07:15\n",
      "epoch [14/100] batch [3/4] time 0.325 (0.948) data 0.000 (0.611) loss 0.7851 (0.5304) acc 71.8750 (81.2500) lr 1.9298e-02 eta 0:05:27\n",
      "epoch [14/100] batch [4/4] time 0.351 (0.799) data 0.000 (0.459) loss 0.7553 (0.5866) acc 75.0000 (79.6875) lr 1.9178e-02 eta 0:04:34\n",
      "epoch [15/100] batch [1/4] time 1.576 (1.576) data 1.257 (1.257) loss 0.6502 (0.6502) acc 84.3750 (84.3750) lr 1.9178e-02 eta 0:09:00\n",
      "epoch [15/100] batch [2/4] time 0.324 (0.950) data 0.001 (0.629) loss 0.7625 (0.7064) acc 75.0000 (79.6875) lr 1.9178e-02 eta 0:05:24\n",
      "epoch [15/100] batch [3/4] time 0.342 (0.747) data 0.000 (0.419) loss 0.4683 (0.6270) acc 90.6250 (83.3333) lr 1.9178e-02 eta 0:04:14\n",
      "epoch [15/100] batch [4/4] time 0.333 (0.644) data 0.001 (0.315) loss 0.4893 (0.5926) acc 81.2500 (82.8125) lr 1.9048e-02 eta 0:03:38\n",
      "epoch [16/100] batch [1/4] time 1.415 (1.415) data 1.094 (1.094) loss 0.7410 (0.7410) acc 71.8750 (71.8750) lr 1.9048e-02 eta 0:07:59\n",
      "epoch [16/100] batch [2/4] time 0.321 (0.868) data 0.000 (0.547) loss 0.5383 (0.6396) acc 90.6250 (81.2500) lr 1.9048e-02 eta 0:04:53\n",
      "epoch [16/100] batch [3/4] time 0.340 (0.692) data 0.000 (0.365) loss 0.4104 (0.5632) acc 87.5000 (83.3333) lr 1.9048e-02 eta 0:03:53\n",
      "epoch [16/100] batch [4/4] time 0.328 (0.601) data 0.000 (0.274) loss 0.6281 (0.5794) acc 87.5000 (84.3750) lr 1.8910e-02 eta 0:03:21\n",
      "epoch [17/100] batch [1/4] time 1.390 (1.390) data 1.070 (1.070) loss 0.3095 (0.3095) acc 87.5000 (87.5000) lr 1.8910e-02 eta 0:07:45\n",
      "epoch [17/100] batch [2/4] time 0.327 (0.859) data 0.001 (0.535) loss 0.5353 (0.4224) acc 87.5000 (87.5000) lr 1.8910e-02 eta 0:04:46\n",
      "epoch [17/100] batch [3/4] time 0.337 (0.685) data 0.000 (0.357) loss 0.5753 (0.4734) acc 78.1250 (84.3750) lr 1.8910e-02 eta 0:03:48\n",
      "epoch [17/100] batch [4/4] time 0.331 (0.596) data 0.001 (0.268) loss 0.4967 (0.4792) acc 87.5000 (85.1562) lr 1.8763e-02 eta 0:03:18\n",
      "epoch [18/100] batch [1/4] time 1.414 (1.414) data 1.096 (1.096) loss 0.4476 (0.4476) acc 84.3750 (84.3750) lr 1.8763e-02 eta 0:07:47\n",
      "epoch [18/100] batch [2/4] time 0.321 (0.868) data 0.000 (0.548) loss 0.4883 (0.4680) acc 84.3750 (84.3750) lr 1.8763e-02 eta 0:04:46\n",
      "epoch [18/100] batch [3/4] time 0.333 (0.689) data 0.000 (0.366) loss 0.6050 (0.5137) acc 81.2500 (83.3333) lr 1.8763e-02 eta 0:03:46\n",
      "epoch [18/100] batch [4/4] time 0.343 (0.603) data 0.000 (0.274) loss 0.5200 (0.5153) acc 81.2500 (82.8125) lr 1.8607e-02 eta 0:03:17\n",
      "epoch [19/100] batch [1/4] time 2.012 (2.012) data 1.631 (1.631) loss 0.6441 (0.6441) acc 81.2500 (81.2500) lr 1.8607e-02 eta 0:10:57\n",
      "epoch [19/100] batch [2/4] time 0.323 (1.167) data 0.001 (0.816) loss 0.9595 (0.8018) acc 71.8750 (76.5625) lr 1.8607e-02 eta 0:06:20\n",
      "epoch [19/100] batch [3/4] time 0.322 (0.886) data 0.000 (0.544) loss 0.3366 (0.6467) acc 87.5000 (80.2083) lr 1.8607e-02 eta 0:04:47\n",
      "epoch [19/100] batch [4/4] time 0.349 (0.751) data 0.000 (0.408) loss 0.5859 (0.6315) acc 81.2500 (80.4688) lr 1.8443e-02 eta 0:04:03\n",
      "epoch [20/100] batch [1/4] time 2.401 (2.401) data 2.042 (2.042) loss 0.4711 (0.4711) acc 84.3750 (84.3750) lr 1.8443e-02 eta 0:12:55\n",
      "epoch [20/100] batch [2/4] time 0.321 (1.361) data 0.001 (1.021) loss 0.8295 (0.6503) acc 71.8750 (78.1250) lr 1.8443e-02 eta 0:07:18\n",
      "epoch [20/100] batch [3/4] time 0.320 (1.014) data 0.000 (0.681) loss 0.5070 (0.6025) acc 84.3750 (80.2083) lr 1.8443e-02 eta 0:05:25\n",
      "epoch [20/100] batch [4/4] time 0.348 (0.848) data 0.000 (0.511) loss 0.4913 (0.5747) acc 84.3750 (81.2500) lr 1.8271e-02 eta 0:04:31\n",
      "epoch [21/100] batch [1/4] time 1.420 (1.420) data 1.105 (1.105) loss 0.5030 (0.5030) acc 84.3750 (84.3750) lr 1.8271e-02 eta 0:07:33\n",
      "epoch [21/100] batch [2/4] time 0.319 (0.870) data 0.000 (0.553) loss 0.3922 (0.4476) acc 93.7500 (89.0625) lr 1.8271e-02 eta 0:04:36\n",
      "epoch [21/100] batch [3/4] time 0.337 (0.692) data 0.000 (0.369) loss 0.4447 (0.4466) acc 81.2500 (86.4583) lr 1.8271e-02 eta 0:03:39\n",
      "epoch [21/100] batch [4/4] time 0.323 (0.600) data 0.000 (0.277) loss 0.5692 (0.4773) acc 81.2500 (85.1562) lr 1.8090e-02 eta 0:03:09\n",
      "epoch [22/100] batch [1/4] time 1.305 (1.305) data 0.986 (0.986) loss 0.5413 (0.5413) acc 81.2500 (81.2500) lr 1.8090e-02 eta 0:06:51\n",
      "epoch [22/100] batch [2/4] time 0.317 (0.811) data 0.001 (0.493) loss 0.3111 (0.4262) acc 93.7500 (87.5000) lr 1.8090e-02 eta 0:04:14\n",
      "epoch [22/100] batch [3/4] time 0.339 (0.654) data 0.000 (0.329) loss 0.6443 (0.4989) acc 75.0000 (83.3333) lr 1.8090e-02 eta 0:03:24\n",
      "epoch [22/100] batch [4/4] time 0.327 (0.572) data 0.000 (0.247) loss 0.6507 (0.5369) acc 84.3750 (83.5938) lr 1.7902e-02 eta 0:02:58\n",
      "epoch [23/100] batch [1/4] time 1.391 (1.391) data 1.074 (1.074) loss 0.5081 (0.5081) acc 87.5000 (87.5000) lr 1.7902e-02 eta 0:07:12\n",
      "epoch [23/100] batch [2/4] time 0.319 (0.855) data 0.001 (0.537) loss 0.4527 (0.4804) acc 81.2500 (84.3750) lr 1.7902e-02 eta 0:04:24\n",
      "epoch [23/100] batch [3/4] time 0.334 (0.681) data 0.000 (0.358) loss 0.3692 (0.4433) acc 93.7500 (87.5000) lr 1.7902e-02 eta 0:03:30\n",
      "epoch [23/100] batch [4/4] time 0.321 (0.591) data 0.000 (0.269) loss 0.6721 (0.5005) acc 81.2500 (85.9375) lr 1.7705e-02 eta 0:03:02\n",
      "epoch [24/100] batch [1/4] time 1.397 (1.397) data 1.083 (1.083) loss 0.3999 (0.3999) acc 84.3750 (84.3750) lr 1.7705e-02 eta 0:07:08\n",
      "epoch [24/100] batch [2/4] time 0.319 (0.858) data 0.000 (0.542) loss 0.3616 (0.3808) acc 90.6250 (87.5000) lr 1.7705e-02 eta 0:04:22\n",
      "epoch [24/100] batch [3/4] time 0.333 (0.683) data 0.000 (0.361) loss 0.5227 (0.4281) acc 90.6250 (88.5417) lr 1.7705e-02 eta 0:03:28\n",
      "epoch [24/100] batch [4/4] time 0.323 (0.593) data 0.000 (0.271) loss 0.4365 (0.4302) acc 87.5000 (88.2812) lr 1.7501e-02 eta 0:03:00\n",
      "epoch [25/100] batch [1/4] time 1.968 (1.968) data 1.622 (1.622) loss 0.6057 (0.6057) acc 81.2500 (81.2500) lr 1.7501e-02 eta 0:09:56\n",
      "epoch [25/100] batch [2/4] time 0.319 (1.144) data 0.001 (0.811) loss 0.7293 (0.6675) acc 75.0000 (78.1250) lr 1.7501e-02 eta 0:05:45\n",
      "epoch [25/100] batch [3/4] time 0.326 (0.871) data 0.000 (0.541) loss 0.5686 (0.6345) acc 78.1250 (78.1250) lr 1.7501e-02 eta 0:04:22\n",
      "epoch [25/100] batch [4/4] time 0.340 (0.738) data 0.000 (0.406) loss 0.5984 (0.6255) acc 78.1250 (78.1250) lr 1.7290e-02 eta 0:03:41\n",
      "epoch [26/100] batch [1/4] time 2.292 (2.292) data 1.929 (1.929) loss 0.3379 (0.3379) acc 93.7500 (93.7500) lr 1.7290e-02 eta 0:11:25\n",
      "epoch [26/100] batch [2/4] time 0.319 (1.306) data 0.001 (0.965) loss 0.5696 (0.4537) acc 81.2500 (87.5000) lr 1.7290e-02 eta 0:06:29\n",
      "epoch [26/100] batch [3/4] time 0.319 (0.977) data 0.000 (0.643) loss 0.6887 (0.5321) acc 84.3750 (86.4583) lr 1.7290e-02 eta 0:04:50\n",
      "epoch [26/100] batch [4/4] time 0.346 (0.819) data 0.000 (0.483) loss 0.5232 (0.5299) acc 81.2500 (85.1562) lr 1.7071e-02 eta 0:04:02\n",
      "epoch [27/100] batch [1/4] time 1.438 (1.438) data 1.129 (1.129) loss 0.5149 (0.5149) acc 87.5000 (87.5000) lr 1.7071e-02 eta 0:07:04\n",
      "epoch [27/100] batch [2/4] time 0.318 (0.878) data 0.000 (0.565) loss 0.6638 (0.5893) acc 84.3750 (85.9375) lr 1.7071e-02 eta 0:04:18\n",
      "epoch [27/100] batch [3/4] time 0.337 (0.698) data 0.000 (0.377) loss 0.4516 (0.5434) acc 90.6250 (87.5000) lr 1.7071e-02 eta 0:03:24\n",
      "epoch [27/100] batch [4/4] time 0.324 (0.604) data 0.000 (0.283) loss 0.9861 (0.6541) acc 75.0000 (84.3750) lr 1.6845e-02 eta 0:02:56\n",
      "epoch [28/100] batch [1/4] time 1.397 (1.397) data 1.085 (1.085) loss 0.6042 (0.6042) acc 78.1250 (78.1250) lr 1.6845e-02 eta 0:06:46\n",
      "epoch [28/100] batch [2/4] time 0.318 (0.858) data 0.000 (0.543) loss 0.4504 (0.5273) acc 84.3750 (81.2500) lr 1.6845e-02 eta 0:04:08\n",
      "epoch [28/100] batch [3/4] time 0.335 (0.683) data 0.000 (0.362) loss 0.5134 (0.5227) acc 87.5000 (83.3333) lr 1.6845e-02 eta 0:03:17\n",
      "epoch [28/100] batch [4/4] time 0.325 (0.594) data 0.000 (0.272) loss 0.2211 (0.4473) acc 90.6250 (85.1562) lr 1.6613e-02 eta 0:02:51\n",
      "epoch [29/100] batch [1/4] time 1.362 (1.362) data 1.046 (1.046) loss 0.7043 (0.7043) acc 81.2500 (81.2500) lr 1.6613e-02 eta 0:06:30\n",
      "epoch [29/100] batch [2/4] time 0.319 (0.841) data 0.001 (0.523) loss 0.3073 (0.5058) acc 96.8750 (89.0625) lr 1.6613e-02 eta 0:04:00\n",
      "epoch [29/100] batch [3/4] time 0.337 (0.673) data 0.000 (0.349) loss 0.5421 (0.5179) acc 87.5000 (88.5417) lr 1.6613e-02 eta 0:03:11\n",
      "epoch [29/100] batch [4/4] time 0.324 (0.586) data 0.000 (0.262) loss 0.3783 (0.4830) acc 87.5000 (88.2812) lr 1.6374e-02 eta 0:02:46\n",
      "epoch [30/100] batch [1/4] time 1.388 (1.388) data 1.073 (1.073) loss 0.3900 (0.3900) acc 87.5000 (87.5000) lr 1.6374e-02 eta 0:06:32\n",
      "epoch [30/100] batch [2/4] time 0.319 (0.854) data 0.001 (0.537) loss 0.2797 (0.3348) acc 96.8750 (92.1875) lr 1.6374e-02 eta 0:04:00\n",
      "epoch [30/100] batch [3/4] time 0.335 (0.681) data 0.000 (0.358) loss 0.4642 (0.3780) acc 84.3750 (89.5833) lr 1.6374e-02 eta 0:03:11\n",
      "epoch [30/100] batch [4/4] time 0.329 (0.593) data 0.000 (0.269) loss 0.6070 (0.4352) acc 75.0000 (85.9375) lr 1.6129e-02 eta 0:02:45\n",
      "epoch [31/100] batch [1/4] time 2.319 (2.319) data 1.965 (1.965) loss 0.6241 (0.6241) acc 87.5000 (87.5000) lr 1.6129e-02 eta 0:10:46\n",
      "epoch [31/100] batch [2/4] time 0.320 (1.320) data 0.001 (0.983) loss 0.6178 (0.6209) acc 81.2500 (84.3750) lr 1.6129e-02 eta 0:06:06\n",
      "epoch [31/100] batch [3/4] time 0.323 (0.987) data 0.000 (0.655) loss 0.5792 (0.6070) acc 78.1250 (82.2917) lr 1.6129e-02 eta 0:04:33\n",
      "epoch [31/100] batch [4/4] time 0.344 (0.826) data 0.000 (0.492) loss 0.4149 (0.5590) acc 81.2500 (82.0312) lr 1.5878e-02 eta 0:03:48\n",
      "epoch [32/100] batch [1/4] time 1.589 (1.589) data 1.275 (1.275) loss 0.9605 (0.9605) acc 65.6250 (65.6250) lr 1.5878e-02 eta 0:07:16\n",
      "epoch [32/100] batch [2/4] time 0.318 (0.953) data 0.001 (0.638) loss 0.5967 (0.7786) acc 84.3750 (75.0000) lr 1.5878e-02 eta 0:04:21\n",
      "epoch [32/100] batch [3/4] time 0.339 (0.748) data 0.000 (0.425) loss 0.5884 (0.7152) acc 84.3750 (78.1250) lr 1.5878e-02 eta 0:03:24\n",
      "epoch [32/100] batch [4/4] time 0.328 (0.643) data 0.000 (0.319) loss 0.2815 (0.6068) acc 90.6250 (81.2500) lr 1.5621e-02 eta 0:02:54\n",
      "epoch [33/100] batch [1/4] time 1.445 (1.445) data 1.130 (1.130) loss 0.4451 (0.4451) acc 87.5000 (87.5000) lr 1.5621e-02 eta 0:06:31\n",
      "epoch [33/100] batch [2/4] time 0.321 (0.883) data 0.000 (0.565) loss 0.4785 (0.4618) acc 81.2500 (84.3750) lr 1.5621e-02 eta 0:03:58\n",
      "epoch [33/100] batch [3/4] time 0.336 (0.701) data 0.001 (0.377) loss 0.5742 (0.4993) acc 90.6250 (86.4583) lr 1.5621e-02 eta 0:03:08\n",
      "epoch [33/100] batch [4/4] time 0.329 (0.608) data 0.000 (0.283) loss 0.3525 (0.4626) acc 87.5000 (86.7188) lr 1.5358e-02 eta 0:02:42\n",
      "epoch [34/100] batch [1/4] time 1.364 (1.364) data 1.049 (1.049) loss 0.5498 (0.5498) acc 78.1250 (78.1250) lr 1.5358e-02 eta 0:06:04\n",
      "epoch [34/100] batch [2/4] time 0.321 (0.842) data 0.000 (0.525) loss 0.4070 (0.4784) acc 87.5000 (82.8125) lr 1.5358e-02 eta 0:03:44\n",
      "epoch [34/100] batch [3/4] time 0.341 (0.675) data 0.000 (0.350) loss 0.2839 (0.4136) acc 96.8750 (87.5000) lr 1.5358e-02 eta 0:02:58\n",
      "epoch [34/100] batch [4/4] time 0.331 (0.589) data 0.000 (0.263) loss 0.2969 (0.3844) acc 93.7500 (89.0625) lr 1.5090e-02 eta 0:02:35\n",
      "epoch [35/100] batch [1/4] time 1.368 (1.368) data 1.051 (1.051) loss 0.4824 (0.4824) acc 87.5000 (87.5000) lr 1.5090e-02 eta 0:05:59\n",
      "epoch [35/100] batch [2/4] time 0.319 (0.843) data 0.000 (0.526) loss 0.3718 (0.4271) acc 87.5000 (87.5000) lr 1.5090e-02 eta 0:03:40\n",
      "epoch [35/100] batch [3/4] time 0.339 (0.675) data 0.000 (0.350) loss 0.2681 (0.3741) acc 96.8750 (90.6250) lr 1.5090e-02 eta 0:02:56\n",
      "epoch [35/100] batch [4/4] time 0.329 (0.589) data 0.000 (0.263) loss 0.2118 (0.3335) acc 100.0000 (92.9688) lr 1.4818e-02 eta 0:02:33\n",
      "epoch [36/100] batch [1/4] time 1.969 (1.969) data 1.639 (1.639) loss 0.4912 (0.4912) acc 90.6250 (90.6250) lr 1.4818e-02 eta 0:08:30\n",
      "epoch [36/100] batch [2/4] time 0.322 (1.146) data 0.001 (0.820) loss 0.4375 (0.4643) acc 90.6250 (90.6250) lr 1.4818e-02 eta 0:04:55\n",
      "epoch [36/100] batch [3/4] time 0.329 (0.874) data 0.000 (0.547) loss 0.3630 (0.4306) acc 90.6250 (90.6250) lr 1.4818e-02 eta 0:03:44\n",
      "epoch [36/100] batch [4/4] time 0.335 (0.739) data 0.001 (0.410) loss 0.5840 (0.4689) acc 81.2500 (88.2812) lr 1.4540e-02 eta 0:03:09\n",
      "epoch [37/100] batch [1/4] time 2.277 (2.277) data 1.916 (1.916) loss 0.3103 (0.3103) acc 90.6250 (90.6250) lr 1.4540e-02 eta 0:09:40\n",
      "epoch [37/100] batch [2/4] time 0.322 (1.300) data 0.001 (0.958) loss 0.4123 (0.3613) acc 84.3750 (87.5000) lr 1.4540e-02 eta 0:05:30\n",
      "epoch [37/100] batch [3/4] time 0.325 (0.975) data 0.000 (0.639) loss 0.5790 (0.4339) acc 84.3750 (86.4583) lr 1.4540e-02 eta 0:04:06\n",
      "epoch [37/100] batch [4/4] time 0.351 (0.819) data 0.000 (0.479) loss 0.2941 (0.3989) acc 93.7500 (88.2812) lr 1.4258e-02 eta 0:03:26\n",
      "epoch [38/100] batch [1/4] time 1.335 (1.335) data 1.007 (1.007) loss 0.4897 (0.4897) acc 81.2500 (81.2500) lr 1.4258e-02 eta 0:05:34\n",
      "epoch [38/100] batch [2/4] time 0.321 (0.828) data 0.000 (0.504) loss 0.7553 (0.6225) acc 75.0000 (78.1250) lr 1.4258e-02 eta 0:03:26\n",
      "epoch [38/100] batch [3/4] time 0.337 (0.664) data 0.000 (0.336) loss 0.3712 (0.5387) acc 93.7500 (83.3333) lr 1.4258e-02 eta 0:02:45\n",
      "epoch [38/100] batch [4/4] time 0.331 (0.581) data 0.000 (0.252) loss 0.4764 (0.5232) acc 90.6250 (85.1562) lr 1.3971e-02 eta 0:02:24\n",
      "epoch [39/100] batch [1/4] time 1.418 (1.418) data 1.102 (1.102) loss 0.5296 (0.5296) acc 84.3750 (84.3750) lr 1.3971e-02 eta 0:05:50\n",
      "epoch [39/100] batch [2/4] time 0.323 (0.871) data 0.000 (0.551) loss 0.3250 (0.4273) acc 90.6250 (87.5000) lr 1.3971e-02 eta 0:03:34\n",
      "epoch [39/100] batch [3/4] time 0.339 (0.694) data 0.000 (0.367) loss 0.3704 (0.4084) acc 93.7500 (89.5833) lr 1.3971e-02 eta 0:02:49\n",
      "epoch [39/100] batch [4/4] time 0.327 (0.602) data 0.000 (0.276) loss 0.4410 (0.4165) acc 87.5000 (89.0625) lr 1.3681e-02 eta 0:02:26\n",
      "epoch [40/100] batch [1/4] time 1.368 (1.368) data 1.054 (1.054) loss 0.4092 (0.4092) acc 75.0000 (75.0000) lr 1.3681e-02 eta 0:05:32\n",
      "epoch [40/100] batch [2/4] time 0.321 (0.845) data 0.001 (0.527) loss 0.3860 (0.3976) acc 84.3750 (79.6875) lr 1.3681e-02 eta 0:03:24\n",
      "epoch [40/100] batch [3/4] time 0.333 (0.674) data 0.000 (0.352) loss 0.5686 (0.4546) acc 81.2500 (80.2083) lr 1.3681e-02 eta 0:02:42\n",
      "epoch [40/100] batch [4/4] time 0.339 (0.590) data 0.000 (0.264) loss 0.3535 (0.4294) acc 90.6250 (82.8125) lr 1.3387e-02 eta 0:02:21\n",
      "epoch [41/100] batch [1/4] time 1.372 (1.372) data 1.055 (1.055) loss 0.7209 (0.7209) acc 81.2500 (81.2500) lr 1.3387e-02 eta 0:05:28\n",
      "epoch [41/100] batch [2/4] time 0.322 (0.847) data 0.000 (0.528) loss 0.4250 (0.5730) acc 87.5000 (84.3750) lr 1.3387e-02 eta 0:03:21\n",
      "epoch [41/100] batch [3/4] time 0.333 (0.676) data 0.000 (0.352) loss 0.3960 (0.5140) acc 81.2500 (83.3333) lr 1.3387e-02 eta 0:02:40\n",
      "epoch [41/100] batch [4/4] time 0.333 (0.590) data 0.000 (0.264) loss 0.5063 (0.5121) acc 87.5000 (84.3750) lr 1.3090e-02 eta 0:02:19\n",
      "epoch [42/100] batch [1/4] time 2.170 (2.170) data 1.813 (1.813) loss 0.2635 (0.2635) acc 93.7500 (93.7500) lr 1.3090e-02 eta 0:08:29\n",
      "epoch [42/100] batch [2/4] time 0.322 (1.246) data 0.004 (0.908) loss 0.3420 (0.3028) acc 90.6250 (92.1875) lr 1.3090e-02 eta 0:04:51\n",
      "epoch [42/100] batch [3/4] time 0.321 (0.938) data 0.000 (0.606) loss 0.2578 (0.2878) acc 96.8750 (93.7500) lr 1.3090e-02 eta 0:03:38\n",
      "epoch [42/100] batch [4/4] time 0.345 (0.790) data 0.000 (0.454) loss 0.3913 (0.3136) acc 93.7500 (93.7500) lr 1.2790e-02 eta 0:03:03\n",
      "epoch [43/100] batch [1/4] time 2.300 (2.300) data 1.917 (1.917) loss 0.6418 (0.6418) acc 78.1250 (78.1250) lr 1.2790e-02 eta 0:08:51\n",
      "epoch [43/100] batch [2/4] time 0.321 (1.310) data 0.001 (0.959) loss 0.5563 (0.5991) acc 84.3750 (81.2500) lr 1.2790e-02 eta 0:05:01\n",
      "epoch [43/100] batch [3/4] time 0.322 (0.981) data 0.000 (0.639) loss 0.9274 (0.7085) acc 68.7500 (77.0833) lr 1.2790e-02 eta 0:03:44\n",
      "epoch [43/100] batch [4/4] time 0.340 (0.820) data 0.000 (0.480) loss 0.5539 (0.6698) acc 81.2500 (78.1250) lr 1.2487e-02 eta 0:03:07\n",
      "epoch [44/100] batch [1/4] time 1.424 (1.424) data 1.105 (1.105) loss 0.8786 (0.8786) acc 68.7500 (68.7500) lr 1.2487e-02 eta 0:05:23\n",
      "epoch [44/100] batch [2/4] time 0.321 (0.872) data 0.000 (0.553) loss 0.3151 (0.5968) acc 90.6250 (79.6875) lr 1.2487e-02 eta 0:03:17\n",
      "epoch [44/100] batch [3/4] time 0.327 (0.691) data 0.000 (0.369) loss 0.8668 (0.6868) acc 71.8750 (77.0833) lr 1.2487e-02 eta 0:02:35\n",
      "epoch [44/100] batch [4/4] time 0.334 (0.601) data 0.000 (0.277) loss 0.6432 (0.6759) acc 84.3750 (78.9062) lr 1.2181e-02 eta 0:02:14\n",
      "epoch [45/100] batch [1/4] time 1.354 (1.354) data 1.037 (1.037) loss 0.5690 (0.5690) acc 84.3750 (84.3750) lr 1.2181e-02 eta 0:05:01\n",
      "epoch [45/100] batch [2/4] time 0.322 (0.838) data 0.001 (0.519) loss 0.5157 (0.5424) acc 87.5000 (85.9375) lr 1.2181e-02 eta 0:03:06\n",
      "epoch [45/100] batch [3/4] time 0.334 (0.670) data 0.000 (0.346) loss 0.4567 (0.5138) acc 87.5000 (86.4583) lr 1.2181e-02 eta 0:02:28\n",
      "epoch [45/100] batch [4/4] time 0.327 (0.584) data 0.000 (0.259) loss 0.4554 (0.4992) acc 93.7500 (88.2812) lr 1.1874e-02 eta 0:02:08\n",
      "epoch [46/100] batch [1/4] time 1.388 (1.388) data 1.069 (1.069) loss 0.5460 (0.5460) acc 87.5000 (87.5000) lr 1.1874e-02 eta 0:05:03\n",
      "epoch [46/100] batch [2/4] time 0.320 (0.854) data 0.000 (0.535) loss 0.5104 (0.5282) acc 81.2500 (84.3750) lr 1.1874e-02 eta 0:03:06\n",
      "epoch [46/100] batch [3/4] time 0.336 (0.681) data 0.000 (0.357) loss 0.3219 (0.4594) acc 90.6250 (86.4583) lr 1.1874e-02 eta 0:02:27\n",
      "epoch [46/100] batch [4/4] time 0.327 (0.593) data 0.000 (0.268) loss 0.4000 (0.4446) acc 90.6250 (87.5000) lr 1.1564e-02 eta 0:02:08\n",
      "epoch [47/100] batch [1/4] time 1.435 (1.435) data 1.121 (1.121) loss 0.3406 (0.3406) acc 93.7500 (93.7500) lr 1.1564e-02 eta 0:05:08\n",
      "epoch [47/100] batch [2/4] time 0.320 (0.878) data 0.000 (0.561) loss 0.2672 (0.3039) acc 87.5000 (90.6250) lr 1.1564e-02 eta 0:03:07\n",
      "epoch [47/100] batch [3/4] time 0.333 (0.696) data 0.000 (0.374) loss 0.6724 (0.4267) acc 75.0000 (85.4167) lr 1.1564e-02 eta 0:02:28\n",
      "epoch [47/100] batch [4/4] time 0.330 (0.604) data 0.000 (0.280) loss 0.4800 (0.4401) acc 81.2500 (84.3750) lr 1.1253e-02 eta 0:02:08\n",
      "epoch [48/100] batch [1/4] time 2.080 (2.080) data 1.714 (1.714) loss 0.5474 (0.5474) acc 87.5000 (87.5000) lr 1.1253e-02 eta 0:07:18\n",
      "epoch [48/100] batch [2/4] time 0.322 (1.201) data 0.001 (0.857) loss 0.4283 (0.4878) acc 84.3750 (85.9375) lr 1.1253e-02 eta 0:04:12\n",
      "epoch [48/100] batch [3/4] time 0.325 (0.909) data 0.000 (0.572) loss 0.3880 (0.4545) acc 84.3750 (85.4167) lr 1.1253e-02 eta 0:03:09\n",
      "epoch [48/100] batch [4/4] time 0.346 (0.768) data 0.000 (0.429) loss 0.4751 (0.4597) acc 81.2500 (84.3750) lr 1.0941e-02 eta 0:02:39\n",
      "epoch [49/100] batch [1/4] time 1.462 (1.462) data 1.142 (1.142) loss 0.3472 (0.3472) acc 90.6250 (90.6250) lr 1.0941e-02 eta 0:05:02\n",
      "epoch [49/100] batch [2/4] time 0.323 (0.892) data 0.001 (0.571) loss 0.4341 (0.3906) acc 81.2500 (85.9375) lr 1.0941e-02 eta 0:03:03\n",
      "epoch [49/100] batch [3/4] time 0.336 (0.707) data 0.000 (0.381) loss 0.4658 (0.4157) acc 81.2500 (84.3750) lr 1.0941e-02 eta 0:02:24\n",
      "epoch [49/100] batch [4/4] time 0.333 (0.613) data 0.000 (0.286) loss 0.5678 (0.4537) acc 81.2500 (83.5938) lr 1.0628e-02 eta 0:02:05\n",
      "epoch [50/100] batch [1/4] time 1.445 (1.445) data 1.131 (1.131) loss 0.5600 (0.5600) acc 84.3750 (84.3750) lr 1.0628e-02 eta 0:04:53\n",
      "epoch [50/100] batch [2/4] time 0.319 (0.882) data 0.000 (0.566) loss 0.7109 (0.6355) acc 81.2500 (82.8125) lr 1.0628e-02 eta 0:02:58\n",
      "epoch [50/100] batch [3/4] time 0.336 (0.700) data 0.000 (0.377) loss 0.4741 (0.5817) acc 90.6250 (85.4167) lr 1.0628e-02 eta 0:02:20\n",
      "epoch [50/100] batch [4/4] time 0.329 (0.607) data 0.000 (0.283) loss 0.6147 (0.5899) acc 75.0000 (82.8125) lr 1.0314e-02 eta 0:02:01\n",
      "epoch [51/100] batch [1/4] time 1.365 (1.365) data 1.048 (1.048) loss 0.4575 (0.4575) acc 87.5000 (87.5000) lr 1.0314e-02 eta 0:04:31\n",
      "epoch [51/100] batch [2/4] time 0.320 (0.842) data 0.000 (0.524) loss 0.6318 (0.5446) acc 81.2500 (84.3750) lr 1.0314e-02 eta 0:02:46\n",
      "epoch [51/100] batch [3/4] time 0.335 (0.673) data 0.000 (0.350) loss 0.5302 (0.5398) acc 84.3750 (84.3750) lr 1.0314e-02 eta 0:02:12\n",
      "epoch [51/100] batch [4/4] time 0.327 (0.587) data 0.000 (0.262) loss 0.3219 (0.4853) acc 87.5000 (85.1562) lr 1.0000e-02 eta 0:01:54\n",
      "epoch [52/100] batch [1/4] time 1.429 (1.429) data 1.115 (1.115) loss 0.2892 (0.2892) acc 93.7500 (93.7500) lr 1.0000e-02 eta 0:04:38\n",
      "epoch [52/100] batch [2/4] time 0.319 (0.874) data 0.001 (0.558) loss 0.4867 (0.3880) acc 84.3750 (89.0625) lr 1.0000e-02 eta 0:02:49\n",
      "epoch [52/100] batch [3/4] time 0.337 (0.695) data 0.000 (0.372) loss 0.2712 (0.3490) acc 90.6250 (89.5833) lr 1.0000e-02 eta 0:02:14\n",
      "epoch [52/100] batch [4/4] time 0.328 (0.603) data 0.000 (0.279) loss 0.5292 (0.3941) acc 84.3750 (88.2812) lr 9.6859e-03 eta 0:01:55\n",
      "epoch [53/100] batch [1/4] time 2.186 (2.186) data 1.828 (1.828) loss 0.3141 (0.3141) acc 87.5000 (87.5000) lr 9.6859e-03 eta 0:06:57\n",
      "epoch [53/100] batch [2/4] time 0.324 (1.255) data 0.001 (0.914) loss 0.5927 (0.4534) acc 78.1250 (82.8125) lr 9.6859e-03 eta 0:03:58\n",
      "epoch [53/100] batch [3/4] time 0.322 (0.944) data 0.000 (0.610) loss 0.3417 (0.4162) acc 90.6250 (85.4167) lr 9.6859e-03 eta 0:02:58\n",
      "epoch [53/100] batch [4/4] time 0.347 (0.795) data 0.000 (0.457) loss 0.5844 (0.4582) acc 81.2500 (84.3750) lr 9.3721e-03 eta 0:02:29\n",
      "epoch [54/100] batch [1/4] time 2.303 (2.303) data 1.926 (1.926) loss 0.4442 (0.4442) acc 84.3750 (84.3750) lr 9.3721e-03 eta 0:07:10\n",
      "epoch [54/100] batch [2/4] time 0.319 (1.311) data 0.001 (0.963) loss 0.4960 (0.4701) acc 84.3750 (84.3750) lr 9.3721e-03 eta 0:04:03\n",
      "epoch [54/100] batch [3/4] time 0.321 (0.981) data 0.000 (0.642) loss 0.4699 (0.4700) acc 84.3750 (84.3750) lr 9.3721e-03 eta 0:03:01\n",
      "epoch [54/100] batch [4/4] time 0.336 (0.820) data 0.000 (0.482) loss 0.1666 (0.3942) acc 93.7500 (86.7188) lr 9.0589e-03 eta 0:02:30\n",
      "epoch [55/100] batch [1/4] time 1.393 (1.393) data 1.069 (1.069) loss 0.4762 (0.4762) acc 90.6250 (90.6250) lr 9.0589e-03 eta 0:04:14\n",
      "epoch [55/100] batch [2/4] time 0.319 (0.856) data 0.001 (0.535) loss 0.9647 (0.7205) acc 75.0000 (82.8125) lr 9.0589e-03 eta 0:02:35\n",
      "epoch [55/100] batch [3/4] time 0.329 (0.680) data 0.000 (0.356) loss 0.7005 (0.7138) acc 78.1250 (81.2500) lr 9.0589e-03 eta 0:02:03\n",
      "epoch [55/100] batch [4/4] time 0.333 (0.594) data 0.000 (0.267) loss 0.3446 (0.6215) acc 87.5000 (82.8125) lr 8.7467e-03 eta 0:01:46\n",
      "epoch [56/100] batch [1/4] time 1.421 (1.421) data 1.105 (1.105) loss 0.5106 (0.5106) acc 87.5000 (87.5000) lr 8.7467e-03 eta 0:04:14\n",
      "epoch [56/100] batch [2/4] time 0.321 (0.871) data 0.000 (0.553) loss 0.2591 (0.3849) acc 100.0000 (93.7500) lr 8.7467e-03 eta 0:02:34\n",
      "epoch [56/100] batch [3/4] time 0.330 (0.690) data 0.000 (0.369) loss 0.6546 (0.4748) acc 78.1250 (88.5417) lr 8.7467e-03 eta 0:02:02\n",
      "epoch [56/100] batch [4/4] time 0.329 (0.600) data 0.000 (0.276) loss 0.3795 (0.4510) acc 90.6250 (89.0625) lr 8.4357e-03 eta 0:01:45\n",
      "epoch [57/100] batch [1/4] time 1.393 (1.393) data 1.076 (1.076) loss 0.4332 (0.4332) acc 90.6250 (90.6250) lr 8.4357e-03 eta 0:04:03\n",
      "epoch [57/100] batch [2/4] time 0.320 (0.856) data 0.001 (0.539) loss 0.2544 (0.3438) acc 96.8750 (93.7500) lr 8.4357e-03 eta 0:02:29\n",
      "epoch [57/100] batch [3/4] time 0.334 (0.682) data 0.000 (0.359) loss 0.4823 (0.3899) acc 87.5000 (91.6667) lr 8.4357e-03 eta 0:01:58\n",
      "epoch [57/100] batch [4/4] time 0.327 (0.593) data 0.000 (0.269) loss 0.3184 (0.3721) acc 87.5000 (90.6250) lr 8.1262e-03 eta 0:01:42\n",
      "epoch [58/100] batch [1/4] time 1.404 (1.404) data 1.089 (1.089) loss 0.5492 (0.5492) acc 81.2500 (81.2500) lr 8.1262e-03 eta 0:04:00\n",
      "epoch [58/100] batch [2/4] time 0.322 (0.863) data 0.000 (0.545) loss 0.5511 (0.5501) acc 81.2500 (81.2500) lr 8.1262e-03 eta 0:02:26\n",
      "epoch [58/100] batch [3/4] time 0.340 (0.689) data 0.000 (0.363) loss 0.7562 (0.6188) acc 68.7500 (77.0833) lr 8.1262e-03 eta 0:01:56\n",
      "epoch [58/100] batch [4/4] time 0.327 (0.598) data 0.000 (0.273) loss 0.4762 (0.5832) acc 84.3750 (78.9062) lr 7.8186e-03 eta 0:01:40\n",
      "epoch [59/100] batch [1/4] time 2.268 (2.268) data 1.888 (1.888) loss 0.4896 (0.4896) acc 87.5000 (87.5000) lr 7.8186e-03 eta 0:06:18\n",
      "epoch [59/100] batch [2/4] time 0.320 (1.294) data 0.001 (0.944) loss 0.4534 (0.4715) acc 81.2500 (84.3750) lr 7.8186e-03 eta 0:03:34\n",
      "epoch [59/100] batch [3/4] time 0.321 (0.970) data 0.000 (0.630) loss 0.5213 (0.4881) acc 75.0000 (81.2500) lr 7.8186e-03 eta 0:02:40\n",
      "epoch [59/100] batch [4/4] time 0.343 (0.813) data 0.000 (0.472) loss 0.4923 (0.4891) acc 87.5000 (82.8125) lr 7.5131e-03 eta 0:02:13\n",
      "epoch [60/100] batch [1/4] time 1.486 (1.486) data 1.170 (1.170) loss 0.7059 (0.7059) acc 78.1250 (78.1250) lr 7.5131e-03 eta 0:04:02\n",
      "epoch [60/100] batch [2/4] time 0.321 (0.904) data 0.001 (0.585) loss 0.3843 (0.5451) acc 93.7500 (85.9375) lr 7.5131e-03 eta 0:02:26\n",
      "epoch [60/100] batch [3/4] time 0.329 (0.712) data 0.000 (0.390) loss 0.4219 (0.5041) acc 87.5000 (86.4583) lr 7.5131e-03 eta 0:01:54\n",
      "epoch [60/100] batch [4/4] time 0.329 (0.616) data 0.000 (0.293) loss 0.2952 (0.4518) acc 87.5000 (86.7188) lr 7.2101e-03 eta 0:01:38\n",
      "epoch [61/100] batch [1/4] time 1.400 (1.400) data 1.082 (1.082) loss 0.4661 (0.4661) acc 93.7500 (93.7500) lr 7.2101e-03 eta 0:03:42\n",
      "epoch [61/100] batch [2/4] time 0.322 (0.861) data 0.001 (0.542) loss 0.4896 (0.4778) acc 84.3750 (89.0625) lr 7.2101e-03 eta 0:02:16\n",
      "epoch [61/100] batch [3/4] time 0.333 (0.685) data 0.000 (0.361) loss 0.2996 (0.4184) acc 96.8750 (91.6667) lr 7.2101e-03 eta 0:01:47\n",
      "epoch [61/100] batch [4/4] time 0.333 (0.597) data 0.000 (0.271) loss 0.4224 (0.4194) acc 84.3750 (89.8438) lr 6.9098e-03 eta 0:01:33\n",
      "epoch [62/100] batch [1/4] time 1.398 (1.398) data 1.081 (1.081) loss 0.3841 (0.3841) acc 90.6250 (90.6250) lr 6.9098e-03 eta 0:03:36\n",
      "epoch [62/100] batch [2/4] time 0.324 (0.861) data 0.000 (0.541) loss 0.5296 (0.4568) acc 87.5000 (89.0625) lr 6.9098e-03 eta 0:02:12\n",
      "epoch [62/100] batch [3/4] time 0.335 (0.685) data 0.000 (0.361) loss 0.7713 (0.5617) acc 84.3750 (87.5000) lr 6.9098e-03 eta 0:01:44\n",
      "epoch [62/100] batch [4/4] time 0.330 (0.597) data 0.000 (0.271) loss 0.2388 (0.4809) acc 93.7500 (89.0625) lr 6.6126e-03 eta 0:01:30\n",
      "epoch [63/100] batch [1/4] time 1.406 (1.406) data 1.088 (1.088) loss 0.3152 (0.3152) acc 87.5000 (87.5000) lr 6.6126e-03 eta 0:03:32\n",
      "epoch [63/100] batch [2/4] time 0.321 (0.863) data 0.000 (0.544) loss 0.2693 (0.2923) acc 90.6250 (89.0625) lr 6.6126e-03 eta 0:02:09\n",
      "epoch [63/100] batch [3/4] time 0.330 (0.686) data 0.000 (0.363) loss 0.4637 (0.3494) acc 90.6250 (89.5833) lr 6.6126e-03 eta 0:01:42\n",
      "epoch [63/100] batch [4/4] time 0.338 (0.599) data 0.000 (0.272) loss 0.6145 (0.4157) acc 84.3750 (88.2812) lr 6.3188e-03 eta 0:01:28\n",
      "epoch [64/100] batch [1/4] time 2.313 (2.313) data 1.932 (1.932) loss 0.3428 (0.3428) acc 87.5000 (87.5000) lr 6.3188e-03 eta 0:05:40\n",
      "epoch [64/100] batch [2/4] time 0.321 (1.317) data 0.001 (0.966) loss 0.2278 (0.2853) acc 93.7500 (90.6250) lr 6.3188e-03 eta 0:03:12\n",
      "epoch [64/100] batch [3/4] time 0.320 (0.985) data 0.000 (0.644) loss 0.5887 (0.3864) acc 81.2500 (87.5000) lr 6.3188e-03 eta 0:02:22\n",
      "epoch [64/100] batch [4/4] time 0.340 (0.824) data 0.000 (0.483) loss 0.2157 (0.3438) acc 96.8750 (89.8438) lr 6.0285e-03 eta 0:01:58\n",
      "epoch [65/100] batch [1/4] time 2.214 (2.214) data 1.835 (1.835) loss 0.5584 (0.5584) acc 87.5000 (87.5000) lr 6.0285e-03 eta 0:05:16\n",
      "epoch [65/100] batch [2/4] time 0.325 (1.269) data 0.001 (0.918) loss 0.3972 (0.4778) acc 87.5000 (87.5000) lr 6.0285e-03 eta 0:03:00\n",
      "epoch [65/100] batch [3/4] time 0.324 (0.954) data 0.000 (0.612) loss 0.6887 (0.5481) acc 71.8750 (82.2917) lr 6.0285e-03 eta 0:02:14\n",
      "epoch [65/100] batch [4/4] time 0.348 (0.803) data 0.000 (0.459) loss 0.3252 (0.4924) acc 84.3750 (82.8125) lr 5.7422e-03 eta 0:01:52\n",
      "epoch [66/100] batch [1/4] time 1.432 (1.432) data 1.116 (1.116) loss 0.4646 (0.4646) acc 90.6250 (90.6250) lr 5.7422e-03 eta 0:03:18\n",
      "epoch [66/100] batch [2/4] time 0.320 (0.876) data 0.001 (0.558) loss 0.3245 (0.3946) acc 96.8750 (93.7500) lr 5.7422e-03 eta 0:02:00\n",
      "epoch [66/100] batch [3/4] time 0.338 (0.696) data 0.000 (0.372) loss 0.2063 (0.3318) acc 96.8750 (94.7917) lr 5.7422e-03 eta 0:01:35\n",
      "epoch [66/100] batch [4/4] time 0.333 (0.606) data 0.000 (0.279) loss 0.5325 (0.3820) acc 93.7500 (94.5312) lr 5.4601e-03 eta 0:01:22\n",
      "epoch [67/100] batch [1/4] time 1.396 (1.396) data 1.076 (1.076) loss 0.3872 (0.3872) acc 96.8750 (96.8750) lr 5.4601e-03 eta 0:03:08\n",
      "epoch [67/100] batch [2/4] time 0.321 (0.858) data 0.000 (0.538) loss 0.4563 (0.4218) acc 90.6250 (93.7500) lr 5.4601e-03 eta 0:01:55\n",
      "epoch [67/100] batch [3/4] time 0.337 (0.685) data 0.000 (0.359) loss 0.2543 (0.3659) acc 93.7500 (93.7500) lr 5.4601e-03 eta 0:01:31\n",
      "epoch [67/100] batch [4/4] time 0.327 (0.595) data 0.000 (0.269) loss 0.5330 (0.4077) acc 90.6250 (92.9688) lr 5.1825e-03 eta 0:01:18\n",
      "epoch [68/100] batch [1/4] time 1.401 (1.401) data 1.084 (1.084) loss 0.4236 (0.4236) acc 84.3750 (84.3750) lr 5.1825e-03 eta 0:03:03\n",
      "epoch [68/100] batch [2/4] time 0.323 (0.862) data 0.001 (0.542) loss 0.8025 (0.6130) acc 81.2500 (82.8125) lr 5.1825e-03 eta 0:01:52\n",
      "epoch [68/100] batch [3/4] time 0.337 (0.687) data 0.000 (0.362) loss 0.4824 (0.5695) acc 90.6250 (85.4167) lr 5.1825e-03 eta 0:01:28\n",
      "epoch [68/100] batch [4/4] time 0.330 (0.598) data 0.000 (0.271) loss 0.3052 (0.5034) acc 84.3750 (85.1562) lr 4.9096e-03 eta 0:01:16\n",
      "epoch [69/100] batch [1/4] time 1.545 (1.545) data 1.226 (1.226) loss 0.4128 (0.4128) acc 87.5000 (87.5000) lr 4.9096e-03 eta 0:03:16\n",
      "epoch [69/100] batch [2/4] time 0.323 (0.934) data 0.000 (0.613) loss 0.3552 (0.3840) acc 90.6250 (89.0625) lr 4.9096e-03 eta 0:01:57\n",
      "epoch [69/100] batch [3/4] time 0.338 (0.735) data 0.000 (0.409) loss 0.3738 (0.3806) acc 90.6250 (89.5833) lr 4.9096e-03 eta 0:01:31\n",
      "epoch [69/100] batch [4/4] time 0.331 (0.634) data 0.000 (0.307) loss 0.3318 (0.3684) acc 84.3750 (88.2812) lr 4.6417e-03 eta 0:01:18\n",
      "epoch [70/100] batch [1/4] time 2.116 (2.116) data 1.734 (1.734) loss 0.5754 (0.5754) acc 84.3750 (84.3750) lr 4.6417e-03 eta 0:04:20\n",
      "epoch [70/100] batch [2/4] time 0.323 (1.219) data 0.001 (0.867) loss 0.4841 (0.5298) acc 87.5000 (85.9375) lr 4.6417e-03 eta 0:02:28\n",
      "epoch [70/100] batch [3/4] time 0.324 (0.921) data 0.000 (0.578) loss 0.2700 (0.4432) acc 93.7500 (88.5417) lr 4.6417e-03 eta 0:01:51\n",
      "epoch [70/100] batch [4/4] time 0.338 (0.775) data 0.000 (0.434) loss 0.3054 (0.4087) acc 93.7500 (89.8438) lr 4.3792e-03 eta 0:01:33\n",
      "epoch [71/100] batch [1/4] time 1.460 (1.460) data 1.144 (1.144) loss 0.3503 (0.3503) acc 96.8750 (96.8750) lr 4.3792e-03 eta 0:02:53\n",
      "epoch [71/100] batch [2/4] time 0.322 (0.891) data 0.000 (0.572) loss 0.5378 (0.4441) acc 81.2500 (89.0625) lr 4.3792e-03 eta 0:01:45\n",
      "epoch [71/100] batch [3/4] time 0.339 (0.707) data 0.001 (0.382) loss 0.4648 (0.4510) acc 87.5000 (88.5417) lr 4.3792e-03 eta 0:01:22\n",
      "epoch [71/100] batch [4/4] time 0.331 (0.613) data 0.000 (0.286) loss 0.6848 (0.5094) acc 81.2500 (86.7188) lr 4.1221e-03 eta 0:01:11\n",
      "epoch [72/100] batch [1/4] time 1.376 (1.376) data 1.052 (1.052) loss 0.5617 (0.5617) acc 71.8750 (71.8750) lr 4.1221e-03 eta 0:02:38\n",
      "epoch [72/100] batch [2/4] time 0.323 (0.849) data 0.001 (0.526) loss 0.3200 (0.4409) acc 96.8750 (84.3750) lr 4.1221e-03 eta 0:01:36\n",
      "epoch [72/100] batch [3/4] time 0.337 (0.679) data 0.000 (0.351) loss 0.3640 (0.4153) acc 84.3750 (84.3750) lr 4.1221e-03 eta 0:01:16\n",
      "epoch [72/100] batch [4/4] time 0.327 (0.591) data 0.000 (0.263) loss 0.2737 (0.3799) acc 90.6250 (85.9375) lr 3.8709e-03 eta 0:01:06\n",
      "epoch [73/100] batch [1/4] time 1.441 (1.441) data 1.125 (1.125) loss 0.5886 (0.5886) acc 75.0000 (75.0000) lr 3.8709e-03 eta 0:02:39\n",
      "epoch [73/100] batch [2/4] time 0.322 (0.881) data 0.001 (0.563) loss 0.4060 (0.4973) acc 81.2500 (78.1250) lr 3.8709e-03 eta 0:01:36\n",
      "epoch [73/100] batch [3/4] time 0.336 (0.700) data 0.000 (0.375) loss 0.7268 (0.5738) acc 84.3750 (80.2083) lr 3.8709e-03 eta 0:01:16\n",
      "epoch [73/100] batch [4/4] time 0.324 (0.606) data 0.000 (0.281) loss 0.2058 (0.4818) acc 96.8750 (84.3750) lr 3.6258e-03 eta 0:01:05\n",
      "epoch [74/100] batch [1/4] time 1.388 (1.388) data 1.075 (1.075) loss 0.6076 (0.6076) acc 84.3750 (84.3750) lr 3.6258e-03 eta 0:02:28\n",
      "epoch [74/100] batch [2/4] time 0.321 (0.855) data 0.000 (0.538) loss 0.4821 (0.5449) acc 87.5000 (85.9375) lr 3.6258e-03 eta 0:01:30\n",
      "epoch [74/100] batch [3/4] time 0.334 (0.681) data 0.000 (0.358) loss 0.2204 (0.4367) acc 96.8750 (89.5833) lr 3.6258e-03 eta 0:01:11\n",
      "epoch [74/100] batch [4/4] time 0.330 (0.593) data 0.000 (0.269) loss 0.4729 (0.4458) acc 78.1250 (86.7188) lr 3.3869e-03 eta 0:01:01\n",
      "epoch [75/100] batch [1/4] time 2.246 (2.246) data 1.882 (1.882) loss 0.3720 (0.3720) acc 93.7500 (93.7500) lr 3.3869e-03 eta 0:03:51\n",
      "epoch [75/100] batch [2/4] time 0.320 (1.283) data 0.001 (0.941) loss 0.4250 (0.3985) acc 87.5000 (90.6250) lr 3.3869e-03 eta 0:02:10\n",
      "epoch [75/100] batch [3/4] time 0.321 (0.962) data 0.000 (0.628) loss 0.5520 (0.4497) acc 78.1250 (86.4583) lr 3.3869e-03 eta 0:01:37\n",
      "epoch [75/100] batch [4/4] time 0.348 (0.809) data 0.000 (0.471) loss 0.4084 (0.4394) acc 87.5000 (86.7188) lr 3.1545e-03 eta 0:01:20\n",
      "epoch [76/100] batch [1/4] time 1.549 (1.549) data 1.236 (1.236) loss 0.4086 (0.4086) acc 93.7500 (93.7500) lr 3.1545e-03 eta 0:02:33\n",
      "epoch [76/100] batch [2/4] time 0.319 (0.934) data 0.000 (0.618) loss 0.3195 (0.3641) acc 87.5000 (90.6250) lr 3.1545e-03 eta 0:01:31\n",
      "epoch [76/100] batch [3/4] time 0.335 (0.734) data 0.000 (0.412) loss 0.2644 (0.3308) acc 93.7500 (91.6667) lr 3.1545e-03 eta 0:01:11\n",
      "epoch [76/100] batch [4/4] time 0.326 (0.632) data 0.000 (0.309) loss 0.2436 (0.3090) acc 93.7500 (92.1875) lr 2.9289e-03 eta 0:01:00\n",
      "epoch [77/100] batch [1/4] time 1.428 (1.428) data 1.113 (1.113) loss 0.2185 (0.2185) acc 96.8750 (96.8750) lr 2.9289e-03 eta 0:02:15\n",
      "epoch [77/100] batch [2/4] time 0.320 (0.874) data 0.001 (0.557) loss 0.5505 (0.3845) acc 81.2500 (89.0625) lr 2.9289e-03 eta 0:01:22\n",
      "epoch [77/100] batch [3/4] time 0.338 (0.695) data 0.000 (0.371) loss 0.5026 (0.4239) acc 87.5000 (88.5417) lr 2.9289e-03 eta 0:01:04\n",
      "epoch [77/100] batch [4/4] time 0.327 (0.603) data 0.000 (0.279) loss 0.4931 (0.4412) acc 84.3750 (87.5000) lr 2.7103e-03 eta 0:00:55\n",
      "epoch [78/100] batch [1/4] time 1.404 (1.404) data 1.091 (1.091) loss 0.3046 (0.3046) acc 96.8750 (96.8750) lr 2.7103e-03 eta 0:02:07\n",
      "epoch [78/100] batch [2/4] time 0.321 (0.863) data 0.000 (0.546) loss 0.7595 (0.5321) acc 75.0000 (85.9375) lr 2.7103e-03 eta 0:01:17\n",
      "epoch [78/100] batch [3/4] time 0.333 (0.686) data 0.000 (0.364) loss 0.2034 (0.4225) acc 96.8750 (89.5833) lr 2.7103e-03 eta 0:01:01\n",
      "epoch [78/100] batch [4/4] time 0.333 (0.598) data 0.000 (0.273) loss 0.6340 (0.4754) acc 75.0000 (85.9375) lr 2.4989e-03 eta 0:00:52\n",
      "epoch [79/100] batch [1/4] time 1.350 (1.350) data 1.025 (1.025) loss 0.5520 (0.5520) acc 84.3750 (84.3750) lr 2.4989e-03 eta 0:01:57\n",
      "epoch [79/100] batch [2/4] time 0.322 (0.836) data 0.001 (0.513) loss 0.3330 (0.4425) acc 90.6250 (87.5000) lr 2.4989e-03 eta 0:01:11\n",
      "epoch [79/100] batch [3/4] time 0.336 (0.669) data 0.000 (0.342) loss 0.2525 (0.3791) acc 96.8750 (90.6250) lr 2.4989e-03 eta 0:00:56\n",
      "epoch [79/100] batch [4/4] time 0.329 (0.584) data 0.000 (0.256) loss 1.0363 (0.5434) acc 71.8750 (85.9375) lr 2.2949e-03 eta 0:00:49\n",
      "epoch [80/100] batch [1/4] time 2.011 (2.011) data 1.691 (1.691) loss 0.5041 (0.5041) acc 84.3750 (84.3750) lr 2.2949e-03 eta 0:02:46\n",
      "epoch [80/100] batch [2/4] time 0.319 (1.165) data 0.001 (0.846) loss 0.5832 (0.5437) acc 84.3750 (84.3750) lr 2.2949e-03 eta 0:01:35\n",
      "epoch [80/100] batch [3/4] time 0.330 (0.887) data 0.000 (0.564) loss 0.3083 (0.4652) acc 90.6250 (86.4583) lr 2.2949e-03 eta 0:01:11\n",
      "epoch [80/100] batch [4/4] time 0.335 (0.749) data 0.000 (0.423) loss 0.3105 (0.4265) acc 87.5000 (86.7188) lr 2.0984e-03 eta 0:00:59\n",
      "epoch [81/100] batch [1/4] time 2.316 (2.316) data 1.948 (1.948) loss 0.3945 (0.3945) acc 90.6250 (90.6250) lr 2.0984e-03 eta 0:03:02\n",
      "epoch [81/100] batch [2/4] time 0.320 (1.318) data 0.001 (0.974) loss 0.5261 (0.4603) acc 84.3750 (87.5000) lr 2.0984e-03 eta 0:01:42\n",
      "epoch [81/100] batch [3/4] time 0.322 (0.986) data 0.000 (0.650) loss 0.5583 (0.4930) acc 84.3750 (86.4583) lr 2.0984e-03 eta 0:01:15\n",
      "epoch [81/100] batch [4/4] time 0.346 (0.826) data 0.000 (0.487) loss 0.6892 (0.5420) acc 84.3750 (85.9375) lr 1.9098e-03 eta 0:01:02\n",
      "epoch [82/100] batch [1/4] time 1.476 (1.476) data 1.155 (1.155) loss 0.5695 (0.5695) acc 81.2500 (81.2500) lr 1.9098e-03 eta 0:01:50\n",
      "epoch [82/100] batch [2/4] time 0.320 (0.898) data 0.001 (0.578) loss 0.2579 (0.4137) acc 93.7500 (87.5000) lr 1.9098e-03 eta 0:01:06\n",
      "epoch [82/100] batch [3/4] time 0.332 (0.710) data 0.000 (0.385) loss 0.3007 (0.3760) acc 90.6250 (88.5417) lr 1.9098e-03 eta 0:00:51\n",
      "epoch [82/100] batch [4/4] time 0.330 (0.615) data 0.000 (0.289) loss 0.4868 (0.4037) acc 87.5000 (88.2812) lr 1.7292e-03 eta 0:00:44\n",
      "epoch [83/100] batch [1/4] time 1.432 (1.432) data 1.119 (1.119) loss 0.3171 (0.3171) acc 90.6250 (90.6250) lr 1.7292e-03 eta 0:01:41\n",
      "epoch [83/100] batch [2/4] time 0.320 (0.876) data 0.000 (0.560) loss 0.4028 (0.3599) acc 90.6250 (90.6250) lr 1.7292e-03 eta 0:01:01\n",
      "epoch [83/100] batch [3/4] time 0.339 (0.697) data 0.000 (0.373) loss 0.4262 (0.3820) acc 87.5000 (89.5833) lr 1.7292e-03 eta 0:00:48\n",
      "epoch [83/100] batch [4/4] time 0.324 (0.604) data 0.000 (0.280) loss 0.3024 (0.3621) acc 96.8750 (91.4062) lr 1.5567e-03 eta 0:00:41\n",
      "epoch [84/100] batch [1/4] time 1.425 (1.425) data 1.113 (1.113) loss 0.8269 (0.8269) acc 81.2500 (81.2500) lr 1.5567e-03 eta 0:01:35\n",
      "epoch [84/100] batch [2/4] time 0.320 (0.873) data 0.000 (0.557) loss 0.3088 (0.5678) acc 96.8750 (89.0625) lr 1.5567e-03 eta 0:00:57\n",
      "epoch [84/100] batch [3/4] time 0.337 (0.694) data 0.000 (0.371) loss 0.4909 (0.5422) acc 93.7500 (90.6250) lr 1.5567e-03 eta 0:00:45\n",
      "epoch [84/100] batch [4/4] time 0.328 (0.602) data 0.000 (0.278) loss 0.2430 (0.4674) acc 87.5000 (89.8438) lr 1.3926e-03 eta 0:00:38\n",
      "epoch [85/100] batch [1/4] time 1.453 (1.453) data 1.134 (1.134) loss 0.6223 (0.6223) acc 81.2500 (81.2500) lr 1.3926e-03 eta 0:01:31\n",
      "epoch [85/100] batch [2/4] time 0.320 (0.886) data 0.000 (0.567) loss 0.6296 (0.6259) acc 78.1250 (79.6875) lr 1.3926e-03 eta 0:00:54\n",
      "epoch [85/100] batch [3/4] time 0.339 (0.704) data 0.000 (0.378) loss 0.4052 (0.5524) acc 84.3750 (81.2500) lr 1.3926e-03 eta 0:00:42\n",
      "epoch [85/100] batch [4/4] time 0.327 (0.609) data 0.001 (0.284) loss 0.3134 (0.4926) acc 87.5000 (82.8125) lr 1.2369e-03 eta 0:00:36\n",
      "epoch [86/100] batch [1/4] time 2.292 (2.292) data 1.929 (1.929) loss 0.3945 (0.3945) acc 87.5000 (87.5000) lr 1.2369e-03 eta 0:02:15\n",
      "epoch [86/100] batch [2/4] time 0.328 (1.310) data 0.006 (0.968) loss 0.3056 (0.3500) acc 93.7500 (90.6250) lr 1.2369e-03 eta 0:01:15\n",
      "epoch [86/100] batch [3/4] time 0.323 (0.981) data 0.000 (0.645) loss 0.7380 (0.4794) acc 78.1250 (86.4583) lr 1.2369e-03 eta 0:00:55\n",
      "epoch [86/100] batch [4/4] time 0.341 (0.821) data 0.000 (0.484) loss 0.3247 (0.4407) acc 90.6250 (87.5000) lr 1.0899e-03 eta 0:00:45\n",
      "epoch [87/100] batch [1/4] time 1.584 (1.584) data 1.268 (1.268) loss 0.5148 (0.5148) acc 90.6250 (90.6250) lr 1.0899e-03 eta 0:01:27\n",
      "epoch [87/100] batch [2/4] time 0.320 (0.952) data 0.000 (0.634) loss 0.6042 (0.5595) acc 81.2500 (85.9375) lr 1.0899e-03 eta 0:00:51\n",
      "epoch [87/100] batch [3/4] time 0.338 (0.748) data 0.000 (0.423) loss 0.3466 (0.4886) acc 87.5000 (86.4583) lr 1.0899e-03 eta 0:00:39\n",
      "epoch [87/100] batch [4/4] time 0.329 (0.643) data 0.000 (0.317) loss 0.3615 (0.4568) acc 93.7500 (88.2812) lr 9.5173e-04 eta 0:00:33\n",
      "epoch [88/100] batch [1/4] time 1.434 (1.434) data 1.118 (1.118) loss 0.5280 (0.5280) acc 87.5000 (87.5000) lr 9.5173e-04 eta 0:01:13\n",
      "epoch [88/100] batch [2/4] time 0.321 (0.877) data 0.001 (0.559) loss 0.2617 (0.3948) acc 93.7500 (90.6250) lr 9.5173e-04 eta 0:00:43\n",
      "epoch [88/100] batch [3/4] time 0.332 (0.696) data 0.000 (0.373) loss 0.4978 (0.4291) acc 84.3750 (88.5417) lr 9.5173e-04 eta 0:00:34\n",
      "epoch [88/100] batch [4/4] time 0.331 (0.605) data 0.000 (0.280) loss 0.5168 (0.4511) acc 87.5000 (88.2812) lr 8.2245e-04 eta 0:00:29\n",
      "epoch [89/100] batch [1/4] time 1.428 (1.428) data 1.111 (1.111) loss 0.3911 (0.3911) acc 84.3750 (84.3750) lr 8.2245e-04 eta 0:01:07\n",
      "epoch [89/100] batch [2/4] time 0.320 (0.874) data 0.001 (0.556) loss 0.3815 (0.3863) acc 87.5000 (85.9375) lr 8.2245e-04 eta 0:00:40\n",
      "epoch [89/100] batch [3/4] time 0.338 (0.695) data 0.000 (0.371) loss 0.3405 (0.3710) acc 90.6250 (87.5000) lr 8.2245e-04 eta 0:00:31\n",
      "epoch [89/100] batch [4/4] time 0.329 (0.604) data 0.000 (0.278) loss 0.5549 (0.4170) acc 81.2500 (85.9375) lr 7.0224e-04 eta 0:00:26\n",
      "epoch [90/100] batch [1/4] time 1.385 (1.385) data 1.063 (1.063) loss 0.4563 (0.4563) acc 93.7500 (93.7500) lr 7.0224e-04 eta 0:00:59\n",
      "epoch [90/100] batch [2/4] time 0.322 (0.853) data 0.000 (0.532) loss 0.2175 (0.3369) acc 96.8750 (95.3125) lr 7.0224e-04 eta 0:00:35\n",
      "epoch [90/100] batch [3/4] time 0.337 (0.681) data 0.000 (0.355) loss 0.7712 (0.4816) acc 75.0000 (88.5417) lr 7.0224e-04 eta 0:00:27\n",
      "epoch [90/100] batch [4/4] time 0.329 (0.593) data 0.000 (0.266) loss 0.3559 (0.4502) acc 90.6250 (89.0625) lr 5.9119e-04 eta 0:00:23\n",
      "epoch [91/100] batch [1/4] time 2.135 (2.135) data 1.766 (1.766) loss 0.3994 (0.3994) acc 87.5000 (87.5000) lr 5.9119e-04 eta 0:01:23\n",
      "epoch [91/100] batch [2/4] time 0.321 (1.228) data 0.001 (0.883) loss 0.4323 (0.4158) acc 87.5000 (87.5000) lr 5.9119e-04 eta 0:00:46\n",
      "epoch [91/100] batch [3/4] time 0.320 (0.925) data 0.000 (0.589) loss 0.3452 (0.3923) acc 90.6250 (88.5417) lr 5.9119e-04 eta 0:00:34\n",
      "epoch [91/100] batch [4/4] time 0.347 (0.781) data 0.000 (0.442) loss 0.1760 (0.3382) acc 100.0000 (91.4062) lr 4.8943e-04 eta 0:00:28\n",
      "epoch [92/100] batch [1/4] time 1.510 (1.510) data 1.193 (1.193) loss 0.3598 (0.3598) acc 93.7500 (93.7500) lr 4.8943e-04 eta 0:00:52\n",
      "epoch [92/100] batch [2/4] time 0.322 (0.916) data 0.000 (0.597) loss 0.4560 (0.4079) acc 87.5000 (90.6250) lr 4.8943e-04 eta 0:00:31\n",
      "epoch [92/100] batch [3/4] time 0.340 (0.724) data 0.000 (0.398) loss 0.9382 (0.5847) acc 68.7500 (83.3333) lr 4.8943e-04 eta 0:00:23\n",
      "epoch [92/100] batch [4/4] time 0.331 (0.626) data 0.000 (0.299) loss 0.5725 (0.5816) acc 84.3750 (83.5938) lr 3.9706e-04 eta 0:00:20\n",
      "epoch [93/100] batch [1/4] time 1.405 (1.405) data 1.088 (1.088) loss 0.4226 (0.4226) acc 84.3750 (84.3750) lr 3.9706e-04 eta 0:00:43\n",
      "epoch [93/100] batch [2/4] time 0.321 (0.863) data 0.000 (0.544) loss 0.3647 (0.3936) acc 87.5000 (85.9375) lr 3.9706e-04 eta 0:00:25\n",
      "epoch [93/100] batch [3/4] time 0.341 (0.689) data 0.000 (0.363) loss 0.4375 (0.4082) acc 87.5000 (86.4583) lr 3.9706e-04 eta 0:00:19\n",
      "epoch [93/100] batch [4/4] time 0.326 (0.598) data 0.000 (0.272) loss 0.7102 (0.4837) acc 81.2500 (85.1562) lr 3.1417e-04 eta 0:00:16\n",
      "epoch [94/100] batch [1/4] time 1.399 (1.399) data 1.084 (1.084) loss 0.3361 (0.3361) acc 90.6250 (90.6250) lr 3.1417e-04 eta 0:00:37\n",
      "epoch [94/100] batch [2/4] time 0.322 (0.861) data 0.001 (0.542) loss 0.2846 (0.3104) acc 90.6250 (90.6250) lr 3.1417e-04 eta 0:00:22\n",
      "epoch [94/100] batch [3/4] time 0.337 (0.686) data 0.000 (0.362) loss 0.6979 (0.4396) acc 81.2500 (87.5000) lr 3.1417e-04 eta 0:00:17\n",
      "epoch [94/100] batch [4/4] time 0.333 (0.598) data 0.000 (0.271) loss 0.2731 (0.3980) acc 90.6250 (88.2812) lr 2.4083e-04 eta 0:00:14\n",
      "epoch [95/100] batch [1/4] time 1.438 (1.438) data 1.123 (1.123) loss 0.4064 (0.4064) acc 84.3750 (84.3750) lr 2.4083e-04 eta 0:00:33\n",
      "epoch [95/100] batch [2/4] time 0.321 (0.879) data 0.000 (0.562) loss 0.9670 (0.6867) acc 68.7500 (76.5625) lr 2.4083e-04 eta 0:00:19\n",
      "epoch [95/100] batch [3/4] time 0.342 (0.700) data 0.000 (0.375) loss 0.5627 (0.6454) acc 87.5000 (80.2083) lr 2.4083e-04 eta 0:00:14\n",
      "epoch [95/100] batch [4/4] time 0.331 (0.608) data 0.000 (0.281) loss 0.2117 (0.5370) acc 93.7500 (83.5938) lr 1.7713e-04 eta 0:00:12\n",
      "epoch [96/100] batch [1/4] time 2.231 (2.231) data 1.876 (1.876) loss 0.5471 (0.5471) acc 87.5000 (87.5000) lr 1.7713e-04 eta 0:00:42\n",
      "epoch [96/100] batch [2/4] time 0.323 (1.277) data 0.001 (0.938) loss 0.5852 (0.5661) acc 75.0000 (81.2500) lr 1.7713e-04 eta 0:00:22\n",
      "epoch [96/100] batch [3/4] time 0.322 (0.959) data 0.000 (0.626) loss 0.9787 (0.7036) acc 68.7500 (77.0833) lr 1.7713e-04 eta 0:00:16\n",
      "epoch [96/100] batch [4/4] time 0.348 (0.806) data 0.000 (0.469) loss 0.3313 (0.6105) acc 93.7500 (81.2500) lr 1.2312e-04 eta 0:00:12\n",
      "epoch [97/100] batch [1/4] time 1.700 (1.700) data 1.381 (1.381) loss 0.6853 (0.6853) acc 75.0000 (75.0000) lr 1.2312e-04 eta 0:00:25\n",
      "epoch [97/100] batch [2/4] time 0.321 (1.010) data 0.000 (0.691) loss 0.3305 (0.5079) acc 87.5000 (81.2500) lr 1.2312e-04 eta 0:00:14\n",
      "epoch [97/100] batch [3/4] time 0.340 (0.787) data 0.000 (0.461) loss 0.3137 (0.4431) acc 96.8750 (86.4583) lr 1.2312e-04 eta 0:00:10\n",
      "epoch [97/100] batch [4/4] time 0.323 (0.671) data 0.000 (0.346) loss 0.3652 (0.4237) acc 87.5000 (86.7188) lr 7.8853e-05 eta 0:00:08\n",
      "epoch [98/100] batch [1/4] time 1.421 (1.421) data 1.107 (1.107) loss 0.2520 (0.2520) acc 90.6250 (90.6250) lr 7.8853e-05 eta 0:00:15\n",
      "epoch [98/100] batch [2/4] time 0.321 (0.871) data 0.000 (0.554) loss 0.3591 (0.3056) acc 90.6250 (90.6250) lr 7.8853e-05 eta 0:00:08\n",
      "epoch [98/100] batch [3/4] time 0.340 (0.694) data 0.000 (0.369) loss 0.8840 (0.4984) acc 75.0000 (85.4167) lr 7.8853e-05 eta 0:00:06\n",
      "epoch [98/100] batch [4/4] time 0.331 (0.603) data 0.000 (0.277) loss 0.4238 (0.4797) acc 93.7500 (87.5000) lr 4.4380e-05 eta 0:00:04\n",
      "epoch [99/100] batch [1/4] time 1.362 (1.362) data 1.046 (1.046) loss 0.3051 (0.3051) acc 87.5000 (87.5000) lr 4.4380e-05 eta 0:00:09\n",
      "epoch [99/100] batch [2/4] time 0.321 (0.842) data 0.001 (0.523) loss 0.5974 (0.4512) acc 87.5000 (87.5000) lr 4.4380e-05 eta 0:00:05\n",
      "epoch [99/100] batch [3/4] time 0.338 (0.674) data 0.000 (0.349) loss 0.4130 (0.4385) acc 90.6250 (88.5417) lr 4.4380e-05 eta 0:00:03\n",
      "epoch [99/100] batch [4/4] time 0.329 (0.587) data 0.000 (0.262) loss 0.3256 (0.4103) acc 90.6250 (89.0625) lr 1.9733e-05 eta 0:00:02\n",
      "epoch [100/100] batch [1/4] time 1.393 (1.393) data 1.080 (1.080) loss 0.2069 (0.2069) acc 93.7500 (93.7500) lr 1.9733e-05 eta 0:00:04\n",
      "epoch [100/100] batch [2/4] time 0.320 (0.856) data 0.001 (0.540) loss 0.5661 (0.3865) acc 78.1250 (85.9375) lr 1.9733e-05 eta 0:00:01\n",
      "epoch [100/100] batch [3/4] time 0.339 (0.684) data 0.000 (0.360) loss 0.8107 (0.5279) acc 75.0000 (82.2917) lr 1.9733e-05 eta 0:00:00\n",
      "epoch [100/100] batch [4/4] time 0.326 (0.594) data 0.000 (0.270) loss 0.5827 (0.5416) acc 81.2500 (82.0312) lr 4.9344e-06 eta 0:00:00\n",
      "Checkpoint saved to output/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1/prompt_learner/model.pth.tar-100\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 37/37 [00:43<00:00,  1.18s/it]\n",
      "=> result\n",
      "* total: 3,669\n",
      "* correct: 3,382\n",
      "* accuracy: 92.2%\n",
      "* error: 7.8%\n",
      "* macro_f1: 92.1%\n",
      "Elapsed: 0:05:25\n"
     ]
    }
   ],
   "source": [
    "#oxford_pets-4shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YT8gJIg_yV1A",
    "outputId": "a0c88055-19a4-432e-e6c0-5c5d272ec17b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:08:28.648403: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:08:28.668091: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:08:28.673994: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:08:28.690349: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:08:29.736745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2']\n",
      "output_dir: output/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordPets\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.02\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 0.1\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: OxfordPets\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_2-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  ----------\n",
      "Dataset    OxfordPets\n",
      "# classes  37\n",
      "# train_x  74\n",
      "# val      74\n",
      "# test     3,669\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1/tensorboard)\n",
      "epoch [1/100] batch [1/2] time 2.169 (2.169) data 0.771 (0.771) loss 3.0888 (3.0888) acc 15.6250 (15.6250) lr 1.0000e-05 eta 0:07:11\n",
      "epoch [1/100] batch [2/2] time 0.313 (1.241) data 0.001 (0.386) loss 2.9013 (2.9950) acc 18.7500 (17.1875) lr 2.0000e-02 eta 0:04:05\n",
      "epoch [2/100] batch [1/2] time 1.032 (1.032) data 0.720 (0.720) loss 2.7606 (2.7606) acc 28.1250 (28.1250) lr 2.0000e-02 eta 0:03:23\n",
      "epoch [2/100] batch [2/2] time 0.318 (0.675) data 0.001 (0.360) loss 2.5429 (2.6518) acc 28.1250 (28.1250) lr 1.9995e-02 eta 0:02:12\n",
      "epoch [3/100] batch [1/2] time 0.965 (0.965) data 0.656 (0.656) loss 2.3704 (2.3704) acc 25.0000 (25.0000) lr 1.9995e-02 eta 0:03:08\n",
      "epoch [3/100] batch [2/2] time 0.317 (0.641) data 0.001 (0.328) loss 2.0636 (2.2170) acc 46.8750 (35.9375) lr 1.9980e-02 eta 0:02:04\n",
      "epoch [4/100] batch [1/2] time 0.950 (0.950) data 0.637 (0.637) loss 1.3079 (1.3079) acc 68.7500 (68.7500) lr 1.9980e-02 eta 0:03:03\n",
      "epoch [4/100] batch [2/2] time 0.318 (0.634) data 0.001 (0.319) loss 1.5269 (1.4174) acc 62.5000 (65.6250) lr 1.9956e-02 eta 0:02:01\n",
      "epoch [5/100] batch [1/2] time 1.019 (1.019) data 0.708 (0.708) loss 1.0707 (1.0707) acc 75.0000 (75.0000) lr 1.9956e-02 eta 0:03:14\n",
      "epoch [5/100] batch [2/2] time 0.320 (0.669) data 0.000 (0.354) loss 0.9321 (1.0014) acc 75.0000 (75.0000) lr 1.9921e-02 eta 0:02:07\n",
      "epoch [6/100] batch [1/2] time 1.301 (1.301) data 0.983 (0.983) loss 1.1728 (1.1728) acc 71.8750 (71.8750) lr 1.9921e-02 eta 0:04:05\n",
      "epoch [6/100] batch [2/2] time 0.322 (0.812) data 0.001 (0.492) loss 1.1989 (1.1859) acc 78.1250 (75.0000) lr 1.9877e-02 eta 0:02:32\n",
      "epoch [7/100] batch [1/2] time 1.451 (1.451) data 1.133 (1.133) loss 1.1617 (1.1617) acc 65.6250 (65.6250) lr 1.9877e-02 eta 0:04:31\n",
      "epoch [7/100] batch [2/2] time 0.320 (0.885) data 0.001 (0.567) loss 0.6903 (0.9260) acc 81.2500 (73.4375) lr 1.9823e-02 eta 0:02:44\n",
      "epoch [8/100] batch [1/2] time 1.058 (1.058) data 0.745 (0.745) loss 0.9166 (0.9166) acc 65.6250 (65.6250) lr 1.9823e-02 eta 0:03:15\n",
      "epoch [8/100] batch [2/2] time 0.316 (0.687) data 0.000 (0.373) loss 0.7094 (0.8130) acc 78.1250 (71.8750) lr 1.9759e-02 eta 0:02:06\n",
      "epoch [9/100] batch [1/2] time 0.978 (0.978) data 0.662 (0.662) loss 0.7927 (0.7927) acc 84.3750 (84.3750) lr 1.9759e-02 eta 0:02:58\n",
      "epoch [9/100] batch [2/2] time 0.320 (0.649) data 0.001 (0.331) loss 0.4960 (0.6443) acc 84.3750 (84.3750) lr 1.9686e-02 eta 0:01:58\n",
      "epoch [10/100] batch [1/2] time 0.969 (0.969) data 0.654 (0.654) loss 0.6168 (0.6168) acc 87.5000 (87.5000) lr 1.9686e-02 eta 0:02:55\n",
      "epoch [10/100] batch [2/2] time 0.320 (0.644) data 0.000 (0.327) loss 0.6389 (0.6278) acc 78.1250 (82.8125) lr 1.9603e-02 eta 0:01:55\n",
      "epoch [11/100] batch [1/2] time 0.979 (0.979) data 0.663 (0.663) loss 0.8506 (0.8506) acc 71.8750 (71.8750) lr 1.9603e-02 eta 0:02:55\n",
      "epoch [11/100] batch [2/2] time 0.321 (0.650) data 0.001 (0.332) loss 0.6517 (0.7512) acc 78.1250 (75.0000) lr 1.9511e-02 eta 0:01:55\n",
      "epoch [12/100] batch [1/2] time 1.018 (1.018) data 0.703 (0.703) loss 0.6467 (0.6467) acc 78.1250 (78.1250) lr 1.9511e-02 eta 0:03:00\n",
      "epoch [12/100] batch [2/2] time 0.324 (0.671) data 0.000 (0.352) loss 0.5695 (0.6081) acc 84.3750 (81.2500) lr 1.9409e-02 eta 0:01:58\n",
      "epoch [13/100] batch [1/2] time 0.978 (0.978) data 0.663 (0.663) loss 0.4684 (0.4684) acc 81.2500 (81.2500) lr 1.9409e-02 eta 0:02:51\n",
      "epoch [13/100] batch [2/2] time 0.323 (0.650) data 0.001 (0.332) loss 0.9005 (0.6845) acc 65.6250 (73.4375) lr 1.9298e-02 eta 0:01:53\n",
      "epoch [14/100] batch [1/2] time 1.044 (1.044) data 0.727 (0.727) loss 0.6422 (0.6422) acc 78.1250 (78.1250) lr 1.9298e-02 eta 0:03:00\n",
      "epoch [14/100] batch [2/2] time 0.323 (0.683) data 0.001 (0.364) loss 0.7688 (0.7055) acc 78.1250 (78.1250) lr 1.9178e-02 eta 0:01:57\n",
      "epoch [15/100] batch [1/2] time 1.297 (1.297) data 0.973 (0.973) loss 0.4051 (0.4051) acc 96.8750 (96.8750) lr 1.9178e-02 eta 0:03:41\n",
      "epoch [15/100] batch [2/2] time 0.329 (0.813) data 0.001 (0.487) loss 0.5452 (0.4751) acc 81.2500 (89.0625) lr 1.9048e-02 eta 0:02:18\n",
      "epoch [16/100] batch [1/2] time 1.361 (1.361) data 1.037 (1.037) loss 0.6981 (0.6981) acc 81.2500 (81.2500) lr 1.9048e-02 eta 0:03:49\n",
      "epoch [16/100] batch [2/2] time 0.325 (0.843) data 0.001 (0.519) loss 1.0511 (0.8746) acc 68.7500 (75.0000) lr 1.8910e-02 eta 0:02:21\n",
      "epoch [17/100] batch [1/2] time 1.061 (1.061) data 0.742 (0.742) loss 0.6014 (0.6014) acc 84.3750 (84.3750) lr 1.8910e-02 eta 0:02:57\n",
      "epoch [17/100] batch [2/2] time 0.324 (0.692) data 0.001 (0.372) loss 0.7825 (0.6919) acc 81.2500 (82.8125) lr 1.8763e-02 eta 0:01:54\n",
      "epoch [18/100] batch [1/2] time 1.016 (1.016) data 0.698 (0.698) loss 0.7565 (0.7565) acc 84.3750 (84.3750) lr 1.8763e-02 eta 0:02:47\n",
      "epoch [18/100] batch [2/2] time 0.323 (0.669) data 0.001 (0.349) loss 0.6363 (0.6964) acc 78.1250 (81.2500) lr 1.8607e-02 eta 0:01:49\n",
      "epoch [19/100] batch [1/2] time 0.991 (0.991) data 0.672 (0.672) loss 0.7490 (0.7490) acc 87.5000 (87.5000) lr 1.8607e-02 eta 0:02:41\n",
      "epoch [19/100] batch [2/2] time 0.324 (0.657) data 0.001 (0.336) loss 0.7758 (0.7624) acc 84.3750 (85.9375) lr 1.8443e-02 eta 0:01:46\n",
      "epoch [20/100] batch [1/2] time 1.019 (1.019) data 0.700 (0.700) loss 0.6351 (0.6351) acc 84.3750 (84.3750) lr 1.8443e-02 eta 0:02:44\n",
      "epoch [20/100] batch [2/2] time 0.323 (0.671) data 0.001 (0.350) loss 0.5608 (0.5980) acc 75.0000 (79.6875) lr 1.8271e-02 eta 0:01:47\n",
      "epoch [21/100] batch [1/2] time 1.029 (1.029) data 0.711 (0.711) loss 0.7741 (0.7741) acc 75.0000 (75.0000) lr 1.8271e-02 eta 0:02:43\n",
      "epoch [21/100] batch [2/2] time 0.323 (0.676) data 0.000 (0.356) loss 0.7628 (0.7684) acc 87.5000 (81.2500) lr 1.8090e-02 eta 0:01:46\n",
      "epoch [22/100] batch [1/2] time 1.052 (1.052) data 0.734 (0.734) loss 0.2752 (0.2752) acc 90.6250 (90.6250) lr 1.8090e-02 eta 0:02:45\n",
      "epoch [22/100] batch [2/2] time 0.325 (0.688) data 0.001 (0.367) loss 0.6636 (0.4694) acc 84.3750 (87.5000) lr 1.7902e-02 eta 0:01:47\n",
      "epoch [23/100] batch [1/2] time 1.013 (1.013) data 0.694 (0.694) loss 0.4418 (0.4418) acc 81.2500 (81.2500) lr 1.7902e-02 eta 0:02:37\n",
      "epoch [23/100] batch [2/2] time 0.330 (0.671) data 0.000 (0.347) loss 0.4325 (0.4371) acc 93.7500 (87.5000) lr 1.7705e-02 eta 0:01:43\n",
      "epoch [24/100] batch [1/2] time 1.447 (1.447) data 1.110 (1.110) loss 0.5940 (0.5940) acc 84.3750 (84.3750) lr 1.7705e-02 eta 0:03:41\n",
      "epoch [24/100] batch [2/2] time 0.324 (0.886) data 0.001 (0.556) loss 0.7951 (0.6945) acc 78.1250 (81.2500) lr 1.7501e-02 eta 0:02:14\n",
      "epoch [25/100] batch [1/2] time 1.525 (1.525) data 1.205 (1.205) loss 0.5396 (0.5396) acc 81.2500 (81.2500) lr 1.7501e-02 eta 0:03:50\n",
      "epoch [25/100] batch [2/2] time 0.324 (0.924) data 0.001 (0.603) loss 0.5947 (0.5672) acc 84.3750 (82.8125) lr 1.7290e-02 eta 0:02:18\n",
      "epoch [26/100] batch [1/2] time 1.057 (1.057) data 0.739 (0.739) loss 0.5121 (0.5121) acc 90.6250 (90.6250) lr 1.7290e-02 eta 0:02:37\n",
      "epoch [26/100] batch [2/2] time 0.325 (0.691) data 0.001 (0.370) loss 0.4061 (0.4591) acc 90.6250 (90.6250) lr 1.7071e-02 eta 0:01:42\n",
      "epoch [27/100] batch [1/2] time 0.980 (0.980) data 0.660 (0.660) loss 0.6941 (0.6941) acc 78.1250 (78.1250) lr 1.7071e-02 eta 0:02:24\n",
      "epoch [27/100] batch [2/2] time 0.324 (0.652) data 0.001 (0.330) loss 0.4987 (0.5964) acc 87.5000 (82.8125) lr 1.6845e-02 eta 0:01:35\n",
      "epoch [28/100] batch [1/2] time 1.029 (1.029) data 0.710 (0.710) loss 0.5411 (0.5411) acc 81.2500 (81.2500) lr 1.6845e-02 eta 0:02:29\n",
      "epoch [28/100] batch [2/2] time 0.324 (0.676) data 0.000 (0.355) loss 0.5669 (0.5540) acc 81.2500 (81.2500) lr 1.6613e-02 eta 0:01:37\n",
      "epoch [29/100] batch [1/2] time 1.021 (1.021) data 0.705 (0.705) loss 0.6679 (0.6679) acc 84.3750 (84.3750) lr 1.6613e-02 eta 0:02:26\n",
      "epoch [29/100] batch [2/2] time 0.323 (0.672) data 0.001 (0.353) loss 0.5006 (0.5842) acc 84.3750 (84.3750) lr 1.6374e-02 eta 0:01:35\n",
      "epoch [30/100] batch [1/2] time 1.010 (1.010) data 0.693 (0.693) loss 0.5769 (0.5769) acc 84.3750 (84.3750) lr 1.6374e-02 eta 0:02:22\n",
      "epoch [30/100] batch [2/2] time 0.323 (0.667) data 0.001 (0.347) loss 0.4170 (0.4970) acc 87.5000 (85.9375) lr 1.6129e-02 eta 0:01:33\n",
      "epoch [31/100] batch [1/2] time 1.004 (1.004) data 0.682 (0.682) loss 0.6579 (0.6579) acc 78.1250 (78.1250) lr 1.6129e-02 eta 0:02:19\n",
      "epoch [31/100] batch [2/2] time 0.320 (0.662) data 0.001 (0.342) loss 0.3295 (0.4937) acc 90.6250 (84.3750) lr 1.5878e-02 eta 0:01:31\n",
      "epoch [32/100] batch [1/2] time 0.987 (0.987) data 0.668 (0.668) loss 0.5201 (0.5201) acc 87.5000 (87.5000) lr 1.5878e-02 eta 0:02:15\n",
      "epoch [32/100] batch [2/2] time 0.325 (0.656) data 0.001 (0.334) loss 0.7339 (0.6270) acc 81.2500 (84.3750) lr 1.5621e-02 eta 0:01:29\n",
      "epoch [33/100] batch [1/2] time 1.453 (1.453) data 1.136 (1.136) loss 0.5610 (0.5610) acc 81.2500 (81.2500) lr 1.5621e-02 eta 0:03:16\n",
      "epoch [33/100] batch [2/2] time 0.324 (0.889) data 0.001 (0.568) loss 0.4185 (0.4898) acc 93.7500 (87.5000) lr 1.5358e-02 eta 0:01:59\n",
      "epoch [34/100] batch [1/2] time 1.510 (1.510) data 1.192 (1.192) loss 0.4434 (0.4434) acc 87.5000 (87.5000) lr 1.5358e-02 eta 0:03:20\n",
      "epoch [34/100] batch [2/2] time 0.322 (0.916) data 0.001 (0.596) loss 0.5864 (0.5149) acc 81.2500 (84.3750) lr 1.5090e-02 eta 0:02:00\n",
      "epoch [35/100] batch [1/2] time 1.045 (1.045) data 0.728 (0.728) loss 0.5190 (0.5190) acc 87.5000 (87.5000) lr 1.5090e-02 eta 0:02:16\n",
      "epoch [35/100] batch [2/2] time 0.322 (0.684) data 0.000 (0.364) loss 0.7905 (0.6547) acc 78.1250 (82.8125) lr 1.4818e-02 eta 0:01:28\n",
      "epoch [36/100] batch [1/2] time 0.977 (0.977) data 0.660 (0.660) loss 0.6254 (0.6254) acc 75.0000 (75.0000) lr 1.4818e-02 eta 0:02:06\n",
      "epoch [36/100] batch [2/2] time 0.323 (0.650) data 0.000 (0.330) loss 0.2385 (0.4319) acc 96.8750 (85.9375) lr 1.4540e-02 eta 0:01:23\n",
      "epoch [37/100] batch [1/2] time 1.036 (1.036) data 0.722 (0.722) loss 0.6816 (0.6816) acc 81.2500 (81.2500) lr 1.4540e-02 eta 0:02:11\n",
      "epoch [37/100] batch [2/2] time 0.322 (0.679) data 0.000 (0.361) loss 0.6313 (0.6564) acc 87.5000 (84.3750) lr 1.4258e-02 eta 0:01:25\n",
      "epoch [38/100] batch [1/2] time 0.958 (0.958) data 0.643 (0.643) loss 0.2505 (0.2505) acc 93.7500 (93.7500) lr 1.4258e-02 eta 0:01:59\n",
      "epoch [38/100] batch [2/2] time 0.320 (0.639) data 0.000 (0.322) loss 0.4478 (0.3492) acc 81.2500 (87.5000) lr 1.3971e-02 eta 0:01:19\n",
      "epoch [39/100] batch [1/2] time 0.987 (0.987) data 0.672 (0.672) loss 0.4422 (0.4422) acc 84.3750 (84.3750) lr 1.3971e-02 eta 0:02:01\n",
      "epoch [39/100] batch [2/2] time 0.321 (0.654) data 0.000 (0.336) loss 0.3479 (0.3951) acc 93.7500 (89.0625) lr 1.3681e-02 eta 0:01:19\n",
      "epoch [40/100] batch [1/2] time 1.011 (1.011) data 0.695 (0.695) loss 0.4737 (0.4737) acc 84.3750 (84.3750) lr 1.3681e-02 eta 0:02:02\n",
      "epoch [40/100] batch [2/2] time 0.320 (0.665) data 0.000 (0.348) loss 0.4709 (0.4723) acc 90.6250 (87.5000) lr 1.3387e-02 eta 0:01:19\n",
      "epoch [41/100] batch [1/2] time 0.929 (0.929) data 0.614 (0.614) loss 1.1589 (1.1589) acc 71.8750 (71.8750) lr 1.3387e-02 eta 0:01:50\n",
      "epoch [41/100] batch [2/2] time 0.321 (0.625) data 0.000 (0.307) loss 0.3660 (0.7624) acc 93.7500 (82.8125) lr 1.3090e-02 eta 0:01:13\n",
      "epoch [42/100] batch [1/2] time 1.428 (1.428) data 1.111 (1.111) loss 0.5857 (0.5857) acc 81.2500 (81.2500) lr 1.3090e-02 eta 0:02:47\n",
      "epoch [42/100] batch [2/2] time 0.320 (0.874) data 0.001 (0.556) loss 0.3118 (0.4487) acc 93.7500 (87.5000) lr 1.2790e-02 eta 0:01:41\n",
      "epoch [43/100] batch [1/2] time 1.520 (1.520) data 1.207 (1.207) loss 0.3384 (0.3384) acc 93.7500 (93.7500) lr 1.2790e-02 eta 0:02:54\n",
      "epoch [43/100] batch [2/2] time 0.318 (0.919) data 0.000 (0.604) loss 0.2274 (0.2829) acc 90.6250 (92.1875) lr 1.2487e-02 eta 0:01:44\n",
      "epoch [44/100] batch [1/2] time 1.103 (1.103) data 0.790 (0.790) loss 0.3796 (0.3796) acc 90.6250 (90.6250) lr 1.2487e-02 eta 0:02:04\n",
      "epoch [44/100] batch [2/2] time 0.317 (0.710) data 0.000 (0.395) loss 0.3940 (0.3868) acc 90.6250 (90.6250) lr 1.2181e-02 eta 0:01:19\n",
      "epoch [45/100] batch [1/2] time 1.029 (1.029) data 0.717 (0.717) loss 0.3537 (0.3537) acc 90.6250 (90.6250) lr 1.2181e-02 eta 0:01:54\n",
      "epoch [45/100] batch [2/2] time 0.317 (0.673) data 0.000 (0.359) loss 0.3236 (0.3386) acc 96.8750 (93.7500) lr 1.1874e-02 eta 0:01:14\n",
      "epoch [46/100] batch [1/2] time 1.054 (1.054) data 0.739 (0.739) loss 0.4720 (0.4720) acc 90.6250 (90.6250) lr 1.1874e-02 eta 0:01:54\n",
      "epoch [46/100] batch [2/2] time 0.320 (0.687) data 0.001 (0.370) loss 0.3680 (0.4200) acc 90.6250 (90.6250) lr 1.1564e-02 eta 0:01:14\n",
      "epoch [47/100] batch [1/2] time 1.013 (1.013) data 0.699 (0.699) loss 0.3130 (0.3130) acc 90.6250 (90.6250) lr 1.1564e-02 eta 0:01:48\n",
      "epoch [47/100] batch [2/2] time 0.320 (0.666) data 0.000 (0.350) loss 0.4600 (0.3865) acc 90.6250 (90.6250) lr 1.1253e-02 eta 0:01:10\n",
      "epoch [48/100] batch [1/2] time 0.965 (0.965) data 0.653 (0.653) loss 0.7031 (0.7031) acc 75.0000 (75.0000) lr 1.1253e-02 eta 0:01:41\n",
      "epoch [48/100] batch [2/2] time 0.318 (0.642) data 0.000 (0.327) loss 0.7768 (0.7399) acc 75.0000 (75.0000) lr 1.0941e-02 eta 0:01:06\n",
      "epoch [49/100] batch [1/2] time 1.026 (1.026) data 0.710 (0.710) loss 0.4345 (0.4345) acc 87.5000 (87.5000) lr 1.0941e-02 eta 0:01:45\n",
      "epoch [49/100] batch [2/2] time 0.318 (0.672) data 0.000 (0.355) loss 0.5536 (0.4940) acc 84.3750 (85.9375) lr 1.0628e-02 eta 0:01:08\n",
      "epoch [50/100] batch [1/2] time 0.972 (0.972) data 0.656 (0.656) loss 0.4542 (0.4542) acc 90.6250 (90.6250) lr 1.0628e-02 eta 0:01:38\n",
      "epoch [50/100] batch [2/2] time 0.320 (0.646) data 0.001 (0.328) loss 0.6249 (0.5396) acc 78.1250 (84.3750) lr 1.0314e-02 eta 0:01:04\n",
      "epoch [51/100] batch [1/2] time 1.465 (1.465) data 1.151 (1.151) loss 0.5424 (0.5424) acc 93.7500 (93.7500) lr 1.0314e-02 eta 0:02:25\n",
      "epoch [51/100] batch [2/2] time 0.320 (0.893) data 0.001 (0.576) loss 0.3824 (0.4624) acc 90.6250 (92.1875) lr 1.0000e-02 eta 0:01:27\n",
      "epoch [52/100] batch [1/2] time 1.441 (1.441) data 1.127 (1.127) loss 0.3446 (0.3446) acc 93.7500 (93.7500) lr 1.0000e-02 eta 0:02:19\n",
      "epoch [52/100] batch [2/2] time 0.317 (0.879) data 0.001 (0.564) loss 0.7513 (0.5480) acc 78.1250 (85.9375) lr 9.6859e-03 eta 0:01:24\n",
      "epoch [53/100] batch [1/2] time 1.034 (1.034) data 0.722 (0.722) loss 0.5043 (0.5043) acc 84.3750 (84.3750) lr 9.6859e-03 eta 0:01:38\n",
      "epoch [53/100] batch [2/2] time 0.318 (0.676) data 0.001 (0.361) loss 0.2164 (0.3603) acc 93.7500 (89.0625) lr 9.3721e-03 eta 0:01:03\n",
      "epoch [54/100] batch [1/2] time 0.955 (0.955) data 0.641 (0.641) loss 0.3014 (0.3014) acc 93.7500 (93.7500) lr 9.3721e-03 eta 0:01:28\n",
      "epoch [54/100] batch [2/2] time 0.318 (0.637) data 0.001 (0.321) loss 0.2652 (0.2833) acc 93.7500 (93.7500) lr 9.0589e-03 eta 0:00:58\n",
      "epoch [55/100] batch [1/2] time 0.944 (0.944) data 0.630 (0.630) loss 0.3826 (0.3826) acc 96.8750 (96.8750) lr 9.0589e-03 eta 0:01:25\n",
      "epoch [55/100] batch [2/2] time 0.318 (0.631) data 0.001 (0.315) loss 0.7015 (0.5421) acc 75.0000 (85.9375) lr 8.7467e-03 eta 0:00:56\n",
      "epoch [56/100] batch [1/2] time 0.971 (0.971) data 0.655 (0.655) loss 0.4063 (0.4063) acc 93.7500 (93.7500) lr 8.7467e-03 eta 0:01:26\n",
      "epoch [56/100] batch [2/2] time 0.319 (0.645) data 0.000 (0.328) loss 0.2544 (0.3304) acc 96.8750 (95.3125) lr 8.4357e-03 eta 0:00:56\n",
      "epoch [57/100] batch [1/2] time 1.009 (1.009) data 0.687 (0.687) loss 0.3528 (0.3528) acc 87.5000 (87.5000) lr 8.4357e-03 eta 0:01:27\n",
      "epoch [57/100] batch [2/2] time 0.320 (0.664) data 0.001 (0.344) loss 0.4133 (0.3831) acc 81.2500 (84.3750) lr 8.1262e-03 eta 0:00:57\n",
      "epoch [58/100] batch [1/2] time 0.984 (0.984) data 0.670 (0.670) loss 0.8928 (0.8928) acc 78.1250 (78.1250) lr 8.1262e-03 eta 0:01:23\n",
      "epoch [58/100] batch [2/2] time 0.320 (0.652) data 0.001 (0.335) loss 0.4710 (0.6819) acc 90.6250 (84.3750) lr 7.8186e-03 eta 0:00:54\n",
      "epoch [59/100] batch [1/2] time 0.989 (0.989) data 0.674 (0.674) loss 0.6702 (0.6702) acc 84.3750 (84.3750) lr 7.8186e-03 eta 0:01:22\n",
      "epoch [59/100] batch [2/2] time 0.318 (0.653) data 0.001 (0.337) loss 0.4598 (0.5650) acc 90.6250 (87.5000) lr 7.5131e-03 eta 0:00:53\n",
      "epoch [60/100] batch [1/2] time 1.460 (1.460) data 1.137 (1.137) loss 0.4065 (0.4065) acc 90.6250 (90.6250) lr 7.5131e-03 eta 0:01:58\n",
      "epoch [60/100] batch [2/2] time 0.321 (0.891) data 0.001 (0.569) loss 0.5103 (0.4584) acc 90.6250 (90.6250) lr 7.2101e-03 eta 0:01:11\n",
      "epoch [61/100] batch [1/2] time 1.483 (1.483) data 1.170 (1.170) loss 0.3675 (0.3675) acc 87.5000 (87.5000) lr 7.2101e-03 eta 0:01:57\n",
      "epoch [61/100] batch [2/2] time 0.322 (0.903) data 0.001 (0.585) loss 0.4756 (0.4216) acc 84.3750 (85.9375) lr 6.9098e-03 eta 0:01:10\n",
      "epoch [62/100] batch [1/2] time 1.067 (1.067) data 0.754 (0.754) loss 0.5515 (0.5515) acc 81.2500 (81.2500) lr 6.9098e-03 eta 0:01:22\n",
      "epoch [62/100] batch [2/2] time 0.321 (0.694) data 0.000 (0.377) loss 0.5098 (0.5307) acc 90.6250 (85.9375) lr 6.6126e-03 eta 0:00:52\n",
      "epoch [63/100] batch [1/2] time 0.985 (0.985) data 0.670 (0.670) loss 0.2218 (0.2218) acc 96.8750 (96.8750) lr 6.6126e-03 eta 0:01:13\n",
      "epoch [63/100] batch [2/2] time 0.320 (0.653) data 0.001 (0.335) loss 0.2637 (0.2427) acc 93.7500 (95.3125) lr 6.3188e-03 eta 0:00:48\n",
      "epoch [64/100] batch [1/2] time 0.951 (0.951) data 0.636 (0.636) loss 0.5188 (0.5188) acc 87.5000 (87.5000) lr 6.3188e-03 eta 0:01:09\n",
      "epoch [64/100] batch [2/2] time 0.322 (0.637) data 0.001 (0.318) loss 0.4340 (0.4764) acc 87.5000 (87.5000) lr 6.0285e-03 eta 0:00:45\n",
      "epoch [65/100] batch [1/2] time 1.013 (1.013) data 0.696 (0.696) loss 0.4729 (0.4729) acc 87.5000 (87.5000) lr 6.0285e-03 eta 0:01:11\n",
      "epoch [65/100] batch [2/2] time 0.322 (0.668) data 0.001 (0.348) loss 0.5856 (0.5293) acc 81.2500 (84.3750) lr 5.7422e-03 eta 0:00:46\n",
      "epoch [66/100] batch [1/2] time 0.995 (0.995) data 0.679 (0.679) loss 0.5300 (0.5300) acc 84.3750 (84.3750) lr 5.7422e-03 eta 0:01:08\n",
      "epoch [66/100] batch [2/2] time 0.319 (0.657) data 0.001 (0.340) loss 0.4570 (0.4935) acc 84.3750 (84.3750) lr 5.4601e-03 eta 0:00:44\n",
      "epoch [67/100] batch [1/2] time 1.017 (1.017) data 0.701 (0.701) loss 0.4741 (0.4741) acc 87.5000 (87.5000) lr 5.4601e-03 eta 0:01:08\n",
      "epoch [67/100] batch [2/2] time 0.322 (0.670) data 0.000 (0.351) loss 0.6247 (0.5494) acc 81.2500 (84.3750) lr 5.1825e-03 eta 0:00:44\n",
      "epoch [68/100] batch [1/2] time 1.018 (1.018) data 0.704 (0.704) loss 0.2488 (0.2488) acc 93.7500 (93.7500) lr 5.1825e-03 eta 0:01:06\n",
      "epoch [68/100] batch [2/2] time 0.321 (0.669) data 0.000 (0.352) loss 0.5670 (0.4079) acc 81.2500 (87.5000) lr 4.9096e-03 eta 0:00:42\n",
      "epoch [69/100] batch [1/2] time 1.438 (1.438) data 1.116 (1.116) loss 0.2559 (0.2559) acc 93.7500 (93.7500) lr 4.9096e-03 eta 0:01:30\n",
      "epoch [69/100] batch [2/2] time 0.326 (0.882) data 0.001 (0.558) loss 0.5949 (0.4254) acc 87.5000 (90.6250) lr 4.6417e-03 eta 0:00:54\n",
      "epoch [70/100] batch [1/2] time 1.331 (1.331) data 1.005 (1.005) loss 0.4643 (0.4643) acc 87.5000 (87.5000) lr 4.6417e-03 eta 0:01:21\n",
      "epoch [70/100] batch [2/2] time 0.324 (0.827) data 0.001 (0.503) loss 0.3219 (0.3931) acc 90.6250 (89.0625) lr 4.3792e-03 eta 0:00:49\n",
      "epoch [71/100] batch [1/2] time 1.096 (1.096) data 0.780 (0.780) loss 0.5441 (0.5441) acc 84.3750 (84.3750) lr 4.3792e-03 eta 0:01:04\n",
      "epoch [71/100] batch [2/2] time 0.320 (0.708) data 0.000 (0.390) loss 0.4237 (0.4839) acc 87.5000 (85.9375) lr 4.1221e-03 eta 0:00:41\n",
      "epoch [72/100] batch [1/2] time 0.973 (0.973) data 0.655 (0.655) loss 0.4306 (0.4306) acc 90.6250 (90.6250) lr 4.1221e-03 eta 0:00:55\n",
      "epoch [72/100] batch [2/2] time 0.321 (0.647) data 0.001 (0.328) loss 0.6344 (0.5325) acc 81.2500 (85.9375) lr 3.8709e-03 eta 0:00:36\n",
      "epoch [73/100] batch [1/2] time 1.032 (1.032) data 0.717 (0.717) loss 0.6202 (0.6202) acc 81.2500 (81.2500) lr 3.8709e-03 eta 0:00:56\n",
      "epoch [73/100] batch [2/2] time 0.323 (0.677) data 0.001 (0.359) loss 0.4064 (0.5133) acc 93.7500 (87.5000) lr 3.6258e-03 eta 0:00:36\n",
      "epoch [74/100] batch [1/2] time 1.035 (1.035) data 0.719 (0.719) loss 0.3446 (0.3446) acc 84.3750 (84.3750) lr 3.6258e-03 eta 0:00:54\n",
      "epoch [74/100] batch [2/2] time 0.322 (0.678) data 0.001 (0.360) loss 0.7081 (0.5263) acc 84.3750 (84.3750) lr 3.3869e-03 eta 0:00:35\n",
      "epoch [75/100] batch [1/2] time 1.036 (1.036) data 0.718 (0.718) loss 0.6241 (0.6241) acc 84.3750 (84.3750) lr 3.3869e-03 eta 0:00:52\n",
      "epoch [75/100] batch [2/2] time 0.323 (0.680) data 0.000 (0.359) loss 0.5201 (0.5721) acc 84.3750 (84.3750) lr 3.1545e-03 eta 0:00:33\n",
      "epoch [76/100] batch [1/2] time 0.954 (0.954) data 0.639 (0.639) loss 0.3091 (0.3091) acc 87.5000 (87.5000) lr 3.1545e-03 eta 0:00:46\n",
      "epoch [76/100] batch [2/2] time 0.324 (0.639) data 0.000 (0.320) loss 0.5381 (0.4236) acc 84.3750 (85.9375) lr 2.9289e-03 eta 0:00:30\n",
      "epoch [77/100] batch [1/2] time 0.986 (0.986) data 0.673 (0.673) loss 0.2791 (0.2791) acc 90.6250 (90.6250) lr 2.9289e-03 eta 0:00:46\n",
      "epoch [77/100] batch [2/2] time 0.322 (0.654) data 0.000 (0.337) loss 0.5918 (0.4355) acc 81.2500 (85.9375) lr 2.7103e-03 eta 0:00:30\n",
      "epoch [78/100] batch [1/2] time 1.493 (1.493) data 1.175 (1.175) loss 0.6509 (0.6509) acc 78.1250 (78.1250) lr 2.7103e-03 eta 0:01:07\n",
      "epoch [78/100] batch [2/2] time 0.325 (0.909) data 0.001 (0.588) loss 0.4937 (0.5723) acc 87.5000 (82.8125) lr 2.4989e-03 eta 0:00:39\n",
      "epoch [79/100] batch [1/2] time 1.478 (1.478) data 1.158 (1.158) loss 0.3255 (0.3255) acc 90.6250 (90.6250) lr 2.4989e-03 eta 0:01:03\n",
      "epoch [79/100] batch [2/2] time 0.322 (0.900) data 0.001 (0.579) loss 0.3257 (0.3256) acc 90.6250 (90.6250) lr 2.2949e-03 eta 0:00:37\n",
      "epoch [80/100] batch [1/2] time 0.988 (0.988) data 0.670 (0.670) loss 0.7696 (0.7696) acc 78.1250 (78.1250) lr 2.2949e-03 eta 0:00:40\n",
      "epoch [80/100] batch [2/2] time 0.317 (0.652) data 0.001 (0.335) loss 0.3604 (0.5650) acc 90.6250 (84.3750) lr 2.0984e-03 eta 0:00:26\n",
      "epoch [81/100] batch [1/2] time 1.046 (1.046) data 0.734 (0.734) loss 0.3658 (0.3658) acc 93.7500 (93.7500) lr 2.0984e-03 eta 0:00:40\n",
      "epoch [81/100] batch [2/2] time 0.322 (0.684) data 0.001 (0.367) loss 0.5357 (0.4507) acc 84.3750 (89.0625) lr 1.9098e-03 eta 0:00:26\n",
      "epoch [82/100] batch [1/2] time 1.003 (1.003) data 0.687 (0.687) loss 0.3931 (0.3931) acc 96.8750 (96.8750) lr 1.9098e-03 eta 0:00:37\n",
      "epoch [82/100] batch [2/2] time 0.318 (0.661) data 0.001 (0.344) loss 0.3404 (0.3668) acc 93.7500 (95.3125) lr 1.7292e-03 eta 0:00:23\n",
      "epoch [83/100] batch [1/2] time 1.054 (1.054) data 0.740 (0.740) loss 0.2372 (0.2372) acc 93.7500 (93.7500) lr 1.7292e-03 eta 0:00:36\n",
      "epoch [83/100] batch [2/2] time 0.320 (0.687) data 0.001 (0.370) loss 0.3917 (0.3145) acc 87.5000 (90.6250) lr 1.5567e-03 eta 0:00:23\n",
      "epoch [84/100] batch [1/2] time 1.005 (1.005) data 0.690 (0.690) loss 0.7574 (0.7574) acc 78.1250 (78.1250) lr 1.5567e-03 eta 0:00:33\n",
      "epoch [84/100] batch [2/2] time 0.321 (0.663) data 0.000 (0.345) loss 0.4546 (0.6060) acc 87.5000 (82.8125) lr 1.3926e-03 eta 0:00:21\n",
      "epoch [85/100] batch [1/2] time 0.971 (0.971) data 0.652 (0.652) loss 0.3338 (0.3338) acc 90.6250 (90.6250) lr 1.3926e-03 eta 0:00:30\n",
      "epoch [85/100] batch [2/2] time 0.319 (0.645) data 0.001 (0.326) loss 0.4549 (0.3943) acc 87.5000 (89.0625) lr 1.2369e-03 eta 0:00:19\n",
      "epoch [86/100] batch [1/2] time 1.004 (1.004) data 0.692 (0.692) loss 0.3145 (0.3145) acc 90.6250 (90.6250) lr 1.2369e-03 eta 0:00:29\n",
      "epoch [86/100] batch [2/2] time 0.322 (0.663) data 0.001 (0.346) loss 0.4395 (0.3770) acc 90.6250 (90.6250) lr 1.0899e-03 eta 0:00:18\n",
      "epoch [87/100] batch [1/2] time 1.426 (1.426) data 1.109 (1.109) loss 0.4361 (0.4361) acc 84.3750 (84.3750) lr 1.0899e-03 eta 0:00:38\n",
      "epoch [87/100] batch [2/2] time 0.323 (0.874) data 0.001 (0.555) loss 0.2947 (0.3654) acc 93.7500 (89.0625) lr 9.5173e-04 eta 0:00:22\n",
      "epoch [88/100] batch [1/2] time 1.575 (1.575) data 1.256 (1.256) loss 0.2188 (0.2188) acc 100.0000 (100.0000) lr 9.5173e-04 eta 0:00:39\n",
      "epoch [88/100] batch [2/2] time 0.321 (0.948) data 0.001 (0.628) loss 0.2553 (0.2371) acc 93.7500 (96.8750) lr 8.2245e-04 eta 0:00:22\n",
      "epoch [89/100] batch [1/2] time 1.037 (1.037) data 0.720 (0.720) loss 0.6514 (0.6514) acc 81.2500 (81.2500) lr 8.2245e-04 eta 0:00:23\n",
      "epoch [89/100] batch [2/2] time 0.321 (0.679) data 0.001 (0.360) loss 0.2584 (0.4549) acc 96.8750 (89.0625) lr 7.0224e-04 eta 0:00:14\n",
      "epoch [90/100] batch [1/2] time 0.969 (0.969) data 0.651 (0.651) loss 0.4971 (0.4971) acc 87.5000 (87.5000) lr 7.0224e-04 eta 0:00:20\n",
      "epoch [90/100] batch [2/2] time 0.320 (0.644) data 0.001 (0.326) loss 0.3826 (0.4399) acc 90.6250 (89.0625) lr 5.9119e-04 eta 0:00:12\n",
      "epoch [91/100] batch [1/2] time 0.980 (0.980) data 0.665 (0.665) loss 0.6680 (0.6680) acc 87.5000 (87.5000) lr 5.9119e-04 eta 0:00:18\n",
      "epoch [91/100] batch [2/2] time 0.323 (0.651) data 0.000 (0.333) loss 0.4153 (0.5417) acc 84.3750 (85.9375) lr 4.8943e-04 eta 0:00:11\n",
      "epoch [92/100] batch [1/2] time 1.003 (1.003) data 0.689 (0.689) loss 0.5157 (0.5157) acc 84.3750 (84.3750) lr 4.8943e-04 eta 0:00:17\n",
      "epoch [92/100] batch [2/2] time 0.319 (0.661) data 0.001 (0.345) loss 0.3992 (0.4574) acc 93.7500 (89.0625) lr 3.9706e-04 eta 0:00:10\n",
      "epoch [93/100] batch [1/2] time 0.960 (0.960) data 0.644 (0.644) loss 0.6021 (0.6021) acc 81.2500 (81.2500) lr 3.9706e-04 eta 0:00:14\n",
      "epoch [93/100] batch [2/2] time 0.322 (0.641) data 0.001 (0.322) loss 0.2708 (0.4364) acc 93.7500 (87.5000) lr 3.1417e-04 eta 0:00:08\n",
      "epoch [94/100] batch [1/2] time 1.007 (1.007) data 0.695 (0.695) loss 0.2842 (0.2842) acc 90.6250 (90.6250) lr 3.1417e-04 eta 0:00:13\n",
      "epoch [94/100] batch [2/2] time 0.322 (0.664) data 0.001 (0.348) loss 0.8174 (0.5508) acc 75.0000 (82.8125) lr 2.4083e-04 eta 0:00:07\n",
      "epoch [95/100] batch [1/2] time 0.971 (0.971) data 0.659 (0.659) loss 0.2184 (0.2184) acc 100.0000 (100.0000) lr 2.4083e-04 eta 0:00:10\n",
      "epoch [95/100] batch [2/2] time 0.321 (0.646) data 0.000 (0.330) loss 0.6998 (0.4591) acc 78.1250 (89.0625) lr 1.7713e-04 eta 0:00:06\n",
      "epoch [96/100] batch [1/2] time 1.485 (1.485) data 1.168 (1.168) loss 0.3360 (0.3360) acc 93.7500 (93.7500) lr 1.7713e-04 eta 0:00:13\n",
      "epoch [96/100] batch [2/2] time 0.319 (0.902) data 0.001 (0.584) loss 0.4087 (0.3724) acc 90.6250 (92.1875) lr 1.2312e-04 eta 0:00:07\n",
      "epoch [97/100] batch [1/2] time 1.355 (1.355) data 1.037 (1.037) loss 0.3809 (0.3809) acc 93.7500 (93.7500) lr 1.2312e-04 eta 0:00:09\n",
      "epoch [97/100] batch [2/2] time 0.321 (0.838) data 0.000 (0.518) loss 0.2123 (0.2966) acc 93.7500 (93.7500) lr 7.8853e-05 eta 0:00:05\n",
      "epoch [98/100] batch [1/2] time 1.064 (1.064) data 0.748 (0.748) loss 0.4388 (0.4388) acc 87.5000 (87.5000) lr 7.8853e-05 eta 0:00:05\n",
      "epoch [98/100] batch [2/2] time 0.322 (0.693) data 0.001 (0.374) loss 0.5738 (0.5063) acc 87.5000 (87.5000) lr 4.4380e-05 eta 0:00:02\n",
      "epoch [99/100] batch [1/2] time 0.997 (0.997) data 0.679 (0.679) loss 0.3504 (0.3504) acc 93.7500 (93.7500) lr 4.4380e-05 eta 0:00:02\n",
      "epoch [99/100] batch [2/2] time 0.323 (0.660) data 0.001 (0.340) loss 0.5606 (0.4555) acc 87.5000 (90.6250) lr 1.9733e-05 eta 0:00:01\n",
      "epoch [100/100] batch [1/2] time 0.979 (0.979) data 0.663 (0.663) loss 0.3765 (0.3765) acc 90.6250 (90.6250) lr 1.9733e-05 eta 0:00:00\n",
      "epoch [100/100] batch [2/2] time 0.324 (0.651) data 0.001 (0.332) loss 0.2570 (0.3168) acc 100.0000 (95.3125) lr 4.9344e-06 eta 0:00:00\n",
      "Checkpoint saved to output/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1/prompt_learner/model.pth.tar-100\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 37/37 [00:42<00:00,  1.16s/it]\n",
      "=> result\n",
      "* total: 3,669\n",
      "* correct: 3,333\n",
      "* accuracy: 90.8%\n",
      "* error: 9.2%\n",
      "* macro_f1: 90.7%\n",
      "Elapsed: 0:03:25\n"
     ]
    }
   ],
   "source": [
    "#oxford_pets-2shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElhLl8RMya3G",
    "outputId": "e5b9cfea-bd91-4921-e9e8-01f541ab5b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:12:10.998903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 15:12:11.019526: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 15:12:11.025689: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 15:12:11.041519: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:12:12.058910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
      "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '1']\n",
      "output_dir: output/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordPets\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 1\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.02\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 50\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 0.1\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "CPU family:                           6\n",
      "Model:                                79\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             0\n",
      "BogoMIPS:                             4399.99\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             256 KiB (1 instance)\n",
      "L3 cache:                             55 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: OxfordPets\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_1-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  ----------\n",
      "Dataset    OxfordPets\n",
      "# classes  37\n",
      "# train_x  37\n",
      "# val      37\n",
      "# test     3,669\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
      "epoch [1/50] batch [1/1] time 2.038 (2.038) data 0.618 (0.618) loss 3.1689 (3.1689) acc 18.7500 (18.7500) lr 2.0000e-02 eta 0:01:39\n",
      "epoch [2/50] batch [1/1] time 0.835 (0.835) data 0.525 (0.525) loss 3.0790 (3.0790) acc 25.0000 (25.0000) lr 1.9980e-02 eta 0:00:40\n",
      "epoch [3/50] batch [1/1] time 0.835 (0.835) data 0.522 (0.522) loss 2.7429 (2.7429) acc 28.1250 (28.1250) lr 1.9921e-02 eta 0:00:39\n",
      "epoch [4/50] batch [1/1] time 1.043 (1.043) data 0.727 (0.727) loss 4.2339 (4.2339) acc 9.3750 (9.3750) lr 1.9823e-02 eta 0:00:47\n",
      "epoch [5/50] batch [1/1] time 1.038 (1.038) data 0.726 (0.726) loss 3.2850 (3.2850) acc 9.3750 (9.3750) lr 1.9686e-02 eta 0:00:46\n",
      "epoch [6/50] batch [1/1] time 0.968 (0.968) data 0.653 (0.653) loss 2.1947 (2.1947) acc 37.5000 (37.5000) lr 1.9511e-02 eta 0:00:42\n",
      "epoch [7/50] batch [1/1] time 0.883 (0.883) data 0.571 (0.571) loss 1.9569 (1.9569) acc 37.5000 (37.5000) lr 1.9298e-02 eta 0:00:37\n",
      "epoch [8/50] batch [1/1] time 0.815 (0.815) data 0.500 (0.500) loss 1.7400 (1.7400) acc 59.3750 (59.3750) lr 1.9048e-02 eta 0:00:34\n",
      "epoch [9/50] batch [1/1] time 0.815 (0.815) data 0.500 (0.500) loss 1.1343 (1.1343) acc 65.6250 (65.6250) lr 1.8763e-02 eta 0:00:33\n",
      "epoch [10/50] batch [1/1] time 0.840 (0.840) data 0.525 (0.525) loss 0.9677 (0.9677) acc 75.0000 (75.0000) lr 1.8443e-02 eta 0:00:33\n",
      "epoch [11/50] batch [1/1] time 0.857 (0.857) data 0.544 (0.544) loss 1.2979 (1.2979) acc 65.6250 (65.6250) lr 1.8090e-02 eta 0:00:33\n",
      "epoch [12/50] batch [1/1] time 0.855 (0.855) data 0.541 (0.541) loss 0.9663 (0.9663) acc 71.8750 (71.8750) lr 1.7705e-02 eta 0:00:32\n",
      "epoch [13/50] batch [1/1] time 0.809 (0.809) data 0.494 (0.494) loss 1.1975 (1.1975) acc 71.8750 (71.8750) lr 1.7290e-02 eta 0:00:29\n",
      "epoch [14/50] batch [1/1] time 0.842 (0.842) data 0.530 (0.530) loss 1.3052 (1.3052) acc 56.2500 (56.2500) lr 1.6845e-02 eta 0:00:30\n",
      "epoch [15/50] batch [1/1] time 0.805 (0.805) data 0.492 (0.492) loss 1.3495 (1.3495) acc 65.6250 (65.6250) lr 1.6374e-02 eta 0:00:28\n",
      "epoch [16/50] batch [1/1] time 0.831 (0.831) data 0.516 (0.516) loss 0.7876 (0.7876) acc 81.2500 (81.2500) lr 1.5878e-02 eta 0:00:28\n",
      "epoch [17/50] batch [1/1] time 0.935 (0.935) data 0.618 (0.618) loss 0.7883 (0.7883) acc 78.1250 (78.1250) lr 1.5358e-02 eta 0:00:30\n",
      "epoch [18/50] batch [1/1] time 1.034 (1.034) data 0.717 (0.717) loss 0.7422 (0.7422) acc 81.2500 (81.2500) lr 1.4818e-02 eta 0:00:33\n",
      "epoch [19/50] batch [1/1] time 1.081 (1.081) data 0.766 (0.766) loss 1.2391 (1.2391) acc 68.7500 (68.7500) lr 1.4258e-02 eta 0:00:33\n",
      "epoch [20/50] batch [1/1] time 0.936 (0.936) data 0.618 (0.618) loss 0.9518 (0.9518) acc 75.0000 (75.0000) lr 1.3681e-02 eta 0:00:28\n",
      "epoch [21/50] batch [1/1] time 0.838 (0.838) data 0.523 (0.523) loss 1.0659 (1.0659) acc 75.0000 (75.0000) lr 1.3090e-02 eta 0:00:24\n",
      "epoch [22/50] batch [1/1] time 0.819 (0.819) data 0.505 (0.505) loss 0.5326 (0.5326) acc 84.3750 (84.3750) lr 1.2487e-02 eta 0:00:22\n",
      "epoch [23/50] batch [1/1] time 0.831 (0.831) data 0.513 (0.513) loss 0.7953 (0.7953) acc 81.2500 (81.2500) lr 1.1874e-02 eta 0:00:22\n",
      "epoch [24/50] batch [1/1] time 0.833 (0.833) data 0.515 (0.515) loss 0.6438 (0.6438) acc 81.2500 (81.2500) lr 1.1253e-02 eta 0:00:21\n",
      "epoch [25/50] batch [1/1] time 0.834 (0.834) data 0.517 (0.517) loss 0.5382 (0.5382) acc 87.5000 (87.5000) lr 1.0628e-02 eta 0:00:20\n",
      "epoch [26/50] batch [1/1] time 0.832 (0.832) data 0.521 (0.521) loss 0.6016 (0.6016) acc 84.3750 (84.3750) lr 1.0000e-02 eta 0:00:19\n",
      "epoch [27/50] batch [1/1] time 0.796 (0.796) data 0.503 (0.503) loss 0.7905 (0.7905) acc 71.8750 (71.8750) lr 9.3721e-03 eta 0:00:18\n",
      "epoch [28/50] batch [1/1] time 0.844 (0.844) data 0.529 (0.529) loss 0.6166 (0.6166) acc 84.3750 (84.3750) lr 8.7467e-03 eta 0:00:18\n",
      "epoch [29/50] batch [1/1] time 0.814 (0.814) data 0.497 (0.497) loss 0.5926 (0.5926) acc 81.2500 (81.2500) lr 8.1262e-03 eta 0:00:17\n",
      "epoch [30/50] batch [1/1] time 0.880 (0.880) data 0.566 (0.566) loss 0.6028 (0.6028) acc 75.0000 (75.0000) lr 7.5131e-03 eta 0:00:17\n",
      "epoch [31/50] batch [1/1] time 1.033 (1.033) data 0.716 (0.716) loss 0.6662 (0.6662) acc 90.6250 (90.6250) lr 6.9098e-03 eta 0:00:19\n",
      "epoch [32/50] batch [1/1] time 1.399 (1.399) data 1.077 (1.077) loss 0.4010 (0.4010) acc 84.3750 (84.3750) lr 6.3188e-03 eta 0:00:25\n",
      "epoch [33/50] batch [1/1] time 0.903 (0.903) data 0.588 (0.588) loss 0.3475 (0.3475) acc 87.5000 (87.5000) lr 5.7422e-03 eta 0:00:15\n",
      "epoch [34/50] batch [1/1] time 0.833 (0.833) data 0.518 (0.518) loss 0.4561 (0.4561) acc 87.5000 (87.5000) lr 5.1825e-03 eta 0:00:13\n",
      "epoch [35/50] batch [1/1] time 0.824 (0.824) data 0.503 (0.503) loss 0.5362 (0.5362) acc 81.2500 (81.2500) lr 4.6417e-03 eta 0:00:12\n",
      "epoch [36/50] batch [1/1] time 0.849 (0.849) data 0.533 (0.533) loss 0.6245 (0.6245) acc 78.1250 (78.1250) lr 4.1221e-03 eta 0:00:11\n",
      "epoch [37/50] batch [1/1] time 0.829 (0.829) data 0.509 (0.509) loss 0.6196 (0.6196) acc 81.2500 (81.2500) lr 3.6258e-03 eta 0:00:10\n",
      "epoch [38/50] batch [1/1] time 0.896 (0.896) data 0.580 (0.580) loss 0.5029 (0.5029) acc 84.3750 (84.3750) lr 3.1545e-03 eta 0:00:10\n",
      "epoch [39/50] batch [1/1] time 0.946 (0.946) data 0.628 (0.628) loss 0.3893 (0.3893) acc 90.6250 (90.6250) lr 2.7103e-03 eta 0:00:10\n",
      "epoch [40/50] batch [1/1] time 0.810 (0.810) data 0.492 (0.492) loss 0.3349 (0.3349) acc 90.6250 (90.6250) lr 2.2949e-03 eta 0:00:08\n",
      "epoch [41/50] batch [1/1] time 0.826 (0.826) data 0.509 (0.509) loss 0.5414 (0.5414) acc 87.5000 (87.5000) lr 1.9098e-03 eta 0:00:07\n",
      "epoch [42/50] batch [1/1] time 0.823 (0.823) data 0.504 (0.504) loss 0.3314 (0.3314) acc 90.6250 (90.6250) lr 1.5567e-03 eta 0:00:06\n",
      "epoch [43/50] batch [1/1] time 1.064 (1.064) data 0.746 (0.746) loss 0.4208 (0.4208) acc 87.5000 (87.5000) lr 1.2369e-03 eta 0:00:07\n",
      "epoch [44/50] batch [1/1] time 1.028 (1.028) data 0.710 (0.710) loss 0.4935 (0.4935) acc 84.3750 (84.3750) lr 9.5173e-04 eta 0:00:06\n",
      "epoch [45/50] batch [1/1] time 1.059 (1.059) data 0.741 (0.741) loss 0.7294 (0.7294) acc 84.3750 (84.3750) lr 7.0224e-04 eta 0:00:05\n",
      "epoch [46/50] batch [1/1] time 0.959 (0.959) data 0.644 (0.644) loss 0.3451 (0.3451) acc 93.7500 (93.7500) lr 4.8943e-04 eta 0:00:03\n",
      "epoch [47/50] batch [1/1] time 0.829 (0.829) data 0.514 (0.514) loss 0.3744 (0.3744) acc 93.7500 (93.7500) lr 3.1417e-04 eta 0:00:02\n",
      "epoch [48/50] batch [1/1] time 0.861 (0.861) data 0.548 (0.548) loss 0.6722 (0.6722) acc 90.6250 (90.6250) lr 1.7713e-04 eta 0:00:01\n",
      "epoch [49/50] batch [1/1] time 0.826 (0.826) data 0.510 (0.510) loss 0.6503 (0.6503) acc 84.3750 (84.3750) lr 7.8853e-05 eta 0:00:00\n",
      "epoch [50/50] batch [1/1] time 0.845 (0.845) data 0.527 (0.527) loss 0.3873 (0.3873) acc 87.5000 (87.5000) lr 1.9733e-05 eta 0:00:00\n",
      "Checkpoint saved to output/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1/prompt_learner/model.pth.tar-50\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 37/37 [00:42<00:00,  1.14s/it]\n",
      "=> result\n",
      "* total: 3,669\n",
      "* correct: 3,325\n",
      "* accuracy: 90.6%\n",
      "* error: 9.4%\n",
      "* macro_f1: 90.6%\n",
      "Elapsed: 0:01:37\n"
     ]
    }
   ],
   "source": [
    "#oxford_pets-1shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
    "        --output-dir output/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxAGBzz_ItmA"
   },
   "source": [
    "# Eurosat New Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DjMw5AnIoRc",
    "outputId": "2206e7dd-98a6-4647-bd09-1aac2a69280b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 16:31:50.416615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 16:31:50.436393: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 16:31:50.442244: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 16:31:50.456377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 16:31:51.472536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16']\n",
      "output_dir: output/new_init/eurosat/DAPT/vit_b16_16shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/new_init/eurosat/DAPT/vit_b16_16shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_16-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  160\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.txt_ctx', 'prompt_learner.vis_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/new_init/eurosat/DAPT/vit_b16_16shots/seed1/tensorboard)\n",
      "epoch [1/200] batch [5/5] time 1.205 (1.681) data 0.000 (0.213) loss 11.3727 (11.5112) acc 6.2500 (10.0000) lr 2.0000e+01 eta 0:27:52\n",
      "epoch [2/200] batch [5/5] time 1.127 (1.352) data 0.000 (0.206) loss 9.9746 (10.2003) acc 9.3750 (7.5000) lr 1.9999e+01 eta 0:22:18\n",
      "epoch [3/200] batch [5/5] time 1.142 (1.323) data 0.000 (0.182) loss 9.7696 (9.8522) acc 9.3750 (6.8750) lr 1.9995e+01 eta 0:21:43\n",
      "epoch [4/200] batch [5/5] time 1.135 (1.336) data 0.000 (0.203) loss 9.7224 (9.7068) acc 6.2500 (7.5000) lr 1.9989e+01 eta 0:21:49\n",
      "epoch [5/200] batch [5/5] time 1.111 (1.325) data 0.000 (0.193) loss 9.6219 (9.6199) acc 0.0000 (6.2500) lr 1.9980e+01 eta 0:21:31\n",
      "epoch [6/200] batch [5/5] time 1.142 (1.369) data 0.000 (0.221) loss 9.3592 (9.4803) acc 12.5000 (11.8750) lr 1.9969e+01 eta 0:22:07\n",
      "epoch [7/200] batch [5/5] time 1.113 (1.327) data 0.000 (0.184) loss 8.8540 (9.1470) acc 25.0000 (12.5000) lr 1.9956e+01 eta 0:21:20\n",
      "epoch [8/200] batch [5/5] time 1.145 (1.431) data 0.000 (0.198) loss 5.5685 (7.5086) acc 6.2500 (16.2500) lr 1.9940e+01 eta 0:22:53\n",
      "epoch [9/200] batch [5/5] time 1.180 (1.367) data 0.000 (0.227) loss 10.0554 (10.0809) acc 12.5000 (7.5000) lr 1.9921e+01 eta 0:21:45\n",
      "epoch [10/200] batch [5/5] time 1.193 (1.424) data 0.000 (0.220) loss 8.8993 (9.3707) acc 9.3750 (10.6250) lr 1.9900e+01 eta 0:22:32\n",
      "epoch [11/200] batch [5/5] time 1.190 (1.344) data 0.000 (0.193) loss 7.3192 (7.9389) acc 9.3750 (11.8750) lr 1.9877e+01 eta 0:21:10\n",
      "epoch [12/200] batch [5/5] time 1.130 (1.348) data 0.000 (0.211) loss 5.9271 (6.3061) acc 15.6250 (19.3750) lr 1.9851e+01 eta 0:21:06\n",
      "epoch [13/200] batch [5/5] time 1.121 (1.322) data 0.000 (0.179) loss 4.8929 (5.1716) acc 21.8750 (16.8750) lr 1.9823e+01 eta 0:20:36\n",
      "epoch [14/200] batch [5/5] time 1.135 (1.359) data 0.000 (0.212) loss 4.1240 (4.5030) acc 3.1250 (8.7500) lr 1.9792e+01 eta 0:21:03\n",
      "epoch [15/200] batch [5/5] time 1.126 (1.339) data 0.000 (0.207) loss 6.4162 (5.0521) acc 6.2500 (14.3750) lr 1.9759e+01 eta 0:20:38\n",
      "epoch [16/200] batch [5/5] time 1.146 (1.359) data 0.000 (0.198) loss 8.3752 (10.5575) acc 18.7500 (10.6250) lr 1.9724e+01 eta 0:20:49\n",
      "epoch [17/200] batch [5/5] time 1.094 (1.331) data 0.000 (0.196) loss 5.0554 (5.7663) acc 21.8750 (10.0000) lr 1.9686e+01 eta 0:20:18\n",
      "epoch [18/200] batch [5/5] time 1.172 (1.385) data 0.000 (0.231) loss 3.1571 (3.9100) acc 28.1250 (21.8750) lr 1.9646e+01 eta 0:21:00\n",
      "epoch [19/200] batch [5/5] time 1.099 (1.328) data 0.000 (0.173) loss 3.1721 (3.1737) acc 18.7500 (23.7500) lr 1.9603e+01 eta 0:20:01\n",
      "epoch [20/200] batch [5/5] time 1.142 (1.368) data 0.000 (0.219) loss 2.6671 (2.6250) acc 21.8750 (31.2500) lr 1.9558e+01 eta 0:20:30\n",
      "epoch [21/200] batch [5/5] time 1.111 (1.319) data 0.000 (0.171) loss 2.0637 (2.1163) acc 56.2500 (50.0000) lr 1.9511e+01 eta 0:19:40\n",
      "epoch [22/200] batch [5/5] time 1.111 (1.335) data 0.000 (0.194) loss 1.3411 (2.0310) acc 84.3750 (58.7500) lr 1.9461e+01 eta 0:19:47\n",
      "epoch [23/200] batch [5/5] time 1.114 (1.332) data 0.000 (0.185) loss 5.4138 (3.2817) acc 28.1250 (41.8750) lr 1.9409e+01 eta 0:19:38\n",
      "epoch [24/200] batch [5/5] time 1.141 (1.372) data 0.000 (0.226) loss 2.7215 (4.0751) acc 43.7500 (31.2500) lr 1.9354e+01 eta 0:20:07\n",
      "epoch [25/200] batch [5/5] time 1.107 (1.322) data 0.000 (0.191) loss 1.7249 (2.0290) acc 71.8750 (66.2500) lr 1.9298e+01 eta 0:19:16\n",
      "epoch [26/200] batch [5/5] time 1.136 (1.351) data 0.000 (0.192) loss 1.9663 (1.6587) acc 59.3750 (66.2500) lr 1.9239e+01 eta 0:19:35\n",
      "epoch [27/200] batch [5/5] time 1.133 (1.351) data 0.000 (0.210) loss 2.3516 (1.7434) acc 53.1250 (59.3750) lr 1.9178e+01 eta 0:19:29\n",
      "epoch [28/200] batch [5/5] time 1.146 (1.360) data 0.000 (0.213) loss 1.3235 (1.6326) acc 68.7500 (61.2500) lr 1.9114e+01 eta 0:19:29\n",
      "epoch [29/200] batch [5/5] time 1.129 (1.331) data 0.000 (0.191) loss 4.4299 (2.1338) acc 21.8750 (59.3750) lr 1.9048e+01 eta 0:18:58\n",
      "epoch [30/200] batch [5/5] time 1.139 (1.357) data 0.000 (0.197) loss 5.5069 (3.3184) acc 40.6250 (52.5000) lr 1.8980e+01 eta 0:19:13\n",
      "epoch [31/200] batch [5/5] time 1.135 (1.330) data 0.000 (0.185) loss 2.4148 (3.0654) acc 50.0000 (55.0000) lr 1.8910e+01 eta 0:18:43\n",
      "epoch [32/200] batch [5/5] time 1.103 (1.374) data 0.000 (0.226) loss 1.7719 (2.1223) acc 65.6250 (56.2500) lr 1.8838e+01 eta 0:19:14\n",
      "epoch [33/200] batch [5/5] time 1.123 (1.336) data 0.000 (0.199) loss 1.2437 (1.4045) acc 81.2500 (75.6250) lr 1.8763e+01 eta 0:18:35\n",
      "epoch [34/200] batch [5/5] time 1.135 (1.350) data 0.000 (0.204) loss 1.1549 (1.0883) acc 78.1250 (78.7500) lr 1.8686e+01 eta 0:18:40\n",
      "epoch [35/200] batch [5/5] time 1.100 (1.320) data 0.000 (0.186) loss 2.2547 (1.5140) acc 56.2500 (70.6250) lr 1.8607e+01 eta 0:18:09\n",
      "epoch [36/200] batch [5/5] time 1.137 (1.376) data 0.000 (0.230) loss 1.4196 (1.7319) acc 65.6250 (68.1250) lr 1.8526e+01 eta 0:18:48\n",
      "epoch [37/200] batch [5/5] time 1.126 (1.329) data 0.000 (0.192) loss 2.4950 (3.1232) acc 46.8750 (48.7500) lr 1.8443e+01 eta 0:18:03\n",
      "epoch [38/200] batch [5/5] time 1.152 (1.382) data 0.000 (0.229) loss 2.8393 (3.6037) acc 53.1250 (37.5000) lr 1.8358e+01 eta 0:18:39\n",
      "epoch [39/200] batch [5/5] time 1.076 (1.320) data 0.000 (0.192) loss 1.7779 (1.8341) acc 68.7500 (70.0000) lr 1.8271e+01 eta 0:17:42\n",
      "epoch [40/200] batch [5/5] time 1.131 (1.380) data 0.000 (0.232) loss 1.5145 (1.7226) acc 62.5000 (63.1250) lr 1.8181e+01 eta 0:18:24\n",
      "epoch [41/200] batch [5/5] time 1.118 (1.329) data 0.000 (0.177) loss 2.0077 (1.5203) acc 62.5000 (72.5000) lr 1.8090e+01 eta 0:17:36\n",
      "epoch [42/200] batch [5/5] time 1.138 (1.375) data 0.000 (0.221) loss 1.4899 (1.5632) acc 81.2500 (71.8750) lr 1.7997e+01 eta 0:18:06\n",
      "epoch [43/200] batch [5/5] time 1.113 (1.331) data 0.000 (0.163) loss 1.7598 (1.5600) acc 59.3750 (73.7500) lr 1.7902e+01 eta 0:17:24\n",
      "epoch [44/200] batch [5/5] time 1.126 (1.396) data 0.000 (0.211) loss 2.5293 (1.8418) acc 31.2500 (59.3750) lr 1.7804e+01 eta 0:18:08\n",
      "epoch [45/200] batch [5/5] time 1.171 (1.325) data 0.000 (0.192) loss 2.1350 (2.0487) acc 46.8750 (62.5000) lr 1.7705e+01 eta 0:17:06\n",
      "epoch [46/200] batch [5/5] time 1.142 (1.346) data 0.000 (0.154) loss 1.7762 (2.0701) acc 65.6250 (53.7500) lr 1.7604e+01 eta 0:17:16\n",
      "epoch [47/200] batch [5/5] time 1.134 (1.327) data 0.000 (0.185) loss 1.2552 (1.5434) acc 68.7500 (68.1250) lr 1.7501e+01 eta 0:16:55\n",
      "epoch [48/200] batch [5/5] time 1.122 (1.353) data 0.000 (0.200) loss 1.0248 (1.2339) acc 81.2500 (73.1250) lr 1.7396e+01 eta 0:17:08\n",
      "epoch [49/200] batch [5/5] time 1.091 (1.324) data 0.000 (0.194) loss 1.2159 (1.1369) acc 71.8750 (78.1250) lr 1.7290e+01 eta 0:16:39\n",
      "epoch [50/200] batch [5/5] time 1.137 (1.371) data 0.000 (0.223) loss 0.9437 (1.1413) acc 84.3750 (76.2500) lr 1.7181e+01 eta 0:17:08\n",
      "epoch [51/200] batch [5/5] time 1.136 (1.348) data 0.000 (0.189) loss 1.7274 (1.3532) acc 56.2500 (70.0000) lr 1.7071e+01 eta 0:16:44\n",
      "epoch [52/200] batch [5/5] time 1.139 (1.370) data 0.000 (0.190) loss 2.2783 (2.3602) acc 46.8750 (56.2500) lr 1.6959e+01 eta 0:16:53\n",
      "epoch [53/200] batch [5/5] time 1.153 (1.347) data 0.000 (0.191) loss 1.7183 (2.2196) acc 62.5000 (55.6250) lr 1.6845e+01 eta 0:16:29\n",
      "epoch [54/200] batch [5/5] time 1.221 (1.408) data 0.000 (0.207) loss 1.3292 (1.3729) acc 71.8750 (70.0000) lr 1.6730e+01 eta 0:17:08\n",
      "epoch [55/200] batch [5/5] time 1.166 (1.350) data 0.000 (0.197) loss 1.4186 (1.3753) acc 65.6250 (71.2500) lr 1.6613e+01 eta 0:16:18\n",
      "epoch [56/200] batch [5/5] time 1.152 (1.404) data 0.000 (0.219) loss 1.0713 (1.0497) acc 81.2500 (84.3750) lr 1.6494e+01 eta 0:16:51\n",
      "epoch [57/200] batch [5/5] time 1.205 (1.344) data 0.000 (0.189) loss 1.2419 (1.2508) acc 75.0000 (76.2500) lr 1.6374e+01 eta 0:16:01\n",
      "epoch [58/200] batch [5/5] time 1.135 (1.355) data 0.000 (0.196) loss 2.2005 (1.6625) acc 43.7500 (65.0000) lr 1.6252e+01 eta 0:16:01\n",
      "epoch [59/200] batch [5/5] time 1.119 (1.341) data 0.000 (0.180) loss 1.9970 (2.1125) acc 53.1250 (58.7500) lr 1.6129e+01 eta 0:15:45\n",
      "epoch [60/200] batch [5/5] time 1.129 (1.378) data 0.000 (0.230) loss 2.6093 (1.6549) acc 50.0000 (70.6250) lr 1.6004e+01 eta 0:16:04\n",
      "epoch [61/200] batch [5/5] time 1.090 (1.345) data 0.000 (0.202) loss 1.8236 (1.9339) acc 56.2500 (66.8750) lr 1.5878e+01 eta 0:15:34\n",
      "epoch [62/200] batch [5/5] time 1.137 (1.404) data 0.000 (0.246) loss 1.5361 (1.4741) acc 59.3750 (68.1250) lr 1.5750e+01 eta 0:16:08\n",
      "epoch [63/200] batch [5/5] time 1.108 (1.337) data 0.000 (0.195) loss 1.1073 (1.4030) acc 78.1250 (76.2500) lr 1.5621e+01 eta 0:15:16\n",
      "epoch [64/200] batch [5/5] time 1.151 (1.364) data 0.000 (0.222) loss 1.9865 (1.5534) acc 65.6250 (68.1250) lr 1.5490e+01 eta 0:15:27\n",
      "epoch [65/200] batch [5/5] time 1.123 (1.338) data 0.000 (0.198) loss 1.8098 (1.3384) acc 56.2500 (71.8750) lr 1.5358e+01 eta 0:15:02\n",
      "epoch [66/200] batch [5/5] time 1.130 (1.373) data 0.000 (0.225) loss 1.6040 (1.3115) acc 62.5000 (73.7500) lr 1.5225e+01 eta 0:15:20\n",
      "epoch [67/200] batch [5/5] time 1.153 (1.324) data 0.000 (0.186) loss 1.4428 (1.5446) acc 59.3750 (66.2500) lr 1.5090e+01 eta 0:14:40\n",
      "epoch [68/200] batch [5/5] time 1.108 (1.374) data 0.000 (0.236) loss 1.9656 (1.5734) acc 65.6250 (68.7500) lr 1.4955e+01 eta 0:15:06\n",
      "epoch [69/200] batch [5/5] time 1.104 (1.344) data 0.000 (0.202) loss 1.7288 (1.4471) acc 65.6250 (72.5000) lr 1.4818e+01 eta 0:14:40\n",
      "epoch [70/200] batch [5/5] time 1.153 (1.455) data 0.000 (0.224) loss 0.9520 (1.1068) acc 78.1250 (77.5000) lr 1.4679e+01 eta 0:15:45\n",
      "epoch [71/200] batch [5/5] time 1.177 (1.339) data 0.000 (0.188) loss 1.3056 (1.3849) acc 78.1250 (73.1250) lr 1.4540e+01 eta 0:14:23\n",
      "epoch [72/200] batch [5/5] time 1.125 (1.380) data 0.000 (0.195) loss 1.2615 (1.0971) acc 78.1250 (78.7500) lr 1.4399e+01 eta 0:14:43\n",
      "epoch [73/200] batch [5/5] time 1.143 (1.343) data 0.000 (0.180) loss 1.2293 (0.9885) acc 75.0000 (79.3750) lr 1.4258e+01 eta 0:14:13\n",
      "epoch [74/200] batch [5/5] time 1.137 (1.353) data 0.000 (0.200) loss 2.3376 (1.3317) acc 59.3750 (77.5000) lr 1.4115e+01 eta 0:14:12\n",
      "epoch [75/200] batch [5/5] time 1.162 (1.347) data 0.000 (0.165) loss 2.7447 (2.1556) acc 46.8750 (57.5000) lr 1.3971e+01 eta 0:14:01\n",
      "epoch [76/200] batch [5/5] time 1.138 (1.353) data 0.000 (0.207) loss 1.5600 (1.6956) acc 75.0000 (68.7500) lr 1.3827e+01 eta 0:13:58\n",
      "epoch [77/200] batch [5/5] time 1.168 (1.335) data 0.000 (0.162) loss 1.4248 (1.4297) acc 71.8750 (74.3750) lr 1.3681e+01 eta 0:13:40\n",
      "epoch [78/200] batch [5/5] time 1.154 (1.391) data 0.000 (0.236) loss 1.1309 (1.1708) acc 75.0000 (76.8750) lr 1.3535e+01 eta 0:14:08\n",
      "epoch [79/200] batch [5/5] time 1.184 (1.368) data 0.000 (0.192) loss 0.9668 (1.0723) acc 78.1250 (79.3750) lr 1.3387e+01 eta 0:13:47\n",
      "epoch [80/200] batch [5/5] time 1.144 (1.346) data 0.001 (0.213) loss 1.7243 (1.2430) acc 62.5000 (73.1250) lr 1.3239e+01 eta 0:13:27\n",
      "epoch [81/200] batch [5/5] time 1.117 (1.323) data 0.000 (0.190) loss 1.5901 (1.3498) acc 62.5000 (71.2500) lr 1.3090e+01 eta 0:13:07\n",
      "epoch [82/200] batch [5/5] time 1.115 (1.376) data 0.000 (0.222) loss 1.2421 (1.1163) acc 75.0000 (77.5000) lr 1.2940e+01 eta 0:13:31\n",
      "epoch [83/200] batch [5/5] time 1.202 (1.350) data 0.000 (0.178) loss 1.1082 (1.3616) acc 71.8750 (73.7500) lr 1.2790e+01 eta 0:13:09\n",
      "epoch [84/200] batch [5/5] time 1.130 (1.365) data 0.000 (0.203) loss 2.8399 (2.3079) acc 43.7500 (60.0000) lr 1.2639e+01 eta 0:13:11\n",
      "epoch [85/200] batch [5/5] time 1.104 (1.339) data 0.000 (0.192) loss 1.4884 (2.1259) acc 65.6250 (60.6250) lr 1.2487e+01 eta 0:12:49\n",
      "epoch [86/200] batch [5/5] time 1.142 (1.384) data 0.000 (0.222) loss 2.1806 (1.9291) acc 53.1250 (58.1250) lr 1.2334e+01 eta 0:13:08\n",
      "epoch [87/200] batch [5/5] time 1.093 (1.350) data 0.000 (0.188) loss 1.5807 (1.6722) acc 71.8750 (66.2500) lr 1.2181e+01 eta 0:12:42\n",
      "epoch [88/200] batch [5/5] time 1.144 (1.387) data 0.000 (0.216) loss 1.5332 (1.4891) acc 75.0000 (73.1250) lr 1.2028e+01 eta 0:12:56\n",
      "epoch [89/200] batch [5/5] time 1.137 (1.354) data 0.000 (0.170) loss 0.8882 (1.2120) acc 71.8750 (70.0000) lr 1.1874e+01 eta 0:12:31\n",
      "epoch [90/200] batch [5/5] time 1.150 (1.366) data 0.000 (0.212) loss 1.1472 (1.0997) acc 75.0000 (78.7500) lr 1.1719e+01 eta 0:12:31\n",
      "epoch [91/200] batch [5/5] time 1.128 (1.336) data 0.000 (0.206) loss 0.7995 (0.9819) acc 84.3750 (77.5000) lr 1.1564e+01 eta 0:12:08\n",
      "epoch [92/200] batch [5/5] time 1.133 (1.395) data 0.000 (0.243) loss 1.5477 (1.1768) acc 62.5000 (76.8750) lr 1.1409e+01 eta 0:12:33\n",
      "epoch [93/200] batch [5/5] time 1.118 (1.339) data 0.000 (0.187) loss 1.7558 (1.6841) acc 78.1250 (71.2500) lr 1.1253e+01 eta 0:11:56\n",
      "epoch [94/200] batch [5/5] time 1.147 (1.373) data 0.000 (0.215) loss 2.0170 (1.7510) acc 62.5000 (71.8750) lr 1.1097e+01 eta 0:12:07\n",
      "epoch [95/200] batch [5/5] time 1.189 (1.352) data 0.000 (0.203) loss 1.5836 (1.3309) acc 56.2500 (73.1250) lr 1.0941e+01 eta 0:11:49\n",
      "epoch [96/200] batch [5/5] time 1.149 (1.365) data 0.000 (0.203) loss 1.0667 (1.3564) acc 75.0000 (69.3750) lr 1.0785e+01 eta 0:11:49\n",
      "epoch [97/200] batch [5/5] time 1.251 (1.356) data 0.002 (0.172) loss 1.2569 (1.1754) acc 75.0000 (78.1250) lr 1.0628e+01 eta 0:11:38\n",
      "epoch [98/200] batch [5/5] time 1.174 (1.393) data 0.000 (0.207) loss 0.9469 (0.9287) acc 87.5000 (85.0000) lr 1.0471e+01 eta 0:11:50\n",
      "epoch [99/200] batch [5/5] time 1.147 (1.356) data 0.000 (0.200) loss 0.9857 (1.0419) acc 87.5000 (80.6250) lr 1.0314e+01 eta 0:11:24\n",
      "epoch [100/200] batch [5/5] time 1.143 (1.368) data 0.000 (0.216) loss 1.3668 (1.1007) acc 65.6250 (77.5000) lr 1.0157e+01 eta 0:11:24\n",
      "epoch [101/200] batch [5/5] time 1.139 (1.345) data 0.000 (0.208) loss 1.5512 (1.0959) acc 65.6250 (78.1250) lr 1.0000e+01 eta 0:11:05\n",
      "epoch [102/200] batch [5/5] time 1.120 (1.373) data 0.000 (0.203) loss 1.1616 (1.1430) acc 81.2500 (76.2500) lr 9.8429e+00 eta 0:11:12\n",
      "epoch [103/200] batch [5/5] time 1.126 (1.336) data 0.000 (0.177) loss 1.1217 (1.2304) acc 84.3750 (75.0000) lr 9.6859e+00 eta 0:10:48\n",
      "epoch [104/200] batch [5/5] time 1.140 (1.373) data 0.000 (0.203) loss 1.3060 (1.0210) acc 68.7500 (81.8750) lr 9.5289e+00 eta 0:10:59\n",
      "epoch [105/200] batch [5/5] time 1.138 (1.380) data 0.000 (0.240) loss 0.9436 (1.2275) acc 81.2500 (81.8750) lr 9.3721e+00 eta 0:10:55\n",
      "epoch [106/200] batch [5/5] time 1.142 (1.371) data 0.000 (0.213) loss 0.6418 (0.9450) acc 93.7500 (86.8750) lr 9.2154e+00 eta 0:10:44\n",
      "epoch [107/200] batch [5/5] time 1.136 (1.326) data 0.000 (0.161) loss 1.3938 (1.4573) acc 68.7500 (76.2500) lr 9.0589e+00 eta 0:10:16\n",
      "epoch [108/200] batch [5/5] time 1.135 (1.385) data 0.000 (0.232) loss 1.0289 (1.2539) acc 84.3750 (78.1250) lr 8.9027e+00 eta 0:10:36\n",
      "epoch [109/200] batch [5/5] time 1.109 (1.331) data 0.000 (0.176) loss 1.0773 (1.2168) acc 78.1250 (75.6250) lr 8.7467e+00 eta 0:10:05\n",
      "epoch [110/200] batch [5/5] time 1.172 (1.384) data 0.000 (0.227) loss 0.9019 (1.1019) acc 90.6250 (79.3750) lr 8.5910e+00 eta 0:10:22\n",
      "epoch [111/200] batch [5/5] time 1.115 (1.326) data 0.000 (0.171) loss 0.8006 (0.9398) acc 84.3750 (84.3750) lr 8.4357e+00 eta 0:09:50\n",
      "epoch [112/200] batch [5/5] time 1.131 (1.379) data 0.000 (0.232) loss 1.0401 (0.9957) acc 84.3750 (82.5000) lr 8.2807e+00 eta 0:10:06\n",
      "epoch [113/200] batch [5/5] time 1.150 (1.359) data 0.000 (0.190) loss 0.8160 (1.1374) acc 87.5000 (76.8750) lr 8.1262e+00 eta 0:09:50\n",
      "epoch [114/200] batch [5/5] time 1.110 (1.361) data 0.000 (0.214) loss 1.0162 (0.9881) acc 87.5000 (81.2500) lr 7.9721e+00 eta 0:09:45\n",
      "epoch [115/200] batch [5/5] time 1.091 (1.325) data 0.000 (0.195) loss 1.1765 (1.3168) acc 75.0000 (75.6250) lr 7.8186e+00 eta 0:09:22\n",
      "epoch [116/200] batch [5/5] time 1.144 (1.382) data 0.000 (0.216) loss 0.9895 (1.0103) acc 81.2500 (80.6250) lr 7.6655e+00 eta 0:09:40\n",
      "epoch [117/200] batch [5/5] time 1.119 (1.342) data 0.000 (0.197) loss 1.5920 (1.1981) acc 62.5000 (71.2500) lr 7.5131e+00 eta 0:09:16\n",
      "epoch [118/200] batch [5/5] time 1.117 (1.412) data 0.000 (0.249) loss 0.6561 (0.9815) acc 87.5000 (81.2500) lr 7.3613e+00 eta 0:09:39\n",
      "epoch [119/200] batch [5/5] time 1.142 (1.343) data 0.000 (0.190) loss 1.2657 (0.9333) acc 71.8750 (83.1250) lr 7.2101e+00 eta 0:09:04\n",
      "epoch [120/200] batch [5/5] time 1.124 (1.354) data 0.000 (0.196) loss 1.0166 (0.9694) acc 81.2500 (81.8750) lr 7.0596e+00 eta 0:09:01\n",
      "epoch [121/200] batch [5/5] time 1.166 (1.348) data 0.000 (0.203) loss 1.0387 (0.9237) acc 71.8750 (79.3750) lr 6.9098e+00 eta 0:08:52\n",
      "epoch [122/200] batch [5/5] time 1.124 (1.350) data 0.000 (0.190) loss 1.2990 (0.9360) acc 65.6250 (79.3750) lr 6.7608e+00 eta 0:08:46\n",
      "epoch [123/200] batch [5/5] time 1.114 (1.323) data 0.000 (0.191) loss 1.0884 (1.4325) acc 81.2500 (70.0000) lr 6.6126e+00 eta 0:08:29\n",
      "epoch [124/200] batch [5/5] time 1.141 (1.391) data 0.000 (0.232) loss 1.1145 (1.1298) acc 75.0000 (76.2500) lr 6.4653e+00 eta 0:08:48\n",
      "epoch [125/200] batch [5/5] time 1.146 (1.348) data 0.000 (0.188) loss 0.9015 (1.0892) acc 81.2500 (75.0000) lr 6.3188e+00 eta 0:08:25\n",
      "epoch [126/200] batch [5/5] time 1.183 (1.372) data 0.000 (0.220) loss 0.8412 (0.9392) acc 81.2500 (80.6250) lr 6.1732e+00 eta 0:08:27\n",
      "epoch [127/200] batch [5/5] time 1.147 (1.420) data 0.000 (0.194) loss 0.8132 (0.7207) acc 90.6250 (88.7500) lr 6.0285e+00 eta 0:08:38\n",
      "epoch [128/200] batch [5/5] time 1.149 (1.373) data 0.000 (0.218) loss 0.5546 (0.6304) acc 93.7500 (91.2500) lr 5.8849e+00 eta 0:08:14\n",
      "epoch [129/200] batch [5/5] time 1.134 (1.333) data 0.000 (0.189) loss 1.2691 (0.9939) acc 75.0000 (83.1250) lr 5.7422e+00 eta 0:07:53\n",
      "epoch [130/200] batch [5/5] time 1.138 (1.369) data 0.000 (0.201) loss 1.0632 (1.1653) acc 78.1250 (76.2500) lr 5.6006e+00 eta 0:07:59\n",
      "epoch [131/200] batch [5/5] time 1.119 (1.354) data 0.000 (0.173) loss 0.7317 (0.9712) acc 90.6250 (81.2500) lr 5.4601e+00 eta 0:07:47\n",
      "epoch [132/200] batch [5/5] time 1.133 (1.379) data 0.000 (0.222) loss 0.8625 (0.9924) acc 81.2500 (80.0000) lr 5.3207e+00 eta 0:07:48\n",
      "epoch [133/200] batch [5/5] time 1.105 (1.328) data 0.000 (0.176) loss 0.5531 (0.7490) acc 93.7500 (88.1250) lr 5.1825e+00 eta 0:07:24\n",
      "epoch [134/200] batch [5/5] time 1.112 (1.389) data 0.000 (0.232) loss 0.6890 (0.6876) acc 90.6250 (88.1250) lr 5.0454e+00 eta 0:07:38\n",
      "epoch [135/200] batch [5/5] time 1.092 (1.343) data 0.000 (0.196) loss 0.9519 (0.7184) acc 81.2500 (87.5000) lr 4.9096e+00 eta 0:07:16\n",
      "epoch [136/200] batch [5/5] time 1.113 (1.383) data 0.000 (0.208) loss 0.5038 (0.5846) acc 96.8750 (91.2500) lr 4.7750e+00 eta 0:07:22\n",
      "epoch [137/200] batch [5/5] time 1.121 (1.348) data 0.000 (0.188) loss 0.5996 (0.8341) acc 90.6250 (80.6250) lr 4.6417e+00 eta 0:07:04\n",
      "epoch [138/200] batch [5/5] time 1.148 (1.359) data 0.000 (0.218) loss 0.7936 (0.8311) acc 81.2500 (82.5000) lr 4.5098e+00 eta 0:07:01\n",
      "epoch [139/200] batch [5/5] time 1.174 (1.352) data 0.000 (0.206) loss 0.9580 (0.8611) acc 75.0000 (84.3750) lr 4.3792e+00 eta 0:06:52\n",
      "epoch [140/200] batch [5/5] time 1.125 (1.392) data 0.000 (0.238) loss 0.6493 (0.5598) acc 87.5000 (92.5000) lr 4.2499e+00 eta 0:06:57\n",
      "epoch [141/200] batch [5/5] time 1.177 (1.344) data 0.000 (0.187) loss 0.5733 (0.6356) acc 90.6250 (91.2500) lr 4.1221e+00 eta 0:06:36\n",
      "epoch [142/200] batch [5/5] time 1.163 (1.398) data 0.000 (0.233) loss 1.0853 (0.6667) acc 78.1250 (88.1250) lr 3.9958e+00 eta 0:06:45\n",
      "epoch [143/200] batch [5/5] time 1.178 (1.364) data 0.000 (0.193) loss 0.6284 (0.7246) acc 87.5000 (85.6250) lr 3.8709e+00 eta 0:06:28\n",
      "epoch [144/200] batch [5/5] time 1.136 (1.372) data 0.000 (0.206) loss 0.6706 (0.6612) acc 87.5000 (90.0000) lr 3.7476e+00 eta 0:06:24\n",
      "epoch [145/200] batch [5/5] time 1.123 (1.326) data 0.000 (0.190) loss 0.5629 (0.6671) acc 90.6250 (86.8750) lr 3.6258e+00 eta 0:06:04\n",
      "epoch [146/200] batch [5/5] time 1.146 (1.376) data 0.000 (0.215) loss 0.6739 (0.7028) acc 87.5000 (87.5000) lr 3.5055e+00 eta 0:06:11\n",
      "epoch [147/200] batch [5/5] time 1.190 (1.368) data 0.000 (0.202) loss 0.8816 (0.6699) acc 87.5000 (86.8750) lr 3.3869e+00 eta 0:06:02\n",
      "epoch [148/200] batch [5/5] time 1.121 (1.356) data 0.000 (0.198) loss 0.8762 (0.6174) acc 84.3750 (90.0000) lr 3.2699e+00 eta 0:05:52\n",
      "epoch [149/200] batch [5/5] time 1.160 (1.340) data 0.000 (0.204) loss 0.5641 (0.5587) acc 87.5000 (90.6250) lr 3.1545e+00 eta 0:05:41\n",
      "epoch [150/200] batch [5/5] time 1.129 (1.355) data 0.000 (0.211) loss 0.8415 (0.7380) acc 75.0000 (84.3750) lr 3.0409e+00 eta 0:05:38\n",
      "epoch [151/200] batch [5/5] time 1.116 (1.331) data 0.000 (0.199) loss 0.4210 (0.7012) acc 96.8750 (86.2500) lr 2.9289e+00 eta 0:05:26\n",
      "epoch [152/200] batch [5/5] time 1.129 (1.384) data 0.000 (0.245) loss 0.3727 (0.7194) acc 100.0000 (88.1250) lr 2.8187e+00 eta 0:05:32\n",
      "epoch [153/200] batch [5/5] time 1.125 (1.343) data 0.000 (0.194) loss 0.7228 (0.5557) acc 90.6250 (93.1250) lr 2.7103e+00 eta 0:05:15\n",
      "epoch [154/200] batch [5/5] time 1.146 (1.362) data 0.000 (0.204) loss 0.5457 (0.6833) acc 90.6250 (85.6250) lr 2.6037e+00 eta 0:05:13\n",
      "epoch [155/200] batch [5/5] time 1.122 (1.335) data 0.000 (0.202) loss 0.6252 (0.5760) acc 93.7500 (93.1250) lr 2.4989e+00 eta 0:05:00\n",
      "epoch [156/200] batch [5/5] time 1.139 (1.368) data 0.000 (0.191) loss 0.7711 (0.6819) acc 81.2500 (86.8750) lr 2.3959e+00 eta 0:05:01\n",
      "epoch [157/200] batch [5/5] time 1.218 (1.351) data 0.000 (0.194) loss 0.5340 (0.5876) acc 90.6250 (88.7500) lr 2.2949e+00 eta 0:04:50\n",
      "epoch [158/200] batch [5/5] time 1.128 (1.373) data 0.000 (0.218) loss 0.4928 (0.6750) acc 93.7500 (86.8750) lr 2.1957e+00 eta 0:04:48\n",
      "epoch [159/200] batch [5/5] time 1.116 (1.332) data 0.000 (0.191) loss 0.4261 (0.5598) acc 96.8750 (90.6250) lr 2.0984e+00 eta 0:04:33\n",
      "epoch [160/200] batch [5/5] time 1.121 (1.369) data 0.000 (0.205) loss 0.5747 (0.5943) acc 93.7500 (90.0000) lr 2.0032e+00 eta 0:04:33\n",
      "epoch [161/200] batch [5/5] time 1.107 (1.343) data 0.000 (0.196) loss 0.3948 (0.5418) acc 93.7500 (89.3750) lr 1.9098e+00 eta 0:04:21\n",
      "epoch [162/200] batch [5/5] time 1.132 (1.389) data 0.000 (0.234) loss 0.7376 (0.5763) acc 90.6250 (90.0000) lr 1.8185e+00 eta 0:04:23\n",
      "epoch [163/200] batch [5/5] time 1.123 (1.357) data 0.000 (0.199) loss 0.3190 (0.4534) acc 100.0000 (95.0000) lr 1.7292e+00 eta 0:04:10\n",
      "epoch [164/200] batch [5/5] time 1.118 (1.366) data 0.000 (0.225) loss 0.5096 (0.4792) acc 93.7500 (93.1250) lr 1.6419e+00 eta 0:04:05\n",
      "epoch [165/200] batch [5/5] time 1.111 (1.341) data 0.000 (0.208) loss 0.3348 (0.3485) acc 96.8750 (96.2500) lr 1.5567e+00 eta 0:03:54\n",
      "epoch [166/200] batch [5/5] time 1.113 (1.402) data 0.000 (0.257) loss 0.4501 (0.3903) acc 93.7500 (95.0000) lr 1.4736e+00 eta 0:03:58\n",
      "epoch [167/200] batch [5/5] time 1.160 (1.321) data 0.000 (0.166) loss 0.2900 (0.4691) acc 100.0000 (94.3750) lr 1.3926e+00 eta 0:03:37\n",
      "epoch [168/200] batch [5/5] time 1.136 (1.377) data 0.000 (0.215) loss 0.6548 (0.4931) acc 90.6250 (91.8750) lr 1.3137e+00 eta 0:03:40\n",
      "epoch [169/200] batch [5/5] time 1.119 (1.345) data 0.000 (0.192) loss 0.5398 (0.5520) acc 93.7500 (90.6250) lr 1.2369e+00 eta 0:03:28\n",
      "epoch [170/200] batch [5/5] time 1.153 (1.360) data 0.000 (0.182) loss 0.4255 (0.4343) acc 96.8750 (93.7500) lr 1.1623e+00 eta 0:03:24\n",
      "epoch [171/200] batch [5/5] time 1.111 (1.327) data 0.000 (0.196) loss 0.4584 (0.6475) acc 90.6250 (87.5000) lr 1.0899e+00 eta 0:03:12\n",
      "epoch [172/200] batch [5/5] time 1.096 (1.342) data 0.000 (0.201) loss 0.5689 (0.4682) acc 90.6250 (92.5000) lr 1.0197e+00 eta 0:03:07\n",
      "epoch [173/200] batch [5/5] time 1.157 (1.333) data 0.000 (0.172) loss 0.4181 (0.4537) acc 90.6250 (93.1250) lr 9.5173e-01 eta 0:02:59\n",
      "epoch [174/200] batch [5/5] time 1.142 (1.383) data 0.000 (0.230) loss 0.3882 (0.4237) acc 96.8750 (94.3750) lr 8.8597e-01 eta 0:02:59\n",
      "epoch [175/200] batch [5/5] time 1.115 (1.356) data 0.000 (0.203) loss 0.2961 (0.3800) acc 100.0000 (97.5000) lr 8.2245e-01 eta 0:02:49\n",
      "epoch [176/200] batch [5/5] time 1.175 (1.366) data 0.000 (0.208) loss 0.3219 (0.4554) acc 96.8750 (95.0000) lr 7.6120e-01 eta 0:02:43\n",
      "epoch [177/200] batch [5/5] time 1.137 (1.336) data 0.000 (0.198) loss 0.5490 (0.4602) acc 93.7500 (95.0000) lr 7.0224e-01 eta 0:02:33\n",
      "epoch [178/200] batch [5/5] time 1.137 (1.381) data 0.000 (0.220) loss 0.3374 (0.3988) acc 100.0000 (94.3750) lr 6.4556e-01 eta 0:02:31\n",
      "epoch [179/200] batch [5/5] time 1.136 (1.341) data 0.000 (0.203) loss 0.3030 (0.3806) acc 96.8750 (96.2500) lr 5.9119e-01 eta 0:02:20\n",
      "epoch [180/200] batch [5/5] time 1.133 (1.381) data 0.000 (0.211) loss 0.5422 (0.3617) acc 93.7500 (97.5000) lr 5.3915e-01 eta 0:02:18\n",
      "epoch [181/200] batch [5/5] time 1.144 (1.350) data 0.000 (0.185) loss 0.3303 (0.3509) acc 96.8750 (96.8750) lr 4.8943e-01 eta 0:02:08\n",
      "epoch [182/200] batch [5/5] time 1.154 (1.355) data 0.000 (0.196) loss 0.2693 (0.2962) acc 100.0000 (98.1250) lr 4.4207e-01 eta 0:02:01\n",
      "epoch [183/200] batch [5/5] time 1.084 (1.347) data 0.000 (0.201) loss 0.2787 (0.3271) acc 100.0000 (97.5000) lr 3.9706e-01 eta 0:01:54\n",
      "epoch [184/200] batch [5/5] time 1.139 (1.378) data 0.000 (0.215) loss 0.3173 (0.3503) acc 96.8750 (96.8750) lr 3.5443e-01 eta 0:01:50\n",
      "epoch [185/200] batch [5/5] time 1.196 (1.361) data 0.000 (0.172) loss 0.6177 (0.3980) acc 87.5000 (93.7500) lr 3.1417e-01 eta 0:01:42\n",
      "epoch [186/200] batch [5/5] time 1.172 (1.392) data 0.000 (0.229) loss 0.4287 (0.3139) acc 96.8750 (98.1250) lr 2.7630e-01 eta 0:01:37\n",
      "epoch [187/200] batch [5/5] time 1.195 (1.375) data 0.000 (0.184) loss 0.3357 (0.3426) acc 96.8750 (96.8750) lr 2.4083e-01 eta 0:01:29\n",
      "epoch [188/200] batch [5/5] time 1.160 (1.417) data 0.000 (0.223) loss 0.4247 (0.3132) acc 90.6250 (96.2500) lr 2.0777e-01 eta 0:01:25\n",
      "epoch [189/200] batch [5/5] time 1.141 (1.329) data 0.000 (0.186) loss 0.3175 (0.4175) acc 96.8750 (94.3750) lr 1.7713e-01 eta 0:01:13\n",
      "epoch [190/200] batch [5/5] time 1.136 (1.374) data 0.000 (0.208) loss 0.2747 (0.3294) acc 96.8750 (96.8750) lr 1.4891e-01 eta 0:01:08\n",
      "epoch [191/200] batch [5/5] time 1.163 (1.349) data 0.000 (0.192) loss 0.2453 (0.2788) acc 100.0000 (97.5000) lr 1.2312e-01 eta 0:01:00\n",
      "epoch [192/200] batch [5/5] time 1.121 (1.344) data 0.000 (0.208) loss 0.5085 (0.3700) acc 90.6250 (95.0000) lr 9.9763e-02 eta 0:00:53\n",
      "epoch [193/200] batch [5/5] time 1.172 (1.370) data 0.000 (0.202) loss 0.2947 (0.3017) acc 96.8750 (96.8750) lr 7.8853e-02 eta 0:00:47\n",
      "epoch [194/200] batch [5/5] time 1.131 (1.374) data 0.000 (0.210) loss 0.2523 (0.2514) acc 100.0000 (99.3750) lr 6.0390e-02 eta 0:00:41\n",
      "epoch [195/200] batch [5/5] time 1.182 (1.338) data 0.000 (0.198) loss 0.2541 (0.2635) acc 100.0000 (98.1250) lr 4.4380e-02 eta 0:00:33\n",
      "epoch [196/200] batch [5/5] time 1.129 (1.376) data 0.000 (0.213) loss 0.2437 (0.2769) acc 100.0000 (99.3750) lr 3.0827e-02 eta 0:00:27\n",
      "epoch [197/200] batch [5/5] time 1.132 (1.424) data 0.000 (0.202) loss 0.4525 (0.3744) acc 96.8750 (95.6250) lr 1.9733e-02 eta 0:00:21\n",
      "epoch [198/200] batch [5/5] time 1.160 (1.367) data 0.000 (0.215) loss 0.2301 (0.2980) acc 100.0000 (97.5000) lr 1.1101e-02 eta 0:00:13\n",
      "epoch [199/200] batch [5/5] time 1.105 (1.328) data 0.000 (0.198) loss 0.3002 (0.3213) acc 96.8750 (96.2500) lr 4.9344e-03 eta 0:00:06\n",
      "epoch [200/200] batch [5/5] time 1.116 (1.371) data 0.000 (0.215) loss 0.2664 (0.2857) acc 100.0000 (97.5000) lr 1.2337e-03 eta 0:00:00\n",
      "Checkpoint saved to output/new_init/eurosat/DAPT/vit_b16_16shots/seed1/prompt_learner/model.pth.tar-200\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [06:16<00:00,  4.65s/it]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 7,388\n",
      "* accuracy: 91.2%\n",
      "* error: 8.8%\n",
      "* macro_f1: 91.0%\n",
      "Elapsed: 0:29:40\n"
     ]
    }
   ],
   "source": [
    "#eurosat-16shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "        --output-dir output/new_init/eurosat/DAPT/vit_b16_16shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3Ayvwo4hguN",
    "outputId": "b4d3c045-64ae-4c8b-ae87-af929a78882a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:04:11.945042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:04:11.965536: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:04:11.971966: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:04:11.985750: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:04:13.014944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '8']\n",
      "output_dir: output/new_init/eurosat/DAPT/vit_b16_8shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 8\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/new_init/eurosat/DAPT/vit_b16_8shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  80\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.txt_ctx', 'prompt_learner.vis_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/new_init/eurosat/DAPT/vit_b16_8shots/seed1/tensorboard)\n",
      "epoch [1/200] batch [1/2] time 3.614 (3.614) data 0.901 (0.901) loss 11.7227 (11.7227) acc 12.5000 (12.5000) lr 1.0000e-05 eta 0:24:02\n",
      "epoch [1/200] batch [2/2] time 1.163 (2.389) data 0.004 (0.453) loss 11.4763 (11.5995) acc 9.3750 (10.9375) lr 2.0000e+01 eta 0:15:50\n",
      "epoch [2/200] batch [1/2] time 1.712 (1.712) data 0.578 (0.578) loss 11.6592 (11.6592) acc 9.3750 (9.3750) lr 2.0000e+01 eta 0:11:19\n",
      "epoch [2/200] batch [2/2] time 1.092 (1.402) data 0.001 (0.289) loss 10.8984 (11.2788) acc 6.2500 (7.8125) lr 1.9999e+01 eta 0:09:15\n",
      "epoch [3/200] batch [1/2] time 1.620 (1.620) data 0.492 (0.492) loss 10.6030 (10.6030) acc 0.0000 (0.0000) lr 1.9999e+01 eta 0:10:39\n",
      "epoch [3/200] batch [2/2] time 1.117 (1.369) data 0.000 (0.246) loss 10.3707 (10.4868) acc 0.0000 (0.0000) lr 1.9995e+01 eta 0:08:59\n",
      "epoch [4/200] batch [1/2] time 1.621 (1.621) data 0.509 (0.509) loss 10.2957 (10.2957) acc 3.1250 (3.1250) lr 1.9995e+01 eta 0:10:37\n",
      "epoch [4/200] batch [2/2] time 1.108 (1.365) data 0.000 (0.255) loss 10.2379 (10.2668) acc 6.2500 (4.6875) lr 1.9989e+01 eta 0:08:54\n",
      "epoch [5/200] batch [1/2] time 1.658 (1.658) data 0.527 (0.527) loss 10.2098 (10.2098) acc 6.2500 (6.2500) lr 1.9989e+01 eta 0:10:48\n",
      "epoch [5/200] batch [2/2] time 1.099 (1.379) data 0.001 (0.264) loss 10.1585 (10.1841) acc 6.2500 (6.2500) lr 1.9980e+01 eta 0:08:57\n",
      "epoch [6/200] batch [1/2] time 2.040 (2.040) data 0.798 (0.798) loss 10.1315 (10.1315) acc 12.5000 (12.5000) lr 1.9980e+01 eta 0:13:13\n",
      "epoch [6/200] batch [2/2] time 1.202 (1.621) data 0.001 (0.399) loss 10.1228 (10.1272) acc 9.3750 (10.9375) lr 1.9969e+01 eta 0:10:28\n",
      "epoch [7/200] batch [1/2] time 1.682 (1.682) data 0.575 (0.575) loss 10.0530 (10.0530) acc 9.3750 (9.3750) lr 1.9969e+01 eta 0:10:50\n",
      "epoch [7/200] batch [2/2] time 1.110 (1.396) data 0.001 (0.288) loss 10.0329 (10.0429) acc 9.3750 (9.3750) lr 1.9956e+01 eta 0:08:58\n",
      "epoch [8/200] batch [1/2] time 1.641 (1.641) data 0.501 (0.501) loss 10.0449 (10.0449) acc 6.2500 (6.2500) lr 1.9956e+01 eta 0:10:31\n",
      "epoch [8/200] batch [2/2] time 1.101 (1.371) data 0.001 (0.251) loss 9.9489 (9.9969) acc 3.1250 (4.6875) lr 1.9940e+01 eta 0:08:46\n",
      "epoch [9/200] batch [1/2] time 1.645 (1.645) data 0.494 (0.494) loss 9.9444 (9.9444) acc 0.0000 (0.0000) lr 1.9940e+01 eta 0:10:29\n",
      "epoch [9/200] batch [2/2] time 1.111 (1.378) data 0.001 (0.248) loss 9.8651 (9.9047) acc 12.5000 (6.2500) lr 1.9921e+01 eta 0:08:46\n",
      "epoch [10/200] batch [1/2] time 1.663 (1.663) data 0.517 (0.517) loss 9.9220 (9.9220) acc 3.1250 (3.1250) lr 1.9921e+01 eta 0:10:33\n",
      "epoch [10/200] batch [2/2] time 1.085 (1.374) data 0.001 (0.259) loss 9.8657 (9.8938) acc 6.2500 (4.6875) lr 1.9900e+01 eta 0:08:42\n",
      "epoch [11/200] batch [1/2] time 1.966 (1.966) data 0.786 (0.786) loss 9.7995 (9.7995) acc 3.1250 (3.1250) lr 1.9900e+01 eta 0:12:25\n",
      "epoch [11/200] batch [2/2] time 1.176 (1.571) data 0.001 (0.393) loss 9.7194 (9.7594) acc 3.1250 (3.1250) lr 1.9877e+01 eta 0:09:53\n",
      "epoch [12/200] batch [1/2] time 1.686 (1.686) data 0.559 (0.559) loss 9.7222 (9.7222) acc 3.1250 (3.1250) lr 1.9877e+01 eta 0:10:35\n",
      "epoch [12/200] batch [2/2] time 1.105 (1.395) data 0.001 (0.280) loss 9.5961 (9.6591) acc 15.6250 (9.3750) lr 1.9851e+01 eta 0:08:44\n",
      "epoch [13/200] batch [1/2] time 1.653 (1.653) data 0.503 (0.503) loss 9.4982 (9.4982) acc 15.6250 (15.6250) lr 1.9851e+01 eta 0:10:19\n",
      "epoch [13/200] batch [2/2] time 1.128 (1.391) data 0.001 (0.252) loss 9.4682 (9.4832) acc 3.1250 (9.3750) lr 1.9823e+01 eta 0:08:40\n",
      "epoch [14/200] batch [1/2] time 1.602 (1.602) data 0.481 (0.481) loss 9.2736 (9.2736) acc 25.0000 (25.0000) lr 1.9823e+01 eta 0:09:57\n",
      "epoch [14/200] batch [2/2] time 1.115 (1.358) data 0.001 (0.241) loss 9.2387 (9.2562) acc 9.3750 (17.1875) lr 1.9792e+01 eta 0:08:25\n",
      "epoch [15/200] batch [1/2] time 1.631 (1.631) data 0.481 (0.481) loss 8.9802 (8.9802) acc 12.5000 (12.5000) lr 1.9792e+01 eta 0:10:05\n",
      "epoch [15/200] batch [2/2] time 1.094 (1.362) data 0.000 (0.241) loss 8.7837 (8.8820) acc 15.6250 (14.0625) lr 1.9759e+01 eta 0:08:24\n",
      "epoch [16/200] batch [1/2] time 1.982 (1.982) data 0.778 (0.778) loss 8.4319 (8.4319) acc 6.2500 (6.2500) lr 1.9759e+01 eta 0:12:11\n",
      "epoch [16/200] batch [2/2] time 1.620 (1.801) data 0.001 (0.389) loss 8.0313 (8.2316) acc 15.6250 (10.9375) lr 1.9724e+01 eta 0:11:02\n",
      "epoch [17/200] batch [1/2] time 1.701 (1.701) data 0.597 (0.597) loss 7.3161 (7.3161) acc 15.6250 (15.6250) lr 1.9724e+01 eta 0:10:24\n",
      "epoch [17/200] batch [2/2] time 1.109 (1.405) data 0.001 (0.299) loss 6.7446 (7.0303) acc 12.5000 (14.0625) lr 1.9686e+01 eta 0:08:34\n",
      "epoch [18/200] batch [1/2] time 1.591 (1.591) data 0.486 (0.486) loss 5.9614 (5.9614) acc 15.6250 (15.6250) lr 1.9686e+01 eta 0:09:40\n",
      "epoch [18/200] batch [2/2] time 1.121 (1.356) data 0.001 (0.243) loss 5.8800 (5.9207) acc 6.2500 (10.9375) lr 1.9646e+01 eta 0:08:13\n",
      "epoch [19/200] batch [1/2] time 1.633 (1.633) data 0.504 (0.504) loss 5.0952 (5.0952) acc 18.7500 (18.7500) lr 1.9646e+01 eta 0:09:52\n",
      "epoch [19/200] batch [2/2] time 1.105 (1.369) data 0.001 (0.252) loss 6.8332 (5.9642) acc 3.1250 (10.9375) lr 1.9603e+01 eta 0:08:15\n",
      "epoch [20/200] batch [1/2] time 1.623 (1.623) data 0.469 (0.469) loss 11.9006 (11.9006) acc 9.3750 (9.3750) lr 1.9603e+01 eta 0:09:45\n",
      "epoch [20/200] batch [2/2] time 1.091 (1.357) data 0.001 (0.235) loss 10.9028 (11.4017) acc 15.6250 (12.5000) lr 1.9558e+01 eta 0:08:08\n",
      "epoch [21/200] batch [1/2] time 2.021 (2.021) data 0.822 (0.822) loss 9.2611 (9.2611) acc 15.6250 (15.6250) lr 1.9558e+01 eta 0:12:05\n",
      "epoch [21/200] batch [2/2] time 1.218 (1.619) data 0.001 (0.411) loss 8.4015 (8.8313) acc 9.3750 (12.5000) lr 1.9511e+01 eta 0:09:39\n",
      "epoch [22/200] batch [1/2] time 1.695 (1.695) data 0.577 (0.577) loss 7.5410 (7.5410) acc 12.5000 (12.5000) lr 1.9511e+01 eta 0:10:05\n",
      "epoch [22/200] batch [2/2] time 1.093 (1.394) data 0.001 (0.289) loss 6.9771 (7.2591) acc 6.2500 (9.3750) lr 1.9461e+01 eta 0:08:16\n",
      "epoch [23/200] batch [1/2] time 1.601 (1.601) data 0.492 (0.492) loss 6.2499 (6.2499) acc 18.7500 (18.7500) lr 1.9461e+01 eta 0:09:28\n",
      "epoch [23/200] batch [2/2] time 1.093 (1.347) data 0.001 (0.246) loss 6.2396 (6.2448) acc 3.1250 (10.9375) lr 1.9409e+01 eta 0:07:56\n",
      "epoch [24/200] batch [1/2] time 1.617 (1.617) data 0.507 (0.507) loss 5.5155 (5.5155) acc 12.5000 (12.5000) lr 1.9409e+01 eta 0:09:30\n",
      "epoch [24/200] batch [2/2] time 1.092 (1.355) data 0.000 (0.254) loss 5.6015 (5.5585) acc 3.1250 (7.8125) lr 1.9354e+01 eta 0:07:56\n",
      "epoch [25/200] batch [1/2] time 1.679 (1.679) data 0.541 (0.541) loss 5.0408 (5.0408) acc 12.5000 (12.5000) lr 1.9354e+01 eta 0:09:49\n",
      "epoch [25/200] batch [2/2] time 1.066 (1.373) data 0.001 (0.271) loss 5.0505 (5.0456) acc 6.2500 (9.3750) lr 1.9298e+01 eta 0:08:00\n",
      "epoch [26/200] batch [1/2] time 1.966 (1.966) data 0.769 (0.769) loss 4.8432 (4.8432) acc 6.2500 (6.2500) lr 1.9298e+01 eta 0:11:26\n",
      "epoch [26/200] batch [2/2] time 1.149 (1.557) data 0.001 (0.385) loss 4.9078 (4.8755) acc 9.3750 (7.8125) lr 1.9239e+01 eta 0:09:01\n",
      "epoch [27/200] batch [1/2] time 1.691 (1.691) data 0.566 (0.566) loss 4.6375 (4.6375) acc 9.3750 (9.3750) lr 1.9239e+01 eta 0:09:46\n",
      "epoch [27/200] batch [2/2] time 1.105 (1.398) data 0.000 (0.283) loss 4.5173 (4.5774) acc 3.1250 (6.2500) lr 1.9178e+01 eta 0:08:03\n",
      "epoch [28/200] batch [1/2] time 1.641 (1.641) data 0.532 (0.532) loss 4.4232 (4.4232) acc 9.3750 (9.3750) lr 1.9178e+01 eta 0:09:26\n",
      "epoch [28/200] batch [2/2] time 1.114 (1.378) data 0.001 (0.266) loss 4.2379 (4.3306) acc 6.2500 (7.8125) lr 1.9114e+01 eta 0:07:53\n",
      "epoch [29/200] batch [1/2] time 1.612 (1.612) data 0.502 (0.502) loss 4.1110 (4.1110) acc 15.6250 (15.6250) lr 1.9114e+01 eta 0:09:12\n",
      "epoch [29/200] batch [2/2] time 1.111 (1.361) data 0.000 (0.251) loss 4.3111 (4.2110) acc 9.3750 (12.5000) lr 1.9048e+01 eta 0:07:45\n",
      "epoch [30/200] batch [1/2] time 1.655 (1.655) data 0.524 (0.524) loss 3.9838 (3.9838) acc 18.7500 (18.7500) lr 1.9048e+01 eta 0:09:24\n",
      "epoch [30/200] batch [2/2] time 1.093 (1.374) data 0.001 (0.263) loss 3.9826 (3.9832) acc 9.3750 (14.0625) lr 1.8980e+01 eta 0:07:47\n",
      "epoch [31/200] batch [1/2] time 1.997 (1.997) data 0.805 (0.805) loss 3.7442 (3.7442) acc 6.2500 (6.2500) lr 1.8980e+01 eta 0:11:17\n",
      "epoch [31/200] batch [2/2] time 1.177 (1.587) data 0.001 (0.403) loss 3.6858 (3.7150) acc 15.6250 (10.9375) lr 1.8910e+01 eta 0:08:56\n",
      "epoch [32/200] batch [1/2] time 1.746 (1.746) data 0.585 (0.585) loss 3.7229 (3.7229) acc 9.3750 (9.3750) lr 1.8910e+01 eta 0:09:48\n",
      "epoch [32/200] batch [2/2] time 1.119 (1.433) data 0.001 (0.293) loss 3.5270 (3.6249) acc 9.3750 (9.3750) lr 1.8838e+01 eta 0:08:01\n",
      "epoch [33/200] batch [1/2] time 1.643 (1.643) data 0.524 (0.524) loss 3.2589 (3.2589) acc 15.6250 (15.6250) lr 1.8838e+01 eta 0:09:10\n",
      "epoch [33/200] batch [2/2] time 1.114 (1.379) data 0.001 (0.262) loss 4.0447 (3.6518) acc 9.3750 (12.5000) lr 1.8763e+01 eta 0:07:40\n",
      "epoch [34/200] batch [1/2] time 1.640 (1.640) data 0.512 (0.512) loss 4.8174 (4.8174) acc 3.1250 (3.1250) lr 1.8763e+01 eta 0:09:06\n",
      "epoch [34/200] batch [2/2] time 1.114 (1.377) data 0.001 (0.256) loss 4.2653 (4.5413) acc 21.8750 (12.5000) lr 1.8686e+01 eta 0:07:37\n",
      "epoch [35/200] batch [1/2] time 1.643 (1.643) data 0.518 (0.518) loss 3.9914 (3.9914) acc 15.6250 (15.6250) lr 1.8686e+01 eta 0:09:03\n",
      "epoch [35/200] batch [2/2] time 1.113 (1.378) data 0.001 (0.259) loss 5.9760 (4.9837) acc 6.2500 (10.9375) lr 1.8607e+01 eta 0:07:34\n",
      "epoch [36/200] batch [1/2] time 2.057 (2.057) data 0.844 (0.844) loss 10.9153 (10.9153) acc 6.2500 (6.2500) lr 1.8607e+01 eta 0:11:16\n",
      "epoch [36/200] batch [2/2] time 1.203 (1.630) data 0.001 (0.422) loss 10.9457 (10.9305) acc 6.2500 (6.2500) lr 1.8526e+01 eta 0:08:54\n",
      "epoch [37/200] batch [1/2] time 1.738 (1.738) data 0.612 (0.612) loss 10.9319 (10.9319) acc 12.5000 (12.5000) lr 1.8526e+01 eta 0:09:28\n",
      "epoch [37/200] batch [2/2] time 1.118 (1.428) data 0.001 (0.307) loss 9.2623 (10.0971) acc 6.2500 (9.3750) lr 1.8443e+01 eta 0:07:45\n",
      "epoch [38/200] batch [1/2] time 1.676 (1.676) data 0.538 (0.538) loss 8.2823 (8.2823) acc 9.3750 (9.3750) lr 1.8443e+01 eta 0:09:04\n",
      "epoch [38/200] batch [2/2] time 1.114 (1.395) data 0.001 (0.269) loss 8.3577 (8.3200) acc 9.3750 (9.3750) lr 1.8358e+01 eta 0:07:32\n",
      "epoch [39/200] batch [1/2] time 1.632 (1.632) data 0.496 (0.496) loss 6.6022 (6.6022) acc 6.2500 (6.2500) lr 1.8358e+01 eta 0:08:47\n",
      "epoch [39/200] batch [2/2] time 1.119 (1.376) data 0.001 (0.248) loss 8.1993 (7.4007) acc 9.3750 (7.8125) lr 1.8271e+01 eta 0:07:22\n",
      "epoch [40/200] batch [1/2] time 1.701 (1.701) data 0.537 (0.537) loss 6.3654 (6.3654) acc 9.3750 (9.3750) lr 1.8271e+01 eta 0:09:05\n",
      "epoch [40/200] batch [2/2] time 1.097 (1.399) data 0.000 (0.269) loss 5.2373 (5.8013) acc 6.2500 (7.8125) lr 1.8181e+01 eta 0:07:27\n",
      "epoch [41/200] batch [1/2] time 2.157 (2.157) data 0.852 (0.852) loss 6.3052 (6.3052) acc 12.5000 (12.5000) lr 1.8181e+01 eta 0:11:28\n",
      "epoch [41/200] batch [2/2] time 1.231 (1.694) data 0.001 (0.427) loss 4.6099 (5.4575) acc 12.5000 (12.5000) lr 1.8090e+01 eta 0:08:58\n",
      "epoch [42/200] batch [1/2] time 1.688 (1.688) data 0.584 (0.584) loss 4.3696 (4.3696) acc 15.6250 (15.6250) lr 1.8090e+01 eta 0:08:55\n",
      "epoch [42/200] batch [2/2] time 1.125 (1.406) data 0.001 (0.292) loss 4.1458 (4.2577) acc 15.6250 (15.6250) lr 1.7997e+01 eta 0:07:24\n",
      "epoch [43/200] batch [1/2] time 1.638 (1.638) data 0.529 (0.529) loss 4.0380 (4.0380) acc 18.7500 (18.7500) lr 1.7997e+01 eta 0:08:35\n",
      "epoch [43/200] batch [2/2] time 1.122 (1.380) data 0.001 (0.265) loss 3.9000 (3.9690) acc 0.0000 (9.3750) lr 1.7902e+01 eta 0:07:13\n",
      "epoch [44/200] batch [1/2] time 1.605 (1.605) data 0.499 (0.499) loss 3.3038 (3.3038) acc 28.1250 (28.1250) lr 1.7902e+01 eta 0:08:22\n",
      "epoch [44/200] batch [2/2] time 1.127 (1.366) data 0.001 (0.250) loss 3.6652 (3.4845) acc 3.1250 (15.6250) lr 1.7804e+01 eta 0:07:06\n",
      "epoch [45/200] batch [1/2] time 1.692 (1.692) data 0.510 (0.510) loss 3.4844 (3.4844) acc 15.6250 (15.6250) lr 1.7804e+01 eta 0:08:46\n",
      "epoch [45/200] batch [2/2] time 1.103 (1.397) data 0.001 (0.256) loss 3.1880 (3.3362) acc 31.2500 (23.4375) lr 1.7705e+01 eta 0:07:13\n",
      "epoch [46/200] batch [1/2] time 2.037 (2.037) data 0.802 (0.802) loss 3.0949 (3.0949) acc 21.8750 (21.8750) lr 1.7705e+01 eta 0:10:29\n",
      "epoch [46/200] batch [2/2] time 1.205 (1.621) data 0.001 (0.402) loss 3.1314 (3.1132) acc 21.8750 (21.8750) lr 1.7604e+01 eta 0:08:19\n",
      "epoch [47/200] batch [1/2] time 1.721 (1.721) data 0.588 (0.588) loss 3.1332 (3.1332) acc 25.0000 (25.0000) lr 1.7604e+01 eta 0:08:48\n",
      "epoch [47/200] batch [2/2] time 1.115 (1.418) data 0.001 (0.294) loss 2.7255 (2.9294) acc 18.7500 (21.8750) lr 1.7501e+01 eta 0:07:13\n",
      "epoch [48/200] batch [1/2] time 1.613 (1.613) data 0.504 (0.504) loss 2.5826 (2.5826) acc 37.5000 (37.5000) lr 1.7501e+01 eta 0:08:11\n",
      "epoch [48/200] batch [2/2] time 1.116 (1.364) data 0.001 (0.252) loss 3.1414 (2.8620) acc 25.0000 (31.2500) lr 1.7396e+01 eta 0:06:54\n",
      "epoch [49/200] batch [1/2] time 1.600 (1.600) data 0.495 (0.495) loss 3.0687 (3.0687) acc 28.1250 (28.1250) lr 1.7396e+01 eta 0:08:04\n",
      "epoch [49/200] batch [2/2] time 1.101 (1.351) data 0.001 (0.248) loss 3.7023 (3.3855) acc 18.7500 (23.4375) lr 1.7290e+01 eta 0:06:47\n",
      "epoch [50/200] batch [1/2] time 1.620 (1.620) data 0.519 (0.519) loss 2.9435 (2.9435) acc 25.0000 (25.0000) lr 1.7290e+01 eta 0:08:07\n",
      "epoch [50/200] batch [2/2] time 1.094 (1.357) data 0.001 (0.260) loss 3.4554 (3.1995) acc 21.8750 (23.4375) lr 1.7181e+01 eta 0:06:47\n",
      "epoch [51/200] batch [1/2] time 2.055 (2.055) data 0.843 (0.843) loss 3.3140 (3.3140) acc 37.5000 (37.5000) lr 1.7181e+01 eta 0:10:14\n",
      "epoch [51/200] batch [2/2] time 1.298 (1.676) data 0.001 (0.422) loss 3.9762 (3.6451) acc 21.8750 (29.6875) lr 1.7071e+01 eta 0:08:19\n",
      "epoch [52/200] batch [1/2] time 1.665 (1.665) data 0.558 (0.558) loss 3.8934 (3.8934) acc 9.3750 (9.3750) lr 1.7071e+01 eta 0:08:14\n",
      "epoch [52/200] batch [2/2] time 1.132 (1.399) data 0.001 (0.279) loss 5.7204 (4.8069) acc 15.6250 (12.5000) lr 1.6959e+01 eta 0:06:53\n",
      "epoch [53/200] batch [1/2] time 1.627 (1.627) data 0.488 (0.488) loss 3.8333 (3.8333) acc 6.2500 (6.2500) lr 1.6959e+01 eta 0:07:59\n",
      "epoch [53/200] batch [2/2] time 1.106 (1.366) data 0.001 (0.244) loss 5.0598 (4.4466) acc 9.3750 (7.8125) lr 1.6845e+01 eta 0:06:41\n",
      "epoch [54/200] batch [1/2] time 1.602 (1.602) data 0.492 (0.492) loss 5.3244 (5.3244) acc 12.5000 (12.5000) lr 1.6845e+01 eta 0:07:49\n",
      "epoch [54/200] batch [2/2] time 1.103 (1.353) data 0.001 (0.247) loss 4.4926 (4.9085) acc 6.2500 (9.3750) lr 1.6730e+01 eta 0:06:34\n",
      "epoch [55/200] batch [1/2] time 1.665 (1.665) data 0.506 (0.506) loss 10.4478 (10.4478) acc 12.5000 (12.5000) lr 1.6730e+01 eta 0:08:04\n",
      "epoch [55/200] batch [2/2] time 1.098 (1.382) data 0.001 (0.253) loss 7.5383 (8.9931) acc 9.3750 (10.9375) lr 1.6613e+01 eta 0:06:40\n",
      "epoch [56/200] batch [1/2] time 1.975 (1.975) data 0.797 (0.797) loss 5.6866 (5.6866) acc 15.6250 (15.6250) lr 1.6613e+01 eta 0:09:30\n",
      "epoch [56/200] batch [2/2] time 1.155 (1.565) data 0.001 (0.399) loss 4.9515 (5.3190) acc 21.8750 (18.7500) lr 1.6494e+01 eta 0:07:30\n",
      "epoch [57/200] batch [1/2] time 1.663 (1.663) data 0.565 (0.565) loss 4.0133 (4.0133) acc 21.8750 (21.8750) lr 1.6494e+01 eta 0:07:57\n",
      "epoch [57/200] batch [2/2] time 1.105 (1.384) data 0.001 (0.283) loss 3.8807 (3.9470) acc 9.3750 (15.6250) lr 1.6374e+01 eta 0:06:35\n",
      "epoch [58/200] batch [1/2] time 1.644 (1.644) data 0.502 (0.502) loss 3.8737 (3.8737) acc 25.0000 (25.0000) lr 1.6374e+01 eta 0:07:48\n",
      "epoch [58/200] batch [2/2] time 1.114 (1.379) data 0.001 (0.251) loss 4.6470 (4.2604) acc 6.2500 (15.6250) lr 1.6252e+01 eta 0:06:31\n",
      "epoch [59/200] batch [1/2] time 1.637 (1.637) data 0.493 (0.493) loss 3.5063 (3.5063) acc 12.5000 (12.5000) lr 1.6252e+01 eta 0:07:43\n",
      "epoch [59/200] batch [2/2] time 1.112 (1.374) data 0.000 (0.247) loss 2.9675 (3.2369) acc 18.7500 (15.6250) lr 1.6129e+01 eta 0:06:27\n",
      "epoch [60/200] batch [1/2] time 1.621 (1.621) data 0.485 (0.485) loss 3.3327 (3.3327) acc 15.6250 (15.6250) lr 1.6129e+01 eta 0:07:35\n",
      "epoch [60/200] batch [2/2] time 1.084 (1.353) data 0.001 (0.243) loss 3.0703 (3.2015) acc 18.7500 (17.1875) lr 1.6004e+01 eta 0:06:18\n",
      "epoch [61/200] batch [1/2] time 2.048 (2.048) data 0.813 (0.813) loss 3.1324 (3.1324) acc 21.8750 (21.8750) lr 1.6004e+01 eta 0:09:31\n",
      "epoch [61/200] batch [2/2] time 1.389 (1.719) data 0.001 (0.407) loss 2.5851 (2.8588) acc 40.6250 (31.2500) lr 1.5878e+01 eta 0:07:57\n",
      "epoch [62/200] batch [1/2] time 1.734 (1.734) data 0.599 (0.599) loss 2.5196 (2.5196) acc 31.2500 (31.2500) lr 1.5878e+01 eta 0:08:00\n",
      "epoch [62/200] batch [2/2] time 1.129 (1.431) data 0.000 (0.300) loss 3.6711 (3.0953) acc 25.0000 (28.1250) lr 1.5750e+01 eta 0:06:35\n",
      "epoch [63/200] batch [1/2] time 1.655 (1.655) data 0.518 (0.518) loss 2.5900 (2.5900) acc 37.5000 (37.5000) lr 1.5750e+01 eta 0:07:35\n",
      "epoch [63/200] batch [2/2] time 1.132 (1.393) data 0.001 (0.259) loss 2.7185 (2.6542) acc 50.0000 (43.7500) lr 1.5621e+01 eta 0:06:21\n",
      "epoch [64/200] batch [1/2] time 1.615 (1.615) data 0.492 (0.492) loss 2.2112 (2.2112) acc 50.0000 (50.0000) lr 1.5621e+01 eta 0:07:20\n",
      "epoch [64/200] batch [2/2] time 1.096 (1.356) data 0.001 (0.246) loss 2.3167 (2.2639) acc 31.2500 (40.6250) lr 1.5490e+01 eta 0:06:08\n",
      "epoch [65/200] batch [1/2] time 1.631 (1.631) data 0.503 (0.503) loss 2.6499 (2.6499) acc 50.0000 (50.0000) lr 1.5490e+01 eta 0:07:22\n",
      "epoch [65/200] batch [2/2] time 1.097 (1.364) data 0.000 (0.252) loss 2.2017 (2.4258) acc 43.7500 (46.8750) lr 1.5358e+01 eta 0:06:08\n",
      "epoch [66/200] batch [1/2] time 2.021 (2.021) data 0.812 (0.812) loss 1.9309 (1.9309) acc 53.1250 (53.1250) lr 1.5358e+01 eta 0:09:03\n",
      "epoch [66/200] batch [2/2] time 1.156 (1.588) data 0.001 (0.406) loss 2.0122 (1.9715) acc 56.2500 (54.6875) lr 1.5225e+01 eta 0:07:05\n",
      "epoch [67/200] batch [1/2] time 1.694 (1.694) data 0.569 (0.569) loss 1.9278 (1.9278) acc 59.3750 (59.3750) lr 1.5225e+01 eta 0:07:32\n",
      "epoch [67/200] batch [2/2] time 1.118 (1.406) data 0.001 (0.285) loss 2.2355 (2.0817) acc 53.1250 (56.2500) lr 1.5090e+01 eta 0:06:13\n",
      "epoch [68/200] batch [1/2] time 1.646 (1.646) data 0.502 (0.502) loss 1.6947 (1.6947) acc 65.6250 (65.6250) lr 1.5090e+01 eta 0:07:16\n",
      "epoch [68/200] batch [2/2] time 1.120 (1.383) data 0.001 (0.251) loss 1.6757 (1.6852) acc 65.6250 (65.6250) lr 1.4955e+01 eta 0:06:05\n",
      "epoch [69/200] batch [1/2] time 1.639 (1.639) data 0.506 (0.506) loss 1.2957 (1.2957) acc 75.0000 (75.0000) lr 1.4955e+01 eta 0:07:10\n",
      "epoch [69/200] batch [2/2] time 1.117 (1.378) data 0.001 (0.253) loss 1.4250 (1.3603) acc 71.8750 (73.4375) lr 1.4818e+01 eta 0:06:01\n",
      "epoch [70/200] batch [1/2] time 1.652 (1.652) data 0.490 (0.490) loss 1.4334 (1.4334) acc 68.7500 (68.7500) lr 1.4818e+01 eta 0:07:11\n",
      "epoch [70/200] batch [2/2] time 1.119 (1.386) data 0.001 (0.245) loss 1.7275 (1.5805) acc 65.6250 (67.1875) lr 1.4679e+01 eta 0:06:00\n",
      "epoch [71/200] batch [1/2] time 2.023 (2.023) data 0.790 (0.790) loss 1.3722 (1.3722) acc 75.0000 (75.0000) lr 1.4679e+01 eta 0:08:43\n",
      "epoch [71/200] batch [2/2] time 1.222 (1.623) data 0.001 (0.396) loss 0.8134 (1.0928) acc 90.6250 (82.8125) lr 1.4540e+01 eta 0:06:58\n",
      "epoch [72/200] batch [1/2] time 1.771 (1.771) data 0.601 (0.601) loss 1.2795 (1.2795) acc 78.1250 (78.1250) lr 1.4540e+01 eta 0:07:35\n",
      "epoch [72/200] batch [2/2] time 1.110 (1.440) data 0.001 (0.301) loss 1.0543 (1.1669) acc 84.3750 (81.2500) lr 1.4399e+01 eta 0:06:08\n",
      "epoch [73/200] batch [1/2] time 1.617 (1.617) data 0.502 (0.502) loss 1.4160 (1.4160) acc 71.8750 (71.8750) lr 1.4399e+01 eta 0:06:52\n",
      "epoch [73/200] batch [2/2] time 1.117 (1.367) data 0.001 (0.251) loss 1.2435 (1.3297) acc 78.1250 (75.0000) lr 1.4258e+01 eta 0:05:47\n",
      "epoch [74/200] batch [1/2] time 1.643 (1.643) data 0.492 (0.492) loss 1.3307 (1.3307) acc 75.0000 (75.0000) lr 1.4258e+01 eta 0:06:55\n",
      "epoch [74/200] batch [2/2] time 1.101 (1.372) data 0.001 (0.246) loss 1.6489 (1.4898) acc 78.1250 (76.5625) lr 1.4115e+01 eta 0:05:45\n",
      "epoch [75/200] batch [1/2] time 1.654 (1.654) data 0.524 (0.524) loss 2.0918 (2.0918) acc 62.5000 (62.5000) lr 1.4115e+01 eta 0:06:55\n",
      "epoch [75/200] batch [2/2] time 1.092 (1.373) data 0.001 (0.262) loss 1.2070 (1.6494) acc 75.0000 (68.7500) lr 1.3971e+01 eta 0:05:43\n",
      "epoch [76/200] batch [1/2] time 1.993 (1.993) data 0.796 (0.796) loss 1.2470 (1.2470) acc 75.0000 (75.0000) lr 1.3971e+01 eta 0:08:16\n",
      "epoch [76/200] batch [2/2] time 1.150 (1.572) data 0.001 (0.399) loss 1.3793 (1.3131) acc 78.1250 (76.5625) lr 1.3827e+01 eta 0:06:29\n",
      "epoch [77/200] batch [1/2] time 1.696 (1.696) data 0.594 (0.594) loss 1.3050 (1.3050) acc 71.8750 (71.8750) lr 1.3827e+01 eta 0:06:59\n",
      "epoch [77/200] batch [2/2] time 1.113 (1.405) data 0.001 (0.297) loss 1.9985 (1.6518) acc 53.1250 (62.5000) lr 1.3681e+01 eta 0:05:45\n",
      "epoch [78/200] batch [1/2] time 1.586 (1.586) data 0.489 (0.489) loss 1.9505 (1.9505) acc 56.2500 (56.2500) lr 1.3681e+01 eta 0:06:28\n",
      "epoch [78/200] batch [2/2] time 1.098 (1.342) data 0.001 (0.245) loss 1.5818 (1.7662) acc 75.0000 (65.6250) lr 1.3535e+01 eta 0:05:27\n",
      "epoch [79/200] batch [1/2] time 1.614 (1.614) data 0.488 (0.488) loss 1.2436 (1.2436) acc 84.3750 (84.3750) lr 1.3535e+01 eta 0:06:32\n",
      "epoch [79/200] batch [2/2] time 1.111 (1.363) data 0.001 (0.244) loss 1.4301 (1.3368) acc 71.8750 (78.1250) lr 1.3387e+01 eta 0:05:29\n",
      "epoch [80/200] batch [1/2] time 1.629 (1.629) data 0.493 (0.493) loss 0.9533 (0.9533) acc 78.1250 (78.1250) lr 1.3387e+01 eta 0:06:32\n",
      "epoch [80/200] batch [2/2] time 1.089 (1.359) data 0.001 (0.247) loss 1.0359 (0.9946) acc 75.0000 (76.5625) lr 1.3239e+01 eta 0:05:26\n",
      "epoch [81/200] batch [1/2] time 2.078 (2.078) data 0.872 (0.872) loss 1.0279 (1.0279) acc 81.2500 (81.2500) lr 1.3239e+01 eta 0:08:16\n",
      "epoch [81/200] batch [2/2] time 1.151 (1.615) data 0.001 (0.436) loss 1.2474 (1.1377) acc 71.8750 (76.5625) lr 1.3090e+01 eta 0:06:24\n",
      "epoch [82/200] batch [1/2] time 1.757 (1.757) data 0.580 (0.580) loss 1.7055 (1.7055) acc 75.0000 (75.0000) lr 1.3090e+01 eta 0:06:56\n",
      "epoch [82/200] batch [2/2] time 1.119 (1.438) data 0.001 (0.290) loss 2.3994 (2.0525) acc 65.6250 (70.3125) lr 1.2940e+01 eta 0:05:39\n",
      "epoch [83/200] batch [1/2] time 1.647 (1.647) data 0.517 (0.517) loss 1.9138 (1.9138) acc 62.5000 (62.5000) lr 1.2940e+01 eta 0:06:26\n",
      "epoch [83/200] batch [2/2] time 1.119 (1.383) data 0.001 (0.259) loss 1.4530 (1.6834) acc 71.8750 (67.1875) lr 1.2790e+01 eta 0:05:23\n",
      "epoch [84/200] batch [1/2] time 1.607 (1.607) data 0.486 (0.486) loss 1.5902 (1.5902) acc 65.6250 (65.6250) lr 1.2790e+01 eta 0:06:14\n",
      "epoch [84/200] batch [2/2] time 1.101 (1.354) data 0.001 (0.243) loss 1.4495 (1.5198) acc 68.7500 (67.1875) lr 1.2639e+01 eta 0:05:14\n",
      "epoch [85/200] batch [1/2] time 1.694 (1.694) data 0.526 (0.526) loss 1.3232 (1.3232) acc 68.7500 (68.7500) lr 1.2639e+01 eta 0:06:31\n",
      "epoch [85/200] batch [2/2] time 1.130 (1.412) data 0.001 (0.263) loss 0.9412 (1.1322) acc 84.3750 (76.5625) lr 1.2487e+01 eta 0:05:24\n",
      "epoch [86/200] batch [1/2] time 2.081 (2.081) data 0.855 (0.855) loss 1.0622 (1.0622) acc 84.3750 (84.3750) lr 1.2487e+01 eta 0:07:56\n",
      "epoch [86/200] batch [2/2] time 1.130 (1.605) data 0.001 (0.428) loss 0.8146 (0.9384) acc 93.7500 (89.0625) lr 1.2334e+01 eta 0:06:05\n",
      "epoch [87/200] batch [1/2] time 1.704 (1.704) data 0.580 (0.580) loss 0.7522 (0.7522) acc 96.8750 (96.8750) lr 1.2334e+01 eta 0:06:26\n",
      "epoch [87/200] batch [2/2] time 1.110 (1.407) data 0.001 (0.290) loss 1.4280 (1.0901) acc 81.2500 (89.0625) lr 1.2181e+01 eta 0:05:17\n",
      "epoch [88/200] batch [1/2] time 1.676 (1.676) data 0.532 (0.532) loss 1.0374 (1.0374) acc 87.5000 (87.5000) lr 1.2181e+01 eta 0:06:17\n",
      "epoch [88/200] batch [2/2] time 1.120 (1.398) data 0.001 (0.266) loss 0.9274 (0.9824) acc 87.5000 (87.5000) lr 1.2028e+01 eta 0:05:13\n",
      "epoch [89/200] batch [1/2] time 1.646 (1.646) data 0.520 (0.520) loss 0.9509 (0.9509) acc 81.2500 (81.2500) lr 1.2028e+01 eta 0:06:07\n",
      "epoch [89/200] batch [2/2] time 1.118 (1.382) data 0.001 (0.260) loss 0.9754 (0.9631) acc 78.1250 (79.6875) lr 1.1874e+01 eta 0:05:06\n",
      "epoch [90/200] batch [1/2] time 1.652 (1.652) data 0.512 (0.512) loss 0.9844 (0.9844) acc 84.3750 (84.3750) lr 1.1874e+01 eta 0:06:05\n",
      "epoch [90/200] batch [2/2] time 1.101 (1.377) data 0.001 (0.256) loss 0.9292 (0.9568) acc 90.6250 (87.5000) lr 1.1719e+01 eta 0:05:02\n",
      "epoch [91/200] batch [1/2] time 2.002 (2.002) data 0.804 (0.804) loss 1.0667 (1.0667) acc 84.3750 (84.3750) lr 1.1719e+01 eta 0:07:18\n",
      "epoch [91/200] batch [2/2] time 1.151 (1.576) data 0.001 (0.402) loss 0.9619 (1.0143) acc 75.0000 (79.6875) lr 1.1564e+01 eta 0:05:43\n",
      "epoch [92/200] batch [1/2] time 1.753 (1.753) data 0.604 (0.604) loss 0.7378 (0.7378) acc 90.6250 (90.6250) lr 1.1564e+01 eta 0:06:20\n",
      "epoch [92/200] batch [2/2] time 1.113 (1.433) data 0.000 (0.302) loss 1.4187 (1.0783) acc 62.5000 (76.5625) lr 1.1409e+01 eta 0:05:09\n",
      "epoch [93/200] batch [1/2] time 1.624 (1.624) data 0.504 (0.504) loss 0.8552 (0.8552) acc 87.5000 (87.5000) lr 1.1409e+01 eta 0:05:49\n",
      "epoch [93/200] batch [2/2] time 1.121 (1.372) data 0.001 (0.252) loss 0.8666 (0.8609) acc 84.3750 (85.9375) lr 1.1253e+01 eta 0:04:53\n",
      "epoch [94/200] batch [1/2] time 1.650 (1.650) data 0.507 (0.507) loss 0.6659 (0.6659) acc 87.5000 (87.5000) lr 1.1253e+01 eta 0:05:51\n",
      "epoch [94/200] batch [2/2] time 1.125 (1.387) data 0.001 (0.254) loss 0.9414 (0.8036) acc 81.2500 (84.3750) lr 1.1097e+01 eta 0:04:54\n",
      "epoch [95/200] batch [1/2] time 1.672 (1.672) data 0.492 (0.492) loss 0.6873 (0.6873) acc 93.7500 (93.7500) lr 1.1097e+01 eta 0:05:52\n",
      "epoch [95/200] batch [2/2] time 1.103 (1.387) data 0.001 (0.246) loss 1.1133 (0.9003) acc 84.3750 (89.0625) lr 1.0941e+01 eta 0:04:51\n",
      "epoch [96/200] batch [1/2] time 2.027 (2.027) data 0.809 (0.809) loss 1.0445 (1.0445) acc 78.1250 (78.1250) lr 1.0941e+01 eta 0:07:03\n",
      "epoch [96/200] batch [2/2] time 1.155 (1.591) data 0.001 (0.405) loss 0.7750 (0.9098) acc 90.6250 (84.3750) lr 1.0785e+01 eta 0:05:30\n",
      "epoch [97/200] batch [1/2] time 1.677 (1.677) data 0.544 (0.544) loss 0.7850 (0.7850) acc 87.5000 (87.5000) lr 1.0785e+01 eta 0:05:47\n",
      "epoch [97/200] batch [2/2] time 1.146 (1.411) data 0.001 (0.272) loss 0.9203 (0.8527) acc 90.6250 (89.0625) lr 1.0628e+01 eta 0:04:50\n",
      "epoch [98/200] batch [1/2] time 1.627 (1.627) data 0.480 (0.480) loss 0.7254 (0.7254) acc 90.6250 (90.6250) lr 1.0628e+01 eta 0:05:33\n",
      "epoch [98/200] batch [2/2] time 1.094 (1.360) data 0.001 (0.240) loss 0.6773 (0.7013) acc 93.7500 (92.1875) lr 1.0471e+01 eta 0:04:37\n",
      "epoch [99/200] batch [1/2] time 1.656 (1.656) data 0.511 (0.511) loss 0.9592 (0.9592) acc 84.3750 (84.3750) lr 1.0471e+01 eta 0:05:36\n",
      "epoch [99/200] batch [2/2] time 1.130 (1.393) data 0.001 (0.256) loss 0.7578 (0.8585) acc 84.3750 (84.3750) lr 1.0314e+01 eta 0:04:41\n",
      "epoch [100/200] batch [1/2] time 1.700 (1.700) data 0.516 (0.516) loss 0.9300 (0.9300) acc 81.2500 (81.2500) lr 1.0314e+01 eta 0:05:41\n",
      "epoch [100/200] batch [2/2] time 1.109 (1.404) data 0.001 (0.258) loss 0.8744 (0.9022) acc 90.6250 (85.9375) lr 1.0157e+01 eta 0:04:40\n",
      "epoch [101/200] batch [1/2] time 2.023 (2.023) data 0.820 (0.820) loss 1.0415 (1.0415) acc 75.0000 (75.0000) lr 1.0157e+01 eta 0:06:42\n",
      "epoch [101/200] batch [2/2] time 1.130 (1.576) data 0.001 (0.410) loss 1.1700 (1.1057) acc 75.0000 (75.0000) lr 1.0000e+01 eta 0:05:12\n",
      "epoch [102/200] batch [1/2] time 1.682 (1.682) data 0.559 (0.559) loss 1.1100 (1.1100) acc 75.0000 (75.0000) lr 1.0000e+01 eta 0:05:31\n",
      "epoch [102/200] batch [2/2] time 1.089 (1.386) data 0.001 (0.280) loss 0.6242 (0.8671) acc 100.0000 (87.5000) lr 9.8429e+00 eta 0:04:31\n",
      "epoch [103/200] batch [1/2] time 1.643 (1.643) data 0.526 (0.526) loss 0.5480 (0.5480) acc 96.8750 (96.8750) lr 9.8429e+00 eta 0:05:20\n",
      "epoch [103/200] batch [2/2] time 1.112 (1.377) data 0.001 (0.263) loss 0.6175 (0.5828) acc 93.7500 (95.3125) lr 9.6859e+00 eta 0:04:27\n",
      "epoch [104/200] batch [1/2] time 1.622 (1.622) data 0.513 (0.513) loss 0.8835 (0.8835) acc 87.5000 (87.5000) lr 9.6859e+00 eta 0:05:12\n",
      "epoch [104/200] batch [2/2] time 1.116 (1.369) data 0.001 (0.257) loss 0.7990 (0.8413) acc 87.5000 (87.5000) lr 9.5289e+00 eta 0:04:22\n",
      "epoch [105/200] batch [1/2] time 1.618 (1.618) data 0.491 (0.491) loss 0.8960 (0.8960) acc 84.3750 (84.3750) lr 9.5289e+00 eta 0:05:09\n",
      "epoch [105/200] batch [2/2] time 1.079 (1.349) data 0.001 (0.246) loss 0.6552 (0.7756) acc 87.5000 (85.9375) lr 9.3721e+00 eta 0:04:16\n",
      "epoch [106/200] batch [1/2] time 2.042 (2.042) data 0.835 (0.835) loss 0.9259 (0.9259) acc 84.3750 (84.3750) lr 9.3721e+00 eta 0:06:25\n",
      "epoch [106/200] batch [2/2] time 1.151 (1.596) data 0.001 (0.418) loss 0.5634 (0.7447) acc 96.8750 (90.6250) lr 9.2154e+00 eta 0:05:00\n",
      "epoch [107/200] batch [1/2] time 1.701 (1.701) data 0.595 (0.595) loss 0.7849 (0.7849) acc 90.6250 (90.6250) lr 9.2154e+00 eta 0:05:18\n",
      "epoch [107/200] batch [2/2] time 1.121 (1.411) data 0.001 (0.298) loss 0.8706 (0.8277) acc 90.6250 (90.6250) lr 9.0589e+00 eta 0:04:22\n",
      "epoch [108/200] batch [1/2] time 1.624 (1.624) data 0.495 (0.495) loss 0.5404 (0.5404) acc 93.7500 (93.7500) lr 9.0589e+00 eta 0:05:00\n",
      "epoch [108/200] batch [2/2] time 1.125 (1.375) data 0.001 (0.248) loss 0.4952 (0.5178) acc 100.0000 (96.8750) lr 8.9027e+00 eta 0:04:12\n",
      "epoch [109/200] batch [1/2] time 1.632 (1.632) data 0.509 (0.509) loss 0.5967 (0.5967) acc 96.8750 (96.8750) lr 8.9027e+00 eta 0:04:58\n",
      "epoch [109/200] batch [2/2] time 1.107 (1.369) data 0.001 (0.255) loss 0.5395 (0.5681) acc 93.7500 (95.3125) lr 8.7467e+00 eta 0:04:09\n",
      "epoch [110/200] batch [1/2] time 1.609 (1.609) data 0.471 (0.471) loss 0.6138 (0.6138) acc 93.7500 (93.7500) lr 8.7467e+00 eta 0:04:51\n",
      "epoch [110/200] batch [2/2] time 1.097 (1.353) data 0.000 (0.236) loss 0.7462 (0.6800) acc 90.6250 (92.1875) lr 8.5910e+00 eta 0:04:03\n",
      "epoch [111/200] batch [1/2] time 2.061 (2.061) data 0.777 (0.777) loss 0.5901 (0.5901) acc 93.7500 (93.7500) lr 8.5910e+00 eta 0:06:08\n",
      "epoch [111/200] batch [2/2] time 1.207 (1.634) data 0.001 (0.389) loss 0.5789 (0.5845) acc 90.6250 (92.1875) lr 8.4357e+00 eta 0:04:50\n",
      "epoch [112/200] batch [1/2] time 1.722 (1.722) data 0.571 (0.571) loss 0.5928 (0.5928) acc 93.7500 (93.7500) lr 8.4357e+00 eta 0:05:04\n",
      "epoch [112/200] batch [2/2] time 1.119 (1.420) data 0.000 (0.286) loss 0.6983 (0.6455) acc 93.7500 (93.7500) lr 8.2807e+00 eta 0:04:10\n",
      "epoch [113/200] batch [1/2] time 1.646 (1.646) data 0.502 (0.502) loss 1.0153 (1.0153) acc 84.3750 (84.3750) lr 8.2807e+00 eta 0:04:48\n",
      "epoch [113/200] batch [2/2] time 1.115 (1.381) data 0.000 (0.251) loss 0.8377 (0.9265) acc 90.6250 (87.5000) lr 8.1262e+00 eta 0:04:00\n",
      "epoch [114/200] batch [1/2] time 1.664 (1.664) data 0.514 (0.514) loss 0.4335 (0.4335) acc 96.8750 (96.8750) lr 8.1262e+00 eta 0:04:47\n",
      "epoch [114/200] batch [2/2] time 1.117 (1.390) data 0.001 (0.257) loss 0.8513 (0.6424) acc 84.3750 (90.6250) lr 7.9721e+00 eta 0:03:59\n",
      "epoch [115/200] batch [1/2] time 1.673 (1.673) data 0.530 (0.530) loss 0.5151 (0.5151) acc 90.6250 (90.6250) lr 7.9721e+00 eta 0:04:46\n",
      "epoch [115/200] batch [2/2] time 1.095 (1.384) data 0.001 (0.265) loss 1.1091 (0.8121) acc 75.0000 (82.8125) lr 7.8186e+00 eta 0:03:55\n",
      "epoch [116/200] batch [1/2] time 2.034 (2.034) data 0.799 (0.799) loss 0.9549 (0.9549) acc 87.5000 (87.5000) lr 7.8186e+00 eta 0:05:43\n",
      "epoch [116/200] batch [2/2] time 1.213 (1.624) data 0.001 (0.400) loss 0.7204 (0.8377) acc 87.5000 (87.5000) lr 7.6655e+00 eta 0:04:32\n",
      "epoch [117/200] batch [1/2] time 1.748 (1.748) data 0.606 (0.606) loss 0.8948 (0.8948) acc 87.5000 (87.5000) lr 7.6655e+00 eta 0:04:51\n",
      "epoch [117/200] batch [2/2] time 1.115 (1.432) data 0.001 (0.303) loss 0.7811 (0.8380) acc 90.6250 (89.0625) lr 7.5131e+00 eta 0:03:57\n",
      "epoch [118/200] batch [1/2] time 1.642 (1.642) data 0.511 (0.511) loss 0.6425 (0.6425) acc 93.7500 (93.7500) lr 7.5131e+00 eta 0:04:30\n",
      "epoch [118/200] batch [2/2] time 1.114 (1.378) data 0.000 (0.256) loss 1.0608 (0.8517) acc 75.0000 (84.3750) lr 7.3613e+00 eta 0:03:45\n",
      "epoch [119/200] batch [1/2] time 1.633 (1.633) data 0.523 (0.523) loss 0.6466 (0.6466) acc 93.7500 (93.7500) lr 7.3613e+00 eta 0:04:26\n",
      "epoch [119/200] batch [2/2] time 1.114 (1.374) data 0.000 (0.262) loss 0.6392 (0.6429) acc 93.7500 (93.7500) lr 7.2101e+00 eta 0:03:42\n",
      "epoch [120/200] batch [1/2] time 1.633 (1.633) data 0.481 (0.481) loss 0.7770 (0.7770) acc 96.8750 (96.8750) lr 7.2101e+00 eta 0:04:22\n",
      "epoch [120/200] batch [2/2] time 1.101 (1.367) data 0.001 (0.241) loss 0.5523 (0.6647) acc 96.8750 (96.8750) lr 7.0596e+00 eta 0:03:38\n",
      "epoch [121/200] batch [1/2] time 2.010 (2.010) data 0.807 (0.807) loss 0.6266 (0.6266) acc 90.6250 (90.6250) lr 7.0596e+00 eta 0:05:19\n",
      "epoch [121/200] batch [2/2] time 1.196 (1.603) data 0.001 (0.404) loss 0.6411 (0.6338) acc 90.6250 (90.6250) lr 6.9098e+00 eta 0:04:13\n",
      "epoch [122/200] batch [1/2] time 1.706 (1.706) data 0.584 (0.584) loss 0.5733 (0.5733) acc 90.6250 (90.6250) lr 6.9098e+00 eta 0:04:27\n",
      "epoch [122/200] batch [2/2] time 1.106 (1.406) data 0.001 (0.292) loss 0.4237 (0.4985) acc 96.8750 (93.7500) lr 6.7608e+00 eta 0:03:39\n",
      "epoch [123/200] batch [1/2] time 1.593 (1.593) data 0.488 (0.488) loss 0.4191 (0.4191) acc 96.8750 (96.8750) lr 6.7608e+00 eta 0:04:06\n",
      "epoch [123/200] batch [2/2] time 1.103 (1.348) data 0.000 (0.244) loss 0.3304 (0.3748) acc 100.0000 (98.4375) lr 6.6126e+00 eta 0:03:27\n",
      "epoch [124/200] batch [1/2] time 1.564 (1.564) data 0.475 (0.475) loss 1.0720 (1.0720) acc 84.3750 (84.3750) lr 6.6126e+00 eta 0:03:59\n",
      "epoch [124/200] batch [2/2] time 1.092 (1.328) data 0.001 (0.238) loss 0.7225 (0.8972) acc 87.5000 (85.9375) lr 6.4653e+00 eta 0:03:21\n",
      "epoch [125/200] batch [1/2] time 1.592 (1.592) data 0.484 (0.484) loss 0.8039 (0.8039) acc 84.3750 (84.3750) lr 6.4653e+00 eta 0:04:00\n",
      "epoch [125/200] batch [2/2] time 1.087 (1.340) data 0.001 (0.242) loss 0.6819 (0.7429) acc 87.5000 (85.9375) lr 6.3188e+00 eta 0:03:20\n",
      "epoch [126/200] batch [1/2] time 2.087 (2.087) data 0.858 (0.858) loss 1.0712 (1.0712) acc 84.3750 (84.3750) lr 6.3188e+00 eta 0:05:10\n",
      "epoch [126/200] batch [2/2] time 1.150 (1.619) data 0.001 (0.430) loss 0.8889 (0.9801) acc 87.5000 (85.9375) lr 6.1732e+00 eta 0:03:59\n",
      "epoch [127/200] batch [1/2] time 1.726 (1.726) data 0.591 (0.591) loss 0.9210 (0.9210) acc 87.5000 (87.5000) lr 6.1732e+00 eta 0:04:13\n",
      "epoch [127/200] batch [2/2] time 1.104 (1.415) data 0.000 (0.296) loss 0.4645 (0.6927) acc 96.8750 (92.1875) lr 6.0285e+00 eta 0:03:26\n",
      "epoch [128/200] batch [1/2] time 1.600 (1.600) data 0.491 (0.491) loss 0.9723 (0.9723) acc 81.2500 (81.2500) lr 6.0285e+00 eta 0:03:51\n",
      "epoch [128/200] batch [2/2] time 1.086 (1.343) data 0.000 (0.246) loss 0.5774 (0.7748) acc 93.7500 (87.5000) lr 5.8849e+00 eta 0:03:13\n",
      "epoch [129/200] batch [1/2] time 1.631 (1.631) data 0.514 (0.514) loss 0.4046 (0.4046) acc 100.0000 (100.0000) lr 5.8849e+00 eta 0:03:53\n",
      "epoch [129/200] batch [2/2] time 1.097 (1.364) data 0.000 (0.257) loss 1.0691 (0.7369) acc 59.3750 (79.6875) lr 5.7422e+00 eta 0:03:13\n",
      "epoch [130/200] batch [1/2] time 1.631 (1.631) data 0.498 (0.498) loss 0.4505 (0.4505) acc 100.0000 (100.0000) lr 5.7422e+00 eta 0:03:49\n",
      "epoch [130/200] batch [2/2] time 1.116 (1.374) data 0.000 (0.249) loss 0.5900 (0.5203) acc 93.7500 (96.8750) lr 5.6006e+00 eta 0:03:12\n",
      "epoch [131/200] batch [1/2] time 2.046 (2.046) data 0.836 (0.836) loss 0.7685 (0.7685) acc 84.3750 (84.3750) lr 5.6006e+00 eta 0:04:44\n",
      "epoch [131/200] batch [2/2] time 1.180 (1.613) data 0.001 (0.418) loss 0.4467 (0.6076) acc 96.8750 (90.6250) lr 5.4601e+00 eta 0:03:42\n",
      "epoch [132/200] batch [1/2] time 1.692 (1.692) data 0.574 (0.574) loss 0.8904 (0.8904) acc 87.5000 (87.5000) lr 5.4601e+00 eta 0:03:51\n",
      "epoch [132/200] batch [2/2] time 1.123 (1.407) data 0.001 (0.287) loss 0.4980 (0.6942) acc 93.7500 (90.6250) lr 5.3207e+00 eta 0:03:11\n",
      "epoch [133/200] batch [1/2] time 1.610 (1.610) data 0.495 (0.495) loss 0.4877 (0.4877) acc 96.8750 (96.8750) lr 5.3207e+00 eta 0:03:37\n",
      "epoch [133/200] batch [2/2] time 1.121 (1.366) data 0.000 (0.248) loss 1.0547 (0.7712) acc 81.2500 (89.0625) lr 5.1825e+00 eta 0:03:02\n",
      "epoch [134/200] batch [1/2] time 1.637 (1.637) data 0.542 (0.542) loss 0.7436 (0.7436) acc 93.7500 (93.7500) lr 5.1825e+00 eta 0:03:37\n",
      "epoch [134/200] batch [2/2] time 1.118 (1.377) data 0.001 (0.271) loss 0.7346 (0.7391) acc 87.5000 (90.6250) lr 5.0454e+00 eta 0:03:01\n",
      "epoch [135/200] batch [1/2] time 1.632 (1.632) data 0.481 (0.481) loss 0.4074 (0.4074) acc 100.0000 (100.0000) lr 5.0454e+00 eta 0:03:33\n",
      "epoch [135/200] batch [2/2] time 1.082 (1.357) data 0.001 (0.241) loss 0.5812 (0.4943) acc 96.8750 (98.4375) lr 4.9096e+00 eta 0:02:56\n",
      "epoch [136/200] batch [1/2] time 2.014 (2.014) data 0.797 (0.797) loss 0.5528 (0.5528) acc 96.8750 (96.8750) lr 4.9096e+00 eta 0:04:19\n",
      "epoch [136/200] batch [2/2] time 1.143 (1.579) data 0.004 (0.401) loss 0.4156 (0.4842) acc 100.0000 (98.4375) lr 4.7750e+00 eta 0:03:22\n",
      "epoch [137/200] batch [1/2] time 1.692 (1.692) data 0.558 (0.558) loss 0.5813 (0.5813) acc 93.7500 (93.7500) lr 4.7750e+00 eta 0:03:34\n",
      "epoch [137/200] batch [2/2] time 1.117 (1.404) data 0.001 (0.279) loss 0.9028 (0.7420) acc 90.6250 (92.1875) lr 4.6417e+00 eta 0:02:56\n",
      "epoch [138/200] batch [1/2] time 1.661 (1.661) data 0.526 (0.526) loss 0.4220 (0.4220) acc 100.0000 (100.0000) lr 4.6417e+00 eta 0:03:27\n",
      "epoch [138/200] batch [2/2] time 1.124 (1.392) data 0.001 (0.264) loss 0.5179 (0.4699) acc 93.7500 (96.8750) lr 4.5098e+00 eta 0:02:52\n",
      "epoch [139/200] batch [1/2] time 1.616 (1.616) data 0.497 (0.497) loss 0.7340 (0.7340) acc 93.7500 (93.7500) lr 4.5098e+00 eta 0:03:18\n",
      "epoch [139/200] batch [2/2] time 1.118 (1.367) data 0.001 (0.249) loss 0.6957 (0.7148) acc 93.7500 (93.7500) lr 4.3792e+00 eta 0:02:46\n",
      "epoch [140/200] batch [1/2] time 1.638 (1.638) data 0.495 (0.495) loss 0.8043 (0.8043) acc 87.5000 (87.5000) lr 4.3792e+00 eta 0:03:18\n",
      "epoch [140/200] batch [2/2] time 1.107 (1.372) data 0.001 (0.248) loss 0.4132 (0.6087) acc 96.8750 (92.1875) lr 4.2499e+00 eta 0:02:44\n",
      "epoch [141/200] batch [1/2] time 1.988 (1.988) data 0.783 (0.783) loss 0.5145 (0.5145) acc 90.6250 (90.6250) lr 4.2499e+00 eta 0:03:56\n",
      "epoch [141/200] batch [2/2] time 1.145 (1.567) data 0.001 (0.392) loss 0.3503 (0.4324) acc 100.0000 (95.3125) lr 4.1221e+00 eta 0:03:04\n",
      "epoch [142/200] batch [1/2] time 1.671 (1.671) data 0.567 (0.567) loss 0.6691 (0.6691) acc 93.7500 (93.7500) lr 4.1221e+00 eta 0:03:15\n",
      "epoch [142/200] batch [2/2] time 1.104 (1.387) data 0.001 (0.284) loss 0.4204 (0.5447) acc 96.8750 (95.3125) lr 3.9958e+00 eta 0:02:40\n",
      "epoch [143/200] batch [1/2] time 1.648 (1.648) data 0.525 (0.525) loss 0.4602 (0.4602) acc 96.8750 (96.8750) lr 3.9958e+00 eta 0:03:09\n",
      "epoch [143/200] batch [2/2] time 1.107 (1.377) data 0.001 (0.263) loss 0.4226 (0.4414) acc 96.8750 (96.8750) lr 3.8709e+00 eta 0:02:37\n",
      "epoch [144/200] batch [1/2] time 1.616 (1.616) data 0.520 (0.520) loss 0.3719 (0.3719) acc 100.0000 (100.0000) lr 3.8709e+00 eta 0:03:02\n",
      "epoch [144/200] batch [2/2] time 1.111 (1.364) data 0.001 (0.260) loss 0.4670 (0.4194) acc 96.8750 (98.4375) lr 3.7476e+00 eta 0:02:32\n",
      "epoch [145/200] batch [1/2] time 1.584 (1.584) data 0.478 (0.478) loss 0.4574 (0.4574) acc 96.8750 (96.8750) lr 3.7476e+00 eta 0:02:55\n",
      "epoch [145/200] batch [2/2] time 1.095 (1.340) data 0.001 (0.239) loss 0.3597 (0.4085) acc 96.8750 (96.8750) lr 3.6258e+00 eta 0:02:27\n",
      "epoch [146/200] batch [1/2] time 2.001 (2.001) data 0.779 (0.779) loss 0.3801 (0.3801) acc 96.8750 (96.8750) lr 3.6258e+00 eta 0:03:38\n",
      "epoch [146/200] batch [2/2] time 1.153 (1.577) data 0.001 (0.390) loss 0.3444 (0.3623) acc 100.0000 (98.4375) lr 3.5055e+00 eta 0:02:50\n",
      "epoch [147/200] batch [1/2] time 1.674 (1.674) data 0.552 (0.552) loss 0.3450 (0.3450) acc 100.0000 (100.0000) lr 3.5055e+00 eta 0:02:59\n",
      "epoch [147/200] batch [2/2] time 1.113 (1.394) data 0.000 (0.276) loss 0.6177 (0.4814) acc 90.6250 (95.3125) lr 3.3869e+00 eta 0:02:27\n",
      "epoch [148/200] batch [1/2] time 1.630 (1.630) data 0.478 (0.478) loss 0.3347 (0.3347) acc 100.0000 (100.0000) lr 3.3869e+00 eta 0:02:51\n",
      "epoch [148/200] batch [2/2] time 1.103 (1.367) data 0.001 (0.239) loss 0.5617 (0.4482) acc 87.5000 (93.7500) lr 3.2699e+00 eta 0:02:22\n",
      "epoch [149/200] batch [1/2] time 1.599 (1.599) data 0.480 (0.480) loss 0.3134 (0.3134) acc 100.0000 (100.0000) lr 3.2699e+00 eta 0:02:44\n",
      "epoch [149/200] batch [2/2] time 1.127 (1.363) data 0.001 (0.240) loss 0.4737 (0.3936) acc 93.7500 (96.8750) lr 3.1545e+00 eta 0:02:19\n",
      "epoch [150/200] batch [1/2] time 1.620 (1.620) data 0.504 (0.504) loss 0.3502 (0.3502) acc 96.8750 (96.8750) lr 3.1545e+00 eta 0:02:43\n",
      "epoch [150/200] batch [2/2] time 1.083 (1.351) data 0.001 (0.252) loss 0.3035 (0.3269) acc 100.0000 (98.4375) lr 3.0409e+00 eta 0:02:15\n",
      "epoch [151/200] batch [1/2] time 2.014 (2.014) data 0.808 (0.808) loss 0.5111 (0.5111) acc 87.5000 (87.5000) lr 3.0409e+00 eta 0:03:19\n",
      "epoch [151/200] batch [2/2] time 1.144 (1.579) data 0.001 (0.404) loss 0.2990 (0.4050) acc 100.0000 (93.7500) lr 2.9289e+00 eta 0:02:34\n",
      "epoch [152/200] batch [1/2] time 1.712 (1.712) data 0.569 (0.569) loss 0.3851 (0.3851) acc 90.6250 (90.6250) lr 2.9289e+00 eta 0:02:46\n",
      "epoch [152/200] batch [2/2] time 1.105 (1.409) data 0.001 (0.285) loss 0.3788 (0.3819) acc 96.8750 (93.7500) lr 2.8187e+00 eta 0:02:15\n",
      "epoch [153/200] batch [1/2] time 1.637 (1.637) data 0.500 (0.500) loss 0.3433 (0.3433) acc 96.8750 (96.8750) lr 2.8187e+00 eta 0:02:35\n",
      "epoch [153/200] batch [2/2] time 1.113 (1.375) data 0.001 (0.250) loss 0.3383 (0.3408) acc 100.0000 (98.4375) lr 2.7103e+00 eta 0:02:09\n",
      "epoch [154/200] batch [1/2] time 1.678 (1.678) data 0.557 (0.557) loss 0.6057 (0.6057) acc 93.7500 (93.7500) lr 2.7103e+00 eta 0:02:36\n",
      "epoch [154/200] batch [2/2] time 1.091 (1.384) data 0.001 (0.279) loss 0.3339 (0.4698) acc 96.8750 (95.3125) lr 2.6037e+00 eta 0:02:07\n",
      "epoch [155/200] batch [1/2] time 1.629 (1.629) data 0.487 (0.487) loss 0.3557 (0.3557) acc 96.8750 (96.8750) lr 2.6037e+00 eta 0:02:28\n",
      "epoch [155/200] batch [2/2] time 1.102 (1.365) data 0.001 (0.244) loss 0.4492 (0.4024) acc 93.7500 (95.3125) lr 2.4989e+00 eta 0:02:02\n",
      "epoch [156/200] batch [1/2] time 2.015 (2.015) data 0.813 (0.813) loss 0.6058 (0.6058) acc 90.6250 (90.6250) lr 2.4989e+00 eta 0:02:59\n",
      "epoch [156/200] batch [2/2] time 1.147 (1.581) data 0.001 (0.407) loss 0.4163 (0.5111) acc 93.7500 (92.1875) lr 2.3959e+00 eta 0:02:19\n",
      "epoch [157/200] batch [1/2] time 1.680 (1.680) data 0.579 (0.579) loss 0.6097 (0.6097) acc 93.7500 (93.7500) lr 2.3959e+00 eta 0:02:26\n",
      "epoch [157/200] batch [2/2] time 1.116 (1.398) data 0.000 (0.290) loss 0.8259 (0.7178) acc 93.7500 (93.7500) lr 2.2949e+00 eta 0:02:00\n",
      "epoch [158/200] batch [1/2] time 1.615 (1.615) data 0.488 (0.488) loss 0.3338 (0.3338) acc 100.0000 (100.0000) lr 2.2949e+00 eta 0:02:17\n",
      "epoch [158/200] batch [2/2] time 1.444 (1.530) data 0.001 (0.244) loss 0.6449 (0.4893) acc 93.7500 (96.8750) lr 2.1957e+00 eta 0:02:08\n",
      "epoch [159/200] batch [1/2] time 1.621 (1.621) data 0.489 (0.489) loss 0.4734 (0.4734) acc 96.8750 (96.8750) lr 2.1957e+00 eta 0:02:14\n",
      "epoch [159/200] batch [2/2] time 1.109 (1.365) data 0.001 (0.245) loss 0.4331 (0.4533) acc 96.8750 (96.8750) lr 2.0984e+00 eta 0:01:51\n",
      "epoch [160/200] batch [1/2] time 1.676 (1.676) data 0.500 (0.500) loss 0.4646 (0.4646) acc 93.7500 (93.7500) lr 2.0984e+00 eta 0:02:15\n",
      "epoch [160/200] batch [2/2] time 1.145 (1.410) data 0.001 (0.250) loss 0.3175 (0.3910) acc 100.0000 (96.8750) lr 2.0032e+00 eta 0:01:52\n",
      "epoch [161/200] batch [1/2] time 2.030 (2.030) data 0.803 (0.803) loss 0.4612 (0.4612) acc 93.7500 (93.7500) lr 2.0032e+00 eta 0:02:40\n",
      "epoch [161/200] batch [2/2] time 1.182 (1.606) data 0.001 (0.402) loss 0.2848 (0.3730) acc 100.0000 (96.8750) lr 1.9098e+00 eta 0:02:05\n",
      "epoch [162/200] batch [1/2] time 1.641 (1.641) data 0.549 (0.549) loss 0.2832 (0.2832) acc 100.0000 (100.0000) lr 1.9098e+00 eta 0:02:06\n",
      "epoch [162/200] batch [2/2] time 1.100 (1.371) data 0.000 (0.275) loss 0.5914 (0.4373) acc 93.7500 (96.8750) lr 1.8185e+00 eta 0:01:44\n",
      "epoch [163/200] batch [1/2] time 1.596 (1.596) data 0.476 (0.476) loss 0.3492 (0.3492) acc 96.8750 (96.8750) lr 1.8185e+00 eta 0:01:59\n",
      "epoch [163/200] batch [2/2] time 1.116 (1.356) data 0.001 (0.238) loss 0.3393 (0.3443) acc 96.8750 (96.8750) lr 1.7292e+00 eta 0:01:40\n",
      "epoch [164/200] batch [1/2] time 1.631 (1.631) data 0.508 (0.508) loss 0.3742 (0.3742) acc 93.7500 (93.7500) lr 1.7292e+00 eta 0:01:59\n",
      "epoch [164/200] batch [2/2] time 1.113 (1.372) data 0.001 (0.254) loss 0.4119 (0.3931) acc 93.7500 (93.7500) lr 1.6419e+00 eta 0:01:38\n",
      "epoch [165/200] batch [1/2] time 1.654 (1.654) data 0.500 (0.500) loss 0.2749 (0.2749) acc 100.0000 (100.0000) lr 1.6419e+00 eta 0:01:57\n",
      "epoch [165/200] batch [2/2] time 1.086 (1.370) data 0.001 (0.250) loss 0.5967 (0.4358) acc 90.6250 (95.3125) lr 1.5567e+00 eta 0:01:35\n",
      "epoch [166/200] batch [1/2] time 2.034 (2.034) data 0.841 (0.841) loss 0.3993 (0.3993) acc 96.8750 (96.8750) lr 1.5567e+00 eta 0:02:20\n",
      "epoch [166/200] batch [2/2] time 1.192 (1.613) data 0.001 (0.421) loss 0.5555 (0.4774) acc 90.6250 (93.7500) lr 1.4736e+00 eta 0:01:49\n",
      "epoch [167/200] batch [1/2] time 1.691 (1.691) data 0.559 (0.559) loss 0.5524 (0.5524) acc 93.7500 (93.7500) lr 1.4736e+00 eta 0:01:53\n",
      "epoch [167/200] batch [2/2] time 1.116 (1.403) data 0.001 (0.280) loss 0.3405 (0.4465) acc 96.8750 (95.3125) lr 1.3926e+00 eta 0:01:32\n",
      "epoch [168/200] batch [1/2] time 1.626 (1.626) data 0.497 (0.497) loss 0.2894 (0.2894) acc 96.8750 (96.8750) lr 1.3926e+00 eta 0:01:45\n",
      "epoch [168/200] batch [2/2] time 1.123 (1.375) data 0.001 (0.249) loss 0.2890 (0.2892) acc 100.0000 (98.4375) lr 1.3137e+00 eta 0:01:27\n",
      "epoch [169/200] batch [1/2] time 1.648 (1.648) data 0.506 (0.506) loss 0.5417 (0.5417) acc 90.6250 (90.6250) lr 1.3137e+00 eta 0:01:43\n",
      "epoch [169/200] batch [2/2] time 1.104 (1.376) data 0.001 (0.253) loss 0.4552 (0.4984) acc 90.6250 (90.6250) lr 1.2369e+00 eta 0:01:25\n",
      "epoch [170/200] batch [1/2] time 1.654 (1.654) data 0.497 (0.497) loss 0.3882 (0.3882) acc 96.8750 (96.8750) lr 1.2369e+00 eta 0:01:40\n",
      "epoch [170/200] batch [2/2] time 1.101 (1.377) data 0.000 (0.249) loss 0.3606 (0.3744) acc 96.8750 (96.8750) lr 1.1623e+00 eta 0:01:22\n",
      "epoch [171/200] batch [1/2] time 2.040 (2.040) data 0.795 (0.795) loss 0.2644 (0.2644) acc 100.0000 (100.0000) lr 1.1623e+00 eta 0:02:00\n",
      "epoch [171/200] batch [2/2] time 1.186 (1.613) data 0.001 (0.398) loss 0.4492 (0.3568) acc 90.6250 (95.3125) lr 1.0899e+00 eta 0:01:33\n",
      "epoch [172/200] batch [1/2] time 1.675 (1.675) data 0.562 (0.562) loss 0.3428 (0.3428) acc 96.8750 (96.8750) lr 1.0899e+00 eta 0:01:35\n",
      "epoch [172/200] batch [2/2] time 1.105 (1.390) data 0.001 (0.281) loss 0.4642 (0.4035) acc 96.8750 (96.8750) lr 1.0197e+00 eta 0:01:17\n",
      "epoch [173/200] batch [1/2] time 1.657 (1.657) data 0.517 (0.517) loss 0.2776 (0.2776) acc 96.8750 (96.8750) lr 1.0197e+00 eta 0:01:31\n",
      "epoch [173/200] batch [2/2] time 1.119 (1.388) data 0.001 (0.259) loss 0.2605 (0.2691) acc 100.0000 (98.4375) lr 9.5173e-01 eta 0:01:14\n",
      "epoch [174/200] batch [1/2] time 1.621 (1.621) data 0.504 (0.504) loss 0.2732 (0.2732) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:01:25\n",
      "epoch [174/200] batch [2/2] time 1.108 (1.365) data 0.001 (0.252) loss 0.2723 (0.2727) acc 100.0000 (100.0000) lr 8.8597e-01 eta 0:01:10\n",
      "epoch [175/200] batch [1/2] time 1.615 (1.615) data 0.477 (0.477) loss 0.2553 (0.2553) acc 100.0000 (100.0000) lr 8.8597e-01 eta 0:01:22\n",
      "epoch [175/200] batch [2/2] time 1.107 (1.361) data 0.001 (0.239) loss 0.3149 (0.2851) acc 100.0000 (100.0000) lr 8.2245e-01 eta 0:01:08\n",
      "epoch [176/200] batch [1/2] time 2.006 (2.006) data 0.800 (0.800) loss 0.2992 (0.2992) acc 96.8750 (96.8750) lr 8.2245e-01 eta 0:01:38\n",
      "epoch [176/200] batch [2/2] time 1.140 (1.573) data 0.005 (0.402) loss 0.2971 (0.2982) acc 96.8750 (96.8750) lr 7.6120e-01 eta 0:01:15\n",
      "epoch [177/200] batch [1/2] time 1.664 (1.664) data 0.549 (0.549) loss 0.3192 (0.3192) acc 96.8750 (96.8750) lr 7.6120e-01 eta 0:01:18\n",
      "epoch [177/200] batch [2/2] time 1.120 (1.392) data 0.000 (0.275) loss 0.2633 (0.2913) acc 100.0000 (98.4375) lr 7.0224e-01 eta 0:01:04\n",
      "epoch [178/200] batch [1/2] time 1.604 (1.604) data 0.481 (0.481) loss 0.3442 (0.3442) acc 96.8750 (96.8750) lr 7.0224e-01 eta 0:01:12\n",
      "epoch [178/200] batch [2/2] time 1.111 (1.357) data 0.001 (0.241) loss 0.2725 (0.3083) acc 100.0000 (98.4375) lr 6.4556e-01 eta 0:00:59\n",
      "epoch [179/200] batch [1/2] time 1.612 (1.612) data 0.496 (0.496) loss 0.3389 (0.3389) acc 96.8750 (96.8750) lr 6.4556e-01 eta 0:01:09\n",
      "epoch [179/200] batch [2/2] time 1.107 (1.360) data 0.000 (0.248) loss 0.4257 (0.3823) acc 93.7500 (95.3125) lr 5.9119e-01 eta 0:00:57\n",
      "epoch [180/200] batch [1/2] time 1.615 (1.615) data 0.495 (0.495) loss 0.2908 (0.2908) acc 96.8750 (96.8750) lr 5.9119e-01 eta 0:01:06\n",
      "epoch [180/200] batch [2/2] time 1.095 (1.355) data 0.001 (0.248) loss 0.3515 (0.3212) acc 96.8750 (96.8750) lr 5.3915e-01 eta 0:00:54\n",
      "epoch [181/200] batch [1/2] time 2.025 (2.025) data 0.842 (0.842) loss 0.2336 (0.2336) acc 100.0000 (100.0000) lr 5.3915e-01 eta 0:01:18\n",
      "epoch [181/200] batch [2/2] time 1.125 (1.575) data 0.001 (0.421) loss 0.2947 (0.2641) acc 96.8750 (98.4375) lr 4.8943e-01 eta 0:00:59\n",
      "epoch [182/200] batch [1/2] time 1.691 (1.691) data 0.611 (0.611) loss 0.2641 (0.2641) acc 100.0000 (100.0000) lr 4.8943e-01 eta 0:01:02\n",
      "epoch [182/200] batch [2/2] time 1.089 (1.390) data 0.001 (0.306) loss 0.3118 (0.2879) acc 96.8750 (98.4375) lr 4.4207e-01 eta 0:00:50\n",
      "epoch [183/200] batch [1/2] time 1.623 (1.623) data 0.505 (0.505) loss 0.3726 (0.3726) acc 96.8750 (96.8750) lr 4.4207e-01 eta 0:00:56\n",
      "epoch [183/200] batch [2/2] time 1.090 (1.356) data 0.001 (0.253) loss 0.3137 (0.3431) acc 96.8750 (96.8750) lr 3.9706e-01 eta 0:00:46\n",
      "epoch [184/200] batch [1/2] time 1.614 (1.614) data 0.492 (0.492) loss 0.2619 (0.2619) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:53\n",
      "epoch [184/200] batch [2/2] time 1.110 (1.362) data 0.001 (0.246) loss 0.3999 (0.3309) acc 93.7500 (96.8750) lr 3.5443e-01 eta 0:00:43\n",
      "epoch [185/200] batch [1/2] time 1.623 (1.623) data 0.488 (0.488) loss 0.3257 (0.3257) acc 96.8750 (96.8750) lr 3.5443e-01 eta 0:00:50\n",
      "epoch [185/200] batch [2/2] time 1.102 (1.363) data 0.001 (0.244) loss 0.2791 (0.3024) acc 96.8750 (96.8750) lr 3.1417e-01 eta 0:00:40\n",
      "epoch [186/200] batch [1/2] time 2.017 (2.017) data 0.789 (0.789) loss 0.2944 (0.2944) acc 96.8750 (96.8750) lr 3.1417e-01 eta 0:00:58\n",
      "epoch [186/200] batch [2/2] time 1.165 (1.591) data 0.001 (0.395) loss 0.2991 (0.2967) acc 96.8750 (96.8750) lr 2.7630e-01 eta 0:00:44\n",
      "epoch [187/200] batch [1/2] time 1.644 (1.644) data 0.531 (0.531) loss 0.3800 (0.3800) acc 90.6250 (90.6250) lr 2.7630e-01 eta 0:00:44\n",
      "epoch [187/200] batch [2/2] time 1.113 (1.379) data 0.001 (0.266) loss 0.2341 (0.3070) acc 100.0000 (95.3125) lr 2.4083e-01 eta 0:00:35\n",
      "epoch [188/200] batch [1/2] time 1.632 (1.632) data 0.498 (0.498) loss 0.2303 (0.2303) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:40\n",
      "epoch [188/200] batch [2/2] time 1.103 (1.367) data 0.001 (0.249) loss 0.2880 (0.2591) acc 96.8750 (98.4375) lr 2.0777e-01 eta 0:00:32\n",
      "epoch [189/200] batch [1/2] time 1.621 (1.621) data 0.475 (0.475) loss 0.2279 (0.2279) acc 100.0000 (100.0000) lr 2.0777e-01 eta 0:00:37\n",
      "epoch [189/200] batch [2/2] time 1.109 (1.365) data 0.001 (0.238) loss 0.2440 (0.2359) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:30\n",
      "epoch [190/200] batch [1/2] time 1.643 (1.643) data 0.488 (0.488) loss 0.4015 (0.4015) acc 96.8750 (96.8750) lr 1.7713e-01 eta 0:00:34\n",
      "epoch [190/200] batch [2/2] time 1.112 (1.378) data 0.001 (0.244) loss 0.2406 (0.3210) acc 100.0000 (98.4375) lr 1.4891e-01 eta 0:00:27\n",
      "epoch [191/200] batch [1/2] time 2.115 (2.115) data 0.845 (0.845) loss 0.3651 (0.3651) acc 96.8750 (96.8750) lr 1.4891e-01 eta 0:00:40\n",
      "epoch [191/200] batch [2/2] time 1.204 (1.659) data 0.001 (0.423) loss 0.3361 (0.3506) acc 93.7500 (95.3125) lr 1.2312e-01 eta 0:00:29\n",
      "epoch [192/200] batch [1/2] time 1.700 (1.700) data 0.576 (0.576) loss 0.3934 (0.3934) acc 96.8750 (96.8750) lr 1.2312e-01 eta 0:00:28\n",
      "epoch [192/200] batch [2/2] time 1.111 (1.405) data 0.001 (0.288) loss 0.2817 (0.3376) acc 100.0000 (98.4375) lr 9.9763e-02 eta 0:00:22\n",
      "epoch [193/200] batch [1/2] time 1.625 (1.625) data 0.505 (0.505) loss 0.2352 (0.2352) acc 100.0000 (100.0000) lr 9.9763e-02 eta 0:00:24\n",
      "epoch [193/200] batch [2/2] time 1.100 (1.363) data 0.001 (0.253) loss 0.3109 (0.2731) acc 96.8750 (98.4375) lr 7.8853e-02 eta 0:00:19\n",
      "epoch [194/200] batch [1/2] time 1.623 (1.623) data 0.509 (0.509) loss 0.2647 (0.2647) acc 96.8750 (96.8750) lr 7.8853e-02 eta 0:00:21\n",
      "epoch [194/200] batch [2/2] time 1.117 (1.370) data 0.001 (0.255) loss 0.4551 (0.3599) acc 87.5000 (92.1875) lr 6.0390e-02 eta 0:00:16\n",
      "epoch [195/200] batch [1/2] time 1.645 (1.645) data 0.491 (0.491) loss 0.4559 (0.4559) acc 93.7500 (93.7500) lr 6.0390e-02 eta 0:00:18\n",
      "epoch [195/200] batch [2/2] time 1.104 (1.375) data 0.001 (0.246) loss 0.2362 (0.3461) acc 100.0000 (96.8750) lr 4.4380e-02 eta 0:00:13\n",
      "epoch [196/200] batch [1/2] time 2.025 (2.025) data 0.818 (0.818) loss 0.2391 (0.2391) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:18\n",
      "epoch [196/200] batch [2/2] time 1.186 (1.605) data 0.001 (0.409) loss 0.2865 (0.2628) acc 96.8750 (98.4375) lr 3.0827e-02 eta 0:00:12\n",
      "epoch [197/200] batch [1/2] time 1.674 (1.674) data 0.534 (0.534) loss 0.3959 (0.3959) acc 93.7500 (93.7500) lr 3.0827e-02 eta 0:00:11\n",
      "epoch [197/200] batch [2/2] time 1.132 (1.403) data 0.000 (0.267) loss 0.2673 (0.3316) acc 96.8750 (95.3125) lr 1.9733e-02 eta 0:00:08\n",
      "epoch [198/200] batch [1/2] time 1.633 (1.633) data 0.499 (0.499) loss 0.3182 (0.3182) acc 96.8750 (96.8750) lr 1.9733e-02 eta 0:00:08\n",
      "epoch [198/200] batch [2/2] time 1.131 (1.382) data 0.001 (0.250) loss 0.2699 (0.2941) acc 96.8750 (96.8750) lr 1.1101e-02 eta 0:00:05\n",
      "epoch [199/200] batch [1/2] time 1.674 (1.674) data 0.505 (0.505) loss 0.3341 (0.3341) acc 93.7500 (93.7500) lr 1.1101e-02 eta 0:00:05\n",
      "epoch [199/200] batch [2/2] time 1.134 (1.404) data 0.000 (0.253) loss 0.2471 (0.2906) acc 100.0000 (96.8750) lr 4.9344e-03 eta 0:00:02\n",
      "epoch [200/200] batch [1/2] time 1.664 (1.664) data 0.499 (0.499) loss 0.2982 (0.2982) acc 96.8750 (96.8750) lr 4.9344e-03 eta 0:00:01\n",
      "epoch [200/200] batch [2/2] time 1.142 (1.403) data 0.001 (0.250) loss 0.2676 (0.2829) acc 96.8750 (96.8750) lr 1.2337e-03 eta 0:00:00\n",
      "Checkpoint saved to output/new_init/eurosat/DAPT/vit_b16_8shots/seed1/prompt_learner/model.pth.tar-200\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:57<00:00,  1.40it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 7,048\n",
      "* accuracy: 87.0%\n",
      "* error: 13.0%\n",
      "* macro_f1: 86.8%\n",
      "Elapsed: 0:11:08\n"
     ]
    }
   ],
   "source": [
    "#eurosat-8shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "        --output-dir output/new_init/eurosat/DAPT/vit_b16_8shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_F3fgkyhmFK",
    "outputId": "0aa87778-00dc-4109-d29e-4b8dfaaae3cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:15:43.524821: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:15:43.543964: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:15:43.549733: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:15:43.563561: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:15:44.598486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '4']\n",
      "output_dir: output/new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 4\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  40\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.vis_ctx', 'prompt_learner.txt_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1/tensorboard)\n",
      "epoch [1/100] batch [1/1] time 2.877 (2.877) data 0.503 (0.503) loss 11.6797 (11.6797) acc 12.5000 (12.5000) lr 2.0000e+01 eta 0:04:44\n",
      "epoch [2/100] batch [1/1] time 1.573 (1.573) data 0.428 (0.428) loss 11.5801 (11.5801) acc 12.5000 (12.5000) lr 1.9995e+01 eta 0:02:34\n",
      "epoch [3/100] batch [1/1] time 1.568 (1.568) data 0.434 (0.434) loss 10.9084 (10.9084) acc 0.0000 (0.0000) lr 1.9980e+01 eta 0:02:32\n",
      "epoch [4/100] batch [1/1] time 1.537 (1.537) data 0.406 (0.406) loss 10.5071 (10.5071) acc 0.0000 (0.0000) lr 1.9956e+01 eta 0:02:27\n",
      "epoch [5/100] batch [1/1] time 1.722 (1.722) data 0.603 (0.603) loss 10.4532 (10.4532) acc 0.0000 (0.0000) lr 1.9921e+01 eta 0:02:43\n",
      "epoch [6/100] batch [1/1] time 1.711 (1.711) data 0.557 (0.557) loss 10.2721 (10.2721) acc 6.2500 (6.2500) lr 1.9877e+01 eta 0:02:40\n",
      "epoch [7/100] batch [1/1] time 1.564 (1.564) data 0.428 (0.428) loss 10.1523 (10.1523) acc 3.1250 (3.1250) lr 1.9823e+01 eta 0:02:25\n",
      "epoch [8/100] batch [1/1] time 1.542 (1.542) data 0.428 (0.428) loss 10.0359 (10.0359) acc 9.3750 (9.3750) lr 1.9759e+01 eta 0:02:21\n",
      "epoch [9/100] batch [1/1] time 1.577 (1.577) data 0.434 (0.434) loss 10.0824 (10.0824) acc 0.0000 (0.0000) lr 1.9686e+01 eta 0:02:23\n",
      "epoch [10/100] batch [1/1] time 1.575 (1.575) data 0.431 (0.431) loss 9.9192 (9.9192) acc 9.3750 (9.3750) lr 1.9603e+01 eta 0:02:21\n",
      "epoch [11/100] batch [1/1] time 1.561 (1.561) data 0.419 (0.419) loss 9.9056 (9.9056) acc 15.6250 (15.6250) lr 1.9511e+01 eta 0:02:18\n",
      "epoch [12/100] batch [1/1] time 1.740 (1.740) data 0.626 (0.626) loss 9.8689 (9.8689) acc 6.2500 (6.2500) lr 1.9409e+01 eta 0:02:33\n",
      "epoch [13/100] batch [1/1] time 1.791 (1.791) data 0.586 (0.586) loss 9.7838 (9.7838) acc 9.3750 (9.3750) lr 1.9298e+01 eta 0:02:35\n",
      "epoch [14/100] batch [1/1] time 1.593 (1.593) data 0.476 (0.476) loss 9.7318 (9.7318) acc 25.0000 (25.0000) lr 1.9178e+01 eta 0:02:16\n",
      "epoch [15/100] batch [1/1] time 1.535 (1.535) data 0.414 (0.414) loss 9.6536 (9.6536) acc 12.5000 (12.5000) lr 1.9048e+01 eta 0:02:10\n",
      "epoch [16/100] batch [1/1] time 1.562 (1.562) data 0.433 (0.433) loss 9.5978 (9.5978) acc 21.8750 (21.8750) lr 1.8910e+01 eta 0:02:11\n",
      "epoch [17/100] batch [1/1] time 1.561 (1.561) data 0.405 (0.405) loss 9.5219 (9.5219) acc 21.8750 (21.8750) lr 1.8763e+01 eta 0:02:09\n",
      "epoch [18/100] batch [1/1] time 1.586 (1.586) data 0.434 (0.434) loss 9.4298 (9.4298) acc 15.6250 (15.6250) lr 1.8607e+01 eta 0:02:10\n",
      "epoch [19/100] batch [1/1] time 1.589 (1.589) data 0.434 (0.434) loss 9.3207 (9.3207) acc 25.0000 (25.0000) lr 1.8443e+01 eta 0:02:08\n",
      "epoch [20/100] batch [1/1] time 1.686 (1.686) data 0.568 (0.568) loss 9.2361 (9.2361) acc 21.8750 (21.8750) lr 1.8271e+01 eta 0:02:14\n",
      "epoch [21/100] batch [1/1] time 1.747 (1.747) data 0.546 (0.546) loss 9.1614 (9.1614) acc 21.8750 (21.8750) lr 1.8090e+01 eta 0:02:18\n",
      "epoch [22/100] batch [1/1] time 1.623 (1.623) data 0.472 (0.472) loss 8.9822 (8.9822) acc 34.3750 (34.3750) lr 1.7902e+01 eta 0:02:06\n",
      "epoch [23/100] batch [1/1] time 1.543 (1.543) data 0.405 (0.405) loss 8.8698 (8.8698) acc 31.2500 (31.2500) lr 1.7705e+01 eta 0:01:58\n",
      "epoch [24/100] batch [1/1] time 1.584 (1.584) data 0.438 (0.438) loss 8.6408 (8.6408) acc 25.0000 (25.0000) lr 1.7501e+01 eta 0:02:00\n",
      "epoch [25/100] batch [1/1] time 1.568 (1.568) data 0.448 (0.448) loss 8.4933 (8.4933) acc 25.0000 (25.0000) lr 1.7290e+01 eta 0:01:57\n",
      "epoch [26/100] batch [1/1] time 1.546 (1.546) data 0.421 (0.421) loss 8.2077 (8.2077) acc 28.1250 (28.1250) lr 1.7071e+01 eta 0:01:54\n",
      "epoch [27/100] batch [1/1] time 1.577 (1.577) data 0.420 (0.420) loss 7.9210 (7.9210) acc 25.0000 (25.0000) lr 1.6845e+01 eta 0:01:55\n",
      "epoch [28/100] batch [1/1] time 1.688 (1.688) data 0.563 (0.563) loss 7.6661 (7.6661) acc 25.0000 (25.0000) lr 1.6613e+01 eta 0:02:01\n",
      "epoch [29/100] batch [1/1] time 1.747 (1.747) data 0.561 (0.561) loss 7.0600 (7.0600) acc 37.5000 (37.5000) lr 1.6374e+01 eta 0:02:04\n",
      "epoch [30/100] batch [1/1] time 1.623 (1.623) data 0.469 (0.469) loss 6.6273 (6.6273) acc 53.1250 (53.1250) lr 1.6129e+01 eta 0:01:53\n",
      "epoch [31/100] batch [1/1] time 1.578 (1.578) data 0.417 (0.417) loss 6.1726 (6.1726) acc 46.8750 (46.8750) lr 1.5878e+01 eta 0:01:48\n",
      "epoch [32/100] batch [1/1] time 1.584 (1.584) data 0.438 (0.438) loss 7.2125 (7.2125) acc 15.6250 (15.6250) lr 1.5621e+01 eta 0:01:47\n",
      "epoch [33/100] batch [1/1] time 1.543 (1.543) data 0.410 (0.410) loss 5.8335 (5.8335) acc 28.1250 (28.1250) lr 1.5358e+01 eta 0:01:43\n",
      "epoch [34/100] batch [1/1] time 1.616 (1.616) data 0.464 (0.464) loss 5.5364 (5.5364) acc 15.6250 (15.6250) lr 1.5090e+01 eta 0:01:46\n",
      "epoch [35/100] batch [1/1] time 1.608 (1.608) data 0.442 (0.442) loss 5.7750 (5.7750) acc 3.1250 (3.1250) lr 1.4818e+01 eta 0:01:44\n",
      "epoch [36/100] batch [1/1] time 1.769 (1.769) data 0.607 (0.607) loss 4.7931 (4.7931) acc 9.3750 (9.3750) lr 1.4540e+01 eta 0:01:53\n",
      "epoch [37/100] batch [1/1] time 1.896 (1.896) data 0.616 (0.616) loss 4.4794 (4.4794) acc 15.6250 (15.6250) lr 1.4258e+01 eta 0:01:59\n",
      "epoch [38/100] batch [1/1] time 1.670 (1.670) data 0.524 (0.524) loss 4.0594 (4.0594) acc 15.6250 (15.6250) lr 1.3971e+01 eta 0:01:43\n",
      "epoch [39/100] batch [1/1] time 1.588 (1.588) data 0.433 (0.433) loss 3.7481 (3.7481) acc 25.0000 (25.0000) lr 1.3681e+01 eta 0:01:36\n",
      "epoch [40/100] batch [1/1] time 1.591 (1.591) data 0.455 (0.455) loss 3.6969 (3.6969) acc 34.3750 (34.3750) lr 1.3387e+01 eta 0:01:35\n",
      "epoch [41/100] batch [1/1] time 1.570 (1.570) data 0.414 (0.414) loss 3.5299 (3.5299) acc 28.1250 (28.1250) lr 1.3090e+01 eta 0:01:32\n",
      "epoch [42/100] batch [1/1] time 1.562 (1.562) data 0.439 (0.439) loss 3.8048 (3.8048) acc 18.7500 (18.7500) lr 1.2790e+01 eta 0:01:30\n",
      "epoch [43/100] batch [1/1] time 1.589 (1.589) data 0.418 (0.418) loss 7.8647 (7.8647) acc 12.5000 (12.5000) lr 1.2487e+01 eta 0:01:30\n",
      "epoch [44/100] batch [1/1] time 1.720 (1.720) data 0.596 (0.596) loss 11.6304 (11.6304) acc 9.3750 (9.3750) lr 1.2181e+01 eta 0:01:36\n",
      "epoch [45/100] batch [1/1] time 1.815 (1.815) data 0.619 (0.619) loss 8.3375 (8.3375) acc 6.2500 (6.2500) lr 1.1874e+01 eta 0:01:39\n",
      "epoch [46/100] batch [1/1] time 1.648 (1.648) data 0.507 (0.507) loss 7.1776 (7.1776) acc 12.5000 (12.5000) lr 1.1564e+01 eta 0:01:28\n",
      "epoch [47/100] batch [1/1] time 1.576 (1.576) data 0.432 (0.432) loss 5.1336 (5.1336) acc 15.6250 (15.6250) lr 1.1253e+01 eta 0:01:23\n",
      "epoch [48/100] batch [1/1] time 1.561 (1.561) data 0.427 (0.427) loss 4.3263 (4.3263) acc 3.1250 (3.1250) lr 1.0941e+01 eta 0:01:21\n",
      "epoch [49/100] batch [1/1] time 1.550 (1.550) data 0.415 (0.415) loss 3.7455 (3.7455) acc 6.2500 (6.2500) lr 1.0628e+01 eta 0:01:19\n",
      "epoch [50/100] batch [1/1] time 1.607 (1.607) data 0.452 (0.452) loss 3.5203 (3.5203) acc 9.3750 (9.3750) lr 1.0314e+01 eta 0:01:20\n",
      "epoch [51/100] batch [1/1] time 1.576 (1.576) data 0.418 (0.418) loss 3.3231 (3.3231) acc 21.8750 (21.8750) lr 1.0000e+01 eta 0:01:17\n",
      "epoch [52/100] batch [1/1] time 1.741 (1.741) data 0.581 (0.581) loss 3.1821 (3.1821) acc 25.0000 (25.0000) lr 9.6859e+00 eta 0:01:23\n",
      "epoch [53/100] batch [1/1] time 1.791 (1.791) data 0.594 (0.594) loss 3.1583 (3.1583) acc 25.0000 (25.0000) lr 9.3721e+00 eta 0:01:24\n",
      "epoch [54/100] batch [1/1] time 1.639 (1.639) data 0.499 (0.499) loss 3.0088 (3.0088) acc 25.0000 (25.0000) lr 9.0589e+00 eta 0:01:15\n",
      "epoch [55/100] batch [1/1] time 1.578 (1.578) data 0.423 (0.423) loss 2.9744 (2.9744) acc 21.8750 (21.8750) lr 8.7467e+00 eta 0:01:11\n",
      "epoch [56/100] batch [1/1] time 1.562 (1.562) data 0.430 (0.430) loss 2.8839 (2.8839) acc 31.2500 (31.2500) lr 8.4357e+00 eta 0:01:08\n",
      "epoch [57/100] batch [1/1] time 1.568 (1.568) data 0.445 (0.445) loss 2.6918 (2.6918) acc 34.3750 (34.3750) lr 8.1262e+00 eta 0:01:07\n",
      "epoch [58/100] batch [1/1] time 1.537 (1.537) data 0.401 (0.401) loss 2.5999 (2.5999) acc 28.1250 (28.1250) lr 7.8186e+00 eta 0:01:04\n",
      "epoch [59/100] batch [1/1] time 1.556 (1.556) data 0.436 (0.436) loss 2.6996 (2.6996) acc 40.6250 (40.6250) lr 7.5131e+00 eta 0:01:03\n",
      "epoch [60/100] batch [1/1] time 1.720 (1.720) data 0.580 (0.580) loss 2.5556 (2.5556) acc 46.8750 (46.8750) lr 7.2101e+00 eta 0:01:08\n",
      "epoch [61/100] batch [1/1] time 2.262 (2.262) data 0.610 (0.610) loss 2.3521 (2.3521) acc 53.1250 (53.1250) lr 6.9098e+00 eta 0:01:28\n",
      "epoch [62/100] batch [1/1] time 1.621 (1.621) data 0.486 (0.486) loss 2.2678 (2.2678) acc 43.7500 (43.7500) lr 6.6126e+00 eta 0:01:01\n",
      "epoch [63/100] batch [1/1] time 1.607 (1.607) data 0.441 (0.441) loss 2.2391 (2.2391) acc 37.5000 (37.5000) lr 6.3188e+00 eta 0:00:59\n",
      "epoch [64/100] batch [1/1] time 1.561 (1.561) data 0.420 (0.420) loss 1.9838 (1.9838) acc 56.2500 (56.2500) lr 6.0285e+00 eta 0:00:56\n",
      "epoch [65/100] batch [1/1] time 1.568 (1.568) data 0.427 (0.427) loss 1.9068 (1.9068) acc 62.5000 (62.5000) lr 5.7422e+00 eta 0:00:54\n",
      "epoch [66/100] batch [1/1] time 1.591 (1.591) data 0.448 (0.448) loss 2.0243 (2.0243) acc 37.5000 (37.5000) lr 5.4601e+00 eta 0:00:54\n",
      "epoch [67/100] batch [1/1] time 1.647 (1.647) data 0.485 (0.485) loss 1.9877 (1.9877) acc 56.2500 (56.2500) lr 5.1825e+00 eta 0:00:54\n",
      "epoch [68/100] batch [1/1] time 1.744 (1.744) data 0.583 (0.583) loss 1.6677 (1.6677) acc 68.7500 (68.7500) lr 4.9096e+00 eta 0:00:55\n",
      "epoch [69/100] batch [1/1] time 1.801 (1.801) data 0.606 (0.606) loss 1.7307 (1.7307) acc 59.3750 (59.3750) lr 4.6417e+00 eta 0:00:55\n",
      "epoch [70/100] batch [1/1] time 1.614 (1.614) data 0.477 (0.477) loss 1.7425 (1.7425) acc 56.2500 (56.2500) lr 4.3792e+00 eta 0:00:48\n",
      "epoch [71/100] batch [1/1] time 1.595 (1.595) data 0.452 (0.452) loss 1.5625 (1.5625) acc 65.6250 (65.6250) lr 4.1221e+00 eta 0:00:46\n",
      "epoch [72/100] batch [1/1] time 1.574 (1.574) data 0.421 (0.421) loss 1.5617 (1.5617) acc 65.6250 (65.6250) lr 3.8709e+00 eta 0:00:44\n",
      "epoch [73/100] batch [1/1] time 1.584 (1.584) data 0.438 (0.438) loss 1.6433 (1.6433) acc 50.0000 (50.0000) lr 3.6258e+00 eta 0:00:42\n",
      "epoch [74/100] batch [1/1] time 1.582 (1.582) data 0.442 (0.442) loss 1.5187 (1.5187) acc 68.7500 (68.7500) lr 3.3869e+00 eta 0:00:41\n",
      "epoch [75/100] batch [1/1] time 1.623 (1.623) data 0.457 (0.457) loss 1.5118 (1.5118) acc 68.7500 (68.7500) lr 3.1545e+00 eta 0:00:40\n",
      "epoch [76/100] batch [1/1] time 1.718 (1.718) data 0.585 (0.585) loss 1.4515 (1.4515) acc 71.8750 (71.8750) lr 2.9289e+00 eta 0:00:41\n",
      "epoch [77/100] batch [1/1] time 1.812 (1.812) data 0.607 (0.607) loss 1.2553 (1.2553) acc 75.0000 (75.0000) lr 2.7103e+00 eta 0:00:41\n",
      "epoch [78/100] batch [1/1] time 1.598 (1.598) data 0.455 (0.455) loss 1.1250 (1.1250) acc 93.7500 (93.7500) lr 2.4989e+00 eta 0:00:35\n",
      "epoch [79/100] batch [1/1] time 1.580 (1.580) data 0.438 (0.438) loss 1.3703 (1.3703) acc 68.7500 (68.7500) lr 2.2949e+00 eta 0:00:33\n",
      "epoch [80/100] batch [1/1] time 1.573 (1.573) data 0.432 (0.432) loss 1.2912 (1.2912) acc 81.2500 (81.2500) lr 2.0984e+00 eta 0:00:31\n",
      "epoch [81/100] batch [1/1] time 1.555 (1.555) data 0.426 (0.426) loss 1.0616 (1.0616) acc 81.2500 (81.2500) lr 1.9098e+00 eta 0:00:29\n",
      "epoch [82/100] batch [1/1] time 1.548 (1.548) data 0.419 (0.419) loss 1.0615 (1.0615) acc 84.3750 (84.3750) lr 1.7292e+00 eta 0:00:27\n",
      "epoch [83/100] batch [1/1] time 1.526 (1.526) data 0.410 (0.410) loss 1.0924 (1.0924) acc 84.3750 (84.3750) lr 1.5567e+00 eta 0:00:25\n",
      "epoch [84/100] batch [1/1] time 1.697 (1.697) data 0.567 (0.567) loss 1.0866 (1.0866) acc 87.5000 (87.5000) lr 1.3926e+00 eta 0:00:27\n",
      "epoch [85/100] batch [1/1] time 1.781 (1.781) data 0.560 (0.560) loss 0.9974 (0.9974) acc 87.5000 (87.5000) lr 1.2369e+00 eta 0:00:26\n",
      "epoch [86/100] batch [1/1] time 1.652 (1.652) data 0.507 (0.507) loss 1.0598 (1.0598) acc 90.6250 (90.6250) lr 1.0899e+00 eta 0:00:23\n",
      "epoch [87/100] batch [1/1] time 1.590 (1.590) data 0.446 (0.446) loss 0.9854 (0.9854) acc 90.6250 (90.6250) lr 9.5173e-01 eta 0:00:20\n",
      "epoch [88/100] batch [1/1] time 1.570 (1.570) data 0.437 (0.437) loss 0.9810 (0.9810) acc 87.5000 (87.5000) lr 8.2245e-01 eta 0:00:18\n",
      "epoch [89/100] batch [1/1] time 1.563 (1.563) data 0.418 (0.418) loss 0.9854 (0.9854) acc 87.5000 (87.5000) lr 7.0224e-01 eta 0:00:17\n",
      "epoch [90/100] batch [1/1] time 1.566 (1.566) data 0.437 (0.437) loss 0.9119 (0.9119) acc 90.6250 (90.6250) lr 5.9119e-01 eta 0:00:15\n",
      "epoch [91/100] batch [1/1] time 1.575 (1.575) data 0.413 (0.413) loss 0.9005 (0.9005) acc 87.5000 (87.5000) lr 4.8943e-01 eta 0:00:14\n",
      "epoch [92/100] batch [1/1] time 1.700 (1.700) data 0.571 (0.571) loss 0.8247 (0.8247) acc 93.7500 (93.7500) lr 3.9706e-01 eta 0:00:13\n",
      "epoch [93/100] batch [1/1] time 1.809 (1.809) data 0.603 (0.603) loss 0.8903 (0.8903) acc 90.6250 (90.6250) lr 3.1417e-01 eta 0:00:12\n",
      "epoch [94/100] batch [1/1] time 1.636 (1.636) data 0.473 (0.473) loss 0.8080 (0.8080) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:09\n",
      "epoch [95/100] batch [1/1] time 1.595 (1.595) data 0.451 (0.451) loss 0.8975 (0.8975) acc 96.8750 (96.8750) lr 1.7713e-01 eta 0:00:07\n",
      "epoch [96/100] batch [1/1] time 1.583 (1.583) data 0.439 (0.439) loss 0.9040 (0.9040) acc 96.8750 (96.8750) lr 1.2312e-01 eta 0:00:06\n",
      "epoch [97/100] batch [1/1] time 1.570 (1.570) data 0.412 (0.412) loss 0.7983 (0.7983) acc 96.8750 (96.8750) lr 7.8853e-02 eta 0:00:04\n",
      "epoch [98/100] batch [1/1] time 1.594 (1.594) data 0.445 (0.445) loss 0.8450 (0.8450) acc 93.7500 (93.7500) lr 4.4380e-02 eta 0:00:03\n",
      "epoch [99/100] batch [1/1] time 1.560 (1.560) data 0.408 (0.408) loss 0.8889 (0.8889) acc 93.7500 (93.7500) lr 1.9733e-02 eta 0:00:01\n",
      "epoch [100/100] batch [1/1] time 1.743 (1.743) data 0.584 (0.584) loss 0.9525 (0.9525) acc 90.6250 (90.6250) lr 4.9344e-03 eta 0:00:00\n",
      "Checkpoint saved to output/new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1/prompt_learner/model.pth.tar-100\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:56<00:00,  1.44it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 4,781\n",
      "* accuracy: 59.0%\n",
      "* error: 41.0%\n",
      "* macro_f1: 59.1%\n",
      "Elapsed: 0:04:00\n"
     ]
    }
   ],
   "source": [
    "#eurosat-4shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFzQEbVXhuSO",
    "outputId": "60b876ad-8364-4b1c-d19c-2998c5d77971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:20:04.344358: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:20:04.379687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:20:04.389247: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:20:04.412620: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:20:05.683500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '2']\n",
      "output_dir: output/new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 2\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 100\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  20\n",
      "# val      20\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.txt_ctx', 'prompt_learner.vis_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1/tensorboard)\n",
      "epoch [1/100] batch [1/1] time 2.566 (2.566) data 0.422 (0.422) loss 11.5547 (11.5547) acc 10.0000 (10.0000) lr 2.0000e+01 eta 0:04:14\n",
      "epoch [2/100] batch [1/1] time 1.351 (1.351) data 0.333 (0.333) loss 11.5602 (11.5602) acc 10.0000 (10.0000) lr 1.9995e+01 eta 0:02:12\n",
      "epoch [3/100] batch [1/1] time 1.376 (1.376) data 0.341 (0.341) loss 10.5728 (10.5728) acc 15.0000 (15.0000) lr 1.9980e+01 eta 0:02:13\n",
      "epoch [4/100] batch [1/1] time 1.380 (1.380) data 0.363 (0.363) loss 10.3400 (10.3400) acc 5.0000 (5.0000) lr 1.9956e+01 eta 0:02:12\n",
      "epoch [5/100] batch [1/1] time 1.379 (1.379) data 0.337 (0.337) loss 10.1112 (10.1112) acc 0.0000 (0.0000) lr 1.9921e+01 eta 0:02:10\n",
      "epoch [6/100] batch [1/1] time 1.470 (1.470) data 0.446 (0.446) loss 10.0639 (10.0639) acc 10.0000 (10.0000) lr 1.9877e+01 eta 0:02:18\n",
      "epoch [7/100] batch [1/1] time 1.564 (1.564) data 0.469 (0.469) loss 9.9343 (9.9343) acc 10.0000 (10.0000) lr 1.9823e+01 eta 0:02:25\n",
      "epoch [8/100] batch [1/1] time 1.410 (1.410) data 0.409 (0.409) loss 9.9113 (9.9113) acc 10.0000 (10.0000) lr 1.9759e+01 eta 0:02:09\n",
      "epoch [9/100] batch [1/1] time 1.350 (1.350) data 0.329 (0.329) loss 9.6439 (9.6439) acc 35.0000 (35.0000) lr 1.9686e+01 eta 0:02:02\n",
      "epoch [10/100] batch [1/1] time 1.356 (1.356) data 0.333 (0.333) loss 9.6474 (9.6474) acc 15.0000 (15.0000) lr 1.9603e+01 eta 0:02:02\n",
      "epoch [11/100] batch [1/1] time 1.357 (1.357) data 0.333 (0.333) loss 9.4981 (9.4981) acc 30.0000 (30.0000) lr 1.9511e+01 eta 0:02:00\n",
      "epoch [12/100] batch [1/1] time 1.357 (1.357) data 0.343 (0.343) loss 9.4327 (9.4327) acc 25.0000 (25.0000) lr 1.9409e+01 eta 0:01:59\n",
      "epoch [13/100] batch [1/1] time 1.370 (1.370) data 0.335 (0.335) loss 9.3152 (9.3152) acc 35.0000 (35.0000) lr 1.9298e+01 eta 0:01:59\n",
      "epoch [14/100] batch [1/1] time 1.414 (1.414) data 0.338 (0.338) loss 9.1806 (9.1806) acc 40.0000 (40.0000) lr 1.9178e+01 eta 0:02:01\n",
      "epoch [15/100] batch [1/1] time 1.492 (1.492) data 0.454 (0.454) loss 9.1507 (9.1507) acc 30.0000 (30.0000) lr 1.9048e+01 eta 0:02:06\n",
      "epoch [16/100] batch [1/1] time 1.622 (1.622) data 0.511 (0.511) loss 8.9000 (8.9000) acc 40.0000 (40.0000) lr 1.8910e+01 eta 0:02:16\n",
      "epoch [17/100] batch [1/1] time 1.417 (1.417) data 0.402 (0.402) loss 9.0837 (9.0837) acc 10.0000 (10.0000) lr 1.8763e+01 eta 0:01:57\n",
      "epoch [18/100] batch [1/1] time 1.371 (1.371) data 0.347 (0.347) loss 9.6842 (9.6842) acc 15.0000 (15.0000) lr 1.8607e+01 eta 0:01:52\n",
      "epoch [19/100] batch [1/1] time 1.359 (1.359) data 0.330 (0.330) loss 9.4364 (9.4364) acc 25.0000 (25.0000) lr 1.8443e+01 eta 0:01:50\n",
      "epoch [20/100] batch [1/1] time 1.361 (1.361) data 0.328 (0.328) loss 9.1056 (9.1056) acc 10.0000 (10.0000) lr 1.8271e+01 eta 0:01:48\n",
      "epoch [21/100] batch [1/1] time 1.397 (1.397) data 0.359 (0.359) loss 8.9469 (8.9469) acc 20.0000 (20.0000) lr 1.8090e+01 eta 0:01:50\n",
      "epoch [22/100] batch [1/1] time 1.372 (1.372) data 0.341 (0.341) loss 8.7529 (8.7529) acc 15.0000 (15.0000) lr 1.7902e+01 eta 0:01:47\n",
      "epoch [23/100] batch [1/1] time 1.429 (1.429) data 0.365 (0.365) loss 8.4711 (8.4711) acc 25.0000 (25.0000) lr 1.7705e+01 eta 0:01:50\n",
      "epoch [24/100] batch [1/1] time 1.506 (1.506) data 0.467 (0.467) loss 8.2070 (8.2070) acc 25.0000 (25.0000) lr 1.7501e+01 eta 0:01:54\n",
      "epoch [25/100] batch [1/1] time 1.557 (1.557) data 0.481 (0.481) loss 8.0039 (8.0039) acc 25.0000 (25.0000) lr 1.7290e+01 eta 0:01:56\n",
      "epoch [26/100] batch [1/1] time 1.431 (1.431) data 0.407 (0.407) loss 7.6991 (7.6991) acc 20.0000 (20.0000) lr 1.7071e+01 eta 0:01:45\n",
      "epoch [27/100] batch [1/1] time 1.360 (1.360) data 0.340 (0.340) loss 7.2237 (7.2237) acc 30.0000 (30.0000) lr 1.6845e+01 eta 0:01:39\n",
      "epoch [28/100] batch [1/1] time 1.369 (1.369) data 0.333 (0.333) loss 6.8767 (6.8767) acc 40.0000 (40.0000) lr 1.6613e+01 eta 0:01:38\n",
      "epoch [29/100] batch [1/1] time 1.386 (1.386) data 0.353 (0.353) loss 6.4881 (6.4881) acc 55.0000 (55.0000) lr 1.6374e+01 eta 0:01:38\n",
      "epoch [30/100] batch [1/1] time 1.367 (1.367) data 0.327 (0.327) loss 7.5537 (7.5537) acc 15.0000 (15.0000) lr 1.6129e+01 eta 0:01:35\n",
      "epoch [31/100] batch [1/1] time 1.365 (1.365) data 0.353 (0.353) loss 6.9984 (6.9984) acc 15.0000 (15.0000) lr 1.5878e+01 eta 0:01:34\n",
      "epoch [32/100] batch [1/1] time 1.398 (1.398) data 0.337 (0.337) loss 5.9824 (5.9824) acc 25.0000 (25.0000) lr 1.5621e+01 eta 0:01:35\n",
      "epoch [33/100] batch [1/1] time 1.498 (1.498) data 0.475 (0.475) loss 7.0799 (7.0799) acc 10.0000 (10.0000) lr 1.5358e+01 eta 0:01:40\n",
      "epoch [34/100] batch [1/1] time 1.586 (1.586) data 0.510 (0.510) loss 6.8214 (6.8214) acc 20.0000 (20.0000) lr 1.5090e+01 eta 0:01:44\n",
      "epoch [35/100] batch [1/1] time 1.450 (1.450) data 0.396 (0.396) loss 5.7742 (5.7742) acc 10.0000 (10.0000) lr 1.4818e+01 eta 0:01:34\n",
      "epoch [36/100] batch [1/1] time 1.379 (1.379) data 0.338 (0.338) loss 7.5082 (7.5082) acc 5.0000 (5.0000) lr 1.4540e+01 eta 0:01:28\n",
      "epoch [37/100] batch [1/1] time 1.375 (1.375) data 0.343 (0.343) loss 7.1209 (7.1209) acc 10.0000 (10.0000) lr 1.4258e+01 eta 0:01:26\n",
      "epoch [38/100] batch [1/1] time 1.359 (1.359) data 0.337 (0.337) loss 4.8785 (4.8785) acc 10.0000 (10.0000) lr 1.3971e+01 eta 0:01:24\n",
      "epoch [39/100] batch [1/1] time 1.360 (1.360) data 0.324 (0.324) loss 4.2206 (4.2206) acc 20.0000 (20.0000) lr 1.3681e+01 eta 0:01:22\n",
      "epoch [40/100] batch [1/1] time 1.362 (1.362) data 0.353 (0.353) loss 4.0899 (4.0899) acc 25.0000 (25.0000) lr 1.3387e+01 eta 0:01:21\n",
      "epoch [41/100] batch [1/1] time 1.354 (1.354) data 0.332 (0.332) loss 3.5425 (3.5425) acc 25.0000 (25.0000) lr 1.3090e+01 eta 0:01:19\n",
      "epoch [42/100] batch [1/1] time 1.507 (1.507) data 0.448 (0.448) loss 4.3192 (4.3192) acc 25.0000 (25.0000) lr 1.2790e+01 eta 0:01:27\n",
      "epoch [43/100] batch [1/1] time 1.588 (1.588) data 0.486 (0.486) loss 6.2930 (6.2930) acc 10.0000 (10.0000) lr 1.2487e+01 eta 0:01:30\n",
      "epoch [44/100] batch [1/1] time 1.405 (1.405) data 0.395 (0.395) loss 11.4837 (11.4837) acc 10.0000 (10.0000) lr 1.2181e+01 eta 0:01:18\n",
      "epoch [45/100] batch [1/1] time 1.368 (1.368) data 0.342 (0.342) loss 7.9714 (7.9714) acc 15.0000 (15.0000) lr 1.1874e+01 eta 0:01:15\n",
      "epoch [46/100] batch [1/1] time 1.391 (1.391) data 0.362 (0.362) loss 4.9958 (4.9958) acc 10.0000 (10.0000) lr 1.1564e+01 eta 0:01:15\n",
      "epoch [47/100] batch [1/1] time 1.376 (1.376) data 0.336 (0.336) loss 3.9947 (3.9947) acc 10.0000 (10.0000) lr 1.1253e+01 eta 0:01:12\n",
      "epoch [48/100] batch [1/1] time 1.351 (1.351) data 0.325 (0.325) loss 3.6686 (3.6686) acc 15.0000 (15.0000) lr 1.0941e+01 eta 0:01:10\n",
      "epoch [49/100] batch [1/1] time 1.415 (1.415) data 0.383 (0.383) loss 3.5153 (3.5153) acc 10.0000 (10.0000) lr 1.0628e+01 eta 0:01:12\n",
      "epoch [50/100] batch [1/1] time 1.396 (1.396) data 0.344 (0.344) loss 3.4013 (3.4013) acc 30.0000 (30.0000) lr 1.0314e+01 eta 0:01:09\n",
      "epoch [51/100] batch [1/1] time 1.518 (1.518) data 0.486 (0.486) loss 3.2263 (3.2263) acc 30.0000 (30.0000) lr 1.0000e+01 eta 0:01:14\n",
      "epoch [52/100] batch [1/1] time 1.593 (1.593) data 0.517 (0.517) loss 3.0875 (3.0875) acc 30.0000 (30.0000) lr 9.6859e+00 eta 0:01:16\n",
      "epoch [53/100] batch [1/1] time 1.431 (1.431) data 0.402 (0.402) loss 2.9742 (2.9742) acc 40.0000 (40.0000) lr 9.3721e+00 eta 0:01:07\n",
      "epoch [54/100] batch [1/1] time 1.374 (1.374) data 0.340 (0.340) loss 2.8439 (2.8439) acc 40.0000 (40.0000) lr 9.0589e+00 eta 0:01:03\n",
      "epoch [55/100] batch [1/1] time 1.353 (1.353) data 0.332 (0.332) loss 2.5956 (2.5956) acc 50.0000 (50.0000) lr 8.7467e+00 eta 0:01:00\n",
      "epoch [56/100] batch [1/1] time 1.373 (1.373) data 0.330 (0.330) loss 2.3581 (2.3581) acc 50.0000 (50.0000) lr 8.4357e+00 eta 0:01:00\n",
      "epoch [57/100] batch [1/1] time 1.368 (1.368) data 0.346 (0.346) loss 2.3186 (2.3186) acc 50.0000 (50.0000) lr 8.1262e+00 eta 0:00:58\n",
      "epoch [58/100] batch [1/1] time 1.395 (1.395) data 0.357 (0.357) loss 2.3773 (2.3773) acc 35.0000 (35.0000) lr 7.8186e+00 eta 0:00:58\n",
      "epoch [59/100] batch [1/1] time 1.428 (1.428) data 0.362 (0.362) loss 2.0606 (2.0606) acc 60.0000 (60.0000) lr 7.5131e+00 eta 0:00:58\n",
      "epoch [60/100] batch [1/1] time 1.531 (1.531) data 0.509 (0.509) loss 1.7841 (1.7841) acc 60.0000 (60.0000) lr 7.2101e+00 eta 0:01:01\n",
      "epoch [61/100] batch [1/1] time 2.097 (2.097) data 0.483 (0.483) loss 1.7035 (1.7035) acc 70.0000 (70.0000) lr 6.9098e+00 eta 0:01:21\n",
      "epoch [62/100] batch [1/1] time 1.431 (1.431) data 0.392 (0.392) loss 1.8082 (1.8082) acc 65.0000 (65.0000) lr 6.6126e+00 eta 0:00:54\n",
      "epoch [63/100] batch [1/1] time 1.404 (1.404) data 0.370 (0.370) loss 1.4944 (1.4944) acc 75.0000 (75.0000) lr 6.3188e+00 eta 0:00:51\n",
      "epoch [64/100] batch [1/1] time 1.366 (1.366) data 0.332 (0.332) loss 1.5351 (1.5351) acc 60.0000 (60.0000) lr 6.0285e+00 eta 0:00:49\n",
      "epoch [65/100] batch [1/1] time 1.406 (1.406) data 0.359 (0.359) loss 1.4656 (1.4656) acc 75.0000 (75.0000) lr 5.7422e+00 eta 0:00:49\n",
      "epoch [66/100] batch [1/1] time 1.409 (1.409) data 0.365 (0.365) loss 1.4839 (1.4839) acc 70.0000 (70.0000) lr 5.4601e+00 eta 0:00:47\n",
      "epoch [67/100] batch [1/1] time 1.373 (1.373) data 0.339 (0.339) loss 1.1177 (1.1177) acc 85.0000 (85.0000) lr 5.1825e+00 eta 0:00:45\n",
      "epoch [68/100] batch [1/1] time 1.441 (1.441) data 0.374 (0.374) loss 1.4108 (1.4108) acc 85.0000 (85.0000) lr 4.9096e+00 eta 0:00:46\n",
      "epoch [69/100] batch [1/1] time 1.582 (1.582) data 0.553 (0.553) loss 1.1324 (1.1324) acc 90.0000 (90.0000) lr 4.6417e+00 eta 0:00:49\n",
      "epoch [70/100] batch [1/1] time 1.563 (1.563) data 0.480 (0.480) loss 1.0939 (1.0939) acc 85.0000 (85.0000) lr 4.3792e+00 eta 0:00:46\n",
      "epoch [71/100] batch [1/1] time 1.449 (1.449) data 0.400 (0.400) loss 0.9623 (0.9623) acc 95.0000 (95.0000) lr 4.1221e+00 eta 0:00:42\n",
      "epoch [72/100] batch [1/1] time 1.406 (1.406) data 0.376 (0.376) loss 0.8816 (0.8816) acc 100.0000 (100.0000) lr 3.8709e+00 eta 0:00:39\n",
      "epoch [73/100] batch [1/1] time 1.456 (1.456) data 0.420 (0.420) loss 0.9417 (0.9417) acc 95.0000 (95.0000) lr 3.6258e+00 eta 0:00:39\n",
      "epoch [74/100] batch [1/1] time 1.392 (1.392) data 0.363 (0.363) loss 0.8865 (0.8865) acc 95.0000 (95.0000) lr 3.3869e+00 eta 0:00:36\n",
      "epoch [75/100] batch [1/1] time 1.423 (1.423) data 0.385 (0.385) loss 1.1401 (1.1401) acc 80.0000 (80.0000) lr 3.1545e+00 eta 0:00:35\n",
      "epoch [76/100] batch [1/1] time 1.356 (1.356) data 0.354 (0.354) loss 0.7314 (0.7314) acc 100.0000 (100.0000) lr 2.9289e+00 eta 0:00:32\n",
      "epoch [77/100] batch [1/1] time 1.396 (1.396) data 0.346 (0.346) loss 0.7850 (0.7850) acc 90.0000 (90.0000) lr 2.7103e+00 eta 0:00:32\n",
      "epoch [78/100] batch [1/1] time 1.513 (1.513) data 0.478 (0.478) loss 0.8562 (0.8562) acc 90.0000 (90.0000) lr 2.4989e+00 eta 0:00:33\n",
      "epoch [79/100] batch [1/1] time 1.568 (1.568) data 0.479 (0.479) loss 0.8458 (0.8458) acc 90.0000 (90.0000) lr 2.2949e+00 eta 0:00:32\n",
      "epoch [80/100] batch [1/1] time 1.477 (1.477) data 0.441 (0.441) loss 0.8191 (0.8191) acc 90.0000 (90.0000) lr 2.0984e+00 eta 0:00:29\n",
      "epoch [81/100] batch [1/1] time 1.403 (1.403) data 0.343 (0.343) loss 0.7286 (0.7286) acc 95.0000 (95.0000) lr 1.9098e+00 eta 0:00:26\n",
      "epoch [82/100] batch [1/1] time 1.365 (1.365) data 0.343 (0.343) loss 1.0004 (1.0004) acc 80.0000 (80.0000) lr 1.7292e+00 eta 0:00:24\n",
      "epoch [83/100] batch [1/1] time 1.387 (1.387) data 0.346 (0.346) loss 0.8577 (0.8577) acc 90.0000 (90.0000) lr 1.5567e+00 eta 0:00:23\n",
      "epoch [84/100] batch [1/1] time 1.406 (1.406) data 0.358 (0.358) loss 0.7962 (0.7962) acc 90.0000 (90.0000) lr 1.3926e+00 eta 0:00:22\n",
      "epoch [85/100] batch [1/1] time 1.373 (1.373) data 0.332 (0.332) loss 0.6567 (0.6567) acc 100.0000 (100.0000) lr 1.2369e+00 eta 0:00:20\n",
      "epoch [86/100] batch [1/1] time 1.431 (1.431) data 0.372 (0.372) loss 0.7051 (0.7051) acc 100.0000 (100.0000) lr 1.0899e+00 eta 0:00:20\n",
      "epoch [87/100] batch [1/1] time 1.507 (1.507) data 0.482 (0.482) loss 0.7204 (0.7204) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:19\n",
      "epoch [88/100] batch [1/1] time 1.584 (1.584) data 0.494 (0.494) loss 0.9335 (0.9335) acc 95.0000 (95.0000) lr 8.2245e-01 eta 0:00:19\n",
      "epoch [89/100] batch [1/1] time 1.462 (1.462) data 0.426 (0.426) loss 0.7779 (0.7779) acc 95.0000 (95.0000) lr 7.0224e-01 eta 0:00:16\n",
      "epoch [90/100] batch [1/1] time 1.392 (1.392) data 0.345 (0.345) loss 0.8635 (0.8635) acc 90.0000 (90.0000) lr 5.9119e-01 eta 0:00:13\n",
      "epoch [91/100] batch [1/1] time 1.392 (1.392) data 0.349 (0.349) loss 0.9094 (0.9094) acc 95.0000 (95.0000) lr 4.8943e-01 eta 0:00:12\n",
      "epoch [92/100] batch [1/1] time 1.393 (1.393) data 0.365 (0.365) loss 1.0542 (1.0542) acc 90.0000 (90.0000) lr 3.9706e-01 eta 0:00:11\n",
      "epoch [93/100] batch [1/1] time 1.366 (1.366) data 0.333 (0.333) loss 0.6881 (0.6881) acc 100.0000 (100.0000) lr 3.1417e-01 eta 0:00:09\n",
      "epoch [94/100] batch [1/1] time 1.374 (1.374) data 0.344 (0.344) loss 0.8928 (0.8928) acc 85.0000 (85.0000) lr 2.4083e-01 eta 0:00:08\n",
      "epoch [95/100] batch [1/1] time 1.386 (1.386) data 0.349 (0.349) loss 0.6356 (0.6356) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:06\n",
      "epoch [96/100] batch [1/1] time 1.560 (1.560) data 0.497 (0.497) loss 0.7092 (0.7092) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:06\n",
      "epoch [97/100] batch [1/1] time 1.573 (1.573) data 0.478 (0.478) loss 0.6688 (0.6688) acc 95.0000 (95.0000) lr 7.8853e-02 eta 0:00:04\n",
      "epoch [98/100] batch [1/1] time 1.447 (1.447) data 0.408 (0.408) loss 0.6169 (0.6169) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:02\n",
      "epoch [99/100] batch [1/1] time 1.381 (1.381) data 0.344 (0.344) loss 0.6991 (0.6991) acc 95.0000 (95.0000) lr 1.9733e-02 eta 0:00:01\n",
      "epoch [100/100] batch [1/1] time 1.378 (1.378) data 0.348 (0.348) loss 0.6251 (0.6251) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:00\n",
      "Checkpoint saved to output/new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1/prompt_learner/model.pth.tar-100\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:56<00:00,  1.44it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 5,639\n",
      "* accuracy: 69.6%\n",
      "* error: 30.4%\n",
      "* macro_f1: 68.6%\n",
      "Elapsed: 0:03:39\n"
     ]
    }
   ],
   "source": [
    "#eurosat-2shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTP-NWKohzs2",
    "outputId": "619d97da-b737-45bf-a6b2-d542c3883a23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:24:04.670428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:24:04.689694: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:24:04.695636: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:24:04.710498: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:24:05.738602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '1']\n",
      "output_dir: output/new_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 1\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 20.0\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 50\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/new_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 10.0\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 100.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_1-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  10\n",
      "# val      10\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.vis_ctx', 'prompt_learner.txt_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/new_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
      "epoch [1/50] batch [1/1] time 2.357 (2.357) data 0.345 (0.345) loss 11.6641 (11.6641) acc 10.0000 (10.0000) lr 2.0000e+01 eta 0:01:55\n",
      "epoch [2/50] batch [1/1] time 1.228 (1.228) data 0.261 (0.261) loss 11.6230 (11.6230) acc 10.0000 (10.0000) lr 1.9980e+01 eta 0:00:58\n",
      "epoch [3/50] batch [1/1] time 1.277 (1.277) data 0.284 (0.284) loss 10.8498 (10.8498) acc 0.0000 (0.0000) lr 1.9921e+01 eta 0:01:00\n",
      "epoch [4/50] batch [1/1] time 1.368 (1.368) data 0.389 (0.389) loss 10.5265 (10.5265) acc 10.0000 (10.0000) lr 1.9823e+01 eta 0:01:02\n",
      "epoch [5/50] batch [1/1] time 1.418 (1.418) data 0.370 (0.370) loss 10.3568 (10.3568) acc 10.0000 (10.0000) lr 1.9686e+01 eta 0:01:03\n",
      "epoch [6/50] batch [1/1] time 1.310 (1.310) data 0.334 (0.334) loss 10.2511 (10.2511) acc 0.0000 (0.0000) lr 1.9511e+01 eta 0:00:57\n",
      "epoch [7/50] batch [1/1] time 1.252 (1.252) data 0.262 (0.262) loss 10.1339 (10.1339) acc 0.0000 (0.0000) lr 1.9298e+01 eta 0:00:53\n",
      "epoch [8/50] batch [1/1] time 1.265 (1.265) data 0.288 (0.288) loss 10.0390 (10.0390) acc 0.0000 (0.0000) lr 1.9048e+01 eta 0:00:53\n",
      "epoch [9/50] batch [1/1] time 1.278 (1.278) data 0.271 (0.271) loss 10.0222 (10.0222) acc 0.0000 (0.0000) lr 1.8763e+01 eta 0:00:52\n",
      "epoch [10/50] batch [1/1] time 1.277 (1.277) data 0.278 (0.278) loss 9.9208 (9.9208) acc 0.0000 (0.0000) lr 1.8443e+01 eta 0:00:51\n",
      "epoch [11/50] batch [1/1] time 1.308 (1.308) data 0.286 (0.286) loss 9.8446 (9.8446) acc 0.0000 (0.0000) lr 1.8090e+01 eta 0:00:50\n",
      "epoch [12/50] batch [1/1] time 1.270 (1.270) data 0.268 (0.268) loss 9.7838 (9.7838) acc 20.0000 (20.0000) lr 1.7705e+01 eta 0:00:48\n",
      "epoch [13/50] batch [1/1] time 1.277 (1.277) data 0.318 (0.318) loss 9.7694 (9.7694) acc 10.0000 (10.0000) lr 1.7290e+01 eta 0:00:47\n",
      "epoch [14/50] batch [1/1] time 1.357 (1.357) data 0.397 (0.397) loss 9.6756 (9.6756) acc 10.0000 (10.0000) lr 1.6845e+01 eta 0:00:48\n",
      "epoch [15/50] batch [1/1] time 1.413 (1.413) data 0.403 (0.403) loss 9.6145 (9.6145) acc 10.0000 (10.0000) lr 1.6374e+01 eta 0:00:49\n",
      "epoch [16/50] batch [1/1] time 1.265 (1.265) data 0.280 (0.280) loss 9.5292 (9.5292) acc 20.0000 (20.0000) lr 1.5878e+01 eta 0:00:43\n",
      "epoch [17/50] batch [1/1] time 1.263 (1.263) data 0.278 (0.278) loss 9.4470 (9.4470) acc 20.0000 (20.0000) lr 1.5358e+01 eta 0:00:41\n",
      "epoch [18/50] batch [1/1] time 1.248 (1.248) data 0.266 (0.266) loss 9.3718 (9.3718) acc 20.0000 (20.0000) lr 1.4818e+01 eta 0:00:39\n",
      "epoch [19/50] batch [1/1] time 1.267 (1.267) data 0.285 (0.285) loss 9.3458 (9.3458) acc 20.0000 (20.0000) lr 1.4258e+01 eta 0:00:39\n",
      "epoch [20/50] batch [1/1] time 1.305 (1.305) data 0.296 (0.296) loss 9.2606 (9.2606) acc 20.0000 (20.0000) lr 1.3681e+01 eta 0:00:39\n",
      "epoch [21/50] batch [1/1] time 1.351 (1.351) data 0.265 (0.265) loss 9.1274 (9.1274) acc 40.0000 (40.0000) lr 1.3090e+01 eta 0:00:39\n",
      "epoch [22/50] batch [1/1] time 1.247 (1.247) data 0.281 (0.281) loss 9.0514 (9.0514) acc 50.0000 (50.0000) lr 1.2487e+01 eta 0:00:34\n",
      "epoch [23/50] batch [1/1] time 1.358 (1.358) data 0.398 (0.398) loss 8.9617 (8.9617) acc 40.0000 (40.0000) lr 1.1874e+01 eta 0:00:36\n",
      "epoch [24/50] batch [1/1] time 1.374 (1.374) data 0.373 (0.373) loss 8.7204 (8.7204) acc 50.0000 (50.0000) lr 1.1253e+01 eta 0:00:35\n",
      "epoch [25/50] batch [1/1] time 1.372 (1.372) data 0.333 (0.333) loss 8.5441 (8.5441) acc 40.0000 (40.0000) lr 1.0628e+01 eta 0:00:34\n",
      "epoch [26/50] batch [1/1] time 1.279 (1.279) data 0.269 (0.269) loss 8.4203 (8.4203) acc 50.0000 (50.0000) lr 1.0000e+01 eta 0:00:30\n",
      "epoch [27/50] batch [1/1] time 1.365 (1.365) data 0.280 (0.280) loss 8.2791 (8.2791) acc 50.0000 (50.0000) lr 9.3721e+00 eta 0:00:31\n",
      "epoch [28/50] batch [1/1] time 1.274 (1.274) data 0.284 (0.284) loss 8.1444 (8.1444) acc 50.0000 (50.0000) lr 8.7467e+00 eta 0:00:28\n",
      "epoch [29/50] batch [1/1] time 1.310 (1.310) data 0.294 (0.294) loss 8.0063 (8.0063) acc 50.0000 (50.0000) lr 8.1262e+00 eta 0:00:27\n",
      "epoch [30/50] batch [1/1] time 1.313 (1.313) data 0.271 (0.271) loss 7.6437 (7.6437) acc 60.0000 (60.0000) lr 7.5131e+00 eta 0:00:26\n",
      "epoch [31/50] batch [1/1] time 1.320 (1.320) data 0.281 (0.281) loss 7.5516 (7.5516) acc 70.0000 (70.0000) lr 6.9098e+00 eta 0:00:25\n",
      "epoch [32/50] batch [1/1] time 1.373 (1.373) data 0.387 (0.387) loss 7.2874 (7.2874) acc 70.0000 (70.0000) lr 6.3188e+00 eta 0:00:24\n",
      "epoch [33/50] batch [1/1] time 1.412 (1.412) data 0.386 (0.386) loss 7.2574 (7.2574) acc 60.0000 (60.0000) lr 5.7422e+00 eta 0:00:23\n",
      "epoch [34/50] batch [1/1] time 1.394 (1.394) data 0.380 (0.380) loss 6.8026 (6.8026) acc 90.0000 (90.0000) lr 5.1825e+00 eta 0:00:22\n",
      "epoch [35/50] batch [1/1] time 1.295 (1.295) data 0.292 (0.292) loss 6.6006 (6.6006) acc 90.0000 (90.0000) lr 4.6417e+00 eta 0:00:19\n",
      "epoch [36/50] batch [1/1] time 1.263 (1.263) data 0.275 (0.275) loss 6.6406 (6.6406) acc 80.0000 (80.0000) lr 4.1221e+00 eta 0:00:17\n",
      "epoch [37/50] batch [1/1] time 1.271 (1.271) data 0.285 (0.285) loss 6.3328 (6.3328) acc 80.0000 (80.0000) lr 3.6258e+00 eta 0:00:16\n",
      "epoch [38/50] batch [1/1] time 1.273 (1.273) data 0.278 (0.278) loss 6.1476 (6.1476) acc 80.0000 (80.0000) lr 3.1545e+00 eta 0:00:15\n",
      "epoch [39/50] batch [1/1] time 1.294 (1.294) data 0.279 (0.279) loss 6.5989 (6.5989) acc 60.0000 (60.0000) lr 2.7103e+00 eta 0:00:14\n",
      "epoch [40/50] batch [1/1] time 1.290 (1.290) data 0.275 (0.275) loss 5.9704 (5.9704) acc 90.0000 (90.0000) lr 2.2949e+00 eta 0:00:12\n",
      "epoch [41/50] batch [1/1] time 1.273 (1.273) data 0.274 (0.274) loss 5.8529 (5.8529) acc 90.0000 (90.0000) lr 1.9098e+00 eta 0:00:11\n",
      "epoch [42/50] batch [1/1] time 1.441 (1.441) data 0.424 (0.424) loss 5.6535 (5.6535) acc 100.0000 (100.0000) lr 1.5567e+00 eta 0:00:11\n",
      "epoch [43/50] batch [1/1] time 1.368 (1.368) data 0.343 (0.343) loss 5.8264 (5.8264) acc 80.0000 (80.0000) lr 1.2369e+00 eta 0:00:09\n",
      "epoch [44/50] batch [1/1] time 1.322 (1.322) data 0.340 (0.340) loss 5.6009 (5.6009) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:07\n",
      "epoch [45/50] batch [1/1] time 1.253 (1.253) data 0.272 (0.272) loss 5.6003 (5.6003) acc 90.0000 (90.0000) lr 7.0224e-01 eta 0:00:06\n",
      "epoch [46/50] batch [1/1] time 1.276 (1.276) data 0.306 (0.306) loss 5.6519 (5.6519) acc 90.0000 (90.0000) lr 4.8943e-01 eta 0:00:05\n",
      "epoch [47/50] batch [1/1] time 1.250 (1.250) data 0.278 (0.278) loss 5.6265 (5.6265) acc 90.0000 (90.0000) lr 3.1417e-01 eta 0:00:03\n",
      "epoch [48/50] batch [1/1] time 1.224 (1.224) data 0.267 (0.267) loss 5.4535 (5.4535) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:02\n",
      "epoch [49/50] batch [1/1] time 1.611 (1.611) data 0.265 (0.265) loss 5.6008 (5.6008) acc 90.0000 (90.0000) lr 7.8853e-02 eta 0:00:01\n",
      "epoch [50/50] batch [1/1] time 1.331 (1.331) data 0.286 (0.286) loss 5.4201 (5.4201) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:00\n",
      "Checkpoint saved to output/new_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1/prompt_learner/model.pth.tar-50\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "100% 81/81 [00:56<00:00,  1.44it/s]\n",
      "=> result\n",
      "* total: 8,100\n",
      "* correct: 3,149\n",
      "* accuracy: 38.9%\n",
      "* error: 61.1%\n",
      "* macro_f1: 34.8%\n",
      "Elapsed: 0:02:12\n"
     ]
    }
   ],
   "source": [
    "#eurosat-1shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
    "        --output-dir output/new_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHLKRmMkoG35"
   },
   "source": [
    "# Oxfordpets New Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mRX0taFoUtb",
    "outputId": "21732b27-b2e6-4ea3-c590-e66c39421afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:35:14.392261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 17:35:14.411871: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 17:35:14.417679: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 17:35:14.431889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 17:35:15.459115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16']\n",
      "output_dir: output/new_init/oxford_pets/DAPT/vit_b16_16shots/seed1\n",
      "prototype_gen: False\n",
      "resume: \n",
      "root: /content/drive/MyDrive/DAPT/DATA/\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: DAPT\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordPets\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.02\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 200\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/new_init/oxford_pets/DAPT/vit_b16_16shots/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAPT:\n",
      "    PROTOTYPE_GEN: False\n",
      "    TXT_BETA: 0.1\n",
      "    TXT_NUM_TOKENS: 16\n",
      "    TXT_RBF_T: 2.0\n",
      "    VIS_BETA: 10.0\n",
      "    VIS_DROPOUT: 0.0\n",
      "    VIS_NUM_TOKENS: 16\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: DAPT\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.5.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: 14.0.0-1ubuntu1.1\n",
      "CMake version: version 3.30.5\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.140\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: Tesla T4\n",
      "Nvidia driver version: 535.104.05\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        46 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               2\n",
      "On-line CPU(s) list:                  0,1\n",
      "Vendor ID:                            GenuineIntel\n",
      "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "CPU family:                           6\n",
      "Model:                                85\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   1\n",
      "Socket(s):                            1\n",
      "Stepping:                             3\n",
      "BogoMIPS:                             4000.38\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
      "Hypervisor vendor:                    KVM\n",
      "Virtualization type:                  full\n",
      "L1d cache:                            32 KiB (1 instance)\n",
      "L1i cache:                            32 KiB (1 instance)\n",
      "L2 cache:                             1 MiB (1 instance)\n",
      "L3 cache:                             38.5 MiB (1 instance)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0,1\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
      "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:               Vulnerable\n",
      "Vulnerability Mmio stale data:        Vulnerable\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Vulnerable\n",
      "Vulnerability Spec rstack overflow:   Not affected\n",
      "Vulnerability Spec store bypass:      Vulnerable\n",
      "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Vulnerable\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] optree==0.13.1\n",
      "[pip3] torch==2.5.1+cu121\n",
      "[pip3] torchaudio==2.5.1+cu121\n",
      "[pip3] torchcam==0.4.0\n",
      "[pip3] torchsummary==1.5.1\n",
      "[pip3] torchvision==0.20.1+cu121\n",
      "[conda] Could not collect\n",
      "        Pillow (11.0.0)\n",
      "\n",
      "Loading trainer: DAPT\n",
      "Loading dataset: OxfordPets\n",
      "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
      "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_16-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "---------  ----------\n",
      "Dataset    OxfordPets\n",
      "# classes  37\n",
      "# train_x  592\n",
      "# val      148\n",
      "# test     3,669\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.txt_ctx', 'prompt_learner.vis_ctx'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/new_init/oxford_pets/DAPT/vit_b16_16shots/seed1/tensorboard)\n",
      "epoch [1/200] batch [5/18] time 1.171 (2.198) data 0.000 (0.471) loss 2.8620 (2.9731) acc 34.3750 (23.7500) lr 1.0000e-05 eta 2:11:40\n",
      "epoch [1/200] batch [10/18] time 1.231 (1.719) data 0.000 (0.236) loss 3.0340 (2.9353) acc 15.6250 (21.2500) lr 1.0000e-05 eta 1:42:51\n",
      "epoch [1/200] batch [15/18] time 1.162 (1.532) data 0.000 (0.157) loss 3.1777 (2.9373) acc 21.8750 (21.0417) lr 1.0000e-05 eta 1:31:33\n",
      "epoch [2/200] batch [5/18] time 1.154 (2.633) data 0.000 (0.558) loss 1.8067 (1.9276) acc 62.5000 (45.6250) lr 2.0000e-02 eta 2:36:59\n",
      "epoch [2/200] batch [10/18] time 1.170 (1.900) data 0.000 (0.279) loss 1.7802 (1.7154) acc 53.1250 (52.8125) lr 2.0000e-02 eta 1:53:07\n",
      "epoch [2/200] batch [15/18] time 1.187 (1.653) data 0.000 (0.186) loss 1.3153 (1.6291) acc 62.5000 (55.0000) lr 2.0000e-02 eta 1:38:16\n",
      "epoch [3/200] batch [5/18] time 1.152 (1.946) data 0.000 (0.373) loss 0.4610 (1.0090) acc 84.3750 (71.2500) lr 1.9999e-02 eta 1:55:25\n",
      "epoch [3/200] batch [10/18] time 1.230 (1.563) data 0.000 (0.186) loss 1.1888 (1.0427) acc 68.7500 (70.9375) lr 1.9999e-02 eta 1:32:36\n",
      "epoch [3/200] batch [15/18] time 1.157 (1.444) data 0.000 (0.124) loss 0.7020 (0.9395) acc 81.2500 (73.5417) lr 1.9999e-02 eta 1:25:25\n",
      "epoch [4/200] batch [5/18] time 1.173 (2.008) data 0.000 (0.427) loss 0.7134 (0.7038) acc 81.2500 (82.5000) lr 1.9995e-02 eta 1:58:30\n",
      "epoch [4/200] batch [10/18] time 1.156 (1.596) data 0.000 (0.214) loss 0.5851 (0.7076) acc 81.2500 (80.9375) lr 1.9995e-02 eta 1:34:01\n",
      "epoch [4/200] batch [15/18] time 1.154 (1.449) data 0.000 (0.143) loss 1.1219 (0.7172) acc 59.3750 (80.4167) lr 1.9995e-02 eta 1:25:16\n",
      "epoch [5/200] batch [5/18] time 1.153 (2.765) data 0.000 (0.638) loss 0.2915 (0.4994) acc 90.6250 (83.7500) lr 1.9989e-02 eta 2:42:22\n",
      "epoch [5/200] batch [10/18] time 1.149 (1.960) data 0.000 (0.319) loss 0.9002 (0.6531) acc 68.7500 (80.9375) lr 1.9989e-02 eta 1:54:55\n",
      "epoch [5/200] batch [15/18] time 1.245 (1.700) data 0.001 (0.213) loss 0.9186 (0.6900) acc 71.8750 (80.0000) lr 1.9989e-02 eta 1:39:31\n",
      "epoch [6/200] batch [5/18] time 1.152 (1.989) data 0.000 (0.409) loss 0.7017 (0.5781) acc 78.1250 (83.7500) lr 1.9980e-02 eta 1:56:11\n",
      "epoch [6/200] batch [10/18] time 1.218 (1.589) data 0.000 (0.205) loss 0.6409 (0.6740) acc 78.1250 (81.2500) lr 1.9980e-02 eta 1:32:42\n",
      "epoch [6/200] batch [15/18] time 1.147 (1.453) data 0.000 (0.137) loss 0.5534 (0.6986) acc 90.6250 (81.4583) lr 1.9980e-02 eta 1:24:36\n",
      "epoch [7/200] batch [5/18] time 1.220 (2.040) data 0.000 (0.432) loss 0.5850 (0.4756) acc 84.3750 (85.6250) lr 1.9969e-02 eta 1:58:32\n",
      "epoch [7/200] batch [10/18] time 1.147 (1.613) data 0.000 (0.216) loss 0.5537 (0.5586) acc 78.1250 (82.8125) lr 1.9969e-02 eta 1:33:37\n",
      "epoch [7/200] batch [15/18] time 1.160 (1.461) data 0.001 (0.144) loss 0.9370 (0.6109) acc 75.0000 (81.8750) lr 1.9969e-02 eta 1:24:40\n",
      "epoch [8/200] batch [5/18] time 1.137 (2.739) data 0.000 (0.790) loss 0.2979 (0.5861) acc 96.8750 (87.5000) lr 1.9956e-02 eta 2:38:21\n",
      "epoch [8/200] batch [10/18] time 1.150 (1.943) data 0.000 (0.395) loss 0.6539 (0.6240) acc 84.3750 (84.0625) lr 1.9956e-02 eta 1:52:10\n",
      "epoch [8/200] batch [15/18] time 1.219 (1.698) data 0.000 (0.264) loss 0.3919 (0.6173) acc 87.5000 (83.1250) lr 1.9956e-02 eta 1:37:52\n",
      "epoch [9/200] batch [5/18] time 1.166 (1.945) data 0.000 (0.397) loss 0.5541 (0.5952) acc 78.1250 (82.5000) lr 1.9940e-02 eta 1:51:53\n",
      "epoch [9/200] batch [10/18] time 1.186 (1.561) data 0.000 (0.198) loss 0.7337 (0.5513) acc 78.1250 (83.7500) lr 1.9940e-02 eta 1:29:37\n",
      "epoch [9/200] batch [15/18] time 1.179 (1.435) data 0.000 (0.132) loss 0.5916 (0.5351) acc 81.2500 (84.1667) lr 1.9940e-02 eta 1:22:18\n",
      "epoch [10/200] batch [5/18] time 1.154 (1.950) data 0.000 (0.351) loss 0.7232 (0.6071) acc 81.2500 (81.8750) lr 1.9921e-02 eta 1:51:33\n",
      "epoch [10/200] batch [10/18] time 1.225 (1.571) data 0.000 (0.176) loss 0.4573 (0.6078) acc 90.6250 (82.8125) lr 1.9921e-02 eta 1:29:44\n",
      "epoch [10/200] batch [15/18] time 1.165 (1.432) data 0.000 (0.117) loss 0.5336 (0.6066) acc 87.5000 (83.3333) lr 1.9921e-02 eta 1:21:42\n",
      "epoch [11/200] batch [5/18] time 1.209 (2.483) data 0.000 (0.499) loss 0.6462 (0.5841) acc 84.3750 (83.1250) lr 1.9900e-02 eta 2:21:18\n",
      "epoch [11/200] batch [10/18] time 1.156 (1.819) data 0.000 (0.250) loss 0.5807 (0.5551) acc 84.3750 (85.3125) lr 1.9900e-02 eta 1:43:24\n",
      "epoch [11/200] batch [15/18] time 1.173 (1.601) data 0.000 (0.167) loss 0.4391 (0.5492) acc 84.3750 (84.7917) lr 1.9900e-02 eta 1:30:50\n",
      "epoch [12/200] batch [5/18] time 1.194 (1.973) data 0.000 (0.416) loss 0.6900 (0.6484) acc 81.2500 (81.2500) lr 1.9877e-02 eta 1:51:41\n",
      "epoch [12/200] batch [10/18] time 1.237 (1.584) data 0.000 (0.208) loss 0.5533 (0.5877) acc 84.3750 (84.0625) lr 1.9877e-02 eta 1:29:33\n",
      "epoch [12/200] batch [15/18] time 1.136 (1.451) data 0.000 (0.139) loss 1.1533 (0.6392) acc 75.0000 (82.2917) lr 1.9877e-02 eta 1:21:53\n",
      "epoch [13/200] batch [5/18] time 1.195 (2.430) data 0.000 (0.482) loss 1.1747 (0.7102) acc 68.7500 (80.0000) lr 1.9851e-02 eta 2:16:49\n",
      "epoch [13/200] batch [10/18] time 1.154 (1.807) data 0.000 (0.241) loss 0.8817 (0.7761) acc 71.8750 (77.1875) lr 1.9851e-02 eta 1:41:37\n",
      "epoch [13/200] batch [15/18] time 1.188 (1.596) data 0.000 (0.161) loss 0.3570 (0.6610) acc 87.5000 (80.6250) lr 1.9851e-02 eta 1:29:38\n",
      "epoch [14/200] batch [5/18] time 1.164 (2.020) data 0.000 (0.480) loss 0.7589 (0.5808) acc 75.0000 (83.7500) lr 1.9823e-02 eta 1:53:08\n",
      "epoch [14/200] batch [10/18] time 1.199 (1.601) data 0.000 (0.240) loss 0.4513 (0.6171) acc 90.6250 (82.8125) lr 1.9823e-02 eta 1:29:33\n",
      "epoch [14/200] batch [15/18] time 1.179 (1.464) data 0.000 (0.160) loss 0.3607 (0.5904) acc 87.5000 (82.9167) lr 1.9823e-02 eta 1:21:46\n",
      "epoch [15/200] batch [5/18] time 1.195 (2.033) data 0.000 (0.418) loss 0.7276 (0.7471) acc 81.2500 (75.0000) lr 1.9792e-02 eta 1:53:15\n",
      "epoch [15/200] batch [10/18] time 1.183 (1.618) data 0.000 (0.209) loss 0.1697 (0.6393) acc 100.0000 (80.0000) lr 1.9792e-02 eta 1:30:02\n",
      "epoch [15/200] batch [15/18] time 1.151 (1.463) data 0.000 (0.140) loss 0.4858 (0.6020) acc 78.1250 (80.8333) lr 1.9792e-02 eta 1:21:14\n",
      "epoch [16/200] batch [5/18] time 1.144 (2.654) data 0.000 (0.549) loss 0.5902 (0.5171) acc 84.3750 (84.3750) lr 1.9759e-02 eta 2:27:05\n",
      "epoch [16/200] batch [10/18] time 1.159 (1.905) data 0.000 (0.275) loss 0.6643 (0.5053) acc 78.1250 (85.0000) lr 1.9759e-02 eta 1:45:23\n",
      "epoch [16/200] batch [15/18] time 1.219 (1.661) data 0.001 (0.183) loss 0.5954 (0.5378) acc 87.5000 (84.7917) lr 1.9759e-02 eta 1:31:47\n",
      "epoch [17/200] batch [5/18] time 1.152 (1.977) data 0.000 (0.424) loss 0.7281 (0.5628) acc 75.0000 (85.0000) lr 1.9724e-02 eta 1:48:57\n",
      "epoch [17/200] batch [10/18] time 1.211 (1.582) data 0.001 (0.212) loss 0.9155 (0.6285) acc 78.1250 (82.8125) lr 1.9724e-02 eta 1:27:03\n",
      "epoch [17/200] batch [15/18] time 1.195 (1.455) data 0.000 (0.142) loss 0.6694 (0.6072) acc 78.1250 (82.2917) lr 1.9724e-02 eta 1:19:55\n",
      "epoch [18/200] batch [5/18] time 1.200 (1.987) data 0.000 (0.393) loss 0.8180 (0.5032) acc 62.5000 (83.1250) lr 1.9686e-02 eta 1:48:53\n",
      "epoch [18/200] batch [10/18] time 1.233 (1.605) data 0.000 (0.197) loss 0.6237 (0.5312) acc 87.5000 (83.1250) lr 1.9686e-02 eta 1:27:51\n",
      "epoch [18/200] batch [15/18] time 1.136 (1.456) data 0.000 (0.131) loss 0.2841 (0.5174) acc 93.7500 (83.7500) lr 1.9686e-02 eta 1:19:33\n",
      "epoch [19/200] batch [5/18] time 1.143 (2.712) data 0.000 (0.652) loss 0.5717 (0.5992) acc 84.3750 (80.0000) lr 1.9646e-02 eta 2:27:51\n",
      "epoch [19/200] batch [10/18] time 1.170 (1.940) data 0.000 (0.326) loss 0.5771 (0.6009) acc 81.2500 (80.6250) lr 1.9646e-02 eta 1:45:36\n",
      "epoch [19/200] batch [15/18] time 1.186 (1.681) data 0.000 (0.217) loss 0.3584 (0.5863) acc 90.6250 (81.4583) lr 1.9646e-02 eta 1:31:22\n",
      "epoch [20/200] batch [5/18] time 1.160 (1.954) data 0.000 (0.459) loss 0.2492 (0.4226) acc 90.6250 (86.8750) lr 1.9603e-02 eta 1:45:54\n",
      "epoch [20/200] batch [10/18] time 1.232 (1.567) data 0.000 (0.230) loss 0.4727 (0.4234) acc 87.5000 (87.5000) lr 1.9603e-02 eta 1:24:49\n",
      "epoch [20/200] batch [15/18] time 1.173 (1.445) data 0.000 (0.153) loss 0.3087 (0.4632) acc 90.6250 (86.0417) lr 1.9603e-02 eta 1:18:07\n",
      "epoch [21/200] batch [5/18] time 1.183 (2.111) data 0.000 (0.370) loss 0.5743 (0.4407) acc 78.1250 (85.0000) lr 1.9558e-02 eta 1:53:49\n",
      "epoch [21/200] batch [10/18] time 1.143 (1.652) data 0.000 (0.185) loss 0.6240 (0.4796) acc 78.1250 (84.6875) lr 1.9558e-02 eta 1:28:55\n",
      "epoch [21/200] batch [15/18] time 1.149 (1.488) data 0.000 (0.124) loss 0.4919 (0.4964) acc 93.7500 (84.3750) lr 1.9558e-02 eta 1:19:58\n",
      "epoch [22/200] batch [5/18] time 1.160 (2.762) data 0.000 (0.750) loss 0.2513 (0.5706) acc 96.8750 (80.0000) lr 1.9511e-02 eta 2:28:05\n",
      "epoch [22/200] batch [10/18] time 1.163 (1.965) data 0.000 (0.375) loss 0.4363 (0.5364) acc 84.3750 (81.2500) lr 1.9511e-02 eta 1:45:12\n",
      "epoch [22/200] batch [15/18] time 1.210 (1.703) data 0.000 (0.250) loss 0.4490 (0.5726) acc 84.3750 (81.0417) lr 1.9511e-02 eta 1:31:00\n",
      "epoch [23/200] batch [5/18] time 1.177 (1.972) data 0.000 (0.350) loss 0.6244 (0.4158) acc 84.3750 (88.7500) lr 1.9461e-02 eta 1:45:07\n",
      "epoch [23/200] batch [10/18] time 1.201 (1.578) data 0.000 (0.175) loss 0.3504 (0.4041) acc 90.6250 (88.4375) lr 1.9461e-02 eta 1:23:59\n",
      "epoch [23/200] batch [15/18] time 1.210 (1.450) data 0.000 (0.117) loss 0.5037 (0.4415) acc 84.3750 (87.9167) lr 1.9461e-02 eta 1:17:02\n",
      "epoch [24/200] batch [5/18] time 1.188 (2.066) data 0.000 (0.404) loss 0.4522 (0.4599) acc 84.3750 (87.5000) lr 1.9409e-02 eta 1:49:32\n",
      "epoch [24/200] batch [10/18] time 1.204 (1.631) data 0.000 (0.202) loss 0.3130 (0.4330) acc 93.7500 (89.3750) lr 1.9409e-02 eta 1:26:19\n",
      "epoch [24/200] batch [15/18] time 1.161 (1.473) data 0.000 (0.135) loss 0.5316 (0.4758) acc 84.3750 (87.2917) lr 1.9409e-02 eta 1:17:52\n",
      "epoch [25/200] batch [5/18] time 1.176 (2.619) data 0.000 (0.498) loss 0.3971 (0.4794) acc 81.2500 (84.3750) lr 1.9354e-02 eta 2:18:03\n",
      "epoch [25/200] batch [10/18] time 1.164 (1.891) data 0.000 (0.249) loss 0.4375 (0.5620) acc 90.6250 (83.7500) lr 1.9354e-02 eta 1:39:31\n",
      "epoch [25/200] batch [15/18] time 1.125 (1.646) data 0.000 (0.166) loss 0.2635 (0.5202) acc 87.5000 (85.6250) lr 1.9354e-02 eta 1:26:29\n",
      "epoch [26/200] batch [5/18] time 1.147 (1.971) data 0.000 (0.374) loss 0.2363 (0.4291) acc 96.8750 (86.2500) lr 1.9298e-02 eta 1:43:17\n",
      "epoch [26/200] batch [10/18] time 1.164 (1.565) data 0.000 (0.187) loss 0.5790 (0.4670) acc 90.6250 (87.5000) lr 1.9298e-02 eta 1:21:54\n",
      "epoch [26/200] batch [15/18] time 1.169 (1.435) data 0.000 (0.125) loss 0.5712 (0.4968) acc 78.1250 (85.0000) lr 1.9298e-02 eta 1:14:58\n",
      "epoch [27/200] batch [5/18] time 1.171 (1.975) data 0.000 (0.397) loss 0.4062 (0.4848) acc 84.3750 (84.3750) lr 1.9239e-02 eta 1:42:56\n",
      "epoch [27/200] batch [10/18] time 1.170 (1.580) data 0.000 (0.198) loss 0.5616 (0.5124) acc 84.3750 (84.6875) lr 1.9239e-02 eta 1:22:13\n",
      "epoch [27/200] batch [15/18] time 1.147 (1.441) data 0.000 (0.132) loss 0.3521 (0.4728) acc 87.5000 (85.8333) lr 1.9239e-02 eta 1:14:51\n",
      "epoch [28/200] batch [5/18] time 1.170 (2.469) data 0.005 (0.360) loss 0.8447 (0.6316) acc 75.0000 (80.0000) lr 1.9178e-02 eta 2:07:57\n",
      "epoch [28/200] batch [10/18] time 1.158 (1.816) data 0.000 (0.180) loss 0.4506 (0.5490) acc 87.5000 (83.4375) lr 1.9178e-02 eta 1:33:58\n",
      "epoch [28/200] batch [15/18] time 1.149 (1.617) data 0.000 (0.120) loss 0.5691 (0.5432) acc 84.3750 (83.7500) lr 1.9178e-02 eta 1:23:30\n",
      "epoch [29/200] batch [5/18] time 1.137 (2.815) data 0.000 (0.790) loss 0.6236 (0.4652) acc 75.0000 (85.6250) lr 1.9114e-02 eta 2:25:01\n",
      "epoch [29/200] batch [10/18] time 1.171 (1.996) data 0.000 (0.395) loss 0.3545 (0.4900) acc 90.6250 (86.2500) lr 1.9114e-02 eta 1:42:38\n",
      "epoch [29/200] batch [15/18] time 1.153 (1.718) data 0.000 (0.263) loss 0.6338 (0.5332) acc 81.2500 (84.1667) lr 1.9114e-02 eta 1:28:12\n",
      "epoch [30/200] batch [5/18] time 1.206 (2.302) data 0.000 (0.367) loss 0.5154 (0.3311) acc 87.5000 (91.2500) lr 1.9048e-02 eta 1:57:54\n",
      "epoch [30/200] batch [10/18] time 1.149 (1.733) data 0.000 (0.184) loss 0.5104 (0.4146) acc 81.2500 (88.7500) lr 1.9048e-02 eta 1:28:38\n",
      "epoch [30/200] batch [15/18] time 1.140 (1.539) data 0.000 (0.123) loss 0.6734 (0.4347) acc 78.1250 (87.2917) lr 1.9048e-02 eta 1:18:34\n",
      "epoch [31/200] batch [5/18] time 1.157 (1.991) data 0.000 (0.387) loss 0.3511 (0.4533) acc 87.5000 (87.5000) lr 1.8980e-02 eta 1:41:22\n",
      "epoch [31/200] batch [10/18] time 1.194 (1.577) data 0.000 (0.194) loss 0.5076 (0.5027) acc 81.2500 (86.2500) lr 1.8980e-02 eta 1:20:10\n",
      "epoch [31/200] batch [15/18] time 1.190 (1.449) data 0.000 (0.129) loss 0.3928 (0.5552) acc 90.6250 (83.7500) lr 1.8980e-02 eta 1:13:33\n",
      "epoch [32/200] batch [5/18] time 1.148 (1.953) data 0.000 (0.367) loss 0.7281 (0.5218) acc 75.0000 (81.2500) lr 1.8910e-02 eta 1:38:50\n",
      "epoch [32/200] batch [10/18] time 1.179 (1.581) data 0.000 (0.184) loss 0.4341 (0.5249) acc 87.5000 (83.1250) lr 1.8910e-02 eta 1:19:54\n",
      "epoch [32/200] batch [15/18] time 1.178 (1.451) data 0.000 (0.123) loss 0.4939 (0.5019) acc 87.5000 (84.5833) lr 1.8910e-02 eta 1:13:11\n",
      "epoch [33/200] batch [5/18] time 1.187 (2.519) data 0.000 (0.421) loss 0.3725 (0.4221) acc 87.5000 (85.6250) lr 1.8838e-02 eta 2:06:44\n",
      "epoch [33/200] batch [10/18] time 1.159 (1.840) data 0.000 (0.210) loss 0.5096 (0.4572) acc 84.3750 (85.0000) lr 1.8838e-02 eta 1:32:26\n",
      "epoch [33/200] batch [15/18] time 1.146 (1.612) data 0.000 (0.140) loss 0.5160 (0.4536) acc 90.6250 (86.0417) lr 1.8838e-02 eta 1:20:51\n",
      "epoch [34/200] batch [5/18] time 1.158 (1.968) data 0.000 (0.434) loss 0.4580 (0.4599) acc 84.3750 (84.3750) lr 1.8763e-02 eta 1:38:25\n",
      "epoch [34/200] batch [10/18] time 1.233 (1.583) data 0.000 (0.217) loss 0.6731 (0.4751) acc 75.0000 (85.3125) lr 1.8763e-02 eta 1:19:03\n",
      "epoch [34/200] batch [15/18] time 1.162 (1.452) data 0.000 (0.145) loss 0.8606 (0.4924) acc 75.0000 (85.0000) lr 1.8763e-02 eta 1:12:23\n",
      "epoch [35/200] batch [5/18] time 1.182 (2.108) data 0.000 (0.367) loss 0.3712 (0.5061) acc 90.6250 (84.3750) lr 1.8686e-02 eta 1:44:48\n",
      "epoch [35/200] batch [10/18] time 1.187 (1.657) data 0.000 (0.184) loss 0.5703 (0.5067) acc 78.1250 (84.0625) lr 1.8686e-02 eta 1:22:15\n",
      "epoch [35/200] batch [15/18] time 1.191 (1.501) data 0.000 (0.123) loss 0.3499 (0.4624) acc 93.7500 (85.4167) lr 1.8686e-02 eta 1:14:22\n",
      "epoch [36/200] batch [5/18] time 1.156 (2.685) data 0.000 (0.728) loss 0.5262 (0.4749) acc 78.1250 (85.6250) lr 1.8607e-02 eta 2:12:41\n",
      "epoch [36/200] batch [10/18] time 1.179 (1.925) data 0.000 (0.364) loss 0.4060 (0.5042) acc 87.5000 (85.0000) lr 1.8607e-02 eta 1:34:58\n",
      "epoch [36/200] batch [15/18] time 1.207 (1.677) data 0.000 (0.243) loss 0.3622 (0.5022) acc 93.7500 (83.7500) lr 1.8607e-02 eta 1:22:35\n",
      "epoch [37/200] batch [5/18] time 1.155 (1.975) data 0.000 (0.429) loss 0.5072 (0.4229) acc 84.3750 (86.2500) lr 1.8526e-02 eta 1:36:59\n",
      "epoch [37/200] batch [10/18] time 1.168 (1.577) data 0.000 (0.215) loss 0.5024 (0.4839) acc 84.3750 (84.6875) lr 1.8526e-02 eta 1:17:20\n",
      "epoch [37/200] batch [15/18] time 1.155 (1.441) data 0.000 (0.143) loss 0.2108 (0.4843) acc 90.6250 (84.1667) lr 1.8526e-02 eta 1:10:33\n",
      "epoch [38/200] batch [5/18] time 1.212 (2.045) data 0.000 (0.348) loss 0.4435 (0.4767) acc 84.3750 (86.2500) lr 1.8443e-02 eta 1:39:48\n",
      "epoch [38/200] batch [10/18] time 1.165 (1.626) data 0.000 (0.174) loss 0.5108 (0.4464) acc 84.3750 (86.5625) lr 1.8443e-02 eta 1:19:15\n",
      "epoch [38/200] batch [15/18] time 1.161 (1.470) data 0.000 (0.116) loss 0.7189 (0.4514) acc 78.1250 (86.4583) lr 1.8443e-02 eta 1:11:31\n",
      "epoch [39/200] batch [5/18] time 1.163 (2.710) data 0.000 (0.632) loss 0.4109 (0.3949) acc 87.5000 (87.5000) lr 1.8358e-02 eta 2:11:28\n",
      "epoch [39/200] batch [10/18] time 1.170 (1.934) data 0.000 (0.316) loss 0.4453 (0.4693) acc 90.6250 (87.1875) lr 1.8358e-02 eta 1:33:40\n",
      "epoch [39/200] batch [15/18] time 1.170 (1.674) data 0.000 (0.211) loss 0.3354 (0.4555) acc 90.6250 (86.6667) lr 1.8358e-02 eta 1:20:56\n",
      "epoch [40/200] batch [5/18] time 1.153 (1.961) data 0.000 (0.416) loss 0.2728 (0.3785) acc 87.5000 (86.8750) lr 1.8271e-02 eta 1:34:31\n",
      "epoch [40/200] batch [10/18] time 1.166 (1.562) data 0.000 (0.208) loss 0.3728 (0.4135) acc 90.6250 (86.8750) lr 1.8271e-02 eta 1:15:11\n",
      "epoch [40/200] batch [15/18] time 1.171 (1.445) data 0.000 (0.139) loss 0.6881 (0.4619) acc 84.3750 (86.2500) lr 1.8271e-02 eta 1:09:26\n",
      "epoch [41/200] batch [5/18] time 1.501 (2.508) data 0.000 (0.361) loss 0.4853 (0.4267) acc 84.3750 (86.2500) lr 1.8181e-02 eta 2:00:11\n",
      "epoch [41/200] batch [10/18] time 1.194 (1.853) data 0.000 (0.181) loss 0.5034 (0.4714) acc 78.1250 (85.0000) lr 1.8181e-02 eta 1:28:37\n",
      "epoch [41/200] batch [15/18] time 1.157 (1.639) data 0.000 (0.121) loss 0.6375 (0.5064) acc 81.2500 (84.5833) lr 1.8181e-02 eta 1:18:14\n",
      "epoch [42/200] batch [5/18] time 1.168 (2.008) data 0.000 (0.439) loss 0.4034 (0.4117) acc 87.5000 (87.5000) lr 1.8090e-02 eta 1:35:37\n",
      "epoch [42/200] batch [10/18] time 1.163 (1.608) data 0.000 (0.220) loss 0.4990 (0.4203) acc 90.6250 (87.8125) lr 1.8090e-02 eta 1:16:25\n",
      "epoch [42/200] batch [15/18] time 1.156 (1.458) data 0.000 (0.146) loss 0.4664 (0.4568) acc 87.5000 (86.2500) lr 1.8090e-02 eta 1:09:10\n",
      "epoch [43/200] batch [5/18] time 1.163 (2.754) data 0.000 (0.768) loss 0.3150 (0.3482) acc 93.7500 (90.6250) lr 1.7997e-02 eta 2:10:18\n",
      "epoch [43/200] batch [10/18] time 1.163 (1.953) data 0.000 (0.384) loss 0.5525 (0.4168) acc 90.6250 (86.8750) lr 1.7997e-02 eta 1:32:15\n",
      "epoch [43/200] batch [15/18] time 1.216 (1.694) data 0.000 (0.256) loss 0.4818 (0.4180) acc 84.3750 (86.6667) lr 1.7997e-02 eta 1:19:52\n",
      "epoch [44/200] batch [5/18] time 1.153 (1.998) data 0.000 (0.437) loss 0.5982 (0.4433) acc 81.2500 (87.5000) lr 1.7902e-02 eta 1:33:54\n",
      "epoch [44/200] batch [10/18] time 1.168 (1.585) data 0.000 (0.219) loss 0.5000 (0.4318) acc 84.3750 (86.5625) lr 1.7902e-02 eta 1:14:22\n",
      "epoch [44/200] batch [15/18] time 1.159 (1.450) data 0.000 (0.146) loss 0.1653 (0.4297) acc 93.7500 (86.8750) lr 1.7902e-02 eta 1:07:55\n",
      "epoch [45/200] batch [5/18] time 1.228 (2.037) data 0.001 (0.434) loss 0.3816 (0.4381) acc 87.5000 (85.6250) lr 1.7804e-02 eta 1:35:10\n",
      "epoch [45/200] batch [10/18] time 1.149 (1.611) data 0.000 (0.217) loss 0.5617 (0.4599) acc 81.2500 (85.6250) lr 1.7804e-02 eta 1:15:08\n",
      "epoch [45/200] batch [15/18] time 1.172 (1.463) data 0.000 (0.145) loss 0.4365 (0.4422) acc 90.6250 (86.0417) lr 1.7804e-02 eta 1:08:05\n",
      "epoch [46/200] batch [5/18] time 1.167 (2.728) data 0.000 (0.770) loss 0.4688 (0.5868) acc 84.3750 (85.0000) lr 1.7705e-02 eta 2:06:37\n",
      "epoch [46/200] batch [10/18] time 1.155 (1.940) data 0.000 (0.385) loss 0.2931 (0.5059) acc 93.7500 (85.6250) lr 1.7705e-02 eta 1:29:52\n",
      "epoch [46/200] batch [15/18] time 1.193 (1.684) data 0.000 (0.257) loss 0.2518 (0.4793) acc 93.7500 (86.0417) lr 1.7705e-02 eta 1:17:53\n",
      "epoch [47/200] batch [5/18] time 1.156 (1.974) data 0.000 (0.341) loss 0.2702 (0.3943) acc 93.7500 (85.6250) lr 1.7604e-02 eta 1:31:03\n",
      "epoch [47/200] batch [10/18] time 1.179 (1.575) data 0.000 (0.170) loss 0.3395 (0.3781) acc 87.5000 (87.8125) lr 1.7604e-02 eta 1:12:30\n",
      "epoch [47/200] batch [15/18] time 1.196 (1.443) data 0.000 (0.114) loss 0.3723 (0.4279) acc 84.3750 (85.6250) lr 1.7604e-02 eta 1:06:18\n",
      "epoch [48/200] batch [5/18] time 1.182 (1.957) data 0.000 (0.405) loss 0.2885 (0.3388) acc 87.5000 (89.3750) lr 1.7501e-02 eta 1:29:40\n",
      "epoch [48/200] batch [10/18] time 1.185 (1.573) data 0.000 (0.202) loss 0.2162 (0.3763) acc 96.8750 (88.4375) lr 1.7501e-02 eta 1:11:56\n",
      "epoch [48/200] batch [15/18] time 1.156 (1.440) data 0.000 (0.135) loss 0.7119 (0.4370) acc 84.3750 (86.8750) lr 1.7501e-02 eta 1:05:44\n",
      "epoch [49/200] batch [5/18] time 1.193 (2.472) data 0.000 (0.457) loss 0.6235 (0.4356) acc 81.2500 (87.5000) lr 1.7396e-02 eta 1:52:29\n",
      "epoch [49/200] batch [10/18] time 1.161 (1.812) data 0.000 (0.229) loss 0.3013 (0.3962) acc 93.7500 (89.3750) lr 1.7396e-02 eta 1:22:19\n",
      "epoch [49/200] batch [15/18] time 1.166 (1.596) data 0.000 (0.152) loss 0.5006 (0.4069) acc 87.5000 (88.5417) lr 1.7396e-02 eta 1:12:22\n",
      "epoch [50/200] batch [5/18] time 1.159 (1.954) data 0.000 (0.379) loss 0.5190 (0.5636) acc 84.3750 (81.2500) lr 1.7290e-02 eta 1:28:21\n",
      "epoch [50/200] batch [10/18] time 1.165 (1.554) data 0.000 (0.190) loss 0.8215 (0.5244) acc 71.8750 (82.8125) lr 1.7290e-02 eta 1:10:08\n",
      "epoch [50/200] batch [15/18] time 1.159 (1.427) data 0.000 (0.127) loss 0.5435 (0.4690) acc 78.1250 (85.0000) lr 1.7290e-02 eta 1:04:17\n",
      "epoch [51/200] batch [5/18] time 1.182 (2.009) data 0.000 (0.443) loss 0.2729 (0.3375) acc 93.7500 (90.6250) lr 1.7181e-02 eta 1:30:15\n",
      "epoch [51/200] batch [10/18] time 1.210 (1.608) data 0.001 (0.222) loss 0.4204 (0.3962) acc 87.5000 (87.1875) lr 1.7181e-02 eta 1:12:06\n",
      "epoch [51/200] batch [15/18] time 1.183 (1.460) data 0.000 (0.148) loss 0.3014 (0.4130) acc 90.6250 (87.2917) lr 1.7181e-02 eta 1:05:19\n",
      "epoch [52/200] batch [5/18] time 1.168 (2.776) data 0.000 (0.748) loss 0.2726 (0.3664) acc 93.7500 (88.7500) lr 1.7071e-02 eta 2:03:51\n",
      "epoch [52/200] batch [10/18] time 1.151 (1.964) data 0.000 (0.374) loss 0.6692 (0.4055) acc 84.3750 (88.4375) lr 1.7071e-02 eta 1:27:28\n",
      "epoch [52/200] batch [15/18] time 1.175 (1.704) data 0.000 (0.250) loss 0.4405 (0.3988) acc 87.5000 (88.9583) lr 1.7071e-02 eta 1:15:45\n",
      "epoch [53/200] batch [5/18] time 1.175 (1.963) data 0.000 (0.414) loss 0.5027 (0.4836) acc 81.2500 (84.3750) lr 1.6959e-02 eta 1:26:59\n",
      "epoch [53/200] batch [10/18] time 1.475 (1.639) data 0.000 (0.207) loss 0.3110 (0.5090) acc 90.6250 (84.0625) lr 1.6959e-02 eta 1:12:29\n",
      "epoch [53/200] batch [15/18] time 1.196 (1.508) data 0.000 (0.138) loss 0.2698 (0.4644) acc 90.6250 (85.2083) lr 1.6959e-02 eta 1:06:33\n",
      "epoch [54/200] batch [5/18] time 1.175 (1.952) data 0.000 (0.443) loss 0.4201 (0.4068) acc 84.3750 (87.5000) lr 1.6845e-02 eta 1:25:54\n",
      "epoch [54/200] batch [10/18] time 1.167 (1.570) data 0.000 (0.222) loss 0.6347 (0.4328) acc 87.5000 (87.5000) lr 1.6845e-02 eta 1:08:59\n",
      "epoch [54/200] batch [15/18] time 1.145 (1.438) data 0.000 (0.148) loss 0.1756 (0.4081) acc 96.8750 (88.3333) lr 1.6845e-02 eta 1:03:04\n",
      "epoch [55/200] batch [5/18] time 1.226 (2.032) data 0.000 (0.400) loss 0.6192 (0.4917) acc 84.3750 (84.3750) lr 1.6730e-02 eta 1:28:50\n",
      "epoch [55/200] batch [10/18] time 1.116 (1.607) data 0.000 (0.200) loss 0.6561 (0.4750) acc 75.0000 (85.9375) lr 1.6730e-02 eta 1:10:06\n",
      "epoch [55/200] batch [15/18] time 1.161 (1.457) data 0.000 (0.134) loss 0.3567 (0.4682) acc 93.7500 (85.8333) lr 1.6730e-02 eta 1:03:26\n",
      "epoch [56/200] batch [5/18] time 1.156 (2.726) data 0.000 (0.654) loss 0.3622 (0.4425) acc 87.5000 (86.2500) lr 1.6613e-02 eta 1:58:21\n",
      "epoch [56/200] batch [10/18] time 1.163 (1.942) data 0.000 (0.327) loss 0.4718 (0.4409) acc 84.3750 (86.2500) lr 1.6613e-02 eta 1:24:08\n",
      "epoch [56/200] batch [15/18] time 1.216 (1.685) data 0.001 (0.218) loss 0.2857 (0.4408) acc 93.7500 (86.8750) lr 1.6613e-02 eta 1:12:53\n",
      "epoch [57/200] batch [5/18] time 1.153 (1.949) data 0.000 (0.452) loss 0.5972 (0.4896) acc 84.3750 (85.0000) lr 1.6494e-02 eta 1:24:02\n",
      "epoch [57/200] batch [10/18] time 1.182 (1.556) data 0.000 (0.226) loss 0.4406 (0.5323) acc 87.5000 (83.7500) lr 1.6494e-02 eta 1:06:57\n",
      "epoch [57/200] batch [15/18] time 1.151 (1.437) data 0.000 (0.151) loss 0.4399 (0.4956) acc 84.3750 (85.4167) lr 1.6494e-02 eta 1:01:43\n",
      "epoch [58/200] batch [5/18] time 1.208 (2.023) data 0.000 (0.392) loss 0.4884 (0.4943) acc 81.2500 (83.7500) lr 1.6374e-02 eta 1:26:38\n",
      "epoch [58/200] batch [10/18] time 1.178 (1.612) data 0.000 (0.196) loss 0.3919 (0.4513) acc 90.6250 (86.5625) lr 1.6374e-02 eta 1:08:52\n",
      "epoch [58/200] batch [15/18] time 1.175 (1.468) data 0.000 (0.131) loss 0.3731 (0.4218) acc 90.6250 (87.7083) lr 1.6374e-02 eta 1:02:37\n",
      "epoch [59/200] batch [5/18] time 1.155 (2.764) data 0.000 (0.731) loss 0.3662 (0.4422) acc 90.6250 (88.7500) lr 1.6252e-02 eta 1:57:32\n",
      "epoch [59/200] batch [10/18] time 1.158 (1.961) data 0.000 (0.366) loss 0.2464 (0.4257) acc 93.7500 (87.8125) lr 1.6252e-02 eta 1:23:11\n",
      "epoch [59/200] batch [15/18] time 1.194 (1.692) data 0.000 (0.244) loss 0.6527 (0.4249) acc 68.7500 (87.0833) lr 1.6252e-02 eta 1:11:38\n",
      "epoch [60/200] batch [5/18] time 1.169 (2.007) data 0.000 (0.412) loss 0.3791 (0.4252) acc 90.6250 (89.3750) lr 1.6129e-02 eta 1:24:42\n",
      "epoch [60/200] batch [10/18] time 1.214 (1.593) data 0.000 (0.206) loss 0.4899 (0.4670) acc 81.2500 (87.5000) lr 1.6129e-02 eta 1:07:05\n",
      "epoch [60/200] batch [15/18] time 1.160 (1.456) data 0.000 (0.138) loss 0.3588 (0.4723) acc 90.6250 (86.2500) lr 1.6129e-02 eta 1:01:12\n",
      "epoch [61/200] batch [5/18] time 1.142 (1.936) data 0.000 (0.434) loss 0.3861 (0.4965) acc 87.5000 (83.1250) lr 1.6004e-02 eta 1:21:08\n",
      "epoch [61/200] batch [10/18] time 1.189 (1.561) data 0.000 (0.217) loss 0.7353 (0.4443) acc 75.0000 (85.6250) lr 1.6004e-02 eta 1:05:18\n",
      "epoch [61/200] batch [15/18] time 1.153 (1.428) data 0.000 (0.145) loss 0.3787 (0.4490) acc 96.8750 (86.4583) lr 1.6004e-02 eta 0:59:38\n",
      "epoch [62/200] batch [5/18] time 1.172 (2.361) data 0.000 (0.385) loss 0.2287 (0.3437) acc 96.8750 (90.6250) lr 1.5878e-02 eta 1:38:15\n",
      "epoch [62/200] batch [10/18] time 1.140 (1.757) data 0.000 (0.193) loss 0.2352 (0.3719) acc 93.7500 (89.0625) lr 1.5878e-02 eta 1:12:57\n",
      "epoch [62/200] batch [15/18] time 1.175 (1.558) data 0.000 (0.129) loss 0.4087 (0.3996) acc 81.2500 (88.1250) lr 1.5878e-02 eta 1:04:34\n",
      "epoch [63/200] batch [5/18] time 1.151 (1.947) data 0.000 (0.367) loss 0.7002 (0.4868) acc 81.2500 (86.8750) lr 1.5750e-02 eta 1:20:26\n",
      "epoch [63/200] batch [10/18] time 1.195 (1.559) data 0.000 (0.184) loss 0.6196 (0.4449) acc 75.0000 (87.1875) lr 1.5750e-02 eta 1:04:16\n",
      "epoch [63/200] batch [15/18] time 1.212 (1.433) data 0.000 (0.123) loss 0.5861 (0.4543) acc 84.3750 (86.0417) lr 1.5750e-02 eta 0:58:58\n",
      "epoch [64/200] batch [5/18] time 1.184 (1.948) data 0.000 (0.400) loss 0.1970 (0.3019) acc 96.8750 (91.2500) lr 1.5621e-02 eta 1:19:53\n",
      "epoch [64/200] batch [10/18] time 1.199 (1.573) data 0.000 (0.200) loss 0.1896 (0.3434) acc 96.8750 (89.6875) lr 1.5621e-02 eta 1:04:23\n",
      "epoch [64/200] batch [15/18] time 1.150 (1.434) data 0.000 (0.134) loss 0.5187 (0.4205) acc 84.3750 (86.8750) lr 1.5621e-02 eta 0:58:35\n",
      "epoch [65/200] batch [5/18] time 1.228 (2.391) data 0.000 (0.432) loss 0.2341 (0.4193) acc 93.7500 (85.0000) lr 1.5490e-02 eta 1:37:20\n",
      "epoch [65/200] batch [10/18] time 1.108 (1.772) data 0.000 (0.216) loss 0.1591 (0.4057) acc 93.7500 (88.1250) lr 1.5490e-02 eta 1:12:00\n",
      "epoch [65/200] batch [15/18] time 1.277 (1.587) data 0.000 (0.144) loss 0.4894 (0.4074) acc 84.3750 (87.5000) lr 1.5490e-02 eta 1:04:20\n",
      "epoch [66/200] batch [5/18] time 1.165 (2.745) data 0.000 (0.755) loss 0.3866 (0.2886) acc 87.5000 (91.8750) lr 1.5358e-02 eta 1:50:56\n",
      "epoch [66/200] batch [10/18] time 1.164 (1.951) data 0.000 (0.378) loss 0.4609 (0.3967) acc 87.5000 (88.1250) lr 1.5358e-02 eta 1:18:41\n",
      "epoch [66/200] batch [15/18] time 1.203 (1.695) data 0.000 (0.252) loss 0.3224 (0.3887) acc 93.7500 (88.5417) lr 1.5358e-02 eta 1:08:13\n",
      "epoch [67/200] batch [5/18] time 1.136 (1.989) data 0.000 (0.426) loss 0.2588 (0.4711) acc 96.8750 (88.7500) lr 1.5225e-02 eta 1:19:46\n",
      "epoch [67/200] batch [10/18] time 1.182 (1.580) data 0.000 (0.213) loss 0.1942 (0.4255) acc 96.8750 (89.6875) lr 1.5225e-02 eta 1:03:15\n",
      "epoch [67/200] batch [15/18] time 1.151 (1.448) data 0.000 (0.142) loss 0.3576 (0.4028) acc 93.7500 (90.8333) lr 1.5225e-02 eta 0:57:50\n",
      "epoch [68/200] batch [5/18] time 1.226 (2.013) data 0.000 (0.440) loss 0.4980 (0.4199) acc 87.5000 (88.7500) lr 1.5090e-02 eta 1:20:10\n",
      "epoch [68/200] batch [10/18] time 1.145 (1.612) data 0.000 (0.220) loss 0.2056 (0.4220) acc 96.8750 (88.7500) lr 1.5090e-02 eta 1:04:03\n",
      "epoch [68/200] batch [15/18] time 1.151 (1.460) data 0.000 (0.147) loss 0.2439 (0.4058) acc 93.7500 (88.7500) lr 1.5090e-02 eta 0:57:52\n",
      "epoch [69/200] batch [5/18] time 1.164 (2.733) data 0.000 (0.648) loss 0.6593 (0.3873) acc 78.1250 (88.1250) lr 1.4955e-02 eta 1:48:01\n",
      "epoch [69/200] batch [10/18] time 1.145 (1.948) data 0.000 (0.324) loss 0.7549 (0.4673) acc 78.1250 (85.9375) lr 1.4955e-02 eta 1:16:48\n",
      "epoch [69/200] batch [15/18] time 1.238 (1.717) data 0.000 (0.216) loss 0.4948 (0.4915) acc 90.6250 (85.8333) lr 1.4955e-02 eta 1:07:32\n",
      "epoch [70/200] batch [5/18] time 1.205 (2.030) data 0.000 (0.465) loss 0.5899 (0.4342) acc 84.3750 (90.6250) lr 1.4818e-02 eta 1:19:37\n",
      "epoch [70/200] batch [10/18] time 1.177 (1.610) data 0.000 (0.233) loss 0.5597 (0.4252) acc 87.5000 (89.0625) lr 1.4818e-02 eta 1:03:00\n",
      "epoch [70/200] batch [15/18] time 1.181 (1.466) data 0.000 (0.155) loss 0.2705 (0.3840) acc 93.7500 (90.2083) lr 1.4818e-02 eta 0:57:15\n",
      "epoch [71/200] batch [5/18] time 1.182 (1.936) data 0.000 (0.440) loss 0.2963 (0.4001) acc 93.7500 (90.0000) lr 1.4679e-02 eta 1:15:21\n",
      "epoch [71/200] batch [10/18] time 1.193 (1.575) data 0.000 (0.220) loss 0.2908 (0.4331) acc 87.5000 (86.8750) lr 1.4679e-02 eta 1:01:09\n",
      "epoch [71/200] batch [15/18] time 1.148 (1.432) data 0.000 (0.147) loss 0.2388 (0.4016) acc 96.8750 (88.1250) lr 1.4679e-02 eta 0:55:29\n",
      "epoch [72/200] batch [5/18] time 1.157 (2.730) data 0.000 (0.813) loss 0.4769 (0.3389) acc 87.5000 (90.0000) lr 1.4540e-02 eta 1:45:25\n",
      "epoch [72/200] batch [10/18] time 1.137 (1.938) data 0.000 (0.407) loss 0.5208 (0.3699) acc 87.5000 (90.0000) lr 1.4540e-02 eta 1:14:40\n",
      "epoch [72/200] batch [15/18] time 1.167 (1.685) data 0.000 (0.271) loss 0.5347 (0.3568) acc 78.1250 (89.5833) lr 1.4540e-02 eta 1:04:46\n",
      "epoch [73/200] batch [5/18] time 1.157 (1.927) data 0.000 (0.355) loss 0.6313 (0.3628) acc 78.1250 (91.2500) lr 1.4399e-02 eta 1:13:50\n",
      "epoch [73/200] batch [10/18] time 1.166 (1.541) data 0.000 (0.178) loss 0.5201 (0.3423) acc 84.3750 (90.6250) lr 1.4399e-02 eta 0:58:55\n",
      "epoch [73/200] batch [15/18] time 1.174 (1.420) data 0.000 (0.119) loss 0.6993 (0.3838) acc 78.1250 (88.9583) lr 1.4399e-02 eta 0:54:09\n",
      "epoch [74/200] batch [5/18] time 1.171 (1.961) data 0.000 (0.394) loss 0.2777 (0.2916) acc 87.5000 (91.2500) lr 1.4258e-02 eta 1:14:32\n",
      "epoch [74/200] batch [10/18] time 1.207 (1.571) data 0.000 (0.197) loss 0.5444 (0.3409) acc 84.3750 (90.9375) lr 1.4258e-02 eta 0:59:35\n",
      "epoch [74/200] batch [15/18] time 1.136 (1.434) data 0.000 (0.131) loss 0.5904 (0.3808) acc 78.1250 (88.9583) lr 1.4258e-02 eta 0:54:17\n",
      "epoch [75/200] batch [5/18] time 1.150 (2.582) data 0.000 (0.610) loss 0.3438 (0.3753) acc 87.5000 (88.7500) lr 1.4115e-02 eta 1:37:23\n",
      "epoch [75/200] batch [10/18] time 1.141 (1.866) data 0.000 (0.305) loss 0.2044 (0.3544) acc 93.7500 (88.7500) lr 1.4115e-02 eta 1:10:13\n",
      "epoch [75/200] batch [15/18] time 1.168 (1.627) data 0.000 (0.203) loss 0.3273 (0.3693) acc 87.5000 (88.7500) lr 1.4115e-02 eta 1:01:05\n",
      "epoch [76/200] batch [5/18] time 1.148 (1.937) data 0.000 (0.389) loss 0.2999 (0.3269) acc 90.6250 (88.7500) lr 1.3971e-02 eta 1:12:27\n",
      "epoch [76/200] batch [10/18] time 1.229 (1.557) data 0.001 (0.195) loss 0.2305 (0.3476) acc 96.8750 (89.0625) lr 1.3971e-02 eta 0:58:08\n",
      "epoch [76/200] batch [15/18] time 1.158 (1.433) data 0.000 (0.130) loss 0.5267 (0.3533) acc 75.0000 (88.9583) lr 1.3971e-02 eta 0:53:22\n",
      "epoch [77/200] batch [5/18] time 1.183 (2.013) data 0.000 (0.422) loss 0.3433 (0.3587) acc 90.6250 (90.0000) lr 1.3827e-02 eta 1:14:41\n",
      "epoch [77/200] batch [10/18] time 1.119 (1.608) data 0.000 (0.211) loss 0.2764 (0.3835) acc 93.7500 (88.7500) lr 1.3827e-02 eta 0:59:31\n",
      "epoch [77/200] batch [15/18] time 1.221 (1.464) data 0.000 (0.141) loss 0.4492 (0.3995) acc 84.3750 (87.5000) lr 1.3827e-02 eta 0:54:06\n",
      "epoch [78/200] batch [5/18] time 1.163 (2.734) data 0.000 (0.750) loss 0.3981 (0.4686) acc 84.3750 (85.6250) lr 1.3681e-02 eta 1:40:40\n",
      "epoch [78/200] batch [10/18] time 1.155 (1.951) data 0.000 (0.375) loss 0.2480 (0.4147) acc 93.7500 (88.1250) lr 1.3681e-02 eta 1:11:39\n",
      "epoch [78/200] batch [15/18] time 1.181 (1.689) data 0.000 (0.250) loss 0.3331 (0.4279) acc 87.5000 (87.2917) lr 1.3681e-02 eta 1:01:54\n",
      "epoch [79/200] batch [5/18] time 1.139 (1.935) data 0.000 (0.447) loss 0.4845 (0.3471) acc 84.3750 (91.8750) lr 1.3535e-02 eta 1:10:39\n",
      "epoch [79/200] batch [10/18] time 1.193 (1.551) data 0.000 (0.224) loss 0.2558 (0.3522) acc 93.7500 (91.8750) lr 1.3535e-02 eta 0:56:29\n",
      "epoch [79/200] batch [15/18] time 1.207 (1.433) data 0.000 (0.149) loss 0.3151 (0.3369) acc 90.6250 (91.6667) lr 1.3535e-02 eta 0:52:05\n",
      "epoch [80/200] batch [5/18] time 1.169 (1.964) data 0.000 (0.395) loss 0.4191 (0.3045) acc 87.5000 (92.5000) lr 1.3387e-02 eta 1:11:07\n",
      "epoch [80/200] batch [10/18] time 1.202 (1.582) data 0.001 (0.198) loss 0.4243 (0.3759) acc 87.5000 (90.6250) lr 1.3387e-02 eta 0:57:10\n",
      "epoch [80/200] batch [15/18] time 1.159 (1.441) data 0.000 (0.132) loss 0.3680 (0.4023) acc 90.6250 (90.0000) lr 1.3387e-02 eta 0:51:57\n",
      "epoch [81/200] batch [5/18] time 1.172 (2.622) data 0.000 (0.654) loss 0.5194 (0.4222) acc 87.5000 (89.3750) lr 1.3239e-02 eta 1:34:09\n",
      "epoch [81/200] batch [10/18] time 1.161 (1.889) data 0.000 (0.327) loss 0.4564 (0.4317) acc 81.2500 (86.8750) lr 1.3239e-02 eta 1:07:41\n",
      "epoch [81/200] batch [15/18] time 1.210 (1.649) data 0.000 (0.218) loss 0.3618 (0.4362) acc 87.5000 (86.4583) lr 1.3239e-02 eta 0:58:56\n",
      "epoch [82/200] batch [5/18] time 1.187 (2.035) data 0.000 (0.456) loss 0.2759 (0.3455) acc 93.7500 (89.3750) lr 1.3090e-02 eta 1:12:29\n",
      "epoch [82/200] batch [10/18] time 1.195 (1.607) data 0.000 (0.228) loss 0.4047 (0.3982) acc 87.5000 (88.4375) lr 1.3090e-02 eta 0:57:06\n",
      "epoch [82/200] batch [15/18] time 1.170 (1.468) data 0.000 (0.152) loss 0.6523 (0.4143) acc 75.0000 (87.2917) lr 1.3090e-02 eta 0:52:02\n",
      "epoch [83/200] batch [5/18] time 1.225 (2.104) data 0.000 (0.432) loss 0.4249 (0.2993) acc 87.5000 (90.6250) lr 1.2940e-02 eta 1:14:18\n",
      "epoch [83/200] batch [10/18] time 1.163 (1.643) data 0.000 (0.216) loss 0.5631 (0.3890) acc 87.5000 (89.0625) lr 1.2940e-02 eta 0:57:53\n",
      "epoch [83/200] batch [15/18] time 1.121 (1.475) data 0.000 (0.144) loss 0.7477 (0.3808) acc 78.1250 (89.7917) lr 1.2940e-02 eta 0:51:51\n",
      "epoch [84/200] batch [5/18] time 1.126 (2.725) data 0.001 (0.684) loss 0.3006 (0.4220) acc 90.6250 (86.2500) lr 1.2790e-02 eta 1:35:24\n",
      "epoch [84/200] batch [10/18] time 1.139 (1.938) data 0.000 (0.342) loss 0.4243 (0.3738) acc 87.5000 (88.4375) lr 1.2790e-02 eta 1:07:41\n",
      "epoch [84/200] batch [15/18] time 1.210 (1.680) data 0.000 (0.228) loss 0.5873 (0.4238) acc 81.2500 (87.9167) lr 1.2790e-02 eta 0:58:32\n",
      "epoch [85/200] batch [5/18] time 1.137 (1.952) data 0.000 (0.366) loss 0.2559 (0.3205) acc 93.7500 (91.2500) lr 1.2639e-02 eta 1:07:47\n",
      "epoch [85/200] batch [10/18] time 1.193 (1.570) data 0.000 (0.183) loss 0.3729 (0.3712) acc 87.5000 (89.6875) lr 1.2639e-02 eta 0:54:22\n",
      "epoch [85/200] batch [15/18] time 1.170 (1.442) data 0.000 (0.122) loss 0.1723 (0.3471) acc 93.7500 (90.6250) lr 1.2639e-02 eta 0:49:48\n"
     ]
    }
   ],
   "source": [
    "#oxford_pets-16shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "        --output-dir output/new_init/oxford_pets/DAPT/vit_b16_16shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzxpuId2ogGU"
   },
   "outputs": [],
   "source": [
    "#oxford_pets-8shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
    "        --output-dir output/new_init/oxford_pets/DAPT/vit_b16_8shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNVmrEw9oiyP"
   },
   "outputs": [],
   "source": [
    "#oxford_pets-4shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/new_init/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2I0an_PoqYU"
   },
   "outputs": [],
   "source": [
    "#oxford_pets-2shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
    "        --output-dir output/new_init/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGdx9RbdouY1"
   },
   "outputs": [],
   "source": [
    "#oxford_pets-2shots-seed1\n",
    "!python train.py \\\n",
    "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
    "        --seed 1 \\\n",
    "        --trainer DAPT \\\n",
    "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
    "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
    "        --output-dir output/new_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
    "        DATASET.NUM_SHOTS 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
