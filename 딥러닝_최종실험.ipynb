{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_Fu7pB5A68Z",
        "outputId": "a0151124-9254-4f55-e7a8-57098adc9c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "%cd \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c0ot1hSRqdqH",
        "outputId": "ef890a5f-9978-4136-f0aa-6e0b57befa08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huc--Bv8CWwn",
        "outputId": "63391fe0-52ca-4a8a-facd-4a23ee6df0fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/DAPT\n"
          ]
        }
      ],
      "source": [
        "cd \"DAPT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BuAnHxxTBHYw"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/KaiyangZhou/Dassl.pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDrjvvKVadl0",
        "outputId": "818037cc-b453-45c6-ce15-80ebaf51e038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/DAPT/Dassl.pytorch\n"
          ]
        }
      ],
      "source": [
        " cd \"./Dassl.pytorch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCx7c6eHCwUg",
        "outputId": "5b69a59e-87db-48a7-fd0c-63ac7957c384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flake8==3.7.9 (from -r requirements.txt (line 1))\n",
            "  Downloading flake8-3.7.9-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting yapf==0.29.0 (from -r requirements.txt (line 2))\n",
            "  Downloading yapf-0.29.0-py2.py3-none-any.whl.metadata (30 kB)\n",
            "Collecting isort==4.3.21 (from -r requirements.txt (line 3))\n",
            "  Downloading isort-4.3.21-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting yacs (from -r requirements.txt (line 4))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (5.2.0)\n",
            "Collecting tb-nightly (from -r requirements.txt (line 6))\n",
            "  Downloading tb_nightly-2.19.0a20241207-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.66.6)\n",
            "Collecting ftfy (from -r requirements.txt (line 11))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2024.9.11)\n",
            "Collecting wilds==1.2.2 (from -r requirements.txt (line 13))\n",
            "  Downloading wilds-1.2.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.9.0)\n",
            "Collecting entrypoints<0.4.0,>=0.3.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading entrypoints-0.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyflakes<2.2.0,>=2.1.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading pyflakes-2.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pycodestyle<2.6.0,>=2.5.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading pycodestyle-2.5.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting mccabe<0.7.0,>=0.6.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.26.4)\n",
            "Collecting ogb>=1.2.6 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting outdated>=0.2.0 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.2.2)\n",
            "Requirement already satisfied: pillow>=7.2.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (11.0.0)\n",
            "Requirement already satisfied: pytz>=2020.4 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2024.2)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.20.1+cu121)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (3.1.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements.txt (line 11)) (0.2.13)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (2.2.3)\n",
            "Collecting littleutils (from outdated>=0.2.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2024.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 5)) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (1.7.1)\n",
            "Downloading flake8-3.7.9-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yapf-0.29.0-py2.py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.3/185.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wilds-1.2.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading tb_nightly-2.19.0a20241207-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
            "Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: yapf, mccabe, yacs, pyflakes, pycodestyle, littleutils, isort, ftfy, entrypoints, tb-nightly, outdated, flake8, ogb, wilds\n",
            "  Attempting uninstall: entrypoints\n",
            "    Found existing installation: entrypoints 0.4\n",
            "    Uninstalling entrypoints-0.4:\n",
            "      Successfully uninstalled entrypoints-0.4\n",
            "Successfully installed entrypoints-0.3 flake8-3.7.9 ftfy-6.3.1 isort-4.3.21 littleutils-0.2.4 mccabe-0.6.1 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.5.0 pyflakes-2.1.1 tb-nightly-2.19.0a20241207 wilds-1.2.2 yacs-0.1.8 yapf-0.29.0\n",
            "running develop\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "writing dassl.egg-info/PKG-INFO\n",
            "writing dependency_links to dassl.egg-info/dependency_links.txt\n",
            "writing requirements to dassl.egg-info/requires.txt\n",
            "writing top-level names to dassl.egg-info/top_level.txt\n",
            "reading manifest file 'dassl.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'dassl.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.10/dist-packages/dassl.egg-link (link to .)\n",
            "Adding dassl 0.6.3 to easy-install.pth file\n",
            "\n",
            "Installed /content/drive/My Drive/DAPT/Dassl.pytorch\n",
            "Processing dependencies for dassl==0.6.3\n",
            "Searching for tabulate==0.9.0\n",
            "Best match: tabulate 0.9.0\n",
            "Adding tabulate 0.9.0 to easy-install.pth file\n",
            "Installing tabulate script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for wilds==1.2.2\n",
            "Best match: wilds 1.2.2\n",
            "Adding wilds 1.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for regex==2024.9.11\n",
            "Best match: regex 2024.9.11\n",
            "Adding regex 2024.9.11 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for ftfy==6.3.1\n",
            "Best match: ftfy 6.3.1\n",
            "Adding ftfy 6.3.1 to easy-install.pth file\n",
            "Installing ftfy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tqdm==4.66.6\n",
            "Best match: tqdm 4.66.6\n",
            "Adding tqdm 4.66.6 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for scikit-learn==1.5.2\n",
            "Best match: scikit-learn 1.5.2\n",
            "Adding scikit-learn 1.5.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for scipy==1.13.1\n",
            "Best match: scipy 1.13.1\n",
            "Adding scipy 1.13.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for future==1.0.0\n",
            "Best match: future 1.0.0\n",
            "Adding future 1.0.0 to easy-install.pth file\n",
            "Installing futurize script to /usr/local/bin\n",
            "Installing pasteurize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tb-nightly==2.19.0a20241207\n",
            "Best match: tb-nightly 2.19.0a20241207\n",
            "Adding tb-nightly 2.19.0a20241207 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for gdown==5.2.0\n",
            "Best match: gdown 5.2.0\n",
            "Adding gdown 5.2.0 to easy-install.pth file\n",
            "Installing gdown script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for yacs==0.1.8\n",
            "Best match: yacs 0.1.8\n",
            "Adding yacs 0.1.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for isort==4.3.21\n",
            "Best match: isort 4.3.21\n",
            "Adding isort 4.3.21 to easy-install.pth file\n",
            "Installing isort script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for yapf==0.29.0\n",
            "Best match: yapf 0.29.0\n",
            "Adding yapf 0.29.0 to easy-install.pth file\n",
            "Installing yapf script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for flake8==3.7.9\n",
            "Best match: flake8 3.7.9\n",
            "Adding flake8 3.7.9 to easy-install.pth file\n",
            "Installing flake8 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for torchvision==0.20.1+cu121\n",
            "Best match: torchvision 0.20.1+cu121\n",
            "Adding torchvision 0.20.1+cu121 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for torch==2.5.1+cu121\n",
            "Best match: torch 2.5.1+cu121\n",
            "Adding torch 2.5.1+cu121 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "Installing torchfrtrace script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for pytz==2024.2\n",
            "Best match: pytz 2024.2\n",
            "Adding pytz 2024.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for pillow==11.0.0\n",
            "Best match: pillow 11.0.0\n",
            "Adding pillow 11.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for pandas==2.2.2\n",
            "Best match: pandas 2.2.2\n",
            "Adding pandas 2.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for outdated==0.2.2\n",
            "Best match: outdated 0.2.2\n",
            "Adding outdated 0.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for ogb==1.3.6\n",
            "Best match: ogb 1.3.6\n",
            "Adding ogb 1.3.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for numpy==1.26.4\n",
            "Best match: numpy 1.26.4\n",
            "Adding numpy 1.26.4 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for wcwidth==0.2.13\n",
            "Best match: wcwidth 0.2.13\n",
            "Adding wcwidth 0.2.13 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for threadpoolctl==3.5.0\n",
            "Best match: threadpoolctl 3.5.0\n",
            "Adding threadpoolctl 3.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for joblib==1.4.2\n",
            "Best match: joblib 1.4.2\n",
            "Adding joblib 1.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for werkzeug==3.1.3\n",
            "Best match: werkzeug 3.1.3\n",
            "Adding werkzeug 3.1.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tensorboard-data-server==0.7.2\n",
            "Best match: tensorboard-data-server 0.7.2\n",
            "Adding tensorboard-data-server 0.7.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for six==1.16.0\n",
            "Best match: six 1.16.0\n",
            "Adding six 1.16.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for setuptools==75.1.0\n",
            "Best match: setuptools 75.1.0\n",
            "Adding setuptools 75.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for protobuf==4.25.5\n",
            "Best match: protobuf 4.25.5\n",
            "Adding protobuf 4.25.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for packaging==24.2\n",
            "Best match: packaging 24.2\n",
            "Adding packaging 24.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for Markdown==3.7\n",
            "Best match: Markdown 3.7\n",
            "Adding Markdown 3.7 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for grpcio==1.68.1\n",
            "Best match: grpcio 1.68.1\n",
            "Adding grpcio 1.68.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for absl-py==1.4.0\n",
            "Best match: absl-py 1.4.0\n",
            "Adding absl-py 1.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for requests==2.32.3\n",
            "Best match: requests 2.32.3\n",
            "Adding requests 2.32.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for filelock==3.16.1\n",
            "Best match: filelock 3.16.1\n",
            "Adding filelock 3.16.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for beautifulsoup4==4.12.3\n",
            "Best match: beautifulsoup4 4.12.3\n",
            "Adding beautifulsoup4 4.12.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for PyYAML==6.0.2\n",
            "Best match: PyYAML 6.0.2\n",
            "Adding PyYAML 6.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for mccabe==0.6.1\n",
            "Best match: mccabe 0.6.1\n",
            "Adding mccabe 0.6.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for pycodestyle==2.5.0\n",
            "Best match: pycodestyle 2.5.0\n",
            "Adding pycodestyle 2.5.0 to easy-install.pth file\n",
            "Installing pycodestyle script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for pyflakes==2.1.1\n",
            "Best match: pyflakes 2.1.1\n",
            "Adding pyflakes 2.1.1 to easy-install.pth file\n",
            "Installing pyflakes script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for entrypoints==0.3\n",
            "Best match: entrypoints 0.3\n",
            "Adding entrypoints 0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for sympy==1.13.1\n",
            "Best match: sympy 1.13.1\n",
            "Adding sympy 1.13.1 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for fsspec==2024.10.0\n",
            "Best match: fsspec 2024.10.0\n",
            "Adding fsspec 2024.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for jinja2==3.1.4\n",
            "Best match: jinja2 3.1.4\n",
            "Adding jinja2 3.1.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for networkx==3.4.2\n",
            "Best match: networkx 3.4.2\n",
            "Adding networkx 3.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for typing-extensions==4.12.2\n",
            "Best match: typing-extensions 4.12.2\n",
            "Adding typing-extensions 4.12.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages/setuptools/_vendor\n",
            "Searching for tzdata==2024.2\n",
            "Best match: tzdata 2024.2\n",
            "Adding tzdata 2024.2 to easy-install.pth file\n",
            "detected new path './setuptools/_vendor'\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for python-dateutil==2.8.2\n",
            "Best match: python-dateutil 2.8.2\n",
            "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for littleutils==0.2.4\n",
            "Best match: littleutils 0.2.4\n",
            "Adding littleutils 0.2.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for urllib3==2.2.3\n",
            "Best match: urllib3 2.2.3\n",
            "Adding urllib3 2.2.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for MarkupSafe==3.0.2\n",
            "Best match: MarkupSafe 3.0.2\n",
            "Adding MarkupSafe 3.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for PySocks==1.7.1\n",
            "Best match: PySocks 1.7.1\n",
            "Adding PySocks 1.7.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for certifi==2024.8.30\n",
            "Best match: certifi 2024.8.30\n",
            "Adding certifi 2024.8.30 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for idna==3.10\n",
            "Best match: idna 3.10\n",
            "Adding idna 3.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for charset-normalizer==3.4.0\n",
            "Best match: charset-normalizer 3.4.0\n",
            "Adding charset-normalizer 3.4.0 to easy-install.pth file\n",
            "Installing normalizer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for soupsieve==2.6\n",
            "Best match: soupsieve 2.6\n",
            "Adding soupsieve 2.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for mpmath==1.3.0\n",
            "Best match: mpmath 1.3.0\n",
            "Adding mpmath 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Finished processing dependencies for dassl==0.6.3\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting torchcam\n",
            "  Downloading torchcam-0.4.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchcam) (1.26.4)\n",
            "Requirement already satisfied: Pillow!=9.2.0,>=8.4.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (11.0.0)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.0.0->torchcam) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.0->torchcam) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->torchcam) (3.0.2)\n",
            "Downloading torchcam-0.4.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcam\n",
            "Successfully installed torchcam-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "!pip install regex\n",
        "!pip install ftfy\n",
        "!pip install tqdm\n",
        "!pip install torchcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSJvsvvGC1Ni",
        "outputId": "a953ecb7-4082-4f75-8c3c-76988e1e085e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/DAPT\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ_D5AiOW8aQ",
        "outputId": "27c42b9e-288f-4f40-fda6-2a6c47088584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '../MyDrive/DAPT/'\n",
            "/content/drive/My Drive/DAPT\n"
          ]
        }
      ],
      "source": [
        "%cd ../MyDrive/DAPT/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OI594A7odD9p",
        "outputId": "57fa270e-d5b0-4dfa-9b9b-c750d1ea7b75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/DAPT'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoTEuybYl3OP"
      },
      "source": [
        "#Eurosat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUOOWUBWl7VL"
      },
      "source": [
        "##Prototype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tc1--NsgbQO",
        "outputId": "366a92da-3c22-49e5-b632-7969a6b20a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:05:37.261942: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:05:37.281134: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:05:37.287109: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:05:37.301156: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:05:38.309066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_16-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  160\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "100%|███████████████████████████████████████| 351M/351M [00:03<00:00, 89.3MiB/s]\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "EuroSAT (SHOTS: 16)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#eurosat prototype-16shots-seed1\n",
        "\n",
        "!python train.py \\\n",
        "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "            --seed 1 \\\n",
        "            --trainer DAPT \\\n",
        "            --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "            --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "            DATASET.NUM_SHOTS 16 \\\n",
        "            TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1uGbt9Ut-p3",
        "outputId": "a866645f-ef98-4fe5-db63-e0308d9caf24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:10:59.158421: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:10:59.177772: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:10:59.183537: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:10:59.197937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:11:00.222966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  80\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "EuroSAT (SHOTS: 8)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#eurosat prototype-8shots-seed1\n",
        "\n",
        "!python3 train.py \\\n",
        " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        " --seed 1 \\\n",
        " --trainer DAPT \\\n",
        " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        " --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        " DATASET.NUM_SHOTS 8 \\\n",
        " TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTDAxN44PzlZ",
        "outputId": "6191a019-53d4-4ad3-967f-dac1d95d8492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:12:10.041482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:12:10.061049: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:12:10.066962: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:12:10.081139: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:12:11.079399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  80\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0173, -0.5723,  0.1752,  ...,  0.4941,  0.4187,  0.0375],\n",
            "         [ 0.0214, -0.5706,  0.1699,  ...,  0.5021,  0.4206,  0.0425],\n",
            "         [ 0.0149, -0.5713,  0.1720,  ...,  0.4970,  0.4210,  0.0462],\n",
            "         ...,\n",
            "         [ 0.0324, -0.5680,  0.1748,  ...,  0.5016,  0.4188,  0.0404],\n",
            "         [ 0.0201, -0.5696,  0.1726,  ...,  0.4989,  0.4190,  0.0354],\n",
            "         [ 0.0228, -0.5696,  0.1684,  ...,  0.5053,  0.4212,  0.0412]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5609\n",
            "  Max: 1.0111\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0400\n",
            "  Min: -0.4519\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "EuroSAT (SHOTS: 8)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#eurosat prototype-8shots-seed2\n",
        "\n",
        "!python3 train.py \\\n",
        " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        " --seed 2 \\\n",
        " --trainer DAPT \\\n",
        " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        " --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        " DATASET.NUM_SHOTS 8 \\\n",
        " TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaEnoiCkP2Np",
        "outputId": "e1b4044a-cacf-4525-9e54-731e1fc4fbeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:13:24.461546: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:13:24.480867: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:13:24.486898: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:13:24.501299: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:13:25.517239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  80\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0187, -0.5727,  0.1683,  ...,  0.4947,  0.4188,  0.0344],\n",
            "         [ 0.0173, -0.5677,  0.1739,  ...,  0.4990,  0.4190,  0.0388],\n",
            "         [ 0.0094, -0.5649,  0.1758,  ...,  0.4946,  0.4157,  0.0405],\n",
            "         ...,\n",
            "         [ 0.0227, -0.5676,  0.1747,  ...,  0.5071,  0.4204,  0.0413],\n",
            "         [ 0.0083, -0.5735,  0.1744,  ...,  0.5016,  0.4249,  0.0364],\n",
            "         [ 0.0210, -0.5687,  0.1704,  ...,  0.5030,  0.4152,  0.0323]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2340\n",
            "  Min: -1.5598\n",
            "  Max: 1.0156\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4500\n",
            "  Max: 0.4708\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "EuroSAT (SHOTS: 8)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#eurosat prototype-8shots-seed3\n",
        "\n",
        "!python3 train.py \\\n",
        " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        " --seed 3 \\\n",
        " --trainer DAPT \\\n",
        " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        " --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        " DATASET.NUM_SHOTS 8 \\\n",
        " TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5W7OVsIvYFm",
        "outputId": "6a26f0c6-93fd-43e0-9136-f0471274f02a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:14:37.898370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:14:37.918864: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:14:37.924945: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:14:37.938827: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:14:38.937338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  40\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "EuroSAT (SHOTS: 4)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#eurosat prototype-4shots-seed1\n",
        "\n",
        "!python3 train.py \\\n",
        " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        " --seed 1 \\\n",
        " --trainer DAPT \\\n",
        " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        " DATASET.NUM_SHOTS 4 \\\n",
        " TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHMTdDz3JfqT",
        "outputId": "ca34d68b-00f6-4230-9dfa-a45a05994c76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:15:21.682642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:15:21.703931: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:15:21.710282: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:15:21.725178: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:15:22.767764: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  40\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0173, -0.5723,  0.1752,  ...,  0.4941,  0.4187,  0.0375],\n",
            "         [ 0.0214, -0.5706,  0.1699,  ...,  0.5021,  0.4206,  0.0425],\n",
            "         [ 0.0149, -0.5713,  0.1720,  ...,  0.4970,  0.4210,  0.0462],\n",
            "         ...,\n",
            "         [ 0.0324, -0.5680,  0.1748,  ...,  0.5016,  0.4188,  0.0404],\n",
            "         [ 0.0201, -0.5696,  0.1726,  ...,  0.4989,  0.4190,  0.0354],\n",
            "         [ 0.0228, -0.5696,  0.1684,  ...,  0.5053,  0.4212,  0.0412]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5609\n",
            "  Max: 1.0111\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0400\n",
            "  Min: -0.4519\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "EuroSAT (SHOTS: 4)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#eurosat prototype-4shots-seed2\n",
        "\n",
        "!python3 train.py \\\n",
        " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        " --seed 2 \\\n",
        " --trainer DAPT \\\n",
        " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        " DATASET.NUM_SHOTS 4 \\\n",
        " TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dmEkpJZJ55Q",
        "outputId": "f21a38c3-bb27-4628-ca9d-821c858f420a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:16:07.475497: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:16:07.495069: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:16:07.501263: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:16:07.515665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:16:08.530438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  40\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0187, -0.5727,  0.1683,  ...,  0.4947,  0.4188,  0.0344],\n",
            "         [ 0.0173, -0.5677,  0.1739,  ...,  0.4990,  0.4190,  0.0388],\n",
            "         [ 0.0094, -0.5649,  0.1758,  ...,  0.4946,  0.4157,  0.0405],\n",
            "         ...,\n",
            "         [ 0.0227, -0.5676,  0.1747,  ...,  0.5071,  0.4204,  0.0413],\n",
            "         [ 0.0083, -0.5735,  0.1744,  ...,  0.5016,  0.4249,  0.0364],\n",
            "         [ 0.0210, -0.5687,  0.1704,  ...,  0.5030,  0.4152,  0.0323]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2340\n",
            "  Min: -1.5598\n",
            "  Max: 1.0156\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4500\n",
            "  Max: 0.4708\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "EuroSAT (SHOTS: 4)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#eurosat prototype-4shots-seed3\n",
        "\n",
        "!python3 train.py \\\n",
        " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        " --seed 3 \\\n",
        " --trainer DAPT \\\n",
        " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        " DATASET.NUM_SHOTS 4 \\\n",
        " TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NR797HOvcfw",
        "outputId": "f529c6e1-9916-449c-e8a8-09eb676028a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:16:52.620953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:16:52.642216: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:16:52.648558: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:16:52.663193: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:16:53.689052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  20\n",
            "# val      20\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "EuroSAT (SHOTS: 2)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#eurosat prototype-2shots-seed1\n",
        "\n",
        "!python3 train.py \\\n",
        " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        " --seed 1 \\\n",
        " --trainer DAPT \\\n",
        " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        " DATASET.NUM_SHOTS 2 \\\n",
        " TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0QWMQypILHS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8aa3376-e489-4f45-f77b-448a827f5959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:17:26.690777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:17:26.710225: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:17:26.716508: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:17:26.730447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:17:27.715984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  20\n",
            "# val      20\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0173, -0.5723,  0.1752,  ...,  0.4941,  0.4187,  0.0375],\n",
            "         [ 0.0214, -0.5706,  0.1699,  ...,  0.5021,  0.4206,  0.0425],\n",
            "         [ 0.0149, -0.5713,  0.1720,  ...,  0.4970,  0.4210,  0.0462],\n",
            "         ...,\n",
            "         [ 0.0324, -0.5680,  0.1748,  ...,  0.5016,  0.4188,  0.0404],\n",
            "         [ 0.0201, -0.5696,  0.1726,  ...,  0.4989,  0.4190,  0.0354],\n",
            "         [ 0.0228, -0.5696,  0.1684,  ...,  0.5053,  0.4212,  0.0412]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5609\n",
            "  Max: 1.0111\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0400\n",
            "  Min: -0.4519\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "EuroSAT (SHOTS: 2)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#eurosat prototype-2shots-seed2\n",
        "\n",
        "!python3 train.py \\\n",
        " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        " --seed 2 \\\n",
        " --trainer DAPT \\\n",
        " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        " DATASET.NUM_SHOTS 2 \\\n",
        " TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p9-jk2JLMZD",
        "outputId": "c425a8aa-66b3-426b-947d-9537beb6c7fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:17:59.001679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:17:59.021184: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:17:59.027141: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:17:59.040964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:18:00.038790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  20\n",
            "# val      20\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0187, -0.5727,  0.1683,  ...,  0.4947,  0.4188,  0.0344],\n",
            "         [ 0.0173, -0.5677,  0.1739,  ...,  0.4990,  0.4190,  0.0388],\n",
            "         [ 0.0094, -0.5649,  0.1758,  ...,  0.4946,  0.4157,  0.0405],\n",
            "         ...,\n",
            "         [ 0.0227, -0.5676,  0.1747,  ...,  0.5071,  0.4204,  0.0413],\n",
            "         [ 0.0083, -0.5735,  0.1744,  ...,  0.5016,  0.4249,  0.0364],\n",
            "         [ 0.0210, -0.5687,  0.1704,  ...,  0.5030,  0.4152,  0.0323]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2340\n",
            "  Min: -1.5598\n",
            "  Max: 1.0156\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4500\n",
            "  Max: 0.4708\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "EuroSAT (SHOTS: 2)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#eurosat prototype-2shots-seed3\n",
        "\n",
        "!python3 train.py \\\n",
        " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        " --seed 3 \\\n",
        " --trainer DAPT \\\n",
        " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        " --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        " DATASET.NUM_SHOTS 2 \\\n",
        " TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "C6jIDWhlvl0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477dba76-8e2b-47df-d449-95555219b18e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:18:31.793942: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:18:31.813962: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:18:31.820056: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:18:31.833865: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:18:32.840304: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  10\n",
            "# val      10\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "EuroSAT (SHOTS: 1)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#eurosat prototype-1shots-seed1\n",
        "\n",
        "!python3 train.py \\\n",
        " --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        " --seed 1 \\\n",
        " --trainer DAPT \\\n",
        " --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        " --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        " DATASET.NUM_SHOTS 1 \\\n",
        " TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU6JtYkHunM2"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGklAx4EhWsX",
        "outputId": "3c919ea2-31aa-4e38-d38e-f27ee4bb5972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:18:59.386994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:18:59.406356: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:18:59.412502: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:18:59.426725: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:19:00.411366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16']\n",
            "output_dir: output/1207_new_init/eurosat/DAPT/vit_b16_16shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/eurosat/DAPT/vit_b16_16shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_16-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  160\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/eurosat/DAPT/vit_b16_16shots/seed1/tensorboard)\n",
            "epoch [1/200] batch [5/5] time 0.292 (0.684) data 0.000 (0.134) loss 8.6739 (8.5705) acc 34.3750 (31.8750) lr 2.0000e+01 eta 0:11:20\n",
            "epoch [2/200] batch [5/5] time 0.289 (0.404) data 0.000 (0.114) loss 7.4321 (7.6462) acc 9.3750 (13.7500) lr 1.9999e+01 eta 0:06:39\n",
            "epoch [3/200] batch [5/5] time 0.289 (0.417) data 0.000 (0.128) loss 4.4319 (5.7321) acc 18.7500 (16.2500) lr 1.9995e+01 eta 0:06:51\n",
            "epoch [4/200] batch [5/5] time 0.292 (0.411) data 0.000 (0.120) loss 4.3567 (4.2034) acc 18.7500 (21.8750) lr 1.9989e+01 eta 0:06:42\n",
            "epoch [5/200] batch [5/5] time 0.294 (0.414) data 0.000 (0.123) loss 3.0579 (3.1205) acc 28.1250 (35.6250) lr 1.9980e+01 eta 0:06:44\n",
            "epoch [6/200] batch [5/5] time 0.293 (0.408) data 0.000 (0.117) loss 2.2020 (2.5279) acc 56.2500 (54.3750) lr 1.9969e+01 eta 0:06:35\n",
            "epoch [7/200] batch [5/5] time 0.290 (0.404) data 0.000 (0.113) loss 1.7673 (2.0220) acc 62.5000 (61.8750) lr 1.9956e+01 eta 0:06:29\n",
            "epoch [8/200] batch [5/5] time 0.294 (0.418) data 0.000 (0.124) loss 6.0835 (3.5574) acc 28.1250 (43.7500) lr 1.9940e+01 eta 0:06:41\n",
            "epoch [9/200] batch [5/5] time 0.293 (0.413) data 0.000 (0.120) loss 7.3055 (6.4411) acc 28.1250 (19.3750) lr 1.9921e+01 eta 0:06:34\n",
            "epoch [10/200] batch [5/5] time 0.294 (0.423) data 0.000 (0.128) loss 3.8884 (3.9676) acc 28.1250 (23.7500) lr 1.9900e+01 eta 0:06:41\n",
            "epoch [11/200] batch [5/5] time 0.296 (0.411) data 0.000 (0.116) loss 3.3986 (2.9980) acc 34.3750 (41.8750) lr 1.9877e+01 eta 0:06:28\n",
            "epoch [12/200] batch [5/5] time 0.296 (0.412) data 0.000 (0.117) loss 1.4951 (1.8533) acc 56.2500 (50.0000) lr 1.9851e+01 eta 0:06:27\n",
            "epoch [13/200] batch [5/5] time 0.298 (0.411) data 0.000 (0.114) loss 1.3325 (2.1904) acc 62.5000 (48.1250) lr 1.9823e+01 eta 0:06:24\n",
            "epoch [14/200] batch [5/5] time 0.296 (0.408) data 0.000 (0.111) loss 3.2850 (1.7741) acc 37.5000 (60.0000) lr 1.9792e+01 eta 0:06:19\n",
            "epoch [15/200] batch [5/5] time 0.302 (0.424) data 0.000 (0.124) loss 6.6204 (4.5772) acc 25.0000 (32.5000) lr 1.9759e+01 eta 0:06:32\n",
            "epoch [16/200] batch [5/5] time 0.300 (0.430) data 0.000 (0.129) loss 6.0726 (5.8286) acc 25.0000 (30.6250) lr 1.9724e+01 eta 0:06:35\n",
            "epoch [17/200] batch [5/5] time 0.302 (0.417) data 0.000 (0.115) loss 2.6321 (2.7468) acc 34.3750 (43.1250) lr 1.9686e+01 eta 0:06:21\n",
            "epoch [18/200] batch [5/5] time 0.305 (0.429) data 0.000 (0.126) loss 1.7466 (1.7348) acc 50.0000 (53.7500) lr 1.9646e+01 eta 0:06:30\n",
            "epoch [19/200] batch [5/5] time 0.303 (0.418) data 0.000 (0.114) loss 1.2673 (1.5024) acc 78.1250 (62.5000) lr 1.9603e+01 eta 0:06:18\n",
            "epoch [20/200] batch [5/5] time 0.303 (0.421) data 0.000 (0.116) loss 1.7912 (1.2921) acc 68.7500 (70.6250) lr 1.9558e+01 eta 0:06:18\n",
            "epoch [21/200] batch [5/5] time 0.306 (0.431) data 0.000 (0.124) loss 1.3946 (2.0527) acc 71.8750 (58.1250) lr 1.9511e+01 eta 0:06:25\n",
            "epoch [22/200] batch [5/5] time 0.308 (0.433) data 0.000 (0.125) loss 3.4955 (4.9401) acc 43.7500 (37.5000) lr 1.9461e+01 eta 0:06:25\n",
            "epoch [23/200] batch [5/5] time 0.307 (0.417) data 0.000 (0.112) loss 6.1792 (6.3927) acc 9.3750 (26.8750) lr 1.9409e+01 eta 0:06:09\n",
            "epoch [24/200] batch [5/5] time 0.307 (0.420) data 0.000 (0.113) loss 6.6986 (7.0449) acc 31.2500 (18.1250) lr 1.9354e+01 eta 0:06:09\n",
            "epoch [25/200] batch [5/5] time 0.311 (0.428) data 0.000 (0.121) loss 2.7271 (4.4839) acc 56.2500 (43.7500) lr 1.9298e+01 eta 0:06:14\n",
            "epoch [26/200] batch [5/5] time 0.310 (0.425) data 0.000 (0.119) loss 2.2326 (2.6595) acc 59.3750 (53.7500) lr 1.9239e+01 eta 0:06:10\n",
            "epoch [27/200] batch [5/5] time 0.312 (0.429) data 0.000 (0.123) loss 1.6273 (1.6129) acc 56.2500 (58.7500) lr 1.9178e+01 eta 0:06:11\n",
            "epoch [28/200] batch [5/5] time 0.310 (0.426) data 0.000 (0.116) loss 1.3568 (1.2457) acc 65.6250 (70.0000) lr 1.9114e+01 eta 0:06:06\n",
            "epoch [29/200] batch [5/5] time 0.311 (0.430) data 0.000 (0.120) loss 6.8724 (3.6043) acc 3.1250 (40.6250) lr 1.9048e+01 eta 0:06:07\n",
            "epoch [30/200] batch [5/5] time 0.313 (0.424) data 0.000 (0.112) loss 4.5750 (5.0031) acc 40.6250 (40.0000) lr 1.8980e+01 eta 0:06:00\n",
            "epoch [31/200] batch [5/5] time 0.313 (0.431) data 0.000 (0.118) loss 4.9649 (5.2072) acc 28.1250 (31.8750) lr 1.8910e+01 eta 0:06:04\n",
            "epoch [32/200] batch [5/5] time 0.313 (0.423) data 0.000 (0.110) loss 1.8614 (4.7525) acc 65.6250 (36.8750) lr 1.8838e+01 eta 0:05:55\n",
            "epoch [33/200] batch [5/5] time 0.317 (0.440) data 0.000 (0.126) loss 1.3817 (1.9730) acc 71.8750 (55.0000) lr 1.8763e+01 eta 0:06:07\n",
            "epoch [34/200] batch [5/5] time 0.313 (0.431) data 0.000 (0.118) loss 1.3577 (1.7241) acc 65.6250 (64.3750) lr 1.8686e+01 eta 0:05:57\n",
            "epoch [35/200] batch [5/5] time 0.311 (0.422) data 0.001 (0.109) loss 0.8307 (1.2009) acc 90.6250 (78.1250) lr 1.8607e+01 eta 0:05:48\n",
            "epoch [36/200] batch [5/5] time 0.316 (0.430) data 0.000 (0.118) loss 4.0081 (2.1400) acc 25.0000 (53.1250) lr 1.8526e+01 eta 0:05:52\n",
            "epoch [37/200] batch [5/5] time 0.321 (0.429) data 0.000 (0.112) loss 16.8796 (9.1491) acc 3.1250 (26.2500) lr 1.8443e+01 eta 0:05:49\n",
            "epoch [38/200] batch [5/5] time 0.318 (0.433) data 0.000 (0.114) loss 15.5332 (16.1132) acc 3.1250 (12.5000) lr 1.8358e+01 eta 0:05:50\n",
            "epoch [39/200] batch [5/5] time 0.317 (0.439) data 0.000 (0.122) loss 6.1965 (10.2485) acc 3.1250 (5.0000) lr 1.8271e+01 eta 0:05:53\n",
            "epoch [40/200] batch [5/5] time 0.317 (0.436) data 0.000 (0.119) loss 3.8610 (4.6018) acc 9.3750 (10.0000) lr 1.8181e+01 eta 0:05:48\n",
            "epoch [41/200] batch [5/5] time 0.321 (0.441) data 0.000 (0.122) loss 3.6181 (3.4263) acc 9.3750 (13.1250) lr 1.8090e+01 eta 0:05:50\n",
            "epoch [42/200] batch [5/5] time 0.317 (0.434) data 0.000 (0.116) loss 3.2179 (3.0585) acc 28.1250 (16.2500) lr 1.7997e+01 eta 0:05:42\n",
            "epoch [43/200] batch [5/5] time 0.324 (0.440) data 0.000 (0.118) loss 4.0867 (3.2579) acc 34.3750 (29.3750) lr 1.7902e+01 eta 0:05:45\n",
            "epoch [44/200] batch [5/5] time 0.318 (0.435) data 0.000 (0.115) loss 7.4123 (3.7312) acc 28.1250 (33.7500) lr 1.7804e+01 eta 0:05:39\n",
            "epoch [45/200] batch [5/5] time 0.324 (0.441) data 0.000 (0.121) loss 4.2451 (5.1968) acc 15.6250 (26.8750) lr 1.7705e+01 eta 0:05:41\n",
            "epoch [46/200] batch [5/5] time 0.319 (0.441) data 0.000 (0.121) loss 4.9722 (6.2453) acc 18.7500 (24.3750) lr 1.7604e+01 eta 0:05:39\n",
            "epoch [47/200] batch [5/5] time 0.319 (0.436) data 0.000 (0.117) loss 2.4453 (2.5190) acc 53.1250 (52.5000) lr 1.7501e+01 eta 0:05:33\n",
            "epoch [48/200] batch [5/5] time 0.317 (0.434) data 0.000 (0.116) loss 2.0322 (1.7010) acc 68.7500 (63.7500) lr 1.7396e+01 eta 0:05:30\n",
            "epoch [49/200] batch [5/5] time 0.317 (0.433) data 0.000 (0.116) loss 2.0064 (1.6387) acc 50.0000 (60.6250) lr 1.7290e+01 eta 0:05:26\n",
            "epoch [50/200] batch [5/5] time 0.319 (0.433) data 0.000 (0.116) loss 1.4269 (1.6437) acc 75.0000 (67.5000) lr 1.7181e+01 eta 0:05:24\n",
            "epoch [51/200] batch [5/5] time 0.316 (0.437) data 0.000 (0.121) loss 1.0827 (1.1348) acc 78.1250 (73.1250) lr 1.7071e+01 eta 0:05:25\n",
            "epoch [52/200] batch [5/5] time 0.314 (0.435) data 0.000 (0.119) loss 3.2149 (1.9274) acc 31.2500 (61.2500) lr 1.6959e+01 eta 0:05:22\n",
            "epoch [53/200] batch [5/5] time 0.319 (0.429) data 0.000 (0.110) loss 5.9747 (5.6992) acc 21.8750 (26.8750) lr 1.6845e+01 eta 0:05:15\n",
            "epoch [54/200] batch [5/5] time 0.320 (0.427) data 0.000 (0.112) loss 2.7413 (3.5349) acc 46.8750 (45.6250) lr 1.6730e+01 eta 0:05:11\n",
            "epoch [55/200] batch [5/5] time 0.317 (0.429) data 0.000 (0.113) loss 1.1506 (1.6054) acc 75.0000 (65.0000) lr 1.6613e+01 eta 0:05:10\n",
            "epoch [56/200] batch [5/5] time 0.315 (0.433) data 0.000 (0.119) loss 1.1555 (1.1758) acc 68.7500 (69.3750) lr 1.6494e+01 eta 0:05:11\n",
            "epoch [57/200] batch [5/5] time 0.312 (0.432) data 0.000 (0.120) loss 1.0620 (1.0886) acc 71.8750 (76.2500) lr 1.6374e+01 eta 0:05:09\n",
            "epoch [58/200] batch [5/5] time 0.313 (0.429) data 0.000 (0.115) loss 1.1969 (1.0960) acc 75.0000 (73.7500) lr 1.6252e+01 eta 0:05:04\n",
            "epoch [59/200] batch [5/5] time 0.317 (0.428) data 0.000 (0.115) loss 1.2850 (1.3963) acc 68.7500 (66.8750) lr 1.6129e+01 eta 0:05:02\n",
            "epoch [60/200] batch [5/5] time 0.313 (0.434) data 0.000 (0.119) loss 2.8175 (3.7641) acc 56.2500 (38.7500) lr 1.6004e+01 eta 0:05:03\n",
            "epoch [61/200] batch [5/5] time 0.316 (0.437) data 0.000 (0.122) loss 3.1691 (6.2133) acc 53.1250 (29.3750) lr 1.5878e+01 eta 0:05:04\n",
            "epoch [62/200] batch [5/5] time 0.315 (0.426) data 0.000 (0.112) loss 2.8137 (2.4992) acc 43.7500 (48.1250) lr 1.5750e+01 eta 0:04:54\n",
            "epoch [63/200] batch [5/5] time 0.314 (0.435) data 0.000 (0.121) loss 1.2467 (1.8971) acc 75.0000 (60.0000) lr 1.5621e+01 eta 0:04:58\n",
            "epoch [64/200] batch [5/5] time 0.317 (0.435) data 0.000 (0.120) loss 1.3409 (1.5125) acc 65.6250 (63.1250) lr 1.5490e+01 eta 0:04:56\n",
            "epoch [65/200] batch [5/5] time 0.315 (0.425) data 0.000 (0.111) loss 0.6990 (1.0982) acc 84.3750 (71.2500) lr 1.5358e+01 eta 0:04:46\n",
            "epoch [66/200] batch [5/5] time 0.314 (0.433) data 0.000 (0.119) loss 0.9891 (1.2117) acc 78.1250 (75.6250) lr 1.5225e+01 eta 0:04:49\n",
            "epoch [67/200] batch [5/5] time 0.315 (0.428) data 0.000 (0.113) loss 2.0933 (2.0961) acc 50.0000 (60.0000) lr 1.5090e+01 eta 0:04:44\n",
            "epoch [68/200] batch [5/5] time 0.315 (0.425) data 0.000 (0.112) loss 2.5212 (3.5449) acc 56.2500 (48.7500) lr 1.4955e+01 eta 0:04:40\n",
            "epoch [69/200] batch [5/5] time 0.314 (0.432) data 0.000 (0.118) loss 3.1137 (4.1909) acc 46.8750 (48.7500) lr 1.4818e+01 eta 0:04:43\n",
            "epoch [70/200] batch [5/5] time 0.313 (0.435) data 0.000 (0.120) loss 1.2379 (1.5813) acc 68.7500 (61.2500) lr 1.4679e+01 eta 0:04:42\n",
            "epoch [71/200] batch [5/5] time 0.317 (0.432) data 0.000 (0.117) loss 0.7751 (1.0321) acc 84.3750 (79.3750) lr 1.4540e+01 eta 0:04:38\n",
            "epoch [72/200] batch [5/5] time 0.317 (0.434) data 0.000 (0.120) loss 2.1287 (1.1861) acc 56.2500 (78.1250) lr 1.4399e+01 eta 0:04:38\n",
            "epoch [73/200] batch [5/5] time 0.314 (0.430) data 0.000 (0.115) loss 1.2876 (1.2213) acc 75.0000 (74.3750) lr 1.4258e+01 eta 0:04:32\n",
            "epoch [74/200] batch [5/5] time 0.318 (0.428) data 0.000 (0.113) loss 1.3744 (1.1428) acc 71.8750 (76.8750) lr 1.4115e+01 eta 0:04:29\n",
            "epoch [75/200] batch [5/5] time 0.317 (0.437) data 0.000 (0.121) loss 1.9394 (2.2046) acc 53.1250 (52.5000) lr 1.3971e+01 eta 0:04:33\n",
            "epoch [76/200] batch [5/5] time 0.318 (0.439) data 0.000 (0.124) loss 4.5536 (3.2420) acc 43.7500 (55.6250) lr 1.3827e+01 eta 0:04:32\n",
            "epoch [77/200] batch [5/5] time 0.321 (0.439) data 0.001 (0.122) loss 2.4237 (3.9590) acc 53.1250 (46.8750) lr 1.3681e+01 eta 0:04:30\n",
            "epoch [78/200] batch [5/5] time 0.316 (0.439) data 0.000 (0.123) loss 0.8159 (1.3743) acc 84.3750 (73.7500) lr 1.3535e+01 eta 0:04:27\n",
            "epoch [79/200] batch [5/5] time 0.317 (0.435) data 0.000 (0.120) loss 1.3933 (1.1735) acc 62.5000 (72.5000) lr 1.3387e+01 eta 0:04:23\n",
            "epoch [80/200] batch [5/5] time 0.314 (0.438) data 0.000 (0.123) loss 0.7163 (1.0159) acc 84.3750 (78.1250) lr 1.3239e+01 eta 0:04:22\n",
            "epoch [81/200] batch [5/5] time 0.317 (0.437) data 0.000 (0.122) loss 1.2480 (1.0968) acc 75.0000 (78.1250) lr 1.3090e+01 eta 0:04:20\n",
            "epoch [82/200] batch [5/5] time 0.315 (0.438) data 0.000 (0.122) loss 0.7740 (0.9916) acc 84.3750 (77.5000) lr 1.2940e+01 eta 0:04:18\n",
            "epoch [83/200] batch [5/5] time 0.316 (0.434) data 0.000 (0.118) loss 0.8724 (1.1299) acc 81.2500 (82.5000) lr 1.2790e+01 eta 0:04:13\n",
            "epoch [84/200] batch [5/5] time 0.317 (0.436) data 0.000 (0.120) loss 1.7812 (1.5735) acc 81.2500 (70.6250) lr 1.2639e+01 eta 0:04:13\n",
            "epoch [85/200] batch [5/5] time 0.317 (0.439) data 0.000 (0.122) loss 2.4538 (2.6827) acc 59.3750 (51.8750) lr 1.2487e+01 eta 0:04:12\n",
            "epoch [86/200] batch [5/5] time 0.319 (0.433) data 0.000 (0.117) loss 2.5881 (2.2054) acc 43.7500 (51.8750) lr 1.2334e+01 eta 0:04:06\n",
            "epoch [87/200] batch [5/5] time 0.315 (0.433) data 0.000 (0.118) loss 0.9840 (1.2175) acc 78.1250 (76.2500) lr 1.2181e+01 eta 0:04:04\n",
            "epoch [88/200] batch [5/5] time 0.316 (0.440) data 0.000 (0.123) loss 0.6567 (0.9401) acc 84.3750 (81.8750) lr 1.2028e+01 eta 0:04:06\n",
            "epoch [89/200] batch [5/5] time 0.313 (0.431) data 0.000 (0.118) loss 1.3804 (1.0424) acc 68.7500 (76.8750) lr 1.1874e+01 eta 0:03:59\n",
            "epoch [90/200] batch [5/5] time 0.315 (0.431) data 0.000 (0.116) loss 0.9598 (1.0400) acc 78.1250 (75.6250) lr 1.1719e+01 eta 0:03:56\n",
            "epoch [91/200] batch [5/5] time 0.313 (0.431) data 0.000 (0.117) loss 0.5978 (0.7315) acc 81.2500 (80.0000) lr 1.1564e+01 eta 0:03:54\n",
            "epoch [92/200] batch [5/5] time 0.313 (0.429) data 0.000 (0.116) loss 0.5981 (0.8415) acc 87.5000 (83.7500) lr 1.1409e+01 eta 0:03:51\n",
            "epoch [93/200] batch [5/5] time 0.313 (0.435) data 0.000 (0.121) loss 0.8397 (0.9493) acc 78.1250 (80.0000) lr 1.1253e+01 eta 0:03:52\n",
            "epoch [94/200] batch [5/5] time 0.316 (0.441) data 0.000 (0.126) loss 1.6250 (1.3741) acc 59.3750 (70.0000) lr 1.1097e+01 eta 0:03:53\n",
            "epoch [95/200] batch [5/5] time 0.319 (0.435) data 0.000 (0.121) loss 2.3115 (1.6502) acc 53.1250 (64.3750) lr 1.0941e+01 eta 0:03:48\n",
            "epoch [96/200] batch [5/5] time 0.314 (0.434) data 0.000 (0.121) loss 1.0243 (1.1155) acc 75.0000 (73.7500) lr 1.0785e+01 eta 0:03:45\n",
            "epoch [97/200] batch [5/5] time 0.317 (0.436) data 0.000 (0.122) loss 1.3770 (0.9962) acc 71.8750 (78.7500) lr 1.0628e+01 eta 0:03:44\n",
            "epoch [98/200] batch [5/5] time 0.315 (0.439) data 0.000 (0.126) loss 1.6492 (1.4084) acc 68.7500 (71.2500) lr 1.0471e+01 eta 0:03:43\n",
            "epoch [99/200] batch [5/5] time 0.317 (0.434) data 0.000 (0.119) loss 1.2991 (1.1193) acc 71.8750 (75.6250) lr 1.0314e+01 eta 0:03:39\n",
            "epoch [100/200] batch [5/5] time 0.315 (0.432) data 0.000 (0.118) loss 1.1043 (0.8943) acc 75.0000 (81.8750) lr 1.0157e+01 eta 0:03:35\n",
            "epoch [101/200] batch [5/5] time 0.317 (0.426) data 0.000 (0.112) loss 0.9241 (0.8686) acc 75.0000 (78.1250) lr 1.0000e+01 eta 0:03:31\n",
            "epoch [102/200] batch [5/5] time 0.319 (0.434) data 0.000 (0.119) loss 0.6334 (0.8418) acc 90.6250 (80.6250) lr 9.8429e+00 eta 0:03:32\n",
            "epoch [103/200] batch [5/5] time 0.317 (0.428) data 0.000 (0.113) loss 1.3201 (1.1087) acc 59.3750 (75.6250) lr 9.6859e+00 eta 0:03:27\n",
            "epoch [104/200] batch [5/5] time 0.314 (0.427) data 0.000 (0.113) loss 0.6707 (1.0641) acc 87.5000 (75.6250) lr 9.5289e+00 eta 0:03:24\n",
            "epoch [105/200] batch [5/5] time 0.318 (0.442) data 0.000 (0.126) loss 0.4820 (1.0686) acc 93.7500 (76.8750) lr 9.3721e+00 eta 0:03:29\n",
            "epoch [106/200] batch [5/5] time 0.315 (0.430) data 0.000 (0.114) loss 1.4185 (1.0834) acc 81.2500 (75.6250) lr 9.2154e+00 eta 0:03:21\n",
            "epoch [107/200] batch [5/5] time 0.316 (0.431) data 0.000 (0.116) loss 0.8467 (1.0319) acc 81.2500 (78.7500) lr 9.0589e+00 eta 0:03:20\n",
            "epoch [108/200] batch [5/5] time 0.318 (0.435) data 0.000 (0.120) loss 0.6916 (0.8511) acc 81.2500 (80.6250) lr 8.9027e+00 eta 0:03:20\n",
            "epoch [109/200] batch [5/5] time 0.316 (0.429) data 0.000 (0.115) loss 1.0864 (1.0366) acc 81.2500 (78.7500) lr 8.7467e+00 eta 0:03:15\n",
            "epoch [110/200] batch [5/5] time 0.313 (0.423) data 0.000 (0.109) loss 1.5144 (1.0289) acc 68.7500 (78.1250) lr 8.5910e+00 eta 0:03:10\n",
            "epoch [111/200] batch [5/5] time 0.315 (0.437) data 0.000 (0.122) loss 0.8624 (0.7652) acc 84.3750 (83.7500) lr 8.4357e+00 eta 0:03:14\n",
            "epoch [112/200] batch [5/5] time 0.313 (0.436) data 0.000 (0.121) loss 0.4504 (0.8255) acc 93.7500 (81.2500) lr 8.2807e+00 eta 0:03:11\n",
            "epoch [113/200] batch [5/5] time 0.317 (0.433) data 0.000 (0.118) loss 0.7375 (0.8069) acc 90.6250 (85.0000) lr 8.1262e+00 eta 0:03:08\n",
            "epoch [114/200] batch [5/5] time 0.315 (0.437) data 0.000 (0.123) loss 1.2393 (0.8318) acc 78.1250 (86.2500) lr 7.9721e+00 eta 0:03:07\n",
            "epoch [115/200] batch [5/5] time 0.316 (0.432) data 0.000 (0.116) loss 1.6388 (1.2048) acc 53.1250 (71.8750) lr 7.8186e+00 eta 0:03:03\n",
            "epoch [116/200] batch [5/5] time 0.316 (0.430) data 0.000 (0.113) loss 1.1744 (1.1995) acc 78.1250 (73.7500) lr 7.6655e+00 eta 0:03:00\n",
            "epoch [117/200] batch [5/5] time 0.317 (0.435) data 0.000 (0.119) loss 1.7969 (1.4050) acc 78.1250 (71.8750) lr 7.5131e+00 eta 0:03:00\n",
            "epoch [118/200] batch [5/5] time 0.313 (0.437) data 0.000 (0.123) loss 0.7618 (0.9034) acc 87.5000 (83.7500) lr 7.3613e+00 eta 0:02:59\n",
            "epoch [119/200] batch [5/5] time 0.316 (0.432) data 0.000 (0.118) loss 0.8785 (0.8423) acc 81.2500 (83.1250) lr 7.2101e+00 eta 0:02:54\n",
            "epoch [120/200] batch [5/5] time 0.315 (0.431) data 0.000 (0.117) loss 0.6748 (0.6043) acc 87.5000 (91.8750) lr 7.0596e+00 eta 0:02:52\n",
            "epoch [121/200] batch [5/5] time 0.316 (0.435) data 0.000 (0.120) loss 0.7326 (0.7859) acc 84.3750 (81.8750) lr 6.9098e+00 eta 0:02:51\n",
            "epoch [122/200] batch [5/5] time 0.318 (0.433) data 0.000 (0.119) loss 1.2495 (1.2021) acc 71.8750 (74.3750) lr 6.7608e+00 eta 0:02:48\n",
            "epoch [123/200] batch [5/5] time 0.315 (0.437) data 0.000 (0.122) loss 1.8121 (0.9912) acc 50.0000 (76.2500) lr 6.6126e+00 eta 0:02:48\n",
            "epoch [124/200] batch [5/5] time 0.315 (0.437) data 0.000 (0.123) loss 0.6866 (0.9778) acc 87.5000 (80.6250) lr 6.4653e+00 eta 0:02:46\n",
            "epoch [125/200] batch [5/5] time 0.315 (0.431) data 0.000 (0.118) loss 0.7913 (0.7169) acc 81.2500 (85.0000) lr 6.3188e+00 eta 0:02:41\n",
            "epoch [126/200] batch [5/5] time 0.315 (0.430) data 0.000 (0.116) loss 0.5341 (0.7820) acc 84.3750 (81.2500) lr 6.1732e+00 eta 0:02:39\n",
            "epoch [127/200] batch [5/5] time 0.314 (0.428) data 0.000 (0.113) loss 0.9932 (0.9188) acc 78.1250 (82.5000) lr 6.0285e+00 eta 0:02:36\n",
            "epoch [128/200] batch [5/5] time 0.315 (0.432) data 0.000 (0.118) loss 0.3742 (0.5624) acc 93.7500 (89.3750) lr 5.8849e+00 eta 0:02:35\n",
            "epoch [129/200] batch [5/5] time 0.313 (0.438) data 0.000 (0.123) loss 0.7303 (0.7879) acc 84.3750 (85.6250) lr 5.7422e+00 eta 0:02:35\n",
            "epoch [130/200] batch [5/5] time 0.316 (0.436) data 0.000 (0.121) loss 0.6278 (0.8595) acc 87.5000 (81.2500) lr 5.6006e+00 eta 0:02:32\n",
            "epoch [131/200] batch [5/5] time 0.313 (0.432) data 0.000 (0.119) loss 0.9006 (0.6421) acc 65.6250 (86.2500) lr 5.4601e+00 eta 0:02:29\n",
            "epoch [132/200] batch [5/5] time 0.314 (0.428) data 0.000 (0.114) loss 0.9170 (0.8010) acc 81.2500 (82.5000) lr 5.3207e+00 eta 0:02:25\n",
            "epoch [133/200] batch [5/5] time 0.314 (0.437) data 0.000 (0.123) loss 0.6387 (0.6685) acc 87.5000 (86.8750) lr 5.1825e+00 eta 0:02:26\n",
            "epoch [134/200] batch [5/5] time 0.314 (0.431) data 0.000 (0.117) loss 1.0070 (0.7006) acc 84.3750 (84.3750) lr 5.0454e+00 eta 0:02:22\n",
            "epoch [135/200] batch [5/5] time 0.314 (0.436) data 0.000 (0.121) loss 0.6991 (0.6144) acc 84.3750 (89.3750) lr 4.9096e+00 eta 0:02:21\n",
            "epoch [136/200] batch [5/5] time 0.315 (0.438) data 0.000 (0.124) loss 0.9654 (0.7548) acc 78.1250 (82.5000) lr 4.7750e+00 eta 0:02:20\n",
            "epoch [137/200] batch [5/5] time 0.316 (0.426) data 0.000 (0.112) loss 0.8603 (0.8301) acc 78.1250 (81.2500) lr 4.6417e+00 eta 0:02:14\n",
            "epoch [138/200] batch [5/5] time 0.316 (0.436) data 0.000 (0.120) loss 1.2451 (0.7386) acc 78.1250 (86.2500) lr 4.5098e+00 eta 0:02:15\n",
            "epoch [139/200] batch [5/5] time 0.314 (0.442) data 0.000 (0.129) loss 0.7629 (0.6566) acc 81.2500 (83.7500) lr 4.3792e+00 eta 0:02:14\n",
            "epoch [140/200] batch [5/5] time 0.314 (0.432) data 0.000 (0.118) loss 0.4991 (0.6935) acc 93.7500 (89.3750) lr 4.2499e+00 eta 0:02:09\n",
            "epoch [141/200] batch [5/5] time 0.314 (0.432) data 0.000 (0.117) loss 0.7737 (0.5518) acc 81.2500 (91.2500) lr 4.1221e+00 eta 0:02:07\n",
            "epoch [142/200] batch [5/5] time 0.315 (0.435) data 0.000 (0.121) loss 0.7797 (0.5899) acc 84.3750 (89.3750) lr 3.9958e+00 eta 0:02:06\n",
            "epoch [143/200] batch [5/5] time 0.314 (0.429) data 0.000 (0.115) loss 0.5056 (0.5800) acc 87.5000 (86.8750) lr 3.8709e+00 eta 0:02:02\n",
            "epoch [144/200] batch [5/5] time 0.315 (0.424) data 0.000 (0.111) loss 0.3297 (0.4330) acc 96.8750 (94.3750) lr 3.7476e+00 eta 0:01:58\n",
            "epoch [145/200] batch [5/5] time 0.315 (0.429) data 0.000 (0.116) loss 0.3593 (0.4971) acc 96.8750 (91.8750) lr 3.6258e+00 eta 0:01:58\n",
            "epoch [146/200] batch [5/5] time 0.315 (0.428) data 0.000 (0.115) loss 0.3096 (0.6683) acc 96.8750 (90.0000) lr 3.5055e+00 eta 0:01:55\n",
            "epoch [147/200] batch [5/5] time 0.318 (0.430) data 0.000 (0.116) loss 0.6763 (0.5900) acc 87.5000 (89.3750) lr 3.3869e+00 eta 0:01:53\n",
            "epoch [148/200] batch [5/5] time 0.317 (0.429) data 0.000 (0.115) loss 0.6728 (0.7219) acc 81.2500 (84.3750) lr 3.2699e+00 eta 0:01:51\n",
            "epoch [149/200] batch [5/5] time 0.317 (0.435) data 0.000 (0.122) loss 0.6497 (0.5529) acc 81.2500 (88.7500) lr 3.1545e+00 eta 0:01:51\n",
            "epoch [150/200] batch [5/5] time 0.318 (0.430) data 0.000 (0.117) loss 0.5699 (0.6846) acc 87.5000 (86.8750) lr 3.0409e+00 eta 0:01:47\n",
            "epoch [151/200] batch [5/5] time 0.314 (0.431) data 0.000 (0.118) loss 0.5619 (0.4931) acc 87.5000 (90.0000) lr 2.9289e+00 eta 0:01:45\n",
            "epoch [152/200] batch [5/5] time 0.316 (0.437) data 0.000 (0.124) loss 0.3393 (0.4415) acc 100.0000 (94.3750) lr 2.8187e+00 eta 0:01:44\n",
            "epoch [153/200] batch [5/5] time 0.317 (0.433) data 0.000 (0.119) loss 0.5581 (0.4527) acc 90.6250 (91.8750) lr 2.7103e+00 eta 0:01:41\n",
            "epoch [154/200] batch [5/5] time 0.311 (0.430) data 0.000 (0.118) loss 0.6559 (0.4522) acc 87.5000 (93.1250) lr 2.6037e+00 eta 0:01:38\n",
            "epoch [155/200] batch [5/5] time 0.312 (0.428) data 0.000 (0.115) loss 0.3488 (0.4379) acc 96.8750 (93.7500) lr 2.4989e+00 eta 0:01:36\n",
            "epoch [156/200] batch [5/5] time 0.312 (0.430) data 0.000 (0.117) loss 0.4741 (0.4574) acc 93.7500 (91.8750) lr 2.3959e+00 eta 0:01:34\n",
            "epoch [157/200] batch [5/5] time 0.315 (0.435) data 0.000 (0.120) loss 0.3137 (0.4446) acc 96.8750 (91.8750) lr 2.2949e+00 eta 0:01:33\n",
            "epoch [158/200] batch [5/5] time 0.315 (0.433) data 0.000 (0.119) loss 0.4530 (0.5360) acc 90.6250 (88.7500) lr 2.1957e+00 eta 0:01:30\n",
            "epoch [159/200] batch [5/5] time 0.319 (0.445) data 0.000 (0.130) loss 0.7076 (0.5137) acc 87.5000 (90.6250) lr 2.0984e+00 eta 0:01:31\n",
            "epoch [160/200] batch [5/5] time 0.315 (0.440) data 0.000 (0.127) loss 0.7048 (0.5569) acc 84.3750 (89.3750) lr 2.0032e+00 eta 0:01:28\n",
            "epoch [161/200] batch [5/5] time 0.313 (0.431) data 0.000 (0.117) loss 0.5761 (0.5994) acc 90.6250 (88.7500) lr 1.9098e+00 eta 0:01:24\n",
            "epoch [162/200] batch [5/5] time 0.317 (0.436) data 0.000 (0.122) loss 0.4308 (0.3860) acc 90.6250 (94.3750) lr 1.8185e+00 eta 0:01:22\n",
            "epoch [163/200] batch [5/5] time 0.314 (0.435) data 0.000 (0.121) loss 0.3301 (0.4431) acc 100.0000 (95.0000) lr 1.7292e+00 eta 0:01:20\n",
            "epoch [164/200] batch [5/5] time 0.316 (0.437) data 0.000 (0.123) loss 0.6431 (0.4786) acc 93.7500 (93.7500) lr 1.6419e+00 eta 0:01:18\n",
            "epoch [165/200] batch [5/5] time 0.315 (0.437) data 0.000 (0.124) loss 0.3342 (0.4119) acc 100.0000 (96.8750) lr 1.5567e+00 eta 0:01:16\n",
            "epoch [166/200] batch [5/5] time 0.316 (0.431) data 0.000 (0.117) loss 0.3261 (0.3925) acc 96.8750 (94.3750) lr 1.4736e+00 eta 0:01:13\n",
            "epoch [167/200] batch [5/5] time 0.314 (0.433) data 0.000 (0.119) loss 0.5157 (0.3934) acc 90.6250 (93.7500) lr 1.3926e+00 eta 0:01:11\n",
            "epoch [168/200] batch [5/5] time 0.315 (0.431) data 0.000 (0.119) loss 0.4586 (0.4440) acc 90.6250 (92.5000) lr 1.3137e+00 eta 0:01:09\n",
            "epoch [169/200] batch [5/5] time 0.317 (0.432) data 0.000 (0.118) loss 0.3092 (0.3400) acc 96.8750 (93.7500) lr 1.2369e+00 eta 0:01:06\n",
            "epoch [170/200] batch [5/5] time 0.316 (0.432) data 0.000 (0.118) loss 0.4419 (0.3872) acc 90.6250 (94.3750) lr 1.1623e+00 eta 0:01:04\n",
            "epoch [171/200] batch [5/5] time 0.313 (0.431) data 0.000 (0.117) loss 0.5706 (0.4655) acc 93.7500 (90.0000) lr 1.0899e+00 eta 0:01:02\n",
            "epoch [172/200] batch [5/5] time 0.312 (0.435) data 0.000 (0.122) loss 0.3138 (0.2969) acc 90.6250 (95.6250) lr 1.0197e+00 eta 0:01:00\n",
            "epoch [173/200] batch [5/5] time 0.316 (0.435) data 0.001 (0.121) loss 0.2547 (0.3404) acc 96.8750 (95.0000) lr 9.5173e-01 eta 0:00:58\n",
            "epoch [174/200] batch [5/5] time 0.316 (0.433) data 0.000 (0.119) loss 0.2522 (0.3166) acc 100.0000 (96.8750) lr 8.8597e-01 eta 0:00:56\n",
            "epoch [175/200] batch [5/5] time 0.312 (0.433) data 0.000 (0.120) loss 0.2233 (0.3106) acc 100.0000 (97.5000) lr 8.2245e-01 eta 0:00:54\n",
            "epoch [176/200] batch [5/5] time 0.314 (0.428) data 0.000 (0.115) loss 0.2881 (0.3530) acc 96.8750 (95.6250) lr 7.6120e-01 eta 0:00:51\n",
            "epoch [177/200] batch [5/5] time 0.313 (0.435) data 0.000 (0.123) loss 0.2283 (0.3133) acc 100.0000 (95.6250) lr 7.0224e-01 eta 0:00:49\n",
            "epoch [178/200] batch [5/5] time 0.316 (0.437) data 0.000 (0.122) loss 0.3025 (0.3466) acc 96.8750 (94.3750) lr 6.4556e-01 eta 0:00:48\n",
            "epoch [179/200] batch [5/5] time 0.314 (0.436) data 0.000 (0.123) loss 0.3060 (0.2967) acc 96.8750 (96.8750) lr 5.9119e-01 eta 0:00:45\n",
            "epoch [180/200] batch [5/5] time 0.313 (0.433) data 0.000 (0.121) loss 0.3243 (0.3259) acc 96.8750 (96.8750) lr 5.3915e-01 eta 0:00:43\n",
            "epoch [181/200] batch [5/5] time 0.310 (0.424) data 0.000 (0.114) loss 0.2421 (0.2450) acc 100.0000 (98.7500) lr 4.8943e-01 eta 0:00:40\n",
            "epoch [182/200] batch [5/5] time 0.312 (0.430) data 0.000 (0.118) loss 0.4193 (0.4111) acc 90.6250 (92.5000) lr 4.4207e-01 eta 0:00:38\n",
            "epoch [183/200] batch [5/5] time 0.313 (0.431) data 0.000 (0.118) loss 0.4967 (0.3307) acc 90.6250 (95.6250) lr 3.9706e-01 eta 0:00:36\n",
            "epoch [184/200] batch [5/5] time 0.315 (0.429) data 0.000 (0.115) loss 0.3627 (0.4066) acc 96.8750 (95.0000) lr 3.5443e-01 eta 0:00:34\n",
            "epoch [185/200] batch [5/5] time 0.313 (0.425) data 0.000 (0.112) loss 0.2572 (0.3497) acc 96.8750 (95.0000) lr 3.1417e-01 eta 0:00:31\n",
            "epoch [186/200] batch [5/5] time 0.311 (0.431) data 0.000 (0.120) loss 0.2403 (0.2651) acc 100.0000 (97.5000) lr 2.7630e-01 eta 0:00:30\n",
            "epoch [187/200] batch [5/5] time 0.310 (0.431) data 0.000 (0.119) loss 0.2937 (0.2715) acc 96.8750 (97.5000) lr 2.4083e-01 eta 0:00:28\n",
            "epoch [188/200] batch [5/5] time 0.313 (0.437) data 0.000 (0.124) loss 0.3103 (0.2701) acc 90.6250 (96.2500) lr 2.0777e-01 eta 0:00:26\n",
            "epoch [189/200] batch [5/5] time 0.313 (0.433) data 0.000 (0.120) loss 0.2526 (0.2717) acc 96.8750 (98.1250) lr 1.7713e-01 eta 0:00:23\n",
            "epoch [190/200] batch [5/5] time 0.314 (0.435) data 0.000 (0.121) loss 0.2807 (0.3243) acc 96.8750 (96.8750) lr 1.4891e-01 eta 0:00:21\n",
            "epoch [191/200] batch [5/5] time 0.315 (0.432) data 0.000 (0.119) loss 0.3132 (0.3110) acc 96.8750 (96.8750) lr 1.2312e-01 eta 0:00:19\n",
            "epoch [192/200] batch [5/5] time 0.312 (0.431) data 0.000 (0.120) loss 0.2178 (0.2583) acc 100.0000 (98.7500) lr 9.9763e-02 eta 0:00:17\n",
            "epoch [193/200] batch [5/5] time 0.315 (0.427) data 0.000 (0.114) loss 0.5718 (0.3234) acc 93.7500 (96.8750) lr 7.8853e-02 eta 0:00:14\n",
            "epoch [194/200] batch [5/5] time 0.317 (0.432) data 0.000 (0.119) loss 0.2308 (0.2283) acc 100.0000 (98.7500) lr 6.0390e-02 eta 0:00:12\n",
            "epoch [195/200] batch [5/5] time 0.313 (0.433) data 0.000 (0.119) loss 0.2485 (0.2748) acc 96.8750 (97.5000) lr 4.4380e-02 eta 0:00:10\n",
            "epoch [196/200] batch [5/5] time 0.311 (0.434) data 0.000 (0.122) loss 0.2453 (0.2677) acc 100.0000 (98.7500) lr 3.0827e-02 eta 0:00:08\n",
            "epoch [197/200] batch [5/5] time 0.316 (0.437) data 0.000 (0.124) loss 0.3218 (0.2974) acc 93.7500 (95.6250) lr 1.9733e-02 eta 0:00:06\n",
            "epoch [198/200] batch [5/5] time 0.314 (0.431) data 0.000 (0.118) loss 0.2058 (0.2374) acc 100.0000 (98.7500) lr 1.1101e-02 eta 0:00:04\n",
            "epoch [199/200] batch [5/5] time 0.312 (0.431) data 0.000 (0.119) loss 0.2176 (0.2727) acc 100.0000 (98.1250) lr 4.9344e-03 eta 0:00:02\n",
            "epoch [200/200] batch [5/5] time 0.313 (0.428) data 0.000 (0.115) loss 0.2587 (0.2537) acc 96.8750 (97.5000) lr 1.2337e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/eurosat/DAPT/vit_b16_16shots/seed1/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [11:41<00:00,  8.66s/it]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 7,379\n",
            "* accuracy: 91.1%\n",
            "* error: 8.9%\n",
            "* macro_f1: 90.9%\n",
            "Elapsed: 0:19:07\n"
          ]
        }
      ],
      "source": [
        "#eurosat-16shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_init/eurosat/DAPT/vit_b16_16shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "R407bmfCvusQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8279982f-1a0f-4f39-8b00-731a4d85d79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:38:27.812273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:38:27.832776: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:38:27.839148: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:38:27.854736: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:38:28.856725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8']\n",
            "output_dir: output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  80\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed1/tensorboard)\n",
            "epoch [1/200] batch [1/2] time 1.802 (1.802) data 0.568 (0.568) loss 8.7022 (8.7022) acc 25.0000 (25.0000) lr 1.0000e-05 eta 0:11:59\n",
            "epoch [1/200] batch [2/2] time 0.296 (1.049) data 0.000 (0.284) loss 8.7470 (8.7246) acc 34.3750 (29.6875) lr 2.0000e+01 eta 0:06:57\n",
            "epoch [2/200] batch [1/2] time 0.714 (0.714) data 0.420 (0.420) loss 8.5457 (8.5457) acc 21.8750 (21.8750) lr 2.0000e+01 eta 0:04:43\n",
            "epoch [2/200] batch [2/2] time 0.298 (0.506) data 0.000 (0.210) loss 7.3471 (7.9464) acc 12.5000 (17.1875) lr 1.9999e+01 eta 0:03:20\n",
            "epoch [3/200] batch [1/2] time 0.736 (0.736) data 0.441 (0.441) loss 5.4235 (5.4235) acc 15.6250 (15.6250) lr 1.9999e+01 eta 0:04:50\n",
            "epoch [3/200] batch [2/2] time 0.299 (0.518) data 0.001 (0.221) loss 11.7754 (8.5995) acc 0.0000 (7.8125) lr 1.9995e+01 eta 0:03:24\n",
            "epoch [4/200] batch [1/2] time 0.713 (0.713) data 0.417 (0.417) loss 6.0972 (6.0972) acc 15.6250 (15.6250) lr 1.9995e+01 eta 0:04:40\n",
            "epoch [4/200] batch [2/2] time 0.300 (0.507) data 0.001 (0.209) loss 6.2435 (6.1703) acc 12.5000 (14.0625) lr 1.9989e+01 eta 0:03:18\n",
            "epoch [5/200] batch [1/2] time 0.767 (0.767) data 0.471 (0.471) loss 4.5442 (4.5442) acc 25.0000 (25.0000) lr 1.9989e+01 eta 0:04:59\n",
            "epoch [5/200] batch [2/2] time 0.301 (0.534) data 0.000 (0.236) loss 4.8517 (4.6980) acc 9.3750 (17.1875) lr 1.9980e+01 eta 0:03:28\n",
            "epoch [6/200] batch [1/2] time 0.715 (0.715) data 0.419 (0.419) loss 4.1223 (4.1223) acc 25.0000 (25.0000) lr 1.9980e+01 eta 0:04:37\n",
            "epoch [6/200] batch [2/2] time 0.300 (0.507) data 0.000 (0.209) loss 3.8224 (3.9724) acc 28.1250 (26.5625) lr 1.9969e+01 eta 0:03:16\n",
            "epoch [7/200] batch [1/2] time 0.710 (0.710) data 0.414 (0.414) loss 3.3630 (3.3630) acc 28.1250 (28.1250) lr 1.9969e+01 eta 0:04:34\n",
            "epoch [7/200] batch [2/2] time 0.300 (0.505) data 0.000 (0.207) loss 3.4472 (3.4051) acc 31.2500 (29.6875) lr 1.9956e+01 eta 0:03:14\n",
            "epoch [8/200] batch [1/2] time 0.726 (0.726) data 0.429 (0.429) loss 3.1923 (3.1923) acc 18.7500 (18.7500) lr 1.9956e+01 eta 0:04:39\n",
            "epoch [8/200] batch [2/2] time 0.300 (0.513) data 0.000 (0.215) loss 3.0548 (3.1236) acc 25.0000 (21.8750) lr 1.9940e+01 eta 0:03:16\n",
            "epoch [9/200] batch [1/2] time 0.699 (0.699) data 0.404 (0.404) loss 3.1777 (3.1777) acc 25.0000 (25.0000) lr 1.9940e+01 eta 0:04:27\n",
            "epoch [9/200] batch [2/2] time 0.299 (0.499) data 0.000 (0.202) loss 2.5893 (2.8835) acc 43.7500 (34.3750) lr 1.9921e+01 eta 0:03:10\n",
            "epoch [10/200] batch [1/2] time 0.734 (0.734) data 0.438 (0.438) loss 2.5384 (2.5384) acc 46.8750 (46.8750) lr 1.9921e+01 eta 0:04:39\n",
            "epoch [10/200] batch [2/2] time 0.302 (0.518) data 0.000 (0.219) loss 2.5844 (2.5614) acc 37.5000 (42.1875) lr 1.9900e+01 eta 0:03:16\n",
            "epoch [11/200] batch [1/2] time 0.698 (0.698) data 0.400 (0.400) loss 2.5723 (2.5723) acc 40.6250 (40.6250) lr 1.9900e+01 eta 0:04:24\n",
            "epoch [11/200] batch [2/2] time 0.299 (0.499) data 0.000 (0.200) loss 2.3875 (2.4799) acc 40.6250 (40.6250) lr 1.9877e+01 eta 0:03:08\n",
            "epoch [12/200] batch [1/2] time 0.713 (0.713) data 0.418 (0.418) loss 2.1161 (2.1161) acc 43.7500 (43.7500) lr 1.9877e+01 eta 0:04:28\n",
            "epoch [12/200] batch [2/2] time 0.301 (0.507) data 0.000 (0.209) loss 2.0350 (2.0756) acc 50.0000 (46.8750) lr 1.9851e+01 eta 0:03:10\n",
            "epoch [13/200] batch [1/2] time 0.692 (0.692) data 0.395 (0.395) loss 1.9848 (1.9848) acc 53.1250 (53.1250) lr 1.9851e+01 eta 0:04:19\n",
            "epoch [13/200] batch [2/2] time 0.300 (0.496) data 0.001 (0.198) loss 2.7536 (2.3692) acc 40.6250 (46.8750) lr 1.9823e+01 eta 0:03:05\n",
            "epoch [14/200] batch [1/2] time 0.719 (0.719) data 0.423 (0.423) loss 1.5517 (1.5517) acc 71.8750 (71.8750) lr 1.9823e+01 eta 0:04:28\n",
            "epoch [14/200] batch [2/2] time 0.303 (0.511) data 0.001 (0.212) loss 2.3966 (1.9742) acc 50.0000 (60.9375) lr 1.9792e+01 eta 0:03:10\n",
            "epoch [15/200] batch [1/2] time 0.737 (0.737) data 0.439 (0.439) loss 1.9607 (1.9607) acc 59.3750 (59.3750) lr 1.9792e+01 eta 0:04:33\n",
            "epoch [15/200] batch [2/2] time 0.303 (0.520) data 0.001 (0.220) loss 3.3772 (2.6690) acc 28.1250 (43.7500) lr 1.9759e+01 eta 0:03:12\n",
            "epoch [16/200] batch [1/2] time 0.721 (0.721) data 0.425 (0.425) loss 4.2001 (4.2001) acc 53.1250 (53.1250) lr 1.9759e+01 eta 0:04:26\n",
            "epoch [16/200] batch [2/2] time 0.300 (0.511) data 0.001 (0.213) loss 1.7384 (2.9693) acc 59.3750 (56.2500) lr 1.9724e+01 eta 0:03:07\n",
            "epoch [17/200] batch [1/2] time 0.726 (0.726) data 0.428 (0.428) loss 1.6730 (1.6730) acc 62.5000 (62.5000) lr 1.9724e+01 eta 0:04:26\n",
            "epoch [17/200] batch [2/2] time 0.304 (0.515) data 0.000 (0.214) loss 1.5488 (1.6109) acc 62.5000 (62.5000) lr 1.9686e+01 eta 0:03:08\n",
            "epoch [18/200] batch [1/2] time 0.729 (0.729) data 0.431 (0.431) loss 2.3451 (2.3451) acc 53.1250 (53.1250) lr 1.9686e+01 eta 0:04:26\n",
            "epoch [18/200] batch [2/2] time 0.303 (0.516) data 0.000 (0.216) loss 2.3742 (2.3596) acc 46.8750 (50.0000) lr 1.9646e+01 eta 0:03:07\n",
            "epoch [19/200] batch [1/2] time 0.696 (0.696) data 0.397 (0.397) loss 3.0234 (3.0234) acc 40.6250 (40.6250) lr 1.9646e+01 eta 0:04:12\n",
            "epoch [19/200] batch [2/2] time 0.303 (0.500) data 0.000 (0.199) loss 6.3855 (4.7044) acc 37.5000 (39.0625) lr 1.9603e+01 eta 0:03:00\n",
            "epoch [20/200] batch [1/2] time 0.722 (0.722) data 0.424 (0.424) loss 5.4790 (5.4790) acc 12.5000 (12.5000) lr 1.9603e+01 eta 0:04:20\n",
            "epoch [20/200] batch [2/2] time 0.304 (0.513) data 0.001 (0.212) loss 6.0062 (5.7426) acc 28.1250 (20.3125) lr 1.9558e+01 eta 0:03:04\n",
            "epoch [21/200] batch [1/2] time 0.731 (0.731) data 0.433 (0.433) loss 6.6552 (6.6552) acc 21.8750 (21.8750) lr 1.9558e+01 eta 0:04:22\n",
            "epoch [21/200] batch [2/2] time 0.305 (0.518) data 0.000 (0.217) loss 5.6814 (6.1683) acc 21.8750 (21.8750) lr 1.9511e+01 eta 0:03:05\n",
            "epoch [22/200] batch [1/2] time 0.745 (0.745) data 0.449 (0.449) loss 3.8294 (3.8294) acc 40.6250 (40.6250) lr 1.9511e+01 eta 0:04:26\n",
            "epoch [22/200] batch [2/2] time 0.305 (0.525) data 0.000 (0.225) loss 6.5201 (5.1748) acc 15.6250 (28.1250) lr 1.9461e+01 eta 0:03:06\n",
            "epoch [23/200] batch [1/2] time 0.718 (0.718) data 0.418 (0.418) loss 2.8313 (2.8313) acc 37.5000 (37.5000) lr 1.9461e+01 eta 0:04:15\n",
            "epoch [23/200] batch [2/2] time 0.303 (0.511) data 0.000 (0.209) loss 1.7958 (2.3135) acc 53.1250 (45.3125) lr 1.9409e+01 eta 0:03:00\n",
            "epoch [24/200] batch [1/2] time 0.720 (0.720) data 0.423 (0.423) loss 1.7631 (1.7631) acc 46.8750 (46.8750) lr 1.9409e+01 eta 0:04:14\n",
            "epoch [24/200] batch [2/2] time 0.301 (0.510) data 0.001 (0.212) loss 1.8872 (1.8252) acc 50.0000 (48.4375) lr 1.9354e+01 eta 0:02:59\n",
            "epoch [25/200] batch [1/2] time 0.721 (0.721) data 0.423 (0.423) loss 1.4213 (1.4213) acc 71.8750 (71.8750) lr 1.9354e+01 eta 0:04:13\n",
            "epoch [25/200] batch [2/2] time 0.303 (0.512) data 0.000 (0.212) loss 1.2198 (1.3206) acc 78.1250 (75.0000) lr 1.9298e+01 eta 0:02:59\n",
            "epoch [26/200] batch [1/2] time 0.740 (0.740) data 0.441 (0.441) loss 0.9458 (0.9458) acc 84.3750 (84.3750) lr 1.9298e+01 eta 0:04:18\n",
            "epoch [26/200] batch [2/2] time 0.302 (0.521) data 0.000 (0.221) loss 1.2766 (1.1112) acc 65.6250 (75.0000) lr 1.9239e+01 eta 0:03:01\n",
            "epoch [27/200] batch [1/2] time 0.732 (0.732) data 0.432 (0.432) loss 1.1647 (1.1647) acc 78.1250 (78.1250) lr 1.9239e+01 eta 0:04:14\n",
            "epoch [27/200] batch [2/2] time 0.302 (0.517) data 0.000 (0.216) loss 1.2421 (1.2034) acc 78.1250 (78.1250) lr 1.9178e+01 eta 0:02:58\n",
            "epoch [28/200] batch [1/2] time 0.747 (0.747) data 0.449 (0.449) loss 0.7887 (0.7887) acc 84.3750 (84.3750) lr 1.9178e+01 eta 0:04:17\n",
            "epoch [28/200] batch [2/2] time 0.304 (0.526) data 0.001 (0.225) loss 0.7102 (0.7494) acc 90.6250 (87.5000) lr 1.9114e+01 eta 0:03:00\n",
            "epoch [29/200] batch [1/2] time 0.752 (0.752) data 0.453 (0.453) loss 0.8341 (0.8341) acc 78.1250 (78.1250) lr 1.9114e+01 eta 0:04:17\n",
            "epoch [29/200] batch [2/2] time 0.302 (0.527) data 0.000 (0.227) loss 0.6323 (0.7332) acc 93.7500 (85.9375) lr 1.9048e+01 eta 0:03:00\n",
            "epoch [30/200] batch [1/2] time 0.708 (0.708) data 0.409 (0.409) loss 0.8243 (0.8243) acc 84.3750 (84.3750) lr 1.9048e+01 eta 0:04:01\n",
            "epoch [30/200] batch [2/2] time 0.303 (0.506) data 0.001 (0.205) loss 0.7128 (0.7686) acc 90.6250 (87.5000) lr 1.8980e+01 eta 0:02:51\n",
            "epoch [31/200] batch [1/2] time 0.700 (0.700) data 0.403 (0.403) loss 0.7347 (0.7347) acc 84.3750 (84.3750) lr 1.8980e+01 eta 0:03:57\n",
            "epoch [31/200] batch [2/2] time 0.302 (0.501) data 0.001 (0.202) loss 1.3247 (1.0297) acc 65.6250 (75.0000) lr 1.8910e+01 eta 0:02:49\n",
            "epoch [32/200] batch [1/2] time 0.737 (0.737) data 0.438 (0.438) loss 1.1014 (1.1014) acc 71.8750 (71.8750) lr 1.8910e+01 eta 0:04:08\n",
            "epoch [32/200] batch [2/2] time 0.305 (0.521) data 0.000 (0.219) loss 2.4359 (1.7686) acc 62.5000 (67.1875) lr 1.8838e+01 eta 0:02:55\n",
            "epoch [33/200] batch [1/2] time 0.706 (0.706) data 0.407 (0.407) loss 1.2027 (1.2027) acc 68.7500 (68.7500) lr 1.8838e+01 eta 0:03:56\n",
            "epoch [33/200] batch [2/2] time 0.307 (0.507) data 0.000 (0.204) loss 2.8563 (2.0295) acc 43.7500 (56.2500) lr 1.8763e+01 eta 0:02:49\n",
            "epoch [34/200] batch [1/2] time 0.730 (0.730) data 0.430 (0.430) loss 2.7782 (2.7782) acc 43.7500 (43.7500) lr 1.8763e+01 eta 0:04:02\n",
            "epoch [34/200] batch [2/2] time 0.304 (0.517) data 0.000 (0.215) loss 4.2205 (3.4994) acc 40.6250 (42.1875) lr 1.8686e+01 eta 0:02:51\n",
            "epoch [35/200] batch [1/2] time 0.708 (0.708) data 0.409 (0.409) loss 3.6190 (3.6190) acc 50.0000 (50.0000) lr 1.8686e+01 eta 0:03:54\n",
            "epoch [35/200] batch [2/2] time 0.306 (0.507) data 0.000 (0.205) loss 2.5614 (3.0902) acc 50.0000 (50.0000) lr 1.8607e+01 eta 0:02:47\n",
            "epoch [36/200] batch [1/2] time 0.714 (0.714) data 0.416 (0.416) loss 10.1533 (10.1533) acc 9.3750 (9.3750) lr 1.8607e+01 eta 0:03:54\n",
            "epoch [36/200] batch [2/2] time 0.307 (0.510) data 0.000 (0.208) loss 6.9850 (8.5692) acc 37.5000 (23.4375) lr 1.8526e+01 eta 0:02:47\n",
            "epoch [37/200] batch [1/2] time 0.713 (0.713) data 0.412 (0.412) loss 3.7539 (3.7539) acc 43.7500 (43.7500) lr 1.8526e+01 eta 0:03:53\n",
            "epoch [37/200] batch [2/2] time 0.308 (0.510) data 0.001 (0.206) loss 4.5839 (4.1689) acc 21.8750 (32.8125) lr 1.8443e+01 eta 0:02:46\n",
            "epoch [38/200] batch [1/2] time 0.705 (0.705) data 0.402 (0.402) loss 3.0882 (3.0882) acc 59.3750 (59.3750) lr 1.8443e+01 eta 0:03:49\n",
            "epoch [38/200] batch [2/2] time 0.307 (0.506) data 0.001 (0.202) loss 4.6619 (3.8751) acc 37.5000 (48.4375) lr 1.8358e+01 eta 0:02:44\n",
            "epoch [39/200] batch [1/2] time 0.725 (0.725) data 0.424 (0.424) loss 3.4525 (3.4525) acc 34.3750 (34.3750) lr 1.8358e+01 eta 0:03:54\n",
            "epoch [39/200] batch [2/2] time 0.305 (0.515) data 0.000 (0.212) loss 6.3801 (4.9163) acc 3.1250 (18.7500) lr 1.8271e+01 eta 0:02:45\n",
            "epoch [40/200] batch [1/2] time 0.742 (0.742) data 0.440 (0.440) loss 5.1553 (5.1553) acc 25.0000 (25.0000) lr 1.8271e+01 eta 0:03:58\n",
            "epoch [40/200] batch [2/2] time 0.307 (0.525) data 0.001 (0.220) loss 4.4006 (4.7780) acc 21.8750 (23.4375) lr 1.8181e+01 eta 0:02:47\n",
            "epoch [41/200] batch [1/2] time 0.723 (0.723) data 0.419 (0.419) loss 4.6830 (4.6830) acc 18.7500 (18.7500) lr 1.8181e+01 eta 0:03:50\n",
            "epoch [41/200] batch [2/2] time 0.309 (0.516) data 0.000 (0.210) loss 5.3749 (5.0289) acc 28.1250 (23.4375) lr 1.8090e+01 eta 0:02:44\n",
            "epoch [42/200] batch [1/2] time 0.743 (0.743) data 0.440 (0.440) loss 5.8051 (5.8051) acc 34.3750 (34.3750) lr 1.8090e+01 eta 0:03:55\n",
            "epoch [42/200] batch [2/2] time 0.304 (0.524) data 0.000 (0.220) loss 2.7300 (4.2676) acc 50.0000 (42.1875) lr 1.7997e+01 eta 0:02:45\n",
            "epoch [43/200] batch [1/2] time 0.729 (0.729) data 0.430 (0.430) loss 2.1038 (2.1038) acc 62.5000 (62.5000) lr 1.7997e+01 eta 0:03:49\n",
            "epoch [43/200] batch [2/2] time 0.305 (0.517) data 0.001 (0.215) loss 1.7933 (1.9486) acc 62.5000 (62.5000) lr 1.7902e+01 eta 0:02:42\n",
            "epoch [44/200] batch [1/2] time 0.741 (0.741) data 0.439 (0.439) loss 1.8962 (1.8962) acc 53.1250 (53.1250) lr 1.7902e+01 eta 0:03:51\n",
            "epoch [44/200] batch [2/2] time 0.309 (0.525) data 0.000 (0.220) loss 1.8900 (1.8931) acc 43.7500 (48.4375) lr 1.7804e+01 eta 0:02:43\n",
            "epoch [45/200] batch [1/2] time 0.695 (0.695) data 0.394 (0.394) loss 1.3469 (1.3469) acc 59.3750 (59.3750) lr 1.7804e+01 eta 0:03:36\n",
            "epoch [45/200] batch [2/2] time 0.306 (0.500) data 0.001 (0.197) loss 1.6634 (1.5051) acc 56.2500 (57.8125) lr 1.7705e+01 eta 0:02:35\n",
            "epoch [46/200] batch [1/2] time 0.720 (0.720) data 0.421 (0.421) loss 1.6544 (1.6544) acc 59.3750 (59.3750) lr 1.7705e+01 eta 0:03:42\n",
            "epoch [46/200] batch [2/2] time 0.306 (0.513) data 0.000 (0.211) loss 1.3441 (1.4992) acc 75.0000 (67.1875) lr 1.7604e+01 eta 0:02:37\n",
            "epoch [47/200] batch [1/2] time 0.708 (0.708) data 0.406 (0.406) loss 1.3508 (1.3508) acc 62.5000 (62.5000) lr 1.7604e+01 eta 0:03:37\n",
            "epoch [47/200] batch [2/2] time 0.308 (0.508) data 0.001 (0.203) loss 1.4710 (1.4109) acc 68.7500 (65.6250) lr 1.7501e+01 eta 0:02:35\n",
            "epoch [48/200] batch [1/2] time 0.714 (0.714) data 0.413 (0.413) loss 1.1531 (1.1531) acc 71.8750 (71.8750) lr 1.7501e+01 eta 0:03:37\n",
            "epoch [48/200] batch [2/2] time 0.306 (0.510) data 0.000 (0.207) loss 0.9665 (1.0598) acc 84.3750 (78.1250) lr 1.7396e+01 eta 0:02:35\n",
            "epoch [49/200] batch [1/2] time 0.728 (0.728) data 0.425 (0.425) loss 0.9799 (0.9799) acc 81.2500 (81.2500) lr 1.7396e+01 eta 0:03:40\n",
            "epoch [49/200] batch [2/2] time 0.307 (0.518) data 0.001 (0.213) loss 0.7550 (0.8675) acc 81.2500 (81.2500) lr 1.7290e+01 eta 0:02:36\n",
            "epoch [50/200] batch [1/2] time 0.725 (0.725) data 0.421 (0.421) loss 0.8167 (0.8167) acc 87.5000 (87.5000) lr 1.7290e+01 eta 0:03:38\n",
            "epoch [50/200] batch [2/2] time 0.307 (0.516) data 0.001 (0.211) loss 1.0237 (0.9202) acc 68.7500 (78.1250) lr 1.7181e+01 eta 0:02:34\n",
            "epoch [51/200] batch [1/2] time 0.771 (0.771) data 0.469 (0.469) loss 1.1019 (1.1019) acc 75.0000 (75.0000) lr 1.7181e+01 eta 0:03:50\n",
            "epoch [51/200] batch [2/2] time 0.310 (0.541) data 0.001 (0.235) loss 0.9760 (1.0389) acc 68.7500 (71.8750) lr 1.7071e+01 eta 0:02:41\n",
            "epoch [52/200] batch [1/2] time 0.738 (0.738) data 0.436 (0.436) loss 1.7075 (1.7075) acc 59.3750 (59.3750) lr 1.7071e+01 eta 0:03:39\n",
            "epoch [52/200] batch [2/2] time 0.304 (0.521) data 0.000 (0.218) loss 1.4421 (1.5748) acc 75.0000 (67.1875) lr 1.6959e+01 eta 0:02:34\n",
            "epoch [53/200] batch [1/2] time 0.779 (0.779) data 0.476 (0.476) loss 1.4495 (1.4495) acc 71.8750 (71.8750) lr 1.6959e+01 eta 0:03:49\n",
            "epoch [53/200] batch [2/2] time 0.308 (0.543) data 0.000 (0.238) loss 1.5684 (1.5090) acc 56.2500 (64.0625) lr 1.6845e+01 eta 0:02:39\n",
            "epoch [54/200] batch [1/2] time 0.710 (0.710) data 0.407 (0.407) loss 2.7683 (2.7683) acc 62.5000 (62.5000) lr 1.6845e+01 eta 0:03:27\n",
            "epoch [54/200] batch [2/2] time 0.305 (0.507) data 0.001 (0.204) loss 3.5750 (3.1716) acc 53.1250 (57.8125) lr 1.6730e+01 eta 0:02:28\n",
            "epoch [55/200] batch [1/2] time 0.723 (0.723) data 0.422 (0.422) loss 4.1213 (4.1213) acc 37.5000 (37.5000) lr 1.6730e+01 eta 0:03:30\n",
            "epoch [55/200] batch [2/2] time 0.310 (0.517) data 0.000 (0.211) loss 3.5854 (3.8534) acc 53.1250 (45.3125) lr 1.6613e+01 eta 0:02:29\n",
            "epoch [56/200] batch [1/2] time 0.718 (0.718) data 0.416 (0.416) loss 4.9691 (4.9691) acc 43.7500 (43.7500) lr 1.6613e+01 eta 0:03:27\n",
            "epoch [56/200] batch [2/2] time 0.311 (0.515) data 0.000 (0.208) loss 5.0849 (5.0270) acc 25.0000 (34.3750) lr 1.6494e+01 eta 0:02:28\n",
            "epoch [57/200] batch [1/2] time 0.724 (0.724) data 0.421 (0.421) loss 5.0664 (5.0664) acc 50.0000 (50.0000) lr 1.6494e+01 eta 0:03:27\n",
            "epoch [57/200] batch [2/2] time 0.310 (0.517) data 0.001 (0.211) loss 4.4545 (4.7604) acc 3.1250 (26.5625) lr 1.6374e+01 eta 0:02:27\n",
            "epoch [58/200] batch [1/2] time 0.719 (0.719) data 0.416 (0.416) loss 6.7548 (6.7548) acc 21.8750 (21.8750) lr 1.6374e+01 eta 0:03:24\n",
            "epoch [58/200] batch [2/2] time 0.310 (0.514) data 0.000 (0.208) loss 9.6148 (8.1848) acc 12.5000 (17.1875) lr 1.6252e+01 eta 0:02:26\n",
            "epoch [59/200] batch [1/2] time 0.711 (0.711) data 0.406 (0.406) loss 4.7561 (4.7561) acc 25.0000 (25.0000) lr 1.6252e+01 eta 0:03:21\n",
            "epoch [59/200] batch [2/2] time 0.310 (0.511) data 0.001 (0.203) loss 3.4088 (4.0824) acc 59.3750 (42.1875) lr 1.6129e+01 eta 0:02:24\n",
            "epoch [60/200] batch [1/2] time 0.750 (0.750) data 0.445 (0.445) loss 2.6481 (2.6481) acc 37.5000 (37.5000) lr 1.6129e+01 eta 0:03:30\n",
            "epoch [60/200] batch [2/2] time 0.310 (0.530) data 0.000 (0.223) loss 1.8210 (2.2345) acc 62.5000 (50.0000) lr 1.6004e+01 eta 0:02:28\n",
            "epoch [61/200] batch [1/2] time 0.706 (0.706) data 0.401 (0.401) loss 1.9059 (1.9059) acc 53.1250 (53.1250) lr 1.6004e+01 eta 0:03:16\n",
            "epoch [61/200] batch [2/2] time 0.311 (0.509) data 0.000 (0.201) loss 1.9013 (1.9036) acc 56.2500 (54.6875) lr 1.5878e+01 eta 0:02:21\n",
            "epoch [62/200] batch [1/2] time 0.744 (0.744) data 0.438 (0.438) loss 1.5724 (1.5724) acc 59.3750 (59.3750) lr 1.5878e+01 eta 0:03:26\n",
            "epoch [62/200] batch [2/2] time 0.309 (0.527) data 0.000 (0.219) loss 1.8992 (1.7358) acc 59.3750 (59.3750) lr 1.5750e+01 eta 0:02:25\n",
            "epoch [63/200] batch [1/2] time 0.760 (0.760) data 0.458 (0.458) loss 1.6576 (1.6576) acc 50.0000 (50.0000) lr 1.5750e+01 eta 0:03:29\n",
            "epoch [63/200] batch [2/2] time 0.311 (0.536) data 0.000 (0.229) loss 1.0731 (1.3654) acc 68.7500 (59.3750) lr 1.5621e+01 eta 0:02:26\n",
            "epoch [64/200] batch [1/2] time 0.754 (0.754) data 0.449 (0.449) loss 1.2514 (1.2514) acc 65.6250 (65.6250) lr 1.5621e+01 eta 0:03:25\n",
            "epoch [64/200] batch [2/2] time 0.310 (0.532) data 0.000 (0.225) loss 1.2950 (1.2732) acc 65.6250 (65.6250) lr 1.5490e+01 eta 0:02:24\n",
            "epoch [65/200] batch [1/2] time 0.710 (0.710) data 0.405 (0.405) loss 1.4977 (1.4977) acc 62.5000 (62.5000) lr 1.5490e+01 eta 0:03:12\n",
            "epoch [65/200] batch [2/2] time 0.309 (0.509) data 0.001 (0.203) loss 1.4544 (1.4760) acc 53.1250 (57.8125) lr 1.5358e+01 eta 0:02:17\n",
            "epoch [66/200] batch [1/2] time 0.713 (0.713) data 0.407 (0.407) loss 0.9582 (0.9582) acc 75.0000 (75.0000) lr 1.5358e+01 eta 0:03:11\n",
            "epoch [66/200] batch [2/2] time 0.310 (0.512) data 0.000 (0.204) loss 1.0331 (0.9957) acc 68.7500 (71.8750) lr 1.5225e+01 eta 0:02:17\n",
            "epoch [67/200] batch [1/2] time 0.705 (0.705) data 0.399 (0.399) loss 1.0707 (1.0707) acc 75.0000 (75.0000) lr 1.5225e+01 eta 0:03:08\n",
            "epoch [67/200] batch [2/2] time 0.310 (0.508) data 0.001 (0.200) loss 1.2354 (1.1530) acc 78.1250 (76.5625) lr 1.5090e+01 eta 0:02:15\n",
            "epoch [68/200] batch [1/2] time 0.683 (0.683) data 0.376 (0.376) loss 1.0033 (1.0033) acc 78.1250 (78.1250) lr 1.5090e+01 eta 0:03:00\n",
            "epoch [68/200] batch [2/2] time 0.310 (0.497) data 0.001 (0.188) loss 1.5260 (1.2646) acc 53.1250 (65.6250) lr 1.4955e+01 eta 0:02:11\n",
            "epoch [69/200] batch [1/2] time 0.699 (0.699) data 0.392 (0.392) loss 1.0244 (1.0244) acc 65.6250 (65.6250) lr 1.4955e+01 eta 0:03:03\n",
            "epoch [69/200] batch [2/2] time 0.310 (0.504) data 0.000 (0.196) loss 1.8087 (1.4165) acc 56.2500 (60.9375) lr 1.4818e+01 eta 0:02:12\n",
            "epoch [70/200] batch [1/2] time 0.725 (0.725) data 0.420 (0.420) loss 0.6973 (0.6973) acc 93.7500 (93.7500) lr 1.4818e+01 eta 0:03:09\n",
            "epoch [70/200] batch [2/2] time 0.307 (0.516) data 0.000 (0.210) loss 0.9349 (0.8161) acc 75.0000 (84.3750) lr 1.4679e+01 eta 0:02:14\n",
            "epoch [71/200] batch [1/2] time 0.756 (0.756) data 0.450 (0.450) loss 0.8253 (0.8253) acc 81.2500 (81.2500) lr 1.4679e+01 eta 0:03:15\n",
            "epoch [71/200] batch [2/2] time 0.310 (0.533) data 0.000 (0.225) loss 0.8544 (0.8398) acc 75.0000 (78.1250) lr 1.4540e+01 eta 0:02:17\n",
            "epoch [72/200] batch [1/2] time 0.716 (0.716) data 0.409 (0.409) loss 0.9056 (0.9056) acc 65.6250 (65.6250) lr 1.4540e+01 eta 0:03:03\n",
            "epoch [72/200] batch [2/2] time 0.309 (0.512) data 0.000 (0.205) loss 2.0672 (1.4864) acc 62.5000 (64.0625) lr 1.4399e+01 eta 0:02:11\n",
            "epoch [73/200] batch [1/2] time 0.726 (0.726) data 0.421 (0.421) loss 2.2245 (2.2245) acc 53.1250 (53.1250) lr 1.4399e+01 eta 0:03:05\n",
            "epoch [73/200] batch [2/2] time 0.309 (0.518) data 0.001 (0.211) loss 0.6840 (1.4542) acc 90.6250 (71.8750) lr 1.4258e+01 eta 0:02:11\n",
            "epoch [74/200] batch [1/2] time 0.741 (0.741) data 0.437 (0.437) loss 1.6594 (1.6594) acc 59.3750 (59.3750) lr 1.4258e+01 eta 0:03:07\n",
            "epoch [74/200] batch [2/2] time 0.314 (0.528) data 0.001 (0.219) loss 2.0067 (1.8330) acc 59.3750 (59.3750) lr 1.4115e+01 eta 0:02:12\n",
            "epoch [75/200] batch [1/2] time 0.749 (0.749) data 0.442 (0.442) loss 1.5223 (1.5223) acc 62.5000 (62.5000) lr 1.4115e+01 eta 0:03:07\n",
            "epoch [75/200] batch [2/2] time 0.313 (0.531) data 0.001 (0.221) loss 2.1852 (1.8538) acc 56.2500 (59.3750) lr 1.3971e+01 eta 0:02:12\n",
            "epoch [76/200] batch [1/2] time 0.748 (0.748) data 0.441 (0.441) loss 1.9402 (1.9402) acc 53.1250 (53.1250) lr 1.3971e+01 eta 0:03:06\n",
            "epoch [76/200] batch [2/2] time 0.310 (0.529) data 0.000 (0.221) loss 3.2280 (2.5841) acc 40.6250 (46.8750) lr 1.3827e+01 eta 0:02:11\n",
            "epoch [77/200] batch [1/2] time 0.703 (0.703) data 0.400 (0.400) loss 1.4264 (1.4264) acc 65.6250 (65.6250) lr 1.3827e+01 eta 0:02:53\n",
            "epoch [77/200] batch [2/2] time 0.311 (0.507) data 0.000 (0.200) loss 2.6817 (2.0541) acc 68.7500 (67.1875) lr 1.3681e+01 eta 0:02:04\n",
            "epoch [78/200] batch [1/2] time 0.740 (0.740) data 0.434 (0.434) loss 2.1114 (2.1114) acc 46.8750 (46.8750) lr 1.3681e+01 eta 0:03:01\n",
            "epoch [78/200] batch [2/2] time 0.312 (0.526) data 0.001 (0.217) loss 3.6135 (2.8625) acc 43.7500 (45.3125) lr 1.3535e+01 eta 0:02:08\n",
            "epoch [79/200] batch [1/2] time 0.715 (0.715) data 0.410 (0.410) loss 1.6712 (1.6712) acc 59.3750 (59.3750) lr 1.3535e+01 eta 0:02:53\n",
            "epoch [79/200] batch [2/2] time 0.311 (0.513) data 0.001 (0.205) loss 1.2506 (1.4609) acc 71.8750 (65.6250) lr 1.3387e+01 eta 0:02:04\n",
            "epoch [80/200] batch [1/2] time 0.703 (0.703) data 0.398 (0.398) loss 1.0691 (1.0691) acc 71.8750 (71.8750) lr 1.3387e+01 eta 0:02:49\n",
            "epoch [80/200] batch [2/2] time 0.312 (0.508) data 0.000 (0.199) loss 1.6422 (1.3557) acc 59.3750 (65.6250) lr 1.3239e+01 eta 0:02:01\n",
            "epoch [81/200] batch [1/2] time 0.695 (0.695) data 0.389 (0.389) loss 1.0940 (1.0940) acc 75.0000 (75.0000) lr 1.3239e+01 eta 0:02:46\n",
            "epoch [81/200] batch [2/2] time 0.311 (0.503) data 0.001 (0.195) loss 1.0653 (1.0797) acc 75.0000 (75.0000) lr 1.3090e+01 eta 0:01:59\n",
            "epoch [82/200] batch [1/2] time 0.723 (0.723) data 0.417 (0.417) loss 0.9849 (0.9849) acc 68.7500 (68.7500) lr 1.3090e+01 eta 0:02:51\n",
            "epoch [82/200] batch [2/2] time 0.313 (0.518) data 0.000 (0.209) loss 0.8685 (0.9267) acc 75.0000 (71.8750) lr 1.2940e+01 eta 0:02:02\n",
            "epoch [83/200] batch [1/2] time 0.708 (0.708) data 0.403 (0.403) loss 0.8812 (0.8812) acc 84.3750 (84.3750) lr 1.2940e+01 eta 0:02:46\n",
            "epoch [83/200] batch [2/2] time 0.311 (0.509) data 0.000 (0.202) loss 1.1851 (1.0332) acc 78.1250 (81.2500) lr 1.2790e+01 eta 0:01:59\n",
            "epoch [84/200] batch [1/2] time 0.712 (0.712) data 0.406 (0.406) loss 0.6781 (0.6781) acc 87.5000 (87.5000) lr 1.2790e+01 eta 0:02:45\n",
            "epoch [84/200] batch [2/2] time 0.314 (0.513) data 0.000 (0.203) loss 0.8599 (0.7690) acc 75.0000 (81.2500) lr 1.2639e+01 eta 0:01:59\n",
            "epoch [85/200] batch [1/2] time 0.717 (0.717) data 0.411 (0.411) loss 0.7657 (0.7657) acc 78.1250 (78.1250) lr 1.2639e+01 eta 0:02:45\n",
            "epoch [85/200] batch [2/2] time 0.311 (0.514) data 0.001 (0.206) loss 0.5339 (0.6498) acc 90.6250 (84.3750) lr 1.2487e+01 eta 0:01:58\n",
            "epoch [86/200] batch [1/2] time 0.754 (0.754) data 0.448 (0.448) loss 0.9408 (0.9408) acc 71.8750 (71.8750) lr 1.2487e+01 eta 0:02:52\n",
            "epoch [86/200] batch [2/2] time 0.311 (0.533) data 0.000 (0.224) loss 0.4437 (0.6923) acc 93.7500 (82.8125) lr 1.2334e+01 eta 0:02:01\n",
            "epoch [87/200] batch [1/2] time 0.750 (0.750) data 0.442 (0.442) loss 0.7403 (0.7403) acc 87.5000 (87.5000) lr 1.2334e+01 eta 0:02:50\n",
            "epoch [87/200] batch [2/2] time 0.312 (0.531) data 0.001 (0.222) loss 0.7820 (0.7612) acc 84.3750 (85.9375) lr 1.2181e+01 eta 0:02:00\n",
            "epoch [88/200] batch [1/2] time 0.759 (0.759) data 0.454 (0.454) loss 0.5643 (0.5643) acc 93.7500 (93.7500) lr 1.2181e+01 eta 0:02:50\n",
            "epoch [88/200] batch [2/2] time 0.312 (0.535) data 0.000 (0.227) loss 1.2571 (0.9107) acc 71.8750 (82.8125) lr 1.2028e+01 eta 0:01:59\n",
            "epoch [89/200] batch [1/2] time 0.724 (0.724) data 0.421 (0.421) loss 0.5492 (0.5492) acc 90.6250 (90.6250) lr 1.2028e+01 eta 0:02:41\n",
            "epoch [89/200] batch [2/2] time 0.310 (0.517) data 0.001 (0.211) loss 1.1882 (0.8687) acc 78.1250 (84.3750) lr 1.1874e+01 eta 0:01:54\n",
            "epoch [90/200] batch [1/2] time 0.717 (0.717) data 0.413 (0.413) loss 0.5344 (0.5344) acc 90.6250 (90.6250) lr 1.1874e+01 eta 0:02:38\n",
            "epoch [90/200] batch [2/2] time 0.312 (0.515) data 0.000 (0.207) loss 0.9064 (0.7204) acc 78.1250 (84.3750) lr 1.1719e+01 eta 0:01:53\n",
            "epoch [91/200] batch [1/2] time 0.718 (0.718) data 0.413 (0.413) loss 1.1143 (1.1143) acc 84.3750 (84.3750) lr 1.1719e+01 eta 0:02:37\n",
            "epoch [91/200] batch [2/2] time 0.310 (0.514) data 0.000 (0.207) loss 0.5790 (0.8467) acc 84.3750 (84.3750) lr 1.1564e+01 eta 0:01:52\n",
            "epoch [92/200] batch [1/2] time 0.730 (0.730) data 0.423 (0.423) loss 0.6265 (0.6265) acc 90.6250 (90.6250) lr 1.1564e+01 eta 0:02:38\n",
            "epoch [92/200] batch [2/2] time 0.312 (0.521) data 0.000 (0.212) loss 0.7565 (0.6915) acc 87.5000 (89.0625) lr 1.1409e+01 eta 0:01:52\n",
            "epoch [93/200] batch [1/2] time 0.715 (0.715) data 0.408 (0.408) loss 0.5339 (0.5339) acc 93.7500 (93.7500) lr 1.1409e+01 eta 0:02:33\n",
            "epoch [93/200] batch [2/2] time 0.313 (0.514) data 0.000 (0.204) loss 0.7175 (0.6257) acc 84.3750 (89.0625) lr 1.1253e+01 eta 0:01:49\n",
            "epoch [94/200] batch [1/2] time 0.687 (0.687) data 0.381 (0.381) loss 0.5851 (0.5851) acc 93.7500 (93.7500) lr 1.1253e+01 eta 0:02:26\n",
            "epoch [94/200] batch [2/2] time 0.309 (0.498) data 0.000 (0.191) loss 0.7947 (0.6899) acc 87.5000 (90.6250) lr 1.1097e+01 eta 0:01:45\n",
            "epoch [95/200] batch [1/2] time 0.705 (0.705) data 0.399 (0.399) loss 0.7802 (0.7802) acc 84.3750 (84.3750) lr 1.1097e+01 eta 0:02:28\n",
            "epoch [95/200] batch [2/2] time 0.313 (0.509) data 0.000 (0.200) loss 0.7115 (0.7458) acc 84.3750 (84.3750) lr 1.0941e+01 eta 0:01:46\n",
            "epoch [96/200] batch [1/2] time 0.742 (0.742) data 0.433 (0.433) loss 1.2575 (1.2575) acc 75.0000 (75.0000) lr 1.0941e+01 eta 0:02:35\n",
            "epoch [96/200] batch [2/2] time 0.312 (0.527) data 0.001 (0.217) loss 2.3799 (1.8187) acc 59.3750 (67.1875) lr 1.0785e+01 eta 0:01:49\n",
            "epoch [97/200] batch [1/2] time 0.733 (0.733) data 0.426 (0.426) loss 0.9043 (0.9043) acc 87.5000 (87.5000) lr 1.0785e+01 eta 0:02:31\n",
            "epoch [97/200] batch [2/2] time 0.313 (0.523) data 0.001 (0.213) loss 0.9809 (0.9426) acc 75.0000 (81.2500) lr 1.0628e+01 eta 0:01:47\n",
            "epoch [98/200] batch [1/2] time 0.739 (0.739) data 0.434 (0.434) loss 1.2983 (1.2983) acc 65.6250 (65.6250) lr 1.0628e+01 eta 0:02:31\n",
            "epoch [98/200] batch [2/2] time 0.314 (0.527) data 0.001 (0.217) loss 2.2432 (1.7707) acc 59.3750 (62.5000) lr 1.0471e+01 eta 0:01:47\n",
            "epoch [99/200] batch [1/2] time 0.753 (0.753) data 0.442 (0.442) loss 3.2629 (3.2629) acc 40.6250 (40.6250) lr 1.0471e+01 eta 0:02:32\n",
            "epoch [99/200] batch [2/2] time 0.314 (0.533) data 0.000 (0.221) loss 2.1744 (2.7187) acc 56.2500 (48.4375) lr 1.0314e+01 eta 0:01:47\n",
            "epoch [100/200] batch [1/2] time 0.713 (0.713) data 0.407 (0.407) loss 1.2887 (1.2887) acc 71.8750 (71.8750) lr 1.0314e+01 eta 0:02:23\n",
            "epoch [100/200] batch [2/2] time 0.314 (0.514) data 0.000 (0.204) loss 1.3036 (1.2962) acc 71.8750 (71.8750) lr 1.0157e+01 eta 0:01:42\n",
            "epoch [101/200] batch [1/2] time 0.722 (0.722) data 0.418 (0.418) loss 0.8446 (0.8446) acc 84.3750 (84.3750) lr 1.0157e+01 eta 0:02:23\n",
            "epoch [101/200] batch [2/2] time 0.311 (0.516) data 0.000 (0.209) loss 1.4979 (1.1712) acc 65.6250 (75.0000) lr 1.0000e+01 eta 0:01:42\n",
            "epoch [102/200] batch [1/2] time 0.752 (0.752) data 0.446 (0.446) loss 0.8944 (0.8944) acc 78.1250 (78.1250) lr 1.0000e+01 eta 0:02:28\n",
            "epoch [102/200] batch [2/2] time 0.313 (0.532) data 0.000 (0.223) loss 0.9374 (0.9159) acc 81.2500 (79.6875) lr 9.8429e+00 eta 0:01:44\n",
            "epoch [103/200] batch [1/2] time 0.711 (0.711) data 0.406 (0.406) loss 0.5611 (0.5611) acc 90.6250 (90.6250) lr 9.8429e+00 eta 0:02:18\n",
            "epoch [103/200] batch [2/2] time 0.312 (0.511) data 0.000 (0.203) loss 0.6359 (0.5985) acc 90.6250 (90.6250) lr 9.6859e+00 eta 0:01:39\n",
            "epoch [104/200] batch [1/2] time 0.744 (0.744) data 0.435 (0.435) loss 0.5412 (0.5412) acc 87.5000 (87.5000) lr 9.6859e+00 eta 0:02:23\n",
            "epoch [104/200] batch [2/2] time 0.312 (0.528) data 0.000 (0.218) loss 1.0619 (0.8016) acc 75.0000 (81.2500) lr 9.5289e+00 eta 0:01:41\n",
            "epoch [105/200] batch [1/2] time 0.727 (0.727) data 0.422 (0.422) loss 0.6625 (0.6625) acc 93.7500 (93.7500) lr 9.5289e+00 eta 0:02:18\n",
            "epoch [105/200] batch [2/2] time 0.312 (0.519) data 0.000 (0.211) loss 0.6098 (0.6361) acc 90.6250 (92.1875) lr 9.3721e+00 eta 0:01:38\n",
            "epoch [106/200] batch [1/2] time 0.744 (0.744) data 0.437 (0.437) loss 0.6065 (0.6065) acc 90.6250 (90.6250) lr 9.3721e+00 eta 0:02:20\n",
            "epoch [106/200] batch [2/2] time 0.311 (0.527) data 0.000 (0.219) loss 0.6790 (0.6427) acc 84.3750 (87.5000) lr 9.2154e+00 eta 0:01:39\n",
            "epoch [107/200] batch [1/2] time 0.721 (0.721) data 0.415 (0.415) loss 0.4438 (0.4438) acc 93.7500 (93.7500) lr 9.2154e+00 eta 0:02:14\n",
            "epoch [107/200] batch [2/2] time 0.312 (0.517) data 0.000 (0.208) loss 0.7998 (0.6218) acc 87.5000 (90.6250) lr 9.0589e+00 eta 0:01:36\n",
            "epoch [108/200] batch [1/2] time 0.727 (0.727) data 0.420 (0.420) loss 0.5281 (0.5281) acc 93.7500 (93.7500) lr 9.0589e+00 eta 0:02:14\n",
            "epoch [108/200] batch [2/2] time 0.311 (0.519) data 0.000 (0.210) loss 0.3959 (0.4620) acc 96.8750 (95.3125) lr 8.9027e+00 eta 0:01:35\n",
            "epoch [109/200] batch [1/2] time 0.740 (0.740) data 0.435 (0.435) loss 0.8050 (0.8050) acc 87.5000 (87.5000) lr 8.9027e+00 eta 0:02:15\n",
            "epoch [109/200] batch [2/2] time 0.311 (0.526) data 0.000 (0.218) loss 0.4752 (0.6401) acc 96.8750 (92.1875) lr 8.7467e+00 eta 0:01:35\n",
            "epoch [110/200] batch [1/2] time 0.748 (0.748) data 0.442 (0.442) loss 0.3961 (0.3961) acc 96.8750 (96.8750) lr 8.7467e+00 eta 0:02:15\n",
            "epoch [110/200] batch [2/2] time 0.311 (0.530) data 0.000 (0.221) loss 0.7715 (0.5838) acc 87.5000 (92.1875) lr 8.5910e+00 eta 0:01:35\n",
            "epoch [111/200] batch [1/2] time 0.735 (0.735) data 0.429 (0.429) loss 0.6503 (0.6503) acc 87.5000 (87.5000) lr 8.5910e+00 eta 0:02:11\n",
            "epoch [111/200] batch [2/2] time 0.312 (0.524) data 0.000 (0.215) loss 0.9578 (0.8040) acc 81.2500 (84.3750) lr 8.4357e+00 eta 0:01:33\n",
            "epoch [112/200] batch [1/2] time 0.708 (0.708) data 0.401 (0.401) loss 0.6774 (0.6774) acc 87.5000 (87.5000) lr 8.4357e+00 eta 0:02:05\n",
            "epoch [112/200] batch [2/2] time 0.313 (0.510) data 0.001 (0.201) loss 1.1258 (0.9016) acc 78.1250 (82.8125) lr 8.2807e+00 eta 0:01:29\n",
            "epoch [113/200] batch [1/2] time 0.715 (0.715) data 0.410 (0.410) loss 0.4093 (0.4093) acc 96.8750 (96.8750) lr 8.2807e+00 eta 0:02:05\n",
            "epoch [113/200] batch [2/2] time 0.314 (0.514) data 0.000 (0.205) loss 0.8130 (0.6111) acc 87.5000 (92.1875) lr 8.1262e+00 eta 0:01:29\n",
            "epoch [114/200] batch [1/2] time 0.733 (0.733) data 0.428 (0.428) loss 0.6028 (0.6028) acc 81.2500 (81.2500) lr 8.1262e+00 eta 0:02:06\n",
            "epoch [114/200] batch [2/2] time 0.314 (0.524) data 0.000 (0.214) loss 0.5293 (0.5660) acc 87.5000 (84.3750) lr 7.9721e+00 eta 0:01:30\n",
            "epoch [115/200] batch [1/2] time 0.712 (0.712) data 0.405 (0.405) loss 0.6722 (0.6722) acc 87.5000 (87.5000) lr 7.9721e+00 eta 0:02:01\n",
            "epoch [115/200] batch [2/2] time 0.315 (0.513) data 0.001 (0.203) loss 0.6824 (0.6773) acc 87.5000 (87.5000) lr 7.8186e+00 eta 0:01:27\n",
            "epoch [116/200] batch [1/2] time 0.704 (0.704) data 0.397 (0.397) loss 0.6187 (0.6187) acc 84.3750 (84.3750) lr 7.8186e+00 eta 0:01:59\n",
            "epoch [116/200] batch [2/2] time 0.310 (0.507) data 0.001 (0.199) loss 0.6985 (0.6586) acc 84.3750 (84.3750) lr 7.6655e+00 eta 0:01:25\n",
            "epoch [117/200] batch [1/2] time 0.710 (0.710) data 0.402 (0.402) loss 0.4216 (0.4216) acc 93.7500 (93.7500) lr 7.6655e+00 eta 0:01:58\n",
            "epoch [117/200] batch [2/2] time 0.311 (0.510) data 0.000 (0.201) loss 0.6987 (0.5601) acc 84.3750 (89.0625) lr 7.5131e+00 eta 0:01:24\n",
            "epoch [118/200] batch [1/2] time 0.697 (0.697) data 0.391 (0.391) loss 0.7971 (0.7971) acc 90.6250 (90.6250) lr 7.5131e+00 eta 0:01:54\n",
            "epoch [118/200] batch [2/2] time 0.311 (0.504) data 0.000 (0.195) loss 0.8115 (0.8043) acc 93.7500 (92.1875) lr 7.3613e+00 eta 0:01:22\n",
            "epoch [119/200] batch [1/2] time 0.699 (0.699) data 0.391 (0.391) loss 0.5756 (0.5756) acc 84.3750 (84.3750) lr 7.3613e+00 eta 0:01:53\n",
            "epoch [119/200] batch [2/2] time 0.315 (0.507) data 0.001 (0.196) loss 0.5577 (0.5667) acc 87.5000 (85.9375) lr 7.2101e+00 eta 0:01:22\n",
            "epoch [120/200] batch [1/2] time 0.740 (0.740) data 0.431 (0.431) loss 0.5100 (0.5100) acc 90.6250 (90.6250) lr 7.2101e+00 eta 0:01:59\n",
            "epoch [120/200] batch [2/2] time 0.311 (0.525) data 0.001 (0.216) loss 0.5636 (0.5368) acc 90.6250 (90.6250) lr 7.0596e+00 eta 0:01:24\n",
            "epoch [121/200] batch [1/2] time 0.763 (0.763) data 0.455 (0.455) loss 0.7037 (0.7037) acc 84.3750 (84.3750) lr 7.0596e+00 eta 0:02:01\n",
            "epoch [121/200] batch [2/2] time 0.311 (0.537) data 0.000 (0.228) loss 1.0273 (0.8655) acc 75.0000 (79.6875) lr 6.9098e+00 eta 0:01:24\n",
            "epoch [122/200] batch [1/2] time 0.734 (0.734) data 0.427 (0.427) loss 0.4592 (0.4592) acc 93.7500 (93.7500) lr 6.9098e+00 eta 0:01:55\n",
            "epoch [122/200] batch [2/2] time 0.311 (0.523) data 0.000 (0.214) loss 0.3832 (0.4212) acc 96.8750 (95.3125) lr 6.7608e+00 eta 0:01:21\n",
            "epoch [123/200] batch [1/2] time 0.752 (0.752) data 0.445 (0.445) loss 0.5056 (0.5056) acc 90.6250 (90.6250) lr 6.7608e+00 eta 0:01:56\n",
            "epoch [123/200] batch [2/2] time 0.313 (0.532) data 0.000 (0.223) loss 1.4007 (0.9532) acc 71.8750 (81.2500) lr 6.6126e+00 eta 0:01:21\n",
            "epoch [124/200] batch [1/2] time 0.720 (0.720) data 0.412 (0.412) loss 1.1582 (1.1582) acc 68.7500 (68.7500) lr 6.6126e+00 eta 0:01:50\n",
            "epoch [124/200] batch [2/2] time 0.312 (0.516) data 0.000 (0.206) loss 0.9757 (1.0669) acc 71.8750 (70.3125) lr 6.4653e+00 eta 0:01:18\n",
            "epoch [125/200] batch [1/2] time 0.705 (0.705) data 0.399 (0.399) loss 0.6823 (0.6823) acc 90.6250 (90.6250) lr 6.4653e+00 eta 0:01:46\n",
            "epoch [125/200] batch [2/2] time 0.313 (0.509) data 0.001 (0.200) loss 0.9709 (0.8266) acc 84.3750 (87.5000) lr 6.3188e+00 eta 0:01:16\n",
            "epoch [126/200] batch [1/2] time 0.714 (0.714) data 0.407 (0.407) loss 0.6390 (0.6390) acc 84.3750 (84.3750) lr 6.3188e+00 eta 0:01:46\n",
            "epoch [126/200] batch [2/2] time 0.314 (0.514) data 0.000 (0.204) loss 1.1340 (0.8865) acc 81.2500 (82.8125) lr 6.1732e+00 eta 0:01:16\n",
            "epoch [127/200] batch [1/2] time 0.703 (0.703) data 0.395 (0.395) loss 0.9987 (0.9987) acc 75.0000 (75.0000) lr 6.1732e+00 eta 0:01:43\n",
            "epoch [127/200] batch [2/2] time 0.310 (0.507) data 0.000 (0.198) loss 1.1996 (1.0992) acc 78.1250 (76.5625) lr 6.0285e+00 eta 0:01:13\n",
            "epoch [128/200] batch [1/2] time 0.736 (0.736) data 0.427 (0.427) loss 0.6660 (0.6660) acc 84.3750 (84.3750) lr 6.0285e+00 eta 0:01:46\n",
            "epoch [128/200] batch [2/2] time 0.314 (0.525) data 0.001 (0.214) loss 0.7152 (0.6906) acc 75.0000 (79.6875) lr 5.8849e+00 eta 0:01:15\n",
            "epoch [129/200] batch [1/2] time 0.698 (0.698) data 0.391 (0.391) loss 0.4793 (0.4793) acc 93.7500 (93.7500) lr 5.8849e+00 eta 0:01:39\n",
            "epoch [129/200] batch [2/2] time 0.312 (0.505) data 0.001 (0.196) loss 0.6373 (0.5583) acc 78.1250 (85.9375) lr 5.7422e+00 eta 0:01:11\n",
            "epoch [130/200] batch [1/2] time 0.706 (0.706) data 0.400 (0.400) loss 0.7361 (0.7361) acc 84.3750 (84.3750) lr 5.7422e+00 eta 0:01:39\n",
            "epoch [130/200] batch [2/2] time 0.309 (0.507) data 0.000 (0.200) loss 1.7359 (1.2360) acc 71.8750 (78.1250) lr 5.6006e+00 eta 0:01:11\n",
            "epoch [131/200] batch [1/2] time 0.744 (0.744) data 0.440 (0.440) loss 0.5547 (0.5547) acc 90.6250 (90.6250) lr 5.6006e+00 eta 0:01:43\n",
            "epoch [131/200] batch [2/2] time 0.310 (0.527) data 0.000 (0.220) loss 0.4469 (0.5008) acc 90.6250 (90.6250) lr 5.4601e+00 eta 0:01:12\n",
            "epoch [132/200] batch [1/2] time 0.732 (0.732) data 0.427 (0.427) loss 0.6719 (0.6719) acc 90.6250 (90.6250) lr 5.4601e+00 eta 0:01:40\n",
            "epoch [132/200] batch [2/2] time 0.312 (0.522) data 0.001 (0.214) loss 0.6194 (0.6457) acc 87.5000 (89.0625) lr 5.3207e+00 eta 0:01:11\n",
            "epoch [133/200] batch [1/2] time 0.744 (0.744) data 0.436 (0.436) loss 0.3189 (0.3189) acc 100.0000 (100.0000) lr 5.3207e+00 eta 0:01:40\n",
            "epoch [133/200] batch [2/2] time 0.310 (0.527) data 0.000 (0.218) loss 0.5013 (0.4101) acc 90.6250 (95.3125) lr 5.1825e+00 eta 0:01:10\n",
            "epoch [134/200] batch [1/2] time 0.735 (0.735) data 0.430 (0.430) loss 0.5042 (0.5042) acc 96.8750 (96.8750) lr 5.1825e+00 eta 0:01:37\n",
            "epoch [134/200] batch [2/2] time 0.309 (0.522) data 0.000 (0.215) loss 0.6469 (0.5755) acc 87.5000 (92.1875) lr 5.0454e+00 eta 0:01:08\n",
            "epoch [135/200] batch [1/2] time 0.728 (0.728) data 0.421 (0.421) loss 0.7872 (0.7872) acc 87.5000 (87.5000) lr 5.0454e+00 eta 0:01:35\n",
            "epoch [135/200] batch [2/2] time 0.311 (0.519) data 0.001 (0.211) loss 0.3309 (0.5591) acc 100.0000 (93.7500) lr 4.9096e+00 eta 0:01:07\n",
            "epoch [136/200] batch [1/2] time 0.742 (0.742) data 0.437 (0.437) loss 0.5539 (0.5539) acc 93.7500 (93.7500) lr 4.9096e+00 eta 0:01:35\n",
            "epoch [136/200] batch [2/2] time 0.311 (0.526) data 0.001 (0.219) loss 0.5138 (0.5339) acc 90.6250 (92.1875) lr 4.7750e+00 eta 0:01:07\n",
            "epoch [137/200] batch [1/2] time 0.749 (0.749) data 0.444 (0.444) loss 0.3739 (0.3739) acc 93.7500 (93.7500) lr 4.7750e+00 eta 0:01:35\n",
            "epoch [137/200] batch [2/2] time 0.311 (0.530) data 0.001 (0.222) loss 0.4685 (0.4212) acc 93.7500 (93.7500) lr 4.6417e+00 eta 0:01:06\n",
            "epoch [138/200] batch [1/2] time 0.759 (0.759) data 0.454 (0.454) loss 0.5930 (0.5930) acc 87.5000 (87.5000) lr 4.6417e+00 eta 0:01:34\n",
            "epoch [138/200] batch [2/2] time 0.310 (0.535) data 0.000 (0.227) loss 0.5661 (0.5796) acc 93.7500 (90.6250) lr 4.5098e+00 eta 0:01:06\n",
            "epoch [139/200] batch [1/2] time 0.730 (0.730) data 0.424 (0.424) loss 0.4058 (0.4058) acc 90.6250 (90.6250) lr 4.5098e+00 eta 0:01:29\n",
            "epoch [139/200] batch [2/2] time 0.312 (0.521) data 0.000 (0.212) loss 0.4266 (0.4162) acc 93.7500 (92.1875) lr 4.3792e+00 eta 0:01:03\n",
            "epoch [140/200] batch [1/2] time 0.723 (0.723) data 0.416 (0.416) loss 0.5370 (0.5370) acc 84.3750 (84.3750) lr 4.3792e+00 eta 0:01:27\n",
            "epoch [140/200] batch [2/2] time 0.312 (0.518) data 0.000 (0.208) loss 0.5961 (0.5666) acc 84.3750 (84.3750) lr 4.2499e+00 eta 0:01:02\n",
            "epoch [141/200] batch [1/2] time 0.739 (0.739) data 0.431 (0.431) loss 0.5516 (0.5516) acc 87.5000 (87.5000) lr 4.2499e+00 eta 0:01:27\n",
            "epoch [141/200] batch [2/2] time 0.312 (0.525) data 0.000 (0.216) loss 0.6573 (0.6045) acc 84.3750 (85.9375) lr 4.1221e+00 eta 0:01:02\n",
            "epoch [142/200] batch [1/2] time 0.744 (0.744) data 0.441 (0.441) loss 0.3605 (0.3605) acc 93.7500 (93.7500) lr 4.1221e+00 eta 0:01:27\n",
            "epoch [142/200] batch [2/2] time 0.310 (0.527) data 0.000 (0.221) loss 0.6591 (0.5098) acc 93.7500 (93.7500) lr 3.9958e+00 eta 0:01:01\n",
            "epoch [143/200] batch [1/2] time 0.694 (0.694) data 0.389 (0.389) loss 0.4609 (0.4609) acc 93.7500 (93.7500) lr 3.9958e+00 eta 0:01:19\n",
            "epoch [143/200] batch [2/2] time 0.310 (0.502) data 0.001 (0.195) loss 0.5534 (0.5072) acc 87.5000 (90.6250) lr 3.8709e+00 eta 0:00:57\n",
            "epoch [144/200] batch [1/2] time 0.747 (0.747) data 0.441 (0.441) loss 0.7453 (0.7453) acc 87.5000 (87.5000) lr 3.8709e+00 eta 0:01:24\n",
            "epoch [144/200] batch [2/2] time 0.310 (0.529) data 0.001 (0.221) loss 0.4058 (0.5756) acc 93.7500 (90.6250) lr 3.7476e+00 eta 0:00:59\n",
            "epoch [145/200] batch [1/2] time 0.742 (0.742) data 0.433 (0.433) loss 0.5157 (0.5157) acc 93.7500 (93.7500) lr 3.7476e+00 eta 0:01:22\n",
            "epoch [145/200] batch [2/2] time 0.310 (0.526) data 0.001 (0.217) loss 0.4567 (0.4862) acc 93.7500 (93.7500) lr 3.6258e+00 eta 0:00:57\n",
            "epoch [146/200] batch [1/2] time 0.709 (0.709) data 0.404 (0.404) loss 0.5280 (0.5280) acc 90.6250 (90.6250) lr 3.6258e+00 eta 0:01:17\n",
            "epoch [146/200] batch [2/2] time 0.312 (0.510) data 0.001 (0.202) loss 0.5720 (0.5500) acc 93.7500 (92.1875) lr 3.5055e+00 eta 0:00:55\n",
            "epoch [147/200] batch [1/2] time 0.721 (0.721) data 0.415 (0.415) loss 0.5389 (0.5389) acc 90.6250 (90.6250) lr 3.5055e+00 eta 0:01:17\n",
            "epoch [147/200] batch [2/2] time 0.313 (0.517) data 0.001 (0.208) loss 0.6654 (0.6022) acc 87.5000 (89.0625) lr 3.3869e+00 eta 0:00:54\n",
            "epoch [148/200] batch [1/2] time 0.736 (0.736) data 0.429 (0.429) loss 0.3731 (0.3731) acc 93.7500 (93.7500) lr 3.3869e+00 eta 0:01:17\n",
            "epoch [148/200] batch [2/2] time 0.312 (0.524) data 0.000 (0.215) loss 0.6880 (0.5306) acc 84.3750 (89.0625) lr 3.2699e+00 eta 0:00:54\n",
            "epoch [149/200] batch [1/2] time 0.700 (0.700) data 0.395 (0.395) loss 0.3476 (0.3476) acc 93.7500 (93.7500) lr 3.2699e+00 eta 0:01:12\n",
            "epoch [149/200] batch [2/2] time 0.311 (0.505) data 0.000 (0.198) loss 0.5641 (0.4559) acc 90.6250 (92.1875) lr 3.1545e+00 eta 0:00:51\n",
            "epoch [150/200] batch [1/2] time 0.740 (0.740) data 0.436 (0.436) loss 0.4117 (0.4117) acc 96.8750 (96.8750) lr 3.1545e+00 eta 0:01:14\n",
            "epoch [150/200] batch [2/2] time 0.310 (0.525) data 0.000 (0.218) loss 0.5052 (0.4584) acc 90.6250 (93.7500) lr 3.0409e+00 eta 0:00:52\n",
            "epoch [151/200] batch [1/2] time 0.728 (0.728) data 0.422 (0.422) loss 0.3890 (0.3890) acc 93.7500 (93.7500) lr 3.0409e+00 eta 0:01:12\n",
            "epoch [151/200] batch [2/2] time 0.315 (0.522) data 0.000 (0.211) loss 0.5802 (0.4846) acc 84.3750 (89.0625) lr 2.9289e+00 eta 0:00:51\n",
            "epoch [152/200] batch [1/2] time 0.728 (0.728) data 0.423 (0.423) loss 0.5010 (0.5010) acc 90.6250 (90.6250) lr 2.9289e+00 eta 0:01:10\n",
            "epoch [152/200] batch [2/2] time 0.309 (0.519) data 0.000 (0.212) loss 0.2998 (0.4004) acc 100.0000 (95.3125) lr 2.8187e+00 eta 0:00:49\n",
            "epoch [153/200] batch [1/2] time 0.732 (0.732) data 0.426 (0.426) loss 0.3613 (0.3613) acc 96.8750 (96.8750) lr 2.8187e+00 eta 0:01:09\n",
            "epoch [153/200] batch [2/2] time 0.311 (0.522) data 0.000 (0.213) loss 0.5729 (0.4671) acc 84.3750 (90.6250) lr 2.7103e+00 eta 0:00:49\n",
            "epoch [154/200] batch [1/2] time 0.706 (0.706) data 0.401 (0.401) loss 0.4711 (0.4711) acc 93.7500 (93.7500) lr 2.7103e+00 eta 0:01:05\n",
            "epoch [154/200] batch [2/2] time 0.310 (0.508) data 0.000 (0.201) loss 0.4793 (0.4752) acc 93.7500 (93.7500) lr 2.6037e+00 eta 0:00:46\n",
            "epoch [155/200] batch [1/2] time 0.720 (0.720) data 0.414 (0.414) loss 0.2513 (0.2513) acc 100.0000 (100.0000) lr 2.6037e+00 eta 0:01:05\n",
            "epoch [155/200] batch [2/2] time 0.310 (0.515) data 0.000 (0.207) loss 0.4414 (0.3463) acc 93.7500 (96.8750) lr 2.4989e+00 eta 0:00:46\n",
            "epoch [156/200] batch [1/2] time 0.755 (0.755) data 0.447 (0.447) loss 0.2708 (0.2708) acc 100.0000 (100.0000) lr 2.4989e+00 eta 0:01:07\n",
            "epoch [156/200] batch [2/2] time 0.311 (0.533) data 0.001 (0.224) loss 0.2879 (0.2793) acc 100.0000 (100.0000) lr 2.3959e+00 eta 0:00:46\n",
            "epoch [157/200] batch [1/2] time 0.724 (0.724) data 0.419 (0.419) loss 0.4305 (0.4305) acc 96.8750 (96.8750) lr 2.3959e+00 eta 0:01:03\n",
            "epoch [157/200] batch [2/2] time 0.311 (0.518) data 0.000 (0.210) loss 0.3412 (0.3859) acc 96.8750 (96.8750) lr 2.2949e+00 eta 0:00:44\n",
            "epoch [158/200] batch [1/2] time 0.703 (0.703) data 0.398 (0.398) loss 0.2851 (0.2851) acc 100.0000 (100.0000) lr 2.2949e+00 eta 0:00:59\n",
            "epoch [158/200] batch [2/2] time 0.311 (0.507) data 0.000 (0.199) loss 0.9341 (0.6096) acc 87.5000 (93.7500) lr 2.1957e+00 eta 0:00:42\n",
            "epoch [159/200] batch [1/2] time 0.714 (0.714) data 0.406 (0.406) loss 0.3365 (0.3365) acc 93.7500 (93.7500) lr 2.1957e+00 eta 0:00:59\n",
            "epoch [159/200] batch [2/2] time 0.313 (0.513) data 0.000 (0.203) loss 0.3694 (0.3529) acc 96.8750 (95.3125) lr 2.0984e+00 eta 0:00:42\n",
            "epoch [160/200] batch [1/2] time 0.720 (0.720) data 0.416 (0.416) loss 0.3288 (0.3288) acc 96.8750 (96.8750) lr 2.0984e+00 eta 0:00:58\n",
            "epoch [160/200] batch [2/2] time 0.309 (0.515) data 0.000 (0.208) loss 0.3183 (0.3236) acc 96.8750 (96.8750) lr 2.0032e+00 eta 0:00:41\n",
            "epoch [161/200] batch [1/2] time 0.721 (0.721) data 0.414 (0.414) loss 0.3575 (0.3575) acc 96.8750 (96.8750) lr 2.0032e+00 eta 0:00:56\n",
            "epoch [161/200] batch [2/2] time 0.309 (0.515) data 0.000 (0.207) loss 0.3453 (0.3514) acc 93.7500 (95.3125) lr 1.9098e+00 eta 0:00:40\n",
            "epoch [162/200] batch [1/2] time 0.731 (0.731) data 0.426 (0.426) loss 0.6656 (0.6656) acc 90.6250 (90.6250) lr 1.9098e+00 eta 0:00:56\n",
            "epoch [162/200] batch [2/2] time 0.309 (0.520) data 0.001 (0.214) loss 0.3902 (0.5279) acc 90.6250 (90.6250) lr 1.8185e+00 eta 0:00:39\n",
            "epoch [163/200] batch [1/2] time 0.736 (0.736) data 0.430 (0.430) loss 0.2705 (0.2705) acc 100.0000 (100.0000) lr 1.8185e+00 eta 0:00:55\n",
            "epoch [163/200] batch [2/2] time 0.314 (0.525) data 0.000 (0.215) loss 0.5645 (0.4175) acc 93.7500 (96.8750) lr 1.7292e+00 eta 0:00:38\n",
            "epoch [164/200] batch [1/2] time 0.716 (0.716) data 0.412 (0.412) loss 0.2565 (0.2565) acc 100.0000 (100.0000) lr 1.7292e+00 eta 0:00:52\n",
            "epoch [164/200] batch [2/2] time 0.310 (0.513) data 0.000 (0.206) loss 0.2663 (0.2614) acc 100.0000 (100.0000) lr 1.6419e+00 eta 0:00:36\n",
            "epoch [165/200] batch [1/2] time 0.737 (0.737) data 0.432 (0.432) loss 0.2417 (0.2417) acc 100.0000 (100.0000) lr 1.6419e+00 eta 0:00:52\n",
            "epoch [165/200] batch [2/2] time 0.311 (0.524) data 0.000 (0.216) loss 0.4560 (0.3488) acc 90.6250 (95.3125) lr 1.5567e+00 eta 0:00:36\n",
            "epoch [166/200] batch [1/2] time 0.709 (0.709) data 0.405 (0.405) loss 0.4512 (0.4512) acc 93.7500 (93.7500) lr 1.5567e+00 eta 0:00:48\n",
            "epoch [166/200] batch [2/2] time 0.310 (0.510) data 0.001 (0.203) loss 0.2948 (0.3730) acc 100.0000 (96.8750) lr 1.4736e+00 eta 0:00:34\n",
            "epoch [167/200] batch [1/2] time 0.758 (0.758) data 0.452 (0.452) loss 0.2630 (0.2630) acc 100.0000 (100.0000) lr 1.4736e+00 eta 0:00:50\n",
            "epoch [167/200] batch [2/2] time 0.313 (0.536) data 0.000 (0.226) loss 0.2255 (0.2443) acc 100.0000 (100.0000) lr 1.3926e+00 eta 0:00:35\n",
            "epoch [168/200] batch [1/2] time 0.749 (0.749) data 0.440 (0.440) loss 0.2843 (0.2843) acc 96.8750 (96.8750) lr 1.3926e+00 eta 0:00:48\n",
            "epoch [168/200] batch [2/2] time 0.310 (0.530) data 0.001 (0.221) loss 0.3541 (0.3192) acc 96.8750 (96.8750) lr 1.3137e+00 eta 0:00:33\n",
            "epoch [169/200] batch [1/2] time 0.729 (0.729) data 0.424 (0.424) loss 0.2866 (0.2866) acc 96.8750 (96.8750) lr 1.3137e+00 eta 0:00:45\n",
            "epoch [169/200] batch [2/2] time 0.312 (0.521) data 0.000 (0.212) loss 0.5064 (0.3965) acc 90.6250 (93.7500) lr 1.2369e+00 eta 0:00:32\n",
            "epoch [170/200] batch [1/2] time 0.716 (0.716) data 0.411 (0.411) loss 0.2630 (0.2630) acc 100.0000 (100.0000) lr 1.2369e+00 eta 0:00:43\n",
            "epoch [170/200] batch [2/2] time 0.309 (0.513) data 0.000 (0.206) loss 0.2443 (0.2536) acc 100.0000 (100.0000) lr 1.1623e+00 eta 0:00:30\n",
            "epoch [171/200] batch [1/2] time 0.737 (0.737) data 0.430 (0.430) loss 0.4027 (0.4027) acc 90.6250 (90.6250) lr 1.1623e+00 eta 0:00:43\n",
            "epoch [171/200] batch [2/2] time 0.312 (0.524) data 0.001 (0.215) loss 0.2693 (0.3360) acc 96.8750 (93.7500) lr 1.0899e+00 eta 0:00:30\n",
            "epoch [172/200] batch [1/2] time 0.717 (0.717) data 0.413 (0.413) loss 0.2961 (0.2961) acc 96.8750 (96.8750) lr 1.0899e+00 eta 0:00:40\n",
            "epoch [172/200] batch [2/2] time 0.311 (0.514) data 0.000 (0.207) loss 0.2822 (0.2892) acc 96.8750 (96.8750) lr 1.0197e+00 eta 0:00:28\n",
            "epoch [173/200] batch [1/2] time 0.721 (0.721) data 0.414 (0.414) loss 0.2378 (0.2378) acc 100.0000 (100.0000) lr 1.0197e+00 eta 0:00:39\n",
            "epoch [173/200] batch [2/2] time 0.312 (0.517) data 0.000 (0.207) loss 0.5513 (0.3946) acc 90.6250 (95.3125) lr 9.5173e-01 eta 0:00:27\n",
            "epoch [174/200] batch [1/2] time 0.720 (0.720) data 0.415 (0.415) loss 0.2640 (0.2640) acc 96.8750 (96.8750) lr 9.5173e-01 eta 0:00:38\n",
            "epoch [174/200] batch [2/2] time 0.311 (0.515) data 0.000 (0.208) loss 0.2375 (0.2508) acc 100.0000 (98.4375) lr 8.8597e-01 eta 0:00:26\n",
            "epoch [175/200] batch [1/2] time 0.751 (0.751) data 0.445 (0.445) loss 0.2879 (0.2879) acc 96.8750 (96.8750) lr 8.8597e-01 eta 0:00:38\n",
            "epoch [175/200] batch [2/2] time 0.311 (0.531) data 0.000 (0.223) loss 0.2708 (0.2793) acc 100.0000 (98.4375) lr 8.2245e-01 eta 0:00:26\n",
            "epoch [176/200] batch [1/2] time 0.719 (0.719) data 0.414 (0.414) loss 0.3188 (0.3188) acc 96.8750 (96.8750) lr 8.2245e-01 eta 0:00:35\n",
            "epoch [176/200] batch [2/2] time 0.311 (0.515) data 0.000 (0.207) loss 0.2969 (0.3079) acc 96.8750 (96.8750) lr 7.6120e-01 eta 0:00:24\n",
            "epoch [177/200] batch [1/2] time 0.716 (0.716) data 0.412 (0.412) loss 0.3240 (0.3240) acc 96.8750 (96.8750) lr 7.6120e-01 eta 0:00:33\n",
            "epoch [177/200] batch [2/2] time 0.311 (0.514) data 0.001 (0.206) loss 0.2260 (0.2750) acc 100.0000 (98.4375) lr 7.0224e-01 eta 0:00:23\n",
            "epoch [178/200] batch [1/2] time 0.752 (0.752) data 0.448 (0.448) loss 0.2950 (0.2950) acc 93.7500 (93.7500) lr 7.0224e-01 eta 0:00:33\n",
            "epoch [178/200] batch [2/2] time 0.311 (0.532) data 0.000 (0.224) loss 0.2710 (0.2830) acc 96.8750 (95.3125) lr 6.4556e-01 eta 0:00:23\n",
            "epoch [179/200] batch [1/2] time 0.760 (0.760) data 0.454 (0.454) loss 0.3729 (0.3729) acc 93.7500 (93.7500) lr 6.4556e-01 eta 0:00:32\n",
            "epoch [179/200] batch [2/2] time 0.310 (0.535) data 0.000 (0.227) loss 0.3125 (0.3427) acc 93.7500 (93.7500) lr 5.9119e-01 eta 0:00:22\n",
            "epoch [180/200] batch [1/2] time 0.754 (0.754) data 0.450 (0.450) loss 0.2444 (0.2444) acc 100.0000 (100.0000) lr 5.9119e-01 eta 0:00:30\n",
            "epoch [180/200] batch [2/2] time 0.309 (0.532) data 0.000 (0.225) loss 0.2262 (0.2353) acc 100.0000 (100.0000) lr 5.3915e-01 eta 0:00:21\n",
            "epoch [181/200] batch [1/2] time 0.718 (0.718) data 0.412 (0.412) loss 0.2920 (0.2920) acc 96.8750 (96.8750) lr 5.3915e-01 eta 0:00:27\n",
            "epoch [181/200] batch [2/2] time 0.308 (0.513) data 0.000 (0.206) loss 0.3155 (0.3037) acc 96.8750 (96.8750) lr 4.8943e-01 eta 0:00:19\n",
            "epoch [182/200] batch [1/2] time 0.749 (0.749) data 0.444 (0.444) loss 0.2548 (0.2548) acc 96.8750 (96.8750) lr 4.8943e-01 eta 0:00:27\n",
            "epoch [182/200] batch [2/2] time 0.312 (0.530) data 0.000 (0.222) loss 0.2554 (0.2551) acc 100.0000 (98.4375) lr 4.4207e-01 eta 0:00:19\n",
            "epoch [183/200] batch [1/2] time 0.717 (0.717) data 0.412 (0.412) loss 0.2363 (0.2363) acc 96.8750 (96.8750) lr 4.4207e-01 eta 0:00:25\n",
            "epoch [183/200] batch [2/2] time 0.311 (0.514) data 0.001 (0.206) loss 0.3586 (0.2974) acc 93.7500 (95.3125) lr 3.9706e-01 eta 0:00:17\n",
            "epoch [184/200] batch [1/2] time 0.719 (0.719) data 0.415 (0.415) loss 0.3431 (0.3431) acc 93.7500 (93.7500) lr 3.9706e-01 eta 0:00:23\n",
            "epoch [184/200] batch [2/2] time 0.310 (0.515) data 0.000 (0.208) loss 0.2655 (0.3043) acc 96.8750 (95.3125) lr 3.5443e-01 eta 0:00:16\n",
            "epoch [185/200] batch [1/2] time 0.723 (0.723) data 0.421 (0.421) loss 0.2199 (0.2199) acc 100.0000 (100.0000) lr 3.5443e-01 eta 0:00:22\n",
            "epoch [185/200] batch [2/2] time 0.310 (0.517) data 0.001 (0.211) loss 0.2306 (0.2252) acc 100.0000 (100.0000) lr 3.1417e-01 eta 0:00:15\n",
            "epoch [186/200] batch [1/2] time 0.735 (0.735) data 0.430 (0.430) loss 0.4858 (0.4858) acc 93.7500 (93.7500) lr 3.1417e-01 eta 0:00:21\n",
            "epoch [186/200] batch [2/2] time 0.308 (0.522) data 0.000 (0.215) loss 0.4436 (0.4647) acc 93.7500 (93.7500) lr 2.7630e-01 eta 0:00:14\n",
            "epoch [187/200] batch [1/2] time 0.740 (0.740) data 0.435 (0.435) loss 0.2395 (0.2395) acc 100.0000 (100.0000) lr 2.7630e-01 eta 0:00:19\n",
            "epoch [187/200] batch [2/2] time 0.309 (0.524) data 0.000 (0.218) loss 0.3105 (0.2750) acc 96.8750 (98.4375) lr 2.4083e-01 eta 0:00:13\n",
            "epoch [188/200] batch [1/2] time 0.725 (0.725) data 0.421 (0.421) loss 0.2473 (0.2473) acc 96.8750 (96.8750) lr 2.4083e-01 eta 0:00:18\n",
            "epoch [188/200] batch [2/2] time 0.311 (0.518) data 0.000 (0.211) loss 0.2960 (0.2717) acc 96.8750 (96.8750) lr 2.0777e-01 eta 0:00:12\n",
            "epoch [189/200] batch [1/2] time 0.726 (0.726) data 0.421 (0.421) loss 0.2144 (0.2144) acc 100.0000 (100.0000) lr 2.0777e-01 eta 0:00:16\n",
            "epoch [189/200] batch [2/2] time 0.311 (0.519) data 0.001 (0.211) loss 0.3418 (0.2781) acc 96.8750 (98.4375) lr 1.7713e-01 eta 0:00:11\n",
            "epoch [190/200] batch [1/2] time 0.757 (0.757) data 0.452 (0.452) loss 0.3252 (0.3252) acc 93.7500 (93.7500) lr 1.7713e-01 eta 0:00:15\n",
            "epoch [190/200] batch [2/2] time 0.311 (0.534) data 0.000 (0.226) loss 0.2658 (0.2955) acc 96.8750 (95.3125) lr 1.4891e-01 eta 0:00:10\n",
            "epoch [191/200] batch [1/2] time 0.752 (0.752) data 0.447 (0.447) loss 0.1999 (0.1999) acc 100.0000 (100.0000) lr 1.4891e-01 eta 0:00:14\n",
            "epoch [191/200] batch [2/2] time 0.311 (0.532) data 0.001 (0.224) loss 0.3032 (0.2516) acc 96.8750 (98.4375) lr 1.2312e-01 eta 0:00:09\n",
            "epoch [192/200] batch [1/2] time 0.727 (0.727) data 0.425 (0.425) loss 0.4118 (0.4118) acc 93.7500 (93.7500) lr 1.2312e-01 eta 0:00:12\n",
            "epoch [192/200] batch [2/2] time 0.309 (0.518) data 0.000 (0.213) loss 0.3638 (0.3878) acc 96.8750 (95.3125) lr 9.9763e-02 eta 0:00:08\n",
            "epoch [193/200] batch [1/2] time 0.725 (0.725) data 0.422 (0.422) loss 0.4842 (0.4842) acc 90.6250 (90.6250) lr 9.9763e-02 eta 0:00:10\n",
            "epoch [193/200] batch [2/2] time 0.311 (0.518) data 0.001 (0.211) loss 0.2216 (0.3529) acc 100.0000 (95.3125) lr 7.8853e-02 eta 0:00:07\n",
            "epoch [194/200] batch [1/2] time 0.721 (0.721) data 0.418 (0.418) loss 0.3267 (0.3267) acc 93.7500 (93.7500) lr 7.8853e-02 eta 0:00:09\n",
            "epoch [194/200] batch [2/2] time 0.310 (0.515) data 0.001 (0.209) loss 0.2119 (0.2693) acc 100.0000 (96.8750) lr 6.0390e-02 eta 0:00:06\n",
            "epoch [195/200] batch [1/2] time 0.718 (0.718) data 0.412 (0.412) loss 0.3991 (0.3991) acc 96.8750 (96.8750) lr 6.0390e-02 eta 0:00:07\n",
            "epoch [195/200] batch [2/2] time 0.309 (0.513) data 0.001 (0.206) loss 0.4598 (0.4295) acc 90.6250 (93.7500) lr 4.4380e-02 eta 0:00:05\n",
            "epoch [196/200] batch [1/2] time 0.694 (0.694) data 0.390 (0.390) loss 0.2104 (0.2104) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:06\n",
            "epoch [196/200] batch [2/2] time 0.312 (0.503) data 0.001 (0.195) loss 0.5842 (0.3973) acc 87.5000 (93.7500) lr 3.0827e-02 eta 0:00:04\n",
            "epoch [197/200] batch [1/2] time 0.724 (0.724) data 0.418 (0.418) loss 0.2131 (0.2131) acc 100.0000 (100.0000) lr 3.0827e-02 eta 0:00:05\n",
            "epoch [197/200] batch [2/2] time 0.311 (0.518) data 0.000 (0.209) loss 0.2250 (0.2191) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [1/2] time 0.716 (0.716) data 0.410 (0.410) loss 0.2457 (0.2457) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [2/2] time 0.311 (0.513) data 0.000 (0.205) loss 0.2377 (0.2417) acc 100.0000 (100.0000) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [1/2] time 0.734 (0.734) data 0.427 (0.427) loss 0.2122 (0.2122) acc 100.0000 (100.0000) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [2/2] time 0.310 (0.522) data 0.000 (0.214) loss 0.2223 (0.2172) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:01\n",
            "epoch [200/200] batch [1/2] time 0.721 (0.721) data 0.415 (0.415) loss 0.2232 (0.2232) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:00\n",
            "epoch [200/200] batch [2/2] time 0.311 (0.516) data 0.000 (0.208) loss 0.3356 (0.2794) acc 93.7500 (96.8750) lr 1.2337e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed1/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:33<00:00,  2.42it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 6,738\n",
            "* accuracy: 83.2%\n",
            "* error: 16.8%\n",
            "* macro_f1: 83.0%\n",
            "Elapsed: 0:04:14\n"
          ]
        }
      ],
      "source": [
        "#eurosat-8shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kodoK_RLP-gQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b06c4c2-1694-4920-f7fe-037aa7836291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:43:01.960314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:43:01.980043: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:43:01.985937: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:43:02.000267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:43:03.006053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8']\n",
            "output_dir: output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed2\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  80\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0173, -0.5723,  0.1752,  ...,  0.4941,  0.4187,  0.0375],\n",
            "         [ 0.0214, -0.5706,  0.1699,  ...,  0.5021,  0.4206,  0.0425],\n",
            "         [ 0.0149, -0.5713,  0.1720,  ...,  0.4970,  0.4210,  0.0462],\n",
            "         ...,\n",
            "         [ 0.0324, -0.5680,  0.1748,  ...,  0.5016,  0.4188,  0.0404],\n",
            "         [ 0.0201, -0.5696,  0.1726,  ...,  0.4989,  0.4190,  0.0354],\n",
            "         [ 0.0228, -0.5696,  0.1684,  ...,  0.5053,  0.4212,  0.0412]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5609\n",
            "  Max: 1.0111\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0400\n",
            "  Min: -0.4519\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed2/tensorboard)\n",
            "epoch [1/200] batch [1/2] time 1.764 (1.764) data 0.516 (0.516) loss 8.7584 (8.7584) acc 15.6250 (15.6250) lr 1.0000e-05 eta 0:11:43\n",
            "epoch [1/200] batch [2/2] time 0.302 (1.033) data 0.000 (0.258) loss 8.6645 (8.7115) acc 21.8750 (18.7500) lr 2.0000e+01 eta 0:06:51\n",
            "epoch [2/200] batch [1/2] time 0.732 (0.732) data 0.429 (0.429) loss 8.7674 (8.7674) acc 18.7500 (18.7500) lr 2.0000e+01 eta 0:04:50\n",
            "epoch [2/200] batch [2/2] time 0.305 (0.519) data 0.001 (0.215) loss 7.5834 (8.1754) acc 9.3750 (14.0625) lr 1.9999e+01 eta 0:03:25\n",
            "epoch [3/200] batch [1/2] time 0.780 (0.780) data 0.480 (0.480) loss 6.0623 (6.0623) acc 15.6250 (15.6250) lr 1.9999e+01 eta 0:05:08\n",
            "epoch [3/200] batch [2/2] time 0.306 (0.543) data 0.000 (0.240) loss 5.4153 (5.7388) acc 25.0000 (20.3125) lr 1.9995e+01 eta 0:03:33\n",
            "epoch [4/200] batch [1/2] time 0.768 (0.768) data 0.466 (0.466) loss 4.5234 (4.5234) acc 28.1250 (28.1250) lr 1.9995e+01 eta 0:05:01\n",
            "epoch [4/200] batch [2/2] time 0.307 (0.538) data 0.000 (0.233) loss 6.1376 (5.3305) acc 9.3750 (18.7500) lr 1.9989e+01 eta 0:03:30\n",
            "epoch [5/200] batch [1/2] time 0.749 (0.749) data 0.447 (0.447) loss 6.0489 (6.0489) acc 15.6250 (15.6250) lr 1.9989e+01 eta 0:04:52\n",
            "epoch [5/200] batch [2/2] time 0.320 (0.534) data 0.000 (0.224) loss 4.9845 (5.5167) acc 18.7500 (17.1875) lr 1.9980e+01 eta 0:03:28\n",
            "epoch [6/200] batch [1/2] time 0.756 (0.756) data 0.454 (0.454) loss 4.2410 (4.2410) acc 31.2500 (31.2500) lr 1.9980e+01 eta 0:04:54\n",
            "epoch [6/200] batch [2/2] time 0.308 (0.532) data 0.000 (0.227) loss 3.7220 (3.9815) acc 21.8750 (26.5625) lr 1.9969e+01 eta 0:03:26\n",
            "epoch [7/200] batch [1/2] time 0.712 (0.712) data 0.409 (0.409) loss 3.5745 (3.5745) acc 15.6250 (15.6250) lr 1.9969e+01 eta 0:04:35\n",
            "epoch [7/200] batch [2/2] time 0.317 (0.515) data 0.000 (0.205) loss 3.2023 (3.3884) acc 28.1250 (21.8750) lr 1.9956e+01 eta 0:03:18\n",
            "epoch [8/200] batch [1/2] time 0.720 (0.720) data 0.419 (0.419) loss 3.2492 (3.2492) acc 28.1250 (28.1250) lr 1.9956e+01 eta 0:04:37\n",
            "epoch [8/200] batch [2/2] time 0.308 (0.514) data 0.001 (0.210) loss 3.0077 (3.1285) acc 28.1250 (28.1250) lr 1.9940e+01 eta 0:03:17\n",
            "epoch [9/200] batch [1/2] time 0.744 (0.744) data 0.441 (0.441) loss 2.6085 (2.6085) acc 46.8750 (46.8750) lr 1.9940e+01 eta 0:04:45\n",
            "epoch [9/200] batch [2/2] time 0.308 (0.526) data 0.001 (0.221) loss 2.3057 (2.4571) acc 43.7500 (45.3125) lr 1.9921e+01 eta 0:03:21\n",
            "epoch [10/200] batch [1/2] time 0.735 (0.735) data 0.431 (0.431) loss 2.2550 (2.2550) acc 46.8750 (46.8750) lr 1.9921e+01 eta 0:04:39\n",
            "epoch [10/200] batch [2/2] time 0.312 (0.523) data 0.000 (0.216) loss 2.3050 (2.2800) acc 43.7500 (45.3125) lr 1.9900e+01 eta 0:03:18\n",
            "epoch [11/200] batch [1/2] time 0.751 (0.751) data 0.447 (0.447) loss 2.1699 (2.1699) acc 43.7500 (43.7500) lr 1.9900e+01 eta 0:04:44\n",
            "epoch [11/200] batch [2/2] time 0.310 (0.531) data 0.000 (0.224) loss 2.1583 (2.1641) acc 40.6250 (42.1875) lr 1.9877e+01 eta 0:03:20\n",
            "epoch [12/200] batch [1/2] time 0.720 (0.720) data 0.415 (0.415) loss 1.7110 (1.7110) acc 71.8750 (71.8750) lr 1.9877e+01 eta 0:04:31\n",
            "epoch [12/200] batch [2/2] time 0.310 (0.515) data 0.001 (0.208) loss 2.2596 (1.9853) acc 59.3750 (65.6250) lr 1.9851e+01 eta 0:03:13\n",
            "epoch [13/200] batch [1/2] time 0.753 (0.753) data 0.446 (0.446) loss 1.7948 (1.7948) acc 56.2500 (56.2500) lr 1.9851e+01 eta 0:04:42\n",
            "epoch [13/200] batch [2/2] time 0.310 (0.532) data 0.000 (0.223) loss 1.5756 (1.6852) acc 65.6250 (60.9375) lr 1.9823e+01 eta 0:03:18\n",
            "epoch [14/200] batch [1/2] time 0.758 (0.758) data 0.452 (0.452) loss 2.3695 (2.3695) acc 43.7500 (43.7500) lr 1.9823e+01 eta 0:04:42\n",
            "epoch [14/200] batch [2/2] time 0.312 (0.535) data 0.000 (0.226) loss 1.8714 (2.1204) acc 56.2500 (50.0000) lr 1.9792e+01 eta 0:03:18\n",
            "epoch [15/200] batch [1/2] time 0.752 (0.752) data 0.446 (0.446) loss 1.6515 (1.6515) acc 56.2500 (56.2500) lr 1.9792e+01 eta 0:04:38\n",
            "epoch [15/200] batch [2/2] time 0.310 (0.531) data 0.000 (0.223) loss 1.6094 (1.6305) acc 62.5000 (59.3750) lr 1.9759e+01 eta 0:03:16\n",
            "epoch [16/200] batch [1/2] time 0.724 (0.724) data 0.418 (0.418) loss 1.7780 (1.7780) acc 59.3750 (59.3750) lr 1.9759e+01 eta 0:04:27\n",
            "epoch [16/200] batch [2/2] time 0.313 (0.519) data 0.001 (0.209) loss 1.6744 (1.7262) acc 59.3750 (59.3750) lr 1.9724e+01 eta 0:03:10\n",
            "epoch [17/200] batch [1/2] time 0.733 (0.733) data 0.427 (0.427) loss 1.6910 (1.6910) acc 56.2500 (56.2500) lr 1.9724e+01 eta 0:04:29\n",
            "epoch [17/200] batch [2/2] time 0.312 (0.523) data 0.000 (0.214) loss 1.8527 (1.7719) acc 53.1250 (54.6875) lr 1.9686e+01 eta 0:03:11\n",
            "epoch [18/200] batch [1/2] time 0.726 (0.726) data 0.416 (0.416) loss 2.3439 (2.3439) acc 46.8750 (46.8750) lr 1.9686e+01 eta 0:04:25\n",
            "epoch [18/200] batch [2/2] time 0.315 (0.521) data 0.000 (0.208) loss 6.5054 (4.4247) acc 21.8750 (34.3750) lr 1.9646e+01 eta 0:03:09\n",
            "epoch [19/200] batch [1/2] time 0.714 (0.714) data 0.404 (0.404) loss 4.0132 (4.0132) acc 25.0000 (25.0000) lr 1.9646e+01 eta 0:04:19\n",
            "epoch [19/200] batch [2/2] time 0.329 (0.522) data 0.000 (0.202) loss 8.4412 (6.2272) acc 15.6250 (20.3125) lr 1.9603e+01 eta 0:03:08\n",
            "epoch [20/200] batch [1/2] time 0.722 (0.722) data 0.413 (0.413) loss 6.1729 (6.1729) acc 21.8750 (21.8750) lr 1.9603e+01 eta 0:04:20\n",
            "epoch [20/200] batch [2/2] time 0.318 (0.520) data 0.000 (0.207) loss 5.5988 (5.8859) acc 21.8750 (21.8750) lr 1.9558e+01 eta 0:03:07\n",
            "epoch [21/200] batch [1/2] time 0.722 (0.722) data 0.411 (0.411) loss 5.3379 (5.3379) acc 15.6250 (15.6250) lr 1.9558e+01 eta 0:04:19\n",
            "epoch [21/200] batch [2/2] time 0.317 (0.520) data 0.001 (0.206) loss 4.7324 (5.0351) acc 37.5000 (26.5625) lr 1.9511e+01 eta 0:03:05\n",
            "epoch [22/200] batch [1/2] time 0.746 (0.746) data 0.435 (0.435) loss 9.5190 (9.5190) acc 6.2500 (6.2500) lr 1.9511e+01 eta 0:04:26\n",
            "epoch [22/200] batch [2/2] time 0.317 (0.531) data 0.001 (0.218) loss 7.2085 (8.3637) acc 21.8750 (14.0625) lr 1.9461e+01 eta 0:03:09\n",
            "epoch [23/200] batch [1/2] time 0.712 (0.712) data 0.404 (0.404) loss 5.3850 (5.3850) acc 46.8750 (46.8750) lr 1.9461e+01 eta 0:04:12\n",
            "epoch [23/200] batch [2/2] time 0.319 (0.516) data 0.001 (0.202) loss 6.9982 (6.1916) acc 28.1250 (37.5000) lr 1.9409e+01 eta 0:03:02\n",
            "epoch [24/200] batch [1/2] time 0.738 (0.738) data 0.424 (0.424) loss 2.7800 (2.7800) acc 53.1250 (53.1250) lr 1.9409e+01 eta 0:04:20\n",
            "epoch [24/200] batch [2/2] time 0.319 (0.529) data 0.001 (0.212) loss 2.9156 (2.8478) acc 31.2500 (42.1875) lr 1.9354e+01 eta 0:03:06\n",
            "epoch [25/200] batch [1/2] time 0.775 (0.775) data 0.461 (0.461) loss 2.2942 (2.2942) acc 37.5000 (37.5000) lr 1.9354e+01 eta 0:04:31\n",
            "epoch [25/200] batch [2/2] time 0.320 (0.547) data 0.001 (0.231) loss 2.2562 (2.2752) acc 46.8750 (42.1875) lr 1.9298e+01 eta 0:03:11\n",
            "epoch [26/200] batch [1/2] time 0.782 (0.782) data 0.469 (0.469) loss 2.1601 (2.1601) acc 56.2500 (56.2500) lr 1.9298e+01 eta 0:04:33\n",
            "epoch [26/200] batch [2/2] time 0.318 (0.550) data 0.000 (0.235) loss 1.1568 (1.6585) acc 84.3750 (70.3125) lr 1.9239e+01 eta 0:03:11\n",
            "epoch [27/200] batch [1/2] time 0.779 (0.779) data 0.463 (0.463) loss 1.5804 (1.5804) acc 56.2500 (56.2500) lr 1.9239e+01 eta 0:04:30\n",
            "epoch [27/200] batch [2/2] time 0.323 (0.551) data 0.001 (0.232) loss 1.5368 (1.5586) acc 68.7500 (62.5000) lr 1.9178e+01 eta 0:03:10\n",
            "epoch [28/200] batch [1/2] time 0.767 (0.767) data 0.452 (0.452) loss 1.9076 (1.9076) acc 59.3750 (59.3750) lr 1.9178e+01 eta 0:04:24\n",
            "epoch [28/200] batch [2/2] time 0.324 (0.545) data 0.000 (0.226) loss 1.2845 (1.5961) acc 59.3750 (59.3750) lr 1.9114e+01 eta 0:03:07\n",
            "epoch [29/200] batch [1/2] time 0.754 (0.754) data 0.439 (0.439) loss 1.5131 (1.5131) acc 68.7500 (68.7500) lr 1.9114e+01 eta 0:04:18\n",
            "epoch [29/200] batch [2/2] time 0.322 (0.538) data 0.000 (0.220) loss 1.0801 (1.2966) acc 75.0000 (71.8750) lr 1.9048e+01 eta 0:03:04\n",
            "epoch [30/200] batch [1/2] time 0.737 (0.737) data 0.421 (0.421) loss 1.1906 (1.1906) acc 68.7500 (68.7500) lr 1.9048e+01 eta 0:04:11\n",
            "epoch [30/200] batch [2/2] time 0.319 (0.528) data 0.001 (0.211) loss 0.9692 (1.0799) acc 81.2500 (75.0000) lr 1.8980e+01 eta 0:02:59\n",
            "epoch [31/200] batch [1/2] time 0.721 (0.721) data 0.403 (0.403) loss 1.2234 (1.2234) acc 75.0000 (75.0000) lr 1.8980e+01 eta 0:04:04\n",
            "epoch [31/200] batch [2/2] time 0.320 (0.520) data 0.000 (0.202) loss 0.9342 (1.0788) acc 87.5000 (81.2500) lr 1.8910e+01 eta 0:02:55\n",
            "epoch [32/200] batch [1/2] time 0.716 (0.716) data 0.401 (0.401) loss 1.1378 (1.1378) acc 71.8750 (71.8750) lr 1.8910e+01 eta 0:04:01\n",
            "epoch [32/200] batch [2/2] time 0.321 (0.519) data 0.000 (0.201) loss 0.9888 (1.0633) acc 81.2500 (76.5625) lr 1.8838e+01 eta 0:02:54\n",
            "epoch [33/200] batch [1/2] time 0.734 (0.734) data 0.420 (0.420) loss 1.0500 (1.0500) acc 75.0000 (75.0000) lr 1.8838e+01 eta 0:04:05\n",
            "epoch [33/200] batch [2/2] time 0.320 (0.527) data 0.000 (0.210) loss 1.5822 (1.3161) acc 68.7500 (71.8750) lr 1.8763e+01 eta 0:02:56\n",
            "epoch [34/200] batch [1/2] time 0.737 (0.737) data 0.421 (0.421) loss 0.9983 (0.9983) acc 75.0000 (75.0000) lr 1.8763e+01 eta 0:04:05\n",
            "epoch [34/200] batch [2/2] time 0.324 (0.530) data 0.000 (0.211) loss 1.3793 (1.1888) acc 56.2500 (65.6250) lr 1.8686e+01 eta 0:02:56\n",
            "epoch [35/200] batch [1/2] time 0.757 (0.757) data 0.443 (0.443) loss 1.9783 (1.9783) acc 56.2500 (56.2500) lr 1.8686e+01 eta 0:04:10\n",
            "epoch [35/200] batch [2/2] time 0.321 (0.539) data 0.000 (0.222) loss 2.5421 (2.2602) acc 46.8750 (51.5625) lr 1.8607e+01 eta 0:02:57\n",
            "epoch [36/200] batch [1/2] time 0.773 (0.773) data 0.462 (0.462) loss 4.4006 (4.4006) acc 31.2500 (31.2500) lr 1.8607e+01 eta 0:04:14\n",
            "epoch [36/200] batch [2/2] time 0.324 (0.548) data 0.001 (0.231) loss 3.1301 (3.7653) acc 53.1250 (42.1875) lr 1.8526e+01 eta 0:02:59\n",
            "epoch [37/200] batch [1/2] time 0.736 (0.736) data 0.419 (0.419) loss 4.9579 (4.9579) acc 34.3750 (34.3750) lr 1.8526e+01 eta 0:04:00\n",
            "epoch [37/200] batch [2/2] time 0.323 (0.530) data 0.001 (0.210) loss 7.3598 (6.1588) acc 15.6250 (25.0000) lr 1.8443e+01 eta 0:02:52\n",
            "epoch [38/200] batch [1/2] time 0.762 (0.762) data 0.447 (0.447) loss 6.3630 (6.3630) acc 31.2500 (31.2500) lr 1.8443e+01 eta 0:04:07\n",
            "epoch [38/200] batch [2/2] time 0.322 (0.542) data 0.000 (0.224) loss 3.6183 (4.9906) acc 31.2500 (31.2500) lr 1.8358e+01 eta 0:02:55\n",
            "epoch [39/200] batch [1/2] time 0.719 (0.719) data 0.406 (0.406) loss 3.5653 (3.5653) acc 34.3750 (34.3750) lr 1.8358e+01 eta 0:03:52\n",
            "epoch [39/200] batch [2/2] time 0.322 (0.520) data 0.000 (0.203) loss 6.6511 (5.1082) acc 18.7500 (26.5625) lr 1.8271e+01 eta 0:02:47\n",
            "epoch [40/200] batch [1/2] time 0.734 (0.734) data 0.420 (0.420) loss 7.5522 (7.5522) acc 28.1250 (28.1250) lr 1.8271e+01 eta 0:03:55\n",
            "epoch [40/200] batch [2/2] time 0.320 (0.527) data 0.000 (0.210) loss 4.1320 (5.8421) acc 34.3750 (31.2500) lr 1.8181e+01 eta 0:02:48\n",
            "epoch [41/200] batch [1/2] time 0.724 (0.724) data 0.409 (0.409) loss 3.8077 (3.8077) acc 37.5000 (37.5000) lr 1.8181e+01 eta 0:03:50\n",
            "epoch [41/200] batch [2/2] time 0.320 (0.522) data 0.000 (0.205) loss 2.9234 (3.3656) acc 40.6250 (39.0625) lr 1.8090e+01 eta 0:02:45\n",
            "epoch [42/200] batch [1/2] time 0.721 (0.721) data 0.412 (0.412) loss 2.2083 (2.2083) acc 46.8750 (46.8750) lr 1.8090e+01 eta 0:03:48\n",
            "epoch [42/200] batch [2/2] time 0.320 (0.520) data 0.001 (0.206) loss 3.8718 (3.0401) acc 28.1250 (37.5000) lr 1.7997e+01 eta 0:02:44\n",
            "epoch [43/200] batch [1/2] time 0.741 (0.741) data 0.427 (0.427) loss 1.9447 (1.9447) acc 62.5000 (62.5000) lr 1.7997e+01 eta 0:03:53\n",
            "epoch [43/200] batch [2/2] time 0.319 (0.530) data 0.001 (0.214) loss 2.6474 (2.2961) acc 43.7500 (53.1250) lr 1.7902e+01 eta 0:02:46\n",
            "epoch [44/200] batch [1/2] time 0.741 (0.741) data 0.431 (0.431) loss 1.1423 (1.1423) acc 81.2500 (81.2500) lr 1.7902e+01 eta 0:03:51\n",
            "epoch [44/200] batch [2/2] time 0.316 (0.529) data 0.000 (0.216) loss 1.9343 (1.5383) acc 40.6250 (60.9375) lr 1.7804e+01 eta 0:02:44\n",
            "epoch [45/200] batch [1/2] time 0.728 (0.728) data 0.417 (0.417) loss 1.1681 (1.1681) acc 78.1250 (78.1250) lr 1.7804e+01 eta 0:03:46\n",
            "epoch [45/200] batch [2/2] time 0.321 (0.525) data 0.001 (0.209) loss 1.4400 (1.3040) acc 68.7500 (73.4375) lr 1.7705e+01 eta 0:02:42\n",
            "epoch [46/200] batch [1/2] time 0.721 (0.721) data 0.412 (0.412) loss 1.2907 (1.2907) acc 65.6250 (65.6250) lr 1.7705e+01 eta 0:03:42\n",
            "epoch [46/200] batch [2/2] time 0.315 (0.518) data 0.001 (0.206) loss 1.5581 (1.4244) acc 59.3750 (62.5000) lr 1.7604e+01 eta 0:02:39\n",
            "epoch [47/200] batch [1/2] time 0.774 (0.774) data 0.465 (0.465) loss 1.0192 (1.0192) acc 68.7500 (68.7500) lr 1.7604e+01 eta 0:03:57\n",
            "epoch [47/200] batch [2/2] time 0.317 (0.545) data 0.001 (0.233) loss 1.0669 (1.0430) acc 81.2500 (75.0000) lr 1.7501e+01 eta 0:02:46\n",
            "epoch [48/200] batch [1/2] time 0.738 (0.738) data 0.427 (0.427) loss 1.0934 (1.0934) acc 75.0000 (75.0000) lr 1.7501e+01 eta 0:03:45\n",
            "epoch [48/200] batch [2/2] time 0.314 (0.526) data 0.001 (0.214) loss 0.8708 (0.9821) acc 81.2500 (78.1250) lr 1.7396e+01 eta 0:02:39\n",
            "epoch [49/200] batch [1/2] time 0.763 (0.763) data 0.453 (0.453) loss 1.7202 (1.7202) acc 56.2500 (56.2500) lr 1.7396e+01 eta 0:03:51\n",
            "epoch [49/200] batch [2/2] time 0.312 (0.537) data 0.000 (0.227) loss 1.0565 (1.3883) acc 75.0000 (65.6250) lr 1.7290e+01 eta 0:02:42\n",
            "epoch [50/200] batch [1/2] time 0.730 (0.730) data 0.421 (0.421) loss 0.9036 (0.9036) acc 78.1250 (78.1250) lr 1.7290e+01 eta 0:03:39\n",
            "epoch [50/200] batch [2/2] time 0.313 (0.522) data 0.001 (0.211) loss 0.6816 (0.7926) acc 81.2500 (79.6875) lr 1.7181e+01 eta 0:02:36\n",
            "epoch [51/200] batch [1/2] time 0.744 (0.744) data 0.438 (0.438) loss 0.8345 (0.8345) acc 87.5000 (87.5000) lr 1.7181e+01 eta 0:03:42\n",
            "epoch [51/200] batch [2/2] time 0.312 (0.528) data 0.000 (0.219) loss 1.0762 (0.9553) acc 75.0000 (81.2500) lr 1.7071e+01 eta 0:02:37\n",
            "epoch [52/200] batch [1/2] time 0.745 (0.745) data 0.435 (0.435) loss 1.0193 (1.0193) acc 75.0000 (75.0000) lr 1.7071e+01 eta 0:03:41\n",
            "epoch [52/200] batch [2/2] time 0.315 (0.530) data 0.001 (0.218) loss 1.3844 (1.2018) acc 71.8750 (73.4375) lr 1.6959e+01 eta 0:02:36\n",
            "epoch [53/200] batch [1/2] time 0.734 (0.734) data 0.424 (0.424) loss 2.0458 (2.0458) acc 56.2500 (56.2500) lr 1.6959e+01 eta 0:03:36\n",
            "epoch [53/200] batch [2/2] time 0.314 (0.524) data 0.001 (0.212) loss 5.1826 (3.6142) acc 40.6250 (48.4375) lr 1.6845e+01 eta 0:02:34\n",
            "epoch [54/200] batch [1/2] time 0.732 (0.732) data 0.423 (0.423) loss 3.3055 (3.3055) acc 53.1250 (53.1250) lr 1.6845e+01 eta 0:03:34\n",
            "epoch [54/200] batch [2/2] time 0.316 (0.524) data 0.000 (0.212) loss 3.8554 (3.5805) acc 31.2500 (42.1875) lr 1.6730e+01 eta 0:02:32\n",
            "epoch [55/200] batch [1/2] time 0.738 (0.738) data 0.431 (0.431) loss 3.6223 (3.6223) acc 50.0000 (50.0000) lr 1.6730e+01 eta 0:03:34\n",
            "epoch [55/200] batch [2/2] time 0.315 (0.526) data 0.000 (0.216) loss 4.3200 (3.9711) acc 31.2500 (40.6250) lr 1.6613e+01 eta 0:02:32\n",
            "epoch [56/200] batch [1/2] time 0.732 (0.732) data 0.426 (0.426) loss 7.7671 (7.7671) acc 18.7500 (18.7500) lr 1.6613e+01 eta 0:03:31\n",
            "epoch [56/200] batch [2/2] time 0.314 (0.523) data 0.001 (0.213) loss 3.5309 (5.6490) acc 34.3750 (26.5625) lr 1.6494e+01 eta 0:02:30\n",
            "epoch [57/200] batch [1/2] time 0.761 (0.761) data 0.456 (0.456) loss 2.3493 (2.3493) acc 59.3750 (59.3750) lr 1.6494e+01 eta 0:03:38\n",
            "epoch [57/200] batch [2/2] time 0.312 (0.537) data 0.000 (0.228) loss 2.9310 (2.6402) acc 37.5000 (48.4375) lr 1.6374e+01 eta 0:02:33\n",
            "epoch [58/200] batch [1/2] time 0.774 (0.774) data 0.467 (0.467) loss 2.9314 (2.9314) acc 28.1250 (28.1250) lr 1.6374e+01 eta 0:03:40\n",
            "epoch [58/200] batch [2/2] time 0.309 (0.542) data 0.001 (0.234) loss 3.1501 (3.0408) acc 50.0000 (39.0625) lr 1.6252e+01 eta 0:02:33\n",
            "epoch [59/200] batch [1/2] time 0.756 (0.756) data 0.448 (0.448) loss 2.8072 (2.8072) acc 40.6250 (40.6250) lr 1.6252e+01 eta 0:03:33\n",
            "epoch [59/200] batch [2/2] time 0.313 (0.534) data 0.001 (0.224) loss 1.4471 (2.1271) acc 71.8750 (56.2500) lr 1.6129e+01 eta 0:02:30\n",
            "epoch [60/200] batch [1/2] time 0.748 (0.748) data 0.442 (0.442) loss 1.4092 (1.4092) acc 68.7500 (68.7500) lr 1.6129e+01 eta 0:03:30\n",
            "epoch [60/200] batch [2/2] time 0.313 (0.530) data 0.000 (0.221) loss 1.7453 (1.5773) acc 56.2500 (62.5000) lr 1.6004e+01 eta 0:02:28\n",
            "epoch [61/200] batch [1/2] time 0.731 (0.731) data 0.425 (0.425) loss 1.4338 (1.4338) acc 59.3750 (59.3750) lr 1.6004e+01 eta 0:03:24\n",
            "epoch [61/200] batch [2/2] time 0.308 (0.520) data 0.000 (0.213) loss 1.0778 (1.2558) acc 81.2500 (70.3125) lr 1.5878e+01 eta 0:02:24\n",
            "epoch [62/200] batch [1/2] time 0.726 (0.726) data 0.421 (0.421) loss 0.9722 (0.9722) acc 78.1250 (78.1250) lr 1.5878e+01 eta 0:03:21\n",
            "epoch [62/200] batch [2/2] time 0.311 (0.519) data 0.001 (0.211) loss 1.3523 (1.1623) acc 50.0000 (64.0625) lr 1.5750e+01 eta 0:02:23\n",
            "epoch [63/200] batch [1/2] time 0.745 (0.745) data 0.439 (0.439) loss 0.9887 (0.9887) acc 75.0000 (75.0000) lr 1.5750e+01 eta 0:03:24\n",
            "epoch [63/200] batch [2/2] time 0.308 (0.527) data 0.000 (0.220) loss 1.0519 (1.0203) acc 75.0000 (75.0000) lr 1.5621e+01 eta 0:02:24\n",
            "epoch [64/200] batch [1/2] time 0.718 (0.718) data 0.413 (0.413) loss 0.8353 (0.8353) acc 87.5000 (87.5000) lr 1.5621e+01 eta 0:03:16\n",
            "epoch [64/200] batch [2/2] time 0.311 (0.515) data 0.000 (0.207) loss 0.9742 (0.9047) acc 81.2500 (84.3750) lr 1.5490e+01 eta 0:02:19\n",
            "epoch [65/200] batch [1/2] time 0.756 (0.756) data 0.448 (0.448) loss 1.0454 (1.0454) acc 71.8750 (71.8750) lr 1.5490e+01 eta 0:03:24\n",
            "epoch [65/200] batch [2/2] time 0.311 (0.534) data 0.000 (0.224) loss 1.0610 (1.0532) acc 81.2500 (76.5625) lr 1.5358e+01 eta 0:02:24\n",
            "epoch [66/200] batch [1/2] time 0.735 (0.735) data 0.432 (0.432) loss 0.6542 (0.6542) acc 90.6250 (90.6250) lr 1.5358e+01 eta 0:03:17\n",
            "epoch [66/200] batch [2/2] time 0.307 (0.521) data 0.000 (0.216) loss 0.7387 (0.6964) acc 87.5000 (89.0625) lr 1.5225e+01 eta 0:02:19\n",
            "epoch [67/200] batch [1/2] time 0.715 (0.715) data 0.411 (0.411) loss 1.2176 (1.2176) acc 75.0000 (75.0000) lr 1.5225e+01 eta 0:03:10\n",
            "epoch [67/200] batch [2/2] time 0.308 (0.511) data 0.000 (0.205) loss 0.7785 (0.9981) acc 84.3750 (79.6875) lr 1.5090e+01 eta 0:02:16\n",
            "epoch [68/200] batch [1/2] time 0.748 (0.748) data 0.443 (0.443) loss 0.7865 (0.7865) acc 84.3750 (84.3750) lr 1.5090e+01 eta 0:03:18\n",
            "epoch [68/200] batch [2/2] time 0.309 (0.529) data 0.000 (0.222) loss 1.2321 (1.0093) acc 71.8750 (78.1250) lr 1.4955e+01 eta 0:02:19\n",
            "epoch [69/200] batch [1/2] time 0.722 (0.722) data 0.417 (0.417) loss 0.7412 (0.7412) acc 87.5000 (87.5000) lr 1.4955e+01 eta 0:03:09\n",
            "epoch [69/200] batch [2/2] time 0.310 (0.516) data 0.001 (0.209) loss 1.4284 (1.0848) acc 68.7500 (78.1250) lr 1.4818e+01 eta 0:02:15\n",
            "epoch [70/200] batch [1/2] time 0.747 (0.747) data 0.441 (0.441) loss 1.1818 (1.1818) acc 78.1250 (78.1250) lr 1.4818e+01 eta 0:03:14\n",
            "epoch [70/200] batch [2/2] time 0.309 (0.528) data 0.000 (0.221) loss 0.8475 (1.0147) acc 78.1250 (78.1250) lr 1.4679e+01 eta 0:02:17\n",
            "epoch [71/200] batch [1/2] time 0.777 (0.777) data 0.472 (0.472) loss 0.8902 (0.8902) acc 81.2500 (81.2500) lr 1.4679e+01 eta 0:03:21\n",
            "epoch [71/200] batch [2/2] time 0.308 (0.542) data 0.001 (0.236) loss 0.7543 (0.8223) acc 84.3750 (82.8125) lr 1.4540e+01 eta 0:02:19\n",
            "epoch [72/200] batch [1/2] time 0.753 (0.753) data 0.448 (0.448) loss 1.1421 (1.1421) acc 75.0000 (75.0000) lr 1.4540e+01 eta 0:03:13\n",
            "epoch [72/200] batch [2/2] time 0.309 (0.531) data 0.000 (0.224) loss 1.0544 (1.0983) acc 75.0000 (75.0000) lr 1.4399e+01 eta 0:02:15\n",
            "epoch [73/200] batch [1/2] time 0.730 (0.730) data 0.427 (0.427) loss 1.1999 (1.1999) acc 81.2500 (81.2500) lr 1.4399e+01 eta 0:03:06\n",
            "epoch [73/200] batch [2/2] time 0.309 (0.519) data 0.001 (0.214) loss 1.2697 (1.2348) acc 59.3750 (70.3125) lr 1.4258e+01 eta 0:02:11\n",
            "epoch [74/200] batch [1/2] time 0.732 (0.732) data 0.427 (0.427) loss 2.1517 (2.1517) acc 50.0000 (50.0000) lr 1.4258e+01 eta 0:03:05\n",
            "epoch [74/200] batch [2/2] time 0.310 (0.521) data 0.001 (0.214) loss 2.8391 (2.4954) acc 62.5000 (56.2500) lr 1.4115e+01 eta 0:02:11\n",
            "epoch [75/200] batch [1/2] time 0.734 (0.734) data 0.429 (0.429) loss 2.6696 (2.6696) acc 40.6250 (40.6250) lr 1.4115e+01 eta 0:03:04\n",
            "epoch [75/200] batch [2/2] time 0.309 (0.522) data 0.001 (0.215) loss 2.3716 (2.5206) acc 43.7500 (42.1875) lr 1.3971e+01 eta 0:02:10\n",
            "epoch [76/200] batch [1/2] time 0.704 (0.704) data 0.402 (0.402) loss 3.9259 (3.9259) acc 34.3750 (34.3750) lr 1.3971e+01 eta 0:02:55\n",
            "epoch [76/200] batch [2/2] time 0.310 (0.507) data 0.000 (0.201) loss 3.9245 (3.9252) acc 43.7500 (39.0625) lr 1.3827e+01 eta 0:02:05\n",
            "epoch [77/200] batch [1/2] time 0.733 (0.733) data 0.429 (0.429) loss 2.8021 (2.8021) acc 65.6250 (65.6250) lr 1.3827e+01 eta 0:03:01\n",
            "epoch [77/200] batch [2/2] time 0.310 (0.521) data 0.000 (0.215) loss 1.9543 (2.3782) acc 50.0000 (57.8125) lr 1.3681e+01 eta 0:02:08\n",
            "epoch [78/200] batch [1/2] time 0.717 (0.717) data 0.412 (0.412) loss 1.9526 (1.9526) acc 43.7500 (43.7500) lr 1.3681e+01 eta 0:02:55\n",
            "epoch [78/200] batch [2/2] time 0.311 (0.514) data 0.000 (0.206) loss 2.0496 (2.0011) acc 50.0000 (46.8750) lr 1.3535e+01 eta 0:02:05\n",
            "epoch [79/200] batch [1/2] time 0.754 (0.754) data 0.451 (0.451) loss 1.5980 (1.5980) acc 65.6250 (65.6250) lr 1.3535e+01 eta 0:03:03\n",
            "epoch [79/200] batch [2/2] time 0.310 (0.532) data 0.000 (0.226) loss 1.6542 (1.6261) acc 65.6250 (65.6250) lr 1.3387e+01 eta 0:02:08\n",
            "epoch [80/200] batch [1/2] time 0.768 (0.768) data 0.463 (0.463) loss 1.2790 (1.2790) acc 65.6250 (65.6250) lr 1.3387e+01 eta 0:03:05\n",
            "epoch [80/200] batch [2/2] time 0.311 (0.540) data 0.000 (0.232) loss 1.1662 (1.2226) acc 75.0000 (70.3125) lr 1.3239e+01 eta 0:02:09\n",
            "epoch [81/200] batch [1/2] time 0.777 (0.777) data 0.473 (0.473) loss 0.7690 (0.7690) acc 93.7500 (93.7500) lr 1.3239e+01 eta 0:03:05\n",
            "epoch [81/200] batch [2/2] time 0.311 (0.544) data 0.001 (0.237) loss 1.2738 (1.0214) acc 71.8750 (82.8125) lr 1.3090e+01 eta 0:02:09\n",
            "epoch [82/200] batch [1/2] time 0.757 (0.757) data 0.452 (0.452) loss 1.2866 (1.2866) acc 71.8750 (71.8750) lr 1.3090e+01 eta 0:02:59\n",
            "epoch [82/200] batch [2/2] time 0.310 (0.533) data 0.001 (0.226) loss 0.8564 (1.0715) acc 81.2500 (76.5625) lr 1.2940e+01 eta 0:02:05\n",
            "epoch [83/200] batch [1/2] time 0.763 (0.763) data 0.457 (0.457) loss 0.8576 (0.8576) acc 84.3750 (84.3750) lr 1.2940e+01 eta 0:02:59\n",
            "epoch [83/200] batch [2/2] time 0.310 (0.536) data 0.000 (0.229) loss 1.0569 (0.9573) acc 68.7500 (76.5625) lr 1.2790e+01 eta 0:02:05\n",
            "epoch [84/200] batch [1/2] time 0.728 (0.728) data 0.423 (0.423) loss 1.6362 (1.6362) acc 75.0000 (75.0000) lr 1.2790e+01 eta 0:02:49\n",
            "epoch [84/200] batch [2/2] time 0.310 (0.519) data 0.001 (0.212) loss 0.9905 (1.3134) acc 75.0000 (75.0000) lr 1.2639e+01 eta 0:02:00\n",
            "epoch [85/200] batch [1/2] time 0.724 (0.724) data 0.421 (0.421) loss 0.6351 (0.6351) acc 87.5000 (87.5000) lr 1.2639e+01 eta 0:02:47\n",
            "epoch [85/200] batch [2/2] time 0.311 (0.517) data 0.001 (0.211) loss 1.2705 (0.9528) acc 78.1250 (82.8125) lr 1.2487e+01 eta 0:01:59\n",
            "epoch [86/200] batch [1/2] time 0.737 (0.737) data 0.433 (0.433) loss 0.6114 (0.6114) acc 87.5000 (87.5000) lr 1.2487e+01 eta 0:02:48\n",
            "epoch [86/200] batch [2/2] time 0.313 (0.525) data 0.001 (0.217) loss 0.5078 (0.5596) acc 93.7500 (90.6250) lr 1.2334e+01 eta 0:01:59\n",
            "epoch [87/200] batch [1/2] time 0.740 (0.740) data 0.435 (0.435) loss 0.7598 (0.7598) acc 87.5000 (87.5000) lr 1.2334e+01 eta 0:02:47\n",
            "epoch [87/200] batch [2/2] time 0.310 (0.525) data 0.000 (0.218) loss 0.5905 (0.6751) acc 90.6250 (89.0625) lr 1.2181e+01 eta 0:01:58\n",
            "epoch [88/200] batch [1/2] time 0.715 (0.715) data 0.409 (0.409) loss 0.6429 (0.6429) acc 87.5000 (87.5000) lr 1.2181e+01 eta 0:02:40\n",
            "epoch [88/200] batch [2/2] time 0.312 (0.513) data 0.000 (0.205) loss 0.7303 (0.6866) acc 93.7500 (90.6250) lr 1.2028e+01 eta 0:01:54\n",
            "epoch [89/200] batch [1/2] time 0.738 (0.738) data 0.435 (0.435) loss 0.4986 (0.4986) acc 93.7500 (93.7500) lr 1.2028e+01 eta 0:02:44\n",
            "epoch [89/200] batch [2/2] time 0.309 (0.523) data 0.000 (0.218) loss 0.7960 (0.6473) acc 87.5000 (90.6250) lr 1.1874e+01 eta 0:01:56\n",
            "epoch [90/200] batch [1/2] time 0.756 (0.756) data 0.451 (0.451) loss 0.8317 (0.8317) acc 90.6250 (90.6250) lr 1.1874e+01 eta 0:02:47\n",
            "epoch [90/200] batch [2/2] time 0.310 (0.533) data 0.000 (0.226) loss 0.7787 (0.8052) acc 84.3750 (87.5000) lr 1.1719e+01 eta 0:01:57\n",
            "epoch [91/200] batch [1/2] time 0.705 (0.705) data 0.400 (0.400) loss 0.9120 (0.9120) acc 75.0000 (75.0000) lr 1.1719e+01 eta 0:02:34\n",
            "epoch [91/200] batch [2/2] time 0.310 (0.508) data 0.000 (0.200) loss 0.4701 (0.6911) acc 93.7500 (84.3750) lr 1.1564e+01 eta 0:01:50\n",
            "epoch [92/200] batch [1/2] time 0.730 (0.730) data 0.420 (0.420) loss 1.3397 (1.3397) acc 68.7500 (68.7500) lr 1.1564e+01 eta 0:02:38\n",
            "epoch [92/200] batch [2/2] time 0.313 (0.521) data 0.001 (0.210) loss 1.0309 (1.1853) acc 68.7500 (68.7500) lr 1.1409e+01 eta 0:01:52\n",
            "epoch [93/200] batch [1/2] time 0.757 (0.757) data 0.449 (0.449) loss 0.8404 (0.8404) acc 84.3750 (84.3750) lr 1.1409e+01 eta 0:02:42\n",
            "epoch [93/200] batch [2/2] time 0.314 (0.535) data 0.001 (0.225) loss 1.6707 (1.2555) acc 75.0000 (79.6875) lr 1.1253e+01 eta 0:01:54\n",
            "epoch [94/200] batch [1/2] time 0.756 (0.756) data 0.451 (0.451) loss 1.4168 (1.4168) acc 68.7500 (68.7500) lr 1.1253e+01 eta 0:02:40\n",
            "epoch [94/200] batch [2/2] time 0.313 (0.535) data 0.000 (0.226) loss 1.8505 (1.6337) acc 59.3750 (64.0625) lr 1.1097e+01 eta 0:01:53\n",
            "epoch [95/200] batch [1/2] time 0.711 (0.711) data 0.403 (0.403) loss 0.8600 (0.8600) acc 81.2500 (81.2500) lr 1.1097e+01 eta 0:02:30\n",
            "epoch [95/200] batch [2/2] time 0.311 (0.511) data 0.000 (0.202) loss 0.6051 (0.7325) acc 93.7500 (87.5000) lr 1.0941e+01 eta 0:01:47\n",
            "epoch [96/200] batch [1/2] time 0.721 (0.721) data 0.414 (0.414) loss 1.9103 (1.9103) acc 53.1250 (53.1250) lr 1.0941e+01 eta 0:02:30\n",
            "epoch [96/200] batch [2/2] time 0.312 (0.517) data 0.000 (0.207) loss 2.1382 (2.0243) acc 56.2500 (54.6875) lr 1.0785e+01 eta 0:01:47\n",
            "epoch [97/200] batch [1/2] time 0.725 (0.725) data 0.417 (0.417) loss 2.0860 (2.0860) acc 53.1250 (53.1250) lr 1.0785e+01 eta 0:02:30\n",
            "epoch [97/200] batch [2/2] time 0.314 (0.520) data 0.000 (0.209) loss 2.3401 (2.2130) acc 43.7500 (48.4375) lr 1.0628e+01 eta 0:01:47\n",
            "epoch [98/200] batch [1/2] time 0.752 (0.752) data 0.443 (0.443) loss 1.6719 (1.6719) acc 68.7500 (68.7500) lr 1.0628e+01 eta 0:02:34\n",
            "epoch [98/200] batch [2/2] time 0.313 (0.532) data 0.000 (0.222) loss 1.0031 (1.3375) acc 81.2500 (75.0000) lr 1.0471e+01 eta 0:01:48\n",
            "epoch [99/200] batch [1/2] time 0.745 (0.745) data 0.437 (0.437) loss 1.3216 (1.3216) acc 75.0000 (75.0000) lr 1.0471e+01 eta 0:02:31\n",
            "epoch [99/200] batch [2/2] time 0.311 (0.528) data 0.000 (0.219) loss 0.6754 (0.9985) acc 90.6250 (82.8125) lr 1.0314e+01 eta 0:01:46\n",
            "epoch [100/200] batch [1/2] time 0.737 (0.737) data 0.431 (0.431) loss 0.7873 (0.7873) acc 81.2500 (81.2500) lr 1.0314e+01 eta 0:02:28\n",
            "epoch [100/200] batch [2/2] time 0.313 (0.525) data 0.000 (0.216) loss 0.8985 (0.8429) acc 84.3750 (82.8125) lr 1.0157e+01 eta 0:01:44\n",
            "epoch [101/200] batch [1/2] time 0.743 (0.743) data 0.434 (0.434) loss 0.8180 (0.8180) acc 90.6250 (90.6250) lr 1.0157e+01 eta 0:02:27\n",
            "epoch [101/200] batch [2/2] time 0.316 (0.529) data 0.000 (0.217) loss 0.8977 (0.8579) acc 78.1250 (84.3750) lr 1.0000e+01 eta 0:01:44\n",
            "epoch [102/200] batch [1/2] time 0.725 (0.725) data 0.419 (0.419) loss 0.7207 (0.7207) acc 84.3750 (84.3750) lr 1.0000e+01 eta 0:02:22\n",
            "epoch [102/200] batch [2/2] time 0.311 (0.518) data 0.000 (0.210) loss 0.9755 (0.8481) acc 84.3750 (84.3750) lr 9.8429e+00 eta 0:01:41\n",
            "epoch [103/200] batch [1/2] time 0.738 (0.738) data 0.430 (0.430) loss 0.5216 (0.5216) acc 93.7500 (93.7500) lr 9.8429e+00 eta 0:02:23\n",
            "epoch [103/200] batch [2/2] time 0.313 (0.525) data 0.001 (0.215) loss 0.7187 (0.6201) acc 87.5000 (90.6250) lr 9.6859e+00 eta 0:01:41\n",
            "epoch [104/200] batch [1/2] time 0.760 (0.760) data 0.454 (0.454) loss 0.7711 (0.7711) acc 87.5000 (87.5000) lr 9.6859e+00 eta 0:02:26\n",
            "epoch [104/200] batch [2/2] time 0.314 (0.537) data 0.001 (0.227) loss 1.2022 (0.9867) acc 75.0000 (81.2500) lr 9.5289e+00 eta 0:01:43\n",
            "epoch [105/200] batch [1/2] time 0.772 (0.772) data 0.467 (0.467) loss 0.5469 (0.5469) acc 90.6250 (90.6250) lr 9.5289e+00 eta 0:02:27\n",
            "epoch [105/200] batch [2/2] time 0.315 (0.543) data 0.000 (0.233) loss 0.4766 (0.5118) acc 93.7500 (92.1875) lr 9.3721e+00 eta 0:01:43\n",
            "epoch [106/200] batch [1/2] time 0.741 (0.741) data 0.435 (0.435) loss 0.6544 (0.6544) acc 90.6250 (90.6250) lr 9.3721e+00 eta 0:02:20\n",
            "epoch [106/200] batch [2/2] time 0.315 (0.528) data 0.001 (0.218) loss 0.7737 (0.7140) acc 81.2500 (85.9375) lr 9.2154e+00 eta 0:01:39\n",
            "epoch [107/200] batch [1/2] time 0.722 (0.722) data 0.416 (0.416) loss 0.7212 (0.7212) acc 93.7500 (93.7500) lr 9.2154e+00 eta 0:02:15\n",
            "epoch [107/200] batch [2/2] time 0.312 (0.517) data 0.000 (0.208) loss 0.8694 (0.7953) acc 81.2500 (87.5000) lr 9.0589e+00 eta 0:01:36\n",
            "epoch [108/200] batch [1/2] time 0.764 (0.764) data 0.459 (0.459) loss 0.7209 (0.7209) acc 84.3750 (84.3750) lr 9.0589e+00 eta 0:02:21\n",
            "epoch [108/200] batch [2/2] time 0.312 (0.538) data 0.000 (0.230) loss 1.0622 (0.8915) acc 75.0000 (79.6875) lr 8.9027e+00 eta 0:01:39\n",
            "epoch [109/200] batch [1/2] time 0.764 (0.764) data 0.454 (0.454) loss 0.7338 (0.7338) acc 84.3750 (84.3750) lr 8.9027e+00 eta 0:02:19\n",
            "epoch [109/200] batch [2/2] time 0.313 (0.538) data 0.001 (0.227) loss 0.5181 (0.6260) acc 93.7500 (89.0625) lr 8.7467e+00 eta 0:01:37\n",
            "epoch [110/200] batch [1/2] time 0.765 (0.765) data 0.456 (0.456) loss 0.4399 (0.4399) acc 93.7500 (93.7500) lr 8.7467e+00 eta 0:02:18\n",
            "epoch [110/200] batch [2/2] time 0.314 (0.539) data 0.001 (0.228) loss 0.8230 (0.6314) acc 87.5000 (90.6250) lr 8.5910e+00 eta 0:01:37\n",
            "epoch [111/200] batch [1/2] time 0.753 (0.753) data 0.446 (0.446) loss 0.5528 (0.5528) acc 90.6250 (90.6250) lr 8.5910e+00 eta 0:02:14\n",
            "epoch [111/200] batch [2/2] time 0.314 (0.534) data 0.001 (0.223) loss 0.5041 (0.5285) acc 96.8750 (93.7500) lr 8.4357e+00 eta 0:01:35\n",
            "epoch [112/200] batch [1/2] time 0.733 (0.733) data 0.426 (0.426) loss 0.7416 (0.7416) acc 87.5000 (87.5000) lr 8.4357e+00 eta 0:02:09\n",
            "epoch [112/200] batch [2/2] time 0.314 (0.523) data 0.001 (0.213) loss 0.7572 (0.7494) acc 87.5000 (87.5000) lr 8.2807e+00 eta 0:01:32\n",
            "epoch [113/200] batch [1/2] time 0.720 (0.720) data 0.412 (0.412) loss 1.1111 (1.1111) acc 81.2500 (81.2500) lr 8.2807e+00 eta 0:02:05\n",
            "epoch [113/200] batch [2/2] time 0.313 (0.516) data 0.000 (0.206) loss 0.7964 (0.9538) acc 75.0000 (78.1250) lr 8.1262e+00 eta 0:01:29\n",
            "epoch [114/200] batch [1/2] time 0.727 (0.727) data 0.419 (0.419) loss 0.7876 (0.7876) acc 90.6250 (90.6250) lr 8.1262e+00 eta 0:02:05\n",
            "epoch [114/200] batch [2/2] time 0.311 (0.519) data 0.001 (0.210) loss 0.5113 (0.6495) acc 90.6250 (90.6250) lr 7.9721e+00 eta 0:01:29\n",
            "epoch [115/200] batch [1/2] time 0.779 (0.779) data 0.471 (0.471) loss 0.4339 (0.4339) acc 93.7500 (93.7500) lr 7.9721e+00 eta 0:02:13\n",
            "epoch [115/200] batch [2/2] time 0.314 (0.546) data 0.000 (0.236) loss 0.5461 (0.4900) acc 81.2500 (87.5000) lr 7.8186e+00 eta 0:01:32\n",
            "epoch [116/200] batch [1/2] time 0.756 (0.756) data 0.448 (0.448) loss 0.6914 (0.6914) acc 81.2500 (81.2500) lr 7.8186e+00 eta 0:02:07\n",
            "epoch [116/200] batch [2/2] time 0.315 (0.535) data 0.001 (0.224) loss 0.9187 (0.8051) acc 78.1250 (79.6875) lr 7.6655e+00 eta 0:01:29\n",
            "epoch [117/200] batch [1/2] time 0.746 (0.746) data 0.439 (0.439) loss 0.8295 (0.8295) acc 84.3750 (84.3750) lr 7.6655e+00 eta 0:02:04\n",
            "epoch [117/200] batch [2/2] time 0.315 (0.531) data 0.000 (0.220) loss 1.2772 (1.0534) acc 71.8750 (78.1250) lr 7.5131e+00 eta 0:01:28\n",
            "epoch [118/200] batch [1/2] time 0.736 (0.736) data 0.430 (0.430) loss 0.8736 (0.8736) acc 81.2500 (81.2500) lr 7.5131e+00 eta 0:02:01\n",
            "epoch [118/200] batch [2/2] time 0.314 (0.525) data 0.001 (0.215) loss 2.2581 (1.5659) acc 37.5000 (59.3750) lr 7.3613e+00 eta 0:01:26\n",
            "epoch [119/200] batch [1/2] time 0.745 (0.745) data 0.436 (0.436) loss 1.3357 (1.3357) acc 81.2500 (81.2500) lr 7.3613e+00 eta 0:02:01\n",
            "epoch [119/200] batch [2/2] time 0.315 (0.530) data 0.000 (0.218) loss 2.5574 (1.9466) acc 59.3750 (70.3125) lr 7.2101e+00 eta 0:01:25\n",
            "epoch [120/200] batch [1/2] time 0.727 (0.727) data 0.420 (0.420) loss 1.0862 (1.0862) acc 78.1250 (78.1250) lr 7.2101e+00 eta 0:01:57\n",
            "epoch [120/200] batch [2/2] time 0.316 (0.521) data 0.000 (0.210) loss 1.8547 (1.4704) acc 59.3750 (68.7500) lr 7.0596e+00 eta 0:01:23\n",
            "epoch [121/200] batch [1/2] time 0.733 (0.733) data 0.423 (0.423) loss 0.5686 (0.5686) acc 93.7500 (93.7500) lr 7.0596e+00 eta 0:01:56\n",
            "epoch [121/200] batch [2/2] time 0.313 (0.523) data 0.001 (0.212) loss 0.6423 (0.6054) acc 87.5000 (90.6250) lr 6.9098e+00 eta 0:01:22\n",
            "epoch [122/200] batch [1/2] time 0.714 (0.714) data 0.406 (0.406) loss 0.5903 (0.5903) acc 93.7500 (93.7500) lr 6.9098e+00 eta 0:01:52\n",
            "epoch [122/200] batch [2/2] time 0.312 (0.513) data 0.000 (0.203) loss 0.9082 (0.7493) acc 87.5000 (90.6250) lr 6.7608e+00 eta 0:01:20\n",
            "epoch [123/200] batch [1/2] time 0.745 (0.745) data 0.437 (0.437) loss 1.1867 (1.1867) acc 75.0000 (75.0000) lr 6.7608e+00 eta 0:01:55\n",
            "epoch [123/200] batch [2/2] time 0.315 (0.530) data 0.000 (0.219) loss 1.0032 (1.0949) acc 71.8750 (73.4375) lr 6.6126e+00 eta 0:01:21\n",
            "epoch [124/200] batch [1/2] time 0.750 (0.750) data 0.443 (0.443) loss 1.0138 (1.0138) acc 78.1250 (78.1250) lr 6.6126e+00 eta 0:01:54\n",
            "epoch [124/200] batch [2/2] time 0.313 (0.531) data 0.000 (0.222) loss 0.4769 (0.7453) acc 90.6250 (84.3750) lr 6.4653e+00 eta 0:01:20\n",
            "epoch [125/200] batch [1/2] time 0.767 (0.767) data 0.459 (0.459) loss 0.7661 (0.7661) acc 84.3750 (84.3750) lr 6.4653e+00 eta 0:01:55\n",
            "epoch [125/200] batch [2/2] time 0.316 (0.541) data 0.001 (0.230) loss 0.9289 (0.8475) acc 87.5000 (85.9375) lr 6.3188e+00 eta 0:01:21\n",
            "epoch [126/200] batch [1/2] time 0.754 (0.754) data 0.444 (0.444) loss 0.5707 (0.5707) acc 93.7500 (93.7500) lr 6.3188e+00 eta 0:01:52\n",
            "epoch [126/200] batch [2/2] time 0.316 (0.535) data 0.000 (0.222) loss 0.8379 (0.7043) acc 78.1250 (85.9375) lr 6.1732e+00 eta 0:01:19\n",
            "epoch [127/200] batch [1/2] time 0.750 (0.750) data 0.443 (0.443) loss 0.6070 (0.6070) acc 90.6250 (90.6250) lr 6.1732e+00 eta 0:01:50\n",
            "epoch [127/200] batch [2/2] time 0.311 (0.530) data 0.001 (0.222) loss 0.6877 (0.6474) acc 93.7500 (92.1875) lr 6.0285e+00 eta 0:01:17\n",
            "epoch [128/200] batch [1/2] time 0.728 (0.728) data 0.419 (0.419) loss 0.5850 (0.5850) acc 87.5000 (87.5000) lr 6.0285e+00 eta 0:01:45\n",
            "epoch [128/200] batch [2/2] time 0.312 (0.520) data 0.001 (0.210) loss 0.9958 (0.7904) acc 75.0000 (81.2500) lr 5.8849e+00 eta 0:01:14\n",
            "epoch [129/200] batch [1/2] time 0.745 (0.745) data 0.438 (0.438) loss 0.7409 (0.7409) acc 87.5000 (87.5000) lr 5.8849e+00 eta 0:01:46\n",
            "epoch [129/200] batch [2/2] time 0.313 (0.529) data 0.000 (0.219) loss 0.6015 (0.6712) acc 90.6250 (89.0625) lr 5.7422e+00 eta 0:01:15\n",
            "epoch [130/200] batch [1/2] time 0.715 (0.715) data 0.407 (0.407) loss 0.5640 (0.5640) acc 90.6250 (90.6250) lr 5.7422e+00 eta 0:01:40\n",
            "epoch [130/200] batch [2/2] time 0.311 (0.513) data 0.000 (0.204) loss 0.7959 (0.6799) acc 87.5000 (89.0625) lr 5.6006e+00 eta 0:01:11\n",
            "epoch [131/200] batch [1/2] time 0.749 (0.749) data 0.441 (0.441) loss 0.5818 (0.5818) acc 84.3750 (84.3750) lr 5.6006e+00 eta 0:01:44\n",
            "epoch [131/200] batch [2/2] time 0.313 (0.531) data 0.000 (0.221) loss 0.7046 (0.6432) acc 93.7500 (89.0625) lr 5.4601e+00 eta 0:01:13\n",
            "epoch [132/200] batch [1/2] time 0.716 (0.716) data 0.408 (0.408) loss 1.5399 (1.5399) acc 71.8750 (71.8750) lr 5.4601e+00 eta 0:01:38\n",
            "epoch [132/200] batch [2/2] time 0.312 (0.514) data 0.000 (0.204) loss 0.6186 (1.0793) acc 87.5000 (79.6875) lr 5.3207e+00 eta 0:01:09\n",
            "epoch [133/200] batch [1/2] time 0.717 (0.717) data 0.410 (0.410) loss 0.4748 (0.4748) acc 93.7500 (93.7500) lr 5.3207e+00 eta 0:01:36\n",
            "epoch [133/200] batch [2/2] time 0.315 (0.516) data 0.000 (0.205) loss 0.5004 (0.4876) acc 90.6250 (92.1875) lr 5.1825e+00 eta 0:01:09\n",
            "epoch [134/200] batch [1/2] time 0.721 (0.721) data 0.416 (0.416) loss 0.6365 (0.6365) acc 90.6250 (90.6250) lr 5.1825e+00 eta 0:01:35\n",
            "epoch [134/200] batch [2/2] time 0.312 (0.517) data 0.000 (0.208) loss 0.6220 (0.6293) acc 84.3750 (87.5000) lr 5.0454e+00 eta 0:01:08\n",
            "epoch [135/200] batch [1/2] time 0.724 (0.724) data 0.418 (0.418) loss 0.4358 (0.4358) acc 93.7500 (93.7500) lr 5.0454e+00 eta 0:01:34\n",
            "epoch [135/200] batch [2/2] time 0.314 (0.519) data 0.000 (0.209) loss 0.4762 (0.4560) acc 93.7500 (93.7500) lr 4.9096e+00 eta 0:01:07\n",
            "epoch [136/200] batch [1/2] time 0.759 (0.759) data 0.452 (0.452) loss 0.6319 (0.6319) acc 84.3750 (84.3750) lr 4.9096e+00 eta 0:01:37\n",
            "epoch [136/200] batch [2/2] time 0.310 (0.534) data 0.001 (0.226) loss 0.7520 (0.6920) acc 90.6250 (87.5000) lr 4.7750e+00 eta 0:01:08\n",
            "epoch [137/200] batch [1/2] time 0.767 (0.767) data 0.462 (0.462) loss 0.4926 (0.4926) acc 93.7500 (93.7500) lr 4.7750e+00 eta 0:01:37\n",
            "epoch [137/200] batch [2/2] time 0.314 (0.541) data 0.001 (0.231) loss 0.6796 (0.5861) acc 90.6250 (92.1875) lr 4.6417e+00 eta 0:01:08\n",
            "epoch [138/200] batch [1/2] time 0.746 (0.746) data 0.439 (0.439) loss 0.3361 (0.3361) acc 100.0000 (100.0000) lr 4.6417e+00 eta 0:01:33\n",
            "epoch [138/200] batch [2/2] time 0.312 (0.529) data 0.001 (0.220) loss 0.7988 (0.5675) acc 81.2500 (90.6250) lr 4.5098e+00 eta 0:01:05\n",
            "epoch [139/200] batch [1/2] time 0.746 (0.746) data 0.439 (0.439) loss 0.4454 (0.4454) acc 93.7500 (93.7500) lr 4.5098e+00 eta 0:01:31\n",
            "epoch [139/200] batch [2/2] time 0.312 (0.529) data 0.001 (0.220) loss 0.4668 (0.4561) acc 96.8750 (95.3125) lr 4.3792e+00 eta 0:01:04\n",
            "epoch [140/200] batch [1/2] time 0.718 (0.718) data 0.414 (0.414) loss 0.4150 (0.4150) acc 93.7500 (93.7500) lr 4.3792e+00 eta 0:01:26\n",
            "epoch [140/200] batch [2/2] time 0.309 (0.514) data 0.000 (0.207) loss 0.5341 (0.4746) acc 90.6250 (92.1875) lr 4.2499e+00 eta 0:01:01\n",
            "epoch [141/200] batch [1/2] time 0.747 (0.747) data 0.440 (0.440) loss 0.6815 (0.6815) acc 87.5000 (87.5000) lr 4.2499e+00 eta 0:01:28\n",
            "epoch [141/200] batch [2/2] time 0.311 (0.529) data 0.000 (0.220) loss 0.6420 (0.6618) acc 90.6250 (89.0625) lr 4.1221e+00 eta 0:01:02\n",
            "epoch [142/200] batch [1/2] time 0.721 (0.721) data 0.415 (0.415) loss 0.4686 (0.4686) acc 96.8750 (96.8750) lr 4.1221e+00 eta 0:01:24\n",
            "epoch [142/200] batch [2/2] time 0.311 (0.516) data 0.000 (0.208) loss 0.4975 (0.4830) acc 90.6250 (93.7500) lr 3.9958e+00 eta 0:00:59\n",
            "epoch [143/200] batch [1/2] time 0.698 (0.698) data 0.395 (0.395) loss 0.4359 (0.4359) acc 90.6250 (90.6250) lr 3.9958e+00 eta 0:01:20\n",
            "epoch [143/200] batch [2/2] time 0.309 (0.504) data 0.000 (0.198) loss 0.8156 (0.6258) acc 87.5000 (89.0625) lr 3.8709e+00 eta 0:00:57\n",
            "epoch [144/200] batch [1/2] time 0.738 (0.738) data 0.430 (0.430) loss 0.5902 (0.5902) acc 87.5000 (87.5000) lr 3.8709e+00 eta 0:01:23\n",
            "epoch [144/200] batch [2/2] time 0.311 (0.525) data 0.001 (0.215) loss 0.4395 (0.5148) acc 90.6250 (89.0625) lr 3.7476e+00 eta 0:00:58\n",
            "epoch [145/200] batch [1/2] time 0.744 (0.744) data 0.440 (0.440) loss 0.5104 (0.5104) acc 90.6250 (90.6250) lr 3.7476e+00 eta 0:01:22\n",
            "epoch [145/200] batch [2/2] time 0.309 (0.527) data 0.000 (0.220) loss 0.3916 (0.4510) acc 96.8750 (93.7500) lr 3.6258e+00 eta 0:00:57\n",
            "epoch [146/200] batch [1/2] time 0.743 (0.743) data 0.441 (0.441) loss 0.4240 (0.4240) acc 93.7500 (93.7500) lr 3.6258e+00 eta 0:01:20\n",
            "epoch [146/200] batch [2/2] time 0.308 (0.526) data 0.000 (0.220) loss 0.5365 (0.4803) acc 93.7500 (93.7500) lr 3.5055e+00 eta 0:00:56\n",
            "epoch [147/200] batch [1/2] time 0.737 (0.737) data 0.432 (0.432) loss 0.2931 (0.2931) acc 100.0000 (100.0000) lr 3.5055e+00 eta 0:01:18\n",
            "epoch [147/200] batch [2/2] time 0.310 (0.523) data 0.000 (0.216) loss 0.4878 (0.3904) acc 90.6250 (95.3125) lr 3.3869e+00 eta 0:00:55\n",
            "epoch [148/200] batch [1/2] time 0.750 (0.750) data 0.446 (0.446) loss 0.6150 (0.6150) acc 90.6250 (90.6250) lr 3.3869e+00 eta 0:01:18\n",
            "epoch [148/200] batch [2/2] time 0.311 (0.531) data 0.000 (0.223) loss 0.4641 (0.5396) acc 90.6250 (90.6250) lr 3.2699e+00 eta 0:00:55\n",
            "epoch [149/200] batch [1/2] time 0.737 (0.737) data 0.434 (0.434) loss 0.3030 (0.3030) acc 100.0000 (100.0000) lr 3.2699e+00 eta 0:01:15\n",
            "epoch [149/200] batch [2/2] time 0.312 (0.524) data 0.000 (0.217) loss 0.6198 (0.4614) acc 90.6250 (95.3125) lr 3.1545e+00 eta 0:00:53\n",
            "epoch [150/200] batch [1/2] time 0.742 (0.742) data 0.435 (0.435) loss 0.3202 (0.3202) acc 96.8750 (96.8750) lr 3.1545e+00 eta 0:01:14\n",
            "epoch [150/200] batch [2/2] time 0.309 (0.526) data 0.000 (0.218) loss 0.3276 (0.3239) acc 96.8750 (96.8750) lr 3.0409e+00 eta 0:00:52\n",
            "epoch [151/200] batch [1/2] time 0.731 (0.731) data 0.426 (0.426) loss 0.4640 (0.4640) acc 90.6250 (90.6250) lr 3.0409e+00 eta 0:01:12\n",
            "epoch [151/200] batch [2/2] time 0.310 (0.521) data 0.000 (0.213) loss 0.4014 (0.4327) acc 96.8750 (93.7500) lr 2.9289e+00 eta 0:00:51\n",
            "epoch [152/200] batch [1/2] time 0.745 (0.745) data 0.441 (0.441) loss 0.4217 (0.4217) acc 96.8750 (96.8750) lr 2.9289e+00 eta 0:01:12\n",
            "epoch [152/200] batch [2/2] time 0.310 (0.527) data 0.000 (0.221) loss 0.5649 (0.4933) acc 93.7500 (95.3125) lr 2.8187e+00 eta 0:00:50\n",
            "epoch [153/200] batch [1/2] time 0.738 (0.738) data 0.433 (0.433) loss 0.5617 (0.5617) acc 87.5000 (87.5000) lr 2.8187e+00 eta 0:01:10\n",
            "epoch [153/200] batch [2/2] time 0.308 (0.523) data 0.001 (0.217) loss 0.3024 (0.4321) acc 96.8750 (92.1875) lr 2.7103e+00 eta 0:00:49\n",
            "epoch [154/200] batch [1/2] time 0.716 (0.716) data 0.409 (0.409) loss 0.4669 (0.4669) acc 93.7500 (93.7500) lr 2.7103e+00 eta 0:01:06\n",
            "epoch [154/200] batch [2/2] time 0.309 (0.512) data 0.001 (0.205) loss 0.6227 (0.5448) acc 90.6250 (92.1875) lr 2.6037e+00 eta 0:00:47\n",
            "epoch [155/200] batch [1/2] time 0.720 (0.720) data 0.413 (0.413) loss 0.5869 (0.5869) acc 87.5000 (87.5000) lr 2.6037e+00 eta 0:01:05\n",
            "epoch [155/200] batch [2/2] time 0.311 (0.515) data 0.001 (0.207) loss 0.4777 (0.5323) acc 93.7500 (90.6250) lr 2.4989e+00 eta 0:00:46\n",
            "epoch [156/200] batch [1/2] time 0.732 (0.732) data 0.427 (0.427) loss 0.6361 (0.6361) acc 87.5000 (87.5000) lr 2.4989e+00 eta 0:01:05\n",
            "epoch [156/200] batch [2/2] time 0.309 (0.521) data 0.000 (0.214) loss 0.4993 (0.5677) acc 93.7500 (90.6250) lr 2.3959e+00 eta 0:00:45\n",
            "epoch [157/200] batch [1/2] time 0.708 (0.708) data 0.405 (0.405) loss 0.3460 (0.3460) acc 96.8750 (96.8750) lr 2.3959e+00 eta 0:01:01\n",
            "epoch [157/200] batch [2/2] time 0.309 (0.509) data 0.000 (0.203) loss 0.4454 (0.3957) acc 90.6250 (93.7500) lr 2.2949e+00 eta 0:00:43\n",
            "epoch [158/200] batch [1/2] time 0.717 (0.717) data 0.415 (0.415) loss 0.8798 (0.8798) acc 78.1250 (78.1250) lr 2.2949e+00 eta 0:01:00\n",
            "epoch [158/200] batch [2/2] time 0.311 (0.514) data 0.000 (0.208) loss 0.4981 (0.6890) acc 93.7500 (85.9375) lr 2.1957e+00 eta 0:00:43\n",
            "epoch [159/200] batch [1/2] time 0.716 (0.716) data 0.411 (0.411) loss 0.5307 (0.5307) acc 87.5000 (87.5000) lr 2.1957e+00 eta 0:00:59\n",
            "epoch [159/200] batch [2/2] time 0.308 (0.512) data 0.000 (0.206) loss 0.2885 (0.4096) acc 100.0000 (93.7500) lr 2.0984e+00 eta 0:00:41\n",
            "epoch [160/200] batch [1/2] time 0.750 (0.750) data 0.443 (0.443) loss 0.3877 (0.3877) acc 96.8750 (96.8750) lr 2.0984e+00 eta 0:01:00\n",
            "epoch [160/200] batch [2/2] time 0.310 (0.530) data 0.001 (0.222) loss 0.5280 (0.4578) acc 93.7500 (95.3125) lr 2.0032e+00 eta 0:00:42\n",
            "epoch [161/200] batch [1/2] time 0.752 (0.752) data 0.447 (0.447) loss 0.5100 (0.5100) acc 87.5000 (87.5000) lr 2.0032e+00 eta 0:00:59\n",
            "epoch [161/200] batch [2/2] time 0.309 (0.531) data 0.000 (0.224) loss 0.3258 (0.4179) acc 100.0000 (93.7500) lr 1.9098e+00 eta 0:00:41\n",
            "epoch [162/200] batch [1/2] time 0.779 (0.779) data 0.472 (0.472) loss 0.7182 (0.7182) acc 84.3750 (84.3750) lr 1.9098e+00 eta 0:00:59\n",
            "epoch [162/200] batch [2/2] time 0.308 (0.543) data 0.000 (0.236) loss 0.3299 (0.5240) acc 96.8750 (90.6250) lr 1.8185e+00 eta 0:00:41\n",
            "epoch [163/200] batch [1/2] time 0.689 (0.689) data 0.385 (0.385) loss 0.2906 (0.2906) acc 96.8750 (96.8750) lr 1.8185e+00 eta 0:00:51\n",
            "epoch [163/200] batch [2/2] time 0.309 (0.499) data 0.000 (0.193) loss 0.4433 (0.3669) acc 90.6250 (93.7500) lr 1.7292e+00 eta 0:00:36\n",
            "epoch [164/200] batch [1/2] time 0.715 (0.715) data 0.410 (0.410) loss 0.5395 (0.5395) acc 90.6250 (90.6250) lr 1.7292e+00 eta 0:00:52\n",
            "epoch [164/200] batch [2/2] time 0.310 (0.513) data 0.000 (0.205) loss 0.7547 (0.6471) acc 90.6250 (90.6250) lr 1.6419e+00 eta 0:00:36\n",
            "epoch [165/200] batch [1/2] time 0.729 (0.729) data 0.422 (0.422) loss 0.4311 (0.4311) acc 96.8750 (96.8750) lr 1.6419e+00 eta 0:00:51\n",
            "epoch [165/200] batch [2/2] time 0.312 (0.521) data 0.000 (0.211) loss 0.6804 (0.5557) acc 84.3750 (90.6250) lr 1.5567e+00 eta 0:00:36\n",
            "epoch [166/200] batch [1/2] time 0.760 (0.760) data 0.454 (0.454) loss 0.3997 (0.3997) acc 93.7500 (93.7500) lr 1.5567e+00 eta 0:00:52\n",
            "epoch [166/200] batch [2/2] time 0.309 (0.534) data 0.000 (0.227) loss 0.3568 (0.3783) acc 96.8750 (95.3125) lr 1.4736e+00 eta 0:00:36\n",
            "epoch [167/200] batch [1/2] time 0.736 (0.736) data 0.430 (0.430) loss 0.2561 (0.2561) acc 100.0000 (100.0000) lr 1.4736e+00 eta 0:00:49\n",
            "epoch [167/200] batch [2/2] time 0.310 (0.523) data 0.000 (0.215) loss 0.4537 (0.3549) acc 93.7500 (96.8750) lr 1.3926e+00 eta 0:00:34\n",
            "epoch [168/200] batch [1/2] time 0.752 (0.752) data 0.448 (0.448) loss 0.3125 (0.3125) acc 100.0000 (100.0000) lr 1.3926e+00 eta 0:00:48\n",
            "epoch [168/200] batch [2/2] time 0.311 (0.531) data 0.000 (0.224) loss 0.3645 (0.3385) acc 96.8750 (98.4375) lr 1.3137e+00 eta 0:00:34\n",
            "epoch [169/200] batch [1/2] time 0.748 (0.748) data 0.441 (0.441) loss 0.4616 (0.4616) acc 87.5000 (87.5000) lr 1.3137e+00 eta 0:00:47\n",
            "epoch [169/200] batch [2/2] time 0.312 (0.530) data 0.000 (0.221) loss 0.5497 (0.5056) acc 90.6250 (89.0625) lr 1.2369e+00 eta 0:00:32\n",
            "epoch [170/200] batch [1/2] time 0.742 (0.742) data 0.438 (0.438) loss 0.3864 (0.3864) acc 96.8750 (96.8750) lr 1.2369e+00 eta 0:00:45\n",
            "epoch [170/200] batch [2/2] time 0.309 (0.526) data 0.000 (0.219) loss 0.2986 (0.3425) acc 100.0000 (98.4375) lr 1.1623e+00 eta 0:00:31\n",
            "epoch [171/200] batch [1/2] time 0.757 (0.757) data 0.453 (0.453) loss 0.3689 (0.3689) acc 96.8750 (96.8750) lr 1.1623e+00 eta 0:00:44\n",
            "epoch [171/200] batch [2/2] time 0.308 (0.533) data 0.001 (0.227) loss 0.3721 (0.3705) acc 96.8750 (96.8750) lr 1.0899e+00 eta 0:00:30\n",
            "epoch [172/200] batch [1/2] time 0.770 (0.770) data 0.464 (0.464) loss 0.4923 (0.4923) acc 93.7500 (93.7500) lr 1.0899e+00 eta 0:00:43\n",
            "epoch [172/200] batch [2/2] time 0.310 (0.540) data 0.001 (0.232) loss 0.3498 (0.4211) acc 93.7500 (93.7500) lr 1.0197e+00 eta 0:00:30\n",
            "epoch [173/200] batch [1/2] time 0.752 (0.752) data 0.447 (0.447) loss 0.2914 (0.2914) acc 96.8750 (96.8750) lr 1.0197e+00 eta 0:00:41\n",
            "epoch [173/200] batch [2/2] time 0.309 (0.531) data 0.000 (0.224) loss 0.3221 (0.3068) acc 96.8750 (96.8750) lr 9.5173e-01 eta 0:00:28\n",
            "epoch [174/200] batch [1/2] time 0.738 (0.738) data 0.434 (0.434) loss 0.2395 (0.2395) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:39\n",
            "epoch [174/200] batch [2/2] time 0.309 (0.524) data 0.000 (0.217) loss 0.2524 (0.2459) acc 100.0000 (100.0000) lr 8.8597e-01 eta 0:00:27\n",
            "epoch [175/200] batch [1/2] time 0.728 (0.728) data 0.423 (0.423) loss 0.4652 (0.4652) acc 93.7500 (93.7500) lr 8.8597e-01 eta 0:00:37\n",
            "epoch [175/200] batch [2/2] time 0.311 (0.520) data 0.000 (0.212) loss 0.3292 (0.3972) acc 93.7500 (93.7500) lr 8.2245e-01 eta 0:00:25\n",
            "epoch [176/200] batch [1/2] time 0.737 (0.737) data 0.431 (0.431) loss 0.2641 (0.2641) acc 100.0000 (100.0000) lr 8.2245e-01 eta 0:00:36\n",
            "epoch [176/200] batch [2/2] time 0.311 (0.524) data 0.000 (0.216) loss 0.3116 (0.2878) acc 96.8750 (98.4375) lr 7.6120e-01 eta 0:00:25\n",
            "epoch [177/200] batch [1/2] time 0.731 (0.731) data 0.426 (0.426) loss 0.3046 (0.3046) acc 96.8750 (96.8750) lr 7.6120e-01 eta 0:00:34\n",
            "epoch [177/200] batch [2/2] time 0.310 (0.521) data 0.001 (0.213) loss 0.3047 (0.3046) acc 96.8750 (96.8750) lr 7.0224e-01 eta 0:00:23\n",
            "epoch [178/200] batch [1/2] time 0.719 (0.719) data 0.412 (0.412) loss 0.2608 (0.2608) acc 100.0000 (100.0000) lr 7.0224e-01 eta 0:00:32\n",
            "epoch [178/200] batch [2/2] time 0.309 (0.514) data 0.000 (0.206) loss 0.2923 (0.2766) acc 96.8750 (98.4375) lr 6.4556e-01 eta 0:00:22\n",
            "epoch [179/200] batch [1/2] time 0.738 (0.738) data 0.433 (0.433) loss 0.3406 (0.3406) acc 96.8750 (96.8750) lr 6.4556e-01 eta 0:00:31\n",
            "epoch [179/200] batch [2/2] time 0.308 (0.523) data 0.000 (0.216) loss 0.2948 (0.3177) acc 96.8750 (96.8750) lr 5.9119e-01 eta 0:00:21\n",
            "epoch [180/200] batch [1/2] time 0.733 (0.733) data 0.429 (0.429) loss 0.2574 (0.2574) acc 100.0000 (100.0000) lr 5.9119e-01 eta 0:00:30\n",
            "epoch [180/200] batch [2/2] time 0.312 (0.523) data 0.000 (0.215) loss 0.6072 (0.4323) acc 90.6250 (95.3125) lr 5.3915e-01 eta 0:00:20\n",
            "epoch [181/200] batch [1/2] time 0.732 (0.732) data 0.426 (0.426) loss 0.4058 (0.4058) acc 93.7500 (93.7500) lr 5.3915e-01 eta 0:00:28\n",
            "epoch [181/200] batch [2/2] time 0.312 (0.522) data 0.001 (0.213) loss 0.4938 (0.4498) acc 96.8750 (95.3125) lr 4.8943e-01 eta 0:00:19\n",
            "epoch [182/200] batch [1/2] time 0.699 (0.699) data 0.392 (0.392) loss 0.5185 (0.5185) acc 90.6250 (90.6250) lr 4.8943e-01 eta 0:00:25\n",
            "epoch [182/200] batch [2/2] time 0.310 (0.504) data 0.000 (0.196) loss 0.2414 (0.3800) acc 100.0000 (95.3125) lr 4.4207e-01 eta 0:00:18\n",
            "epoch [183/200] batch [1/2] time 0.756 (0.756) data 0.449 (0.449) loss 0.6647 (0.6647) acc 81.2500 (81.2500) lr 4.4207e-01 eta 0:00:26\n",
            "epoch [183/200] batch [2/2] time 0.312 (0.534) data 0.001 (0.225) loss 0.3264 (0.4956) acc 93.7500 (87.5000) lr 3.9706e-01 eta 0:00:18\n",
            "epoch [184/200] batch [1/2] time 0.762 (0.762) data 0.456 (0.456) loss 0.4077 (0.4077) acc 90.6250 (90.6250) lr 3.9706e-01 eta 0:00:25\n",
            "epoch [184/200] batch [2/2] time 0.310 (0.536) data 0.000 (0.228) loss 0.3816 (0.3947) acc 93.7500 (92.1875) lr 3.5443e-01 eta 0:00:17\n",
            "epoch [185/200] batch [1/2] time 0.736 (0.736) data 0.429 (0.429) loss 0.3304 (0.3304) acc 96.8750 (96.8750) lr 3.5443e-01 eta 0:00:22\n",
            "epoch [185/200] batch [2/2] time 0.311 (0.523) data 0.000 (0.215) loss 0.3837 (0.3570) acc 93.7500 (95.3125) lr 3.1417e-01 eta 0:00:15\n",
            "epoch [186/200] batch [1/2] time 0.756 (0.756) data 0.449 (0.449) loss 0.4008 (0.4008) acc 93.7500 (93.7500) lr 3.1417e-01 eta 0:00:21\n",
            "epoch [186/200] batch [2/2] time 0.309 (0.533) data 0.000 (0.225) loss 0.6514 (0.5261) acc 90.6250 (92.1875) lr 2.7630e-01 eta 0:00:14\n",
            "epoch [187/200] batch [1/2] time 0.743 (0.743) data 0.438 (0.438) loss 0.5507 (0.5507) acc 93.7500 (93.7500) lr 2.7630e-01 eta 0:00:20\n",
            "epoch [187/200] batch [2/2] time 0.309 (0.526) data 0.000 (0.219) loss 0.3760 (0.4633) acc 90.6250 (92.1875) lr 2.4083e-01 eta 0:00:13\n",
            "epoch [188/200] batch [1/2] time 0.746 (0.746) data 0.441 (0.441) loss 0.2721 (0.2721) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:18\n",
            "epoch [188/200] batch [2/2] time 0.312 (0.529) data 0.000 (0.221) loss 0.3165 (0.2943) acc 96.8750 (98.4375) lr 2.0777e-01 eta 0:00:12\n",
            "epoch [189/200] batch [1/2] time 0.747 (0.747) data 0.441 (0.441) loss 0.3016 (0.3016) acc 96.8750 (96.8750) lr 2.0777e-01 eta 0:00:17\n",
            "epoch [189/200] batch [2/2] time 0.313 (0.530) data 0.000 (0.221) loss 0.3355 (0.3185) acc 96.8750 (96.8750) lr 1.7713e-01 eta 0:00:11\n",
            "epoch [190/200] batch [1/2] time 0.720 (0.720) data 0.415 (0.415) loss 0.3862 (0.3862) acc 93.7500 (93.7500) lr 1.7713e-01 eta 0:00:15\n",
            "epoch [190/200] batch [2/2] time 0.313 (0.516) data 0.001 (0.208) loss 0.3145 (0.3503) acc 96.8750 (95.3125) lr 1.4891e-01 eta 0:00:10\n",
            "epoch [191/200] batch [1/2] time 0.733 (0.733) data 0.427 (0.427) loss 0.2635 (0.2635) acc 100.0000 (100.0000) lr 1.4891e-01 eta 0:00:13\n",
            "epoch [191/200] batch [2/2] time 0.312 (0.522) data 0.000 (0.214) loss 0.2423 (0.2529) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:09\n",
            "epoch [192/200] batch [1/2] time 0.755 (0.755) data 0.449 (0.449) loss 0.3830 (0.3830) acc 96.8750 (96.8750) lr 1.2312e-01 eta 0:00:12\n",
            "epoch [192/200] batch [2/2] time 0.313 (0.534) data 0.001 (0.225) loss 0.2865 (0.3347) acc 96.8750 (96.8750) lr 9.9763e-02 eta 0:00:08\n",
            "epoch [193/200] batch [1/2] time 0.745 (0.745) data 0.439 (0.439) loss 0.2939 (0.2939) acc 96.8750 (96.8750) lr 9.9763e-02 eta 0:00:11\n",
            "epoch [193/200] batch [2/2] time 0.313 (0.529) data 0.000 (0.220) loss 0.4039 (0.3489) acc 93.7500 (95.3125) lr 7.8853e-02 eta 0:00:07\n",
            "epoch [194/200] batch [1/2] time 0.766 (0.766) data 0.460 (0.460) loss 0.2488 (0.2488) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:09\n",
            "epoch [194/200] batch [2/2] time 0.311 (0.538) data 0.000 (0.230) loss 0.3590 (0.3039) acc 96.8750 (98.4375) lr 6.0390e-02 eta 0:00:06\n",
            "epoch [195/200] batch [1/2] time 0.774 (0.774) data 0.467 (0.467) loss 0.3348 (0.3348) acc 96.8750 (96.8750) lr 6.0390e-02 eta 0:00:08\n",
            "epoch [195/200] batch [2/2] time 0.312 (0.543) data 0.000 (0.234) loss 0.4052 (0.3700) acc 93.7500 (95.3125) lr 4.4380e-02 eta 0:00:05\n",
            "epoch [196/200] batch [1/2] time 0.762 (0.762) data 0.456 (0.456) loss 0.2518 (0.2518) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:06\n",
            "epoch [196/200] batch [2/2] time 0.310 (0.536) data 0.000 (0.228) loss 0.2685 (0.2602) acc 100.0000 (100.0000) lr 3.0827e-02 eta 0:00:04\n",
            "epoch [197/200] batch [1/2] time 0.710 (0.710) data 0.403 (0.403) loss 0.4172 (0.4172) acc 93.7500 (93.7500) lr 3.0827e-02 eta 0:00:04\n",
            "epoch [197/200] batch [2/2] time 0.311 (0.510) data 0.000 (0.202) loss 0.3617 (0.3895) acc 96.8750 (95.3125) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [1/2] time 0.715 (0.715) data 0.410 (0.410) loss 0.2882 (0.2882) acc 96.8750 (96.8750) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [2/2] time 0.309 (0.512) data 0.000 (0.205) loss 0.2704 (0.2793) acc 100.0000 (98.4375) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [1/2] time 0.723 (0.723) data 0.417 (0.417) loss 0.3283 (0.3283) acc 93.7500 (93.7500) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [2/2] time 0.309 (0.516) data 0.001 (0.209) loss 0.4082 (0.3683) acc 93.7500 (93.7500) lr 4.9344e-03 eta 0:00:01\n",
            "epoch [200/200] batch [1/2] time 0.718 (0.718) data 0.410 (0.410) loss 0.3882 (0.3882) acc 93.7500 (93.7500) lr 4.9344e-03 eta 0:00:00\n",
            "epoch [200/200] batch [2/2] time 0.312 (0.515) data 0.000 (0.205) loss 0.4953 (0.4418) acc 87.5000 (90.6250) lr 1.2337e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed2/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:33<00:00,  2.42it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 6,420\n",
            "* accuracy: 79.3%\n",
            "* error: 20.7%\n",
            "* macro_f1: 79.0%\n",
            "Elapsed: 0:04:18\n"
          ]
        }
      ],
      "source": [
        "#eurosat-8shots-seed2\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 2 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed2 \\\n",
        "        DATASET.NUM_SHOTS 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NfrH5V-LQCl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0e81b0-990b-44ab-e009-2d1980361f54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:47:39.586004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:47:39.605590: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:47:39.611475: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:47:39.625604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:47:40.615397: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8']\n",
            "output_dir: output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed3\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  80\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0187, -0.5727,  0.1683,  ...,  0.4947,  0.4188,  0.0344],\n",
            "         [ 0.0173, -0.5677,  0.1739,  ...,  0.4990,  0.4190,  0.0388],\n",
            "         [ 0.0094, -0.5649,  0.1758,  ...,  0.4946,  0.4157,  0.0405],\n",
            "         ...,\n",
            "         [ 0.0227, -0.5676,  0.1747,  ...,  0.5071,  0.4204,  0.0413],\n",
            "         [ 0.0083, -0.5735,  0.1744,  ...,  0.5016,  0.4249,  0.0364],\n",
            "         [ 0.0210, -0.5687,  0.1704,  ...,  0.5030,  0.4152,  0.0323]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2340\n",
            "  Min: -1.5598\n",
            "  Max: 1.0156\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4500\n",
            "  Max: 0.4708\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed3/tensorboard)\n",
            "epoch [1/200] batch [1/2] time 1.755 (1.755) data 0.525 (0.525) loss 8.5552 (8.5552) acc 21.8750 (21.8750) lr 1.0000e-05 eta 0:11:40\n",
            "epoch [1/200] batch [2/2] time 0.299 (1.027) data 0.000 (0.263) loss 8.5376 (8.5464) acc 28.1250 (25.0000) lr 2.0000e+01 eta 0:06:48\n",
            "epoch [2/200] batch [1/2] time 0.735 (0.735) data 0.434 (0.434) loss 8.2351 (8.2351) acc 34.3750 (34.3750) lr 2.0000e+01 eta 0:04:51\n",
            "epoch [2/200] batch [2/2] time 0.304 (0.519) data 0.000 (0.217) loss 6.8976 (7.5664) acc 28.1250 (31.2500) lr 1.9999e+01 eta 0:03:25\n",
            "epoch [3/200] batch [1/2] time 0.733 (0.733) data 0.433 (0.433) loss 5.7898 (5.7898) acc 21.8750 (21.8750) lr 1.9999e+01 eta 0:04:49\n",
            "epoch [3/200] batch [2/2] time 0.306 (0.519) data 0.000 (0.217) loss 5.7306 (5.7602) acc 6.2500 (14.0625) lr 1.9995e+01 eta 0:03:24\n",
            "epoch [4/200] batch [1/2] time 0.739 (0.739) data 0.437 (0.437) loss 5.5496 (5.5496) acc 15.6250 (15.6250) lr 1.9995e+01 eta 0:04:50\n",
            "epoch [4/200] batch [2/2] time 0.308 (0.524) data 0.000 (0.219) loss 6.4574 (6.0035) acc 15.6250 (15.6250) lr 1.9989e+01 eta 0:03:25\n",
            "epoch [5/200] batch [1/2] time 0.758 (0.758) data 0.456 (0.456) loss 6.9195 (6.9195) acc 12.5000 (12.5000) lr 1.9989e+01 eta 0:04:56\n",
            "epoch [5/200] batch [2/2] time 0.307 (0.532) data 0.001 (0.228) loss 5.4565 (6.1880) acc 18.7500 (15.6250) lr 1.9980e+01 eta 0:03:27\n",
            "epoch [6/200] batch [1/2] time 0.749 (0.749) data 0.446 (0.446) loss 5.9849 (5.9849) acc 31.2500 (31.2500) lr 1.9980e+01 eta 0:04:51\n",
            "epoch [6/200] batch [2/2] time 0.308 (0.529) data 0.001 (0.223) loss 4.9632 (5.4740) acc 21.8750 (26.5625) lr 1.9969e+01 eta 0:03:25\n",
            "epoch [7/200] batch [1/2] time 0.772 (0.772) data 0.468 (0.468) loss 3.4634 (3.4634) acc 43.7500 (43.7500) lr 1.9969e+01 eta 0:04:58\n",
            "epoch [7/200] batch [2/2] time 0.286 (0.529) data 0.000 (0.234) loss 3.3093 (3.3863) acc 31.2500 (37.5000) lr 1.9956e+01 eta 0:03:24\n",
            "epoch [8/200] batch [1/2] time 0.733 (0.733) data 0.431 (0.431) loss 3.0220 (3.0220) acc 34.3750 (34.3750) lr 1.9956e+01 eta 0:04:42\n",
            "epoch [8/200] batch [2/2] time 0.307 (0.520) data 0.000 (0.216) loss 3.4582 (3.2401) acc 12.5000 (23.4375) lr 1.9940e+01 eta 0:03:19\n",
            "epoch [9/200] batch [1/2] time 0.736 (0.736) data 0.434 (0.434) loss 2.8030 (2.8030) acc 28.1250 (28.1250) lr 1.9940e+01 eta 0:04:41\n",
            "epoch [9/200] batch [2/2] time 0.306 (0.521) data 0.001 (0.217) loss 2.9820 (2.8925) acc 34.3750 (31.2500) lr 1.9921e+01 eta 0:03:19\n",
            "epoch [10/200] batch [1/2] time 0.737 (0.737) data 0.434 (0.434) loss 2.6753 (2.6753) acc 53.1250 (53.1250) lr 1.9921e+01 eta 0:04:40\n",
            "epoch [10/200] batch [2/2] time 0.309 (0.523) data 0.000 (0.217) loss 2.8310 (2.7531) acc 31.2500 (42.1875) lr 1.9900e+01 eta 0:03:18\n",
            "epoch [11/200] batch [1/2] time 0.734 (0.734) data 0.430 (0.430) loss 2.3578 (2.3578) acc 59.3750 (59.3750) lr 1.9900e+01 eta 0:04:38\n",
            "epoch [11/200] batch [2/2] time 0.311 (0.522) data 0.000 (0.215) loss 2.5402 (2.4490) acc 46.8750 (53.1250) lr 1.9877e+01 eta 0:03:17\n",
            "epoch [12/200] batch [1/2] time 0.745 (0.745) data 0.440 (0.440) loss 2.1885 (2.1885) acc 59.3750 (59.3750) lr 1.9877e+01 eta 0:04:40\n",
            "epoch [12/200] batch [2/2] time 0.311 (0.528) data 0.000 (0.220) loss 2.2462 (2.2173) acc 43.7500 (51.5625) lr 1.9851e+01 eta 0:03:18\n",
            "epoch [13/200] batch [1/2] time 0.741 (0.741) data 0.437 (0.437) loss 1.7781 (1.7781) acc 65.6250 (65.6250) lr 1.9851e+01 eta 0:04:37\n",
            "epoch [13/200] batch [2/2] time 0.309 (0.525) data 0.001 (0.219) loss 1.8532 (1.8156) acc 53.1250 (59.3750) lr 1.9823e+01 eta 0:03:16\n",
            "epoch [14/200] batch [1/2] time 0.776 (0.776) data 0.470 (0.470) loss 1.6821 (1.6821) acc 59.3750 (59.3750) lr 1.9823e+01 eta 0:04:49\n",
            "epoch [14/200] batch [2/2] time 0.312 (0.544) data 0.001 (0.235) loss 1.7469 (1.7145) acc 62.5000 (60.9375) lr 1.9792e+01 eta 0:03:22\n",
            "epoch [15/200] batch [1/2] time 0.750 (0.750) data 0.445 (0.445) loss 1.2733 (1.2733) acc 68.7500 (68.7500) lr 1.9792e+01 eta 0:04:38\n",
            "epoch [15/200] batch [2/2] time 0.311 (0.531) data 0.001 (0.223) loss 1.9513 (1.6123) acc 46.8750 (57.8125) lr 1.9759e+01 eta 0:03:16\n",
            "epoch [16/200] batch [1/2] time 0.762 (0.762) data 0.456 (0.456) loss 1.2422 (1.2422) acc 81.2500 (81.2500) lr 1.9759e+01 eta 0:04:41\n",
            "epoch [16/200] batch [2/2] time 0.311 (0.537) data 0.001 (0.228) loss 1.4663 (1.3543) acc 62.5000 (71.8750) lr 1.9724e+01 eta 0:03:17\n",
            "epoch [17/200] batch [1/2] time 0.755 (0.755) data 0.448 (0.448) loss 1.5088 (1.5088) acc 62.5000 (62.5000) lr 1.9724e+01 eta 0:04:37\n",
            "epoch [17/200] batch [2/2] time 0.314 (0.535) data 0.000 (0.224) loss 2.4118 (1.9603) acc 53.1250 (57.8125) lr 1.9686e+01 eta 0:03:15\n",
            "epoch [18/200] batch [1/2] time 0.770 (0.770) data 0.463 (0.463) loss 3.8491 (3.8491) acc 34.3750 (34.3750) lr 1.9686e+01 eta 0:04:41\n",
            "epoch [18/200] batch [2/2] time 0.314 (0.542) data 0.001 (0.232) loss 4.0947 (3.9719) acc 46.8750 (40.6250) lr 1.9646e+01 eta 0:03:17\n",
            "epoch [19/200] batch [1/2] time 0.737 (0.737) data 0.429 (0.429) loss 4.7339 (4.7339) acc 40.6250 (40.6250) lr 1.9646e+01 eta 0:04:27\n",
            "epoch [19/200] batch [2/2] time 0.316 (0.527) data 0.000 (0.215) loss 3.1841 (3.9590) acc 46.8750 (43.7500) lr 1.9603e+01 eta 0:03:10\n",
            "epoch [20/200] batch [1/2] time 0.712 (0.712) data 0.402 (0.402) loss 3.1999 (3.1999) acc 59.3750 (59.3750) lr 1.9603e+01 eta 0:04:17\n",
            "epoch [20/200] batch [2/2] time 0.319 (0.515) data 0.001 (0.201) loss 4.0779 (3.6389) acc 28.1250 (43.7500) lr 1.9558e+01 eta 0:03:05\n",
            "epoch [21/200] batch [1/2] time 0.760 (0.760) data 0.452 (0.452) loss 6.4100 (6.4100) acc 31.2500 (31.2500) lr 1.9558e+01 eta 0:04:32\n",
            "epoch [21/200] batch [2/2] time 0.315 (0.538) data 0.000 (0.226) loss 8.7633 (7.5867) acc 18.7500 (25.0000) lr 1.9511e+01 eta 0:03:12\n",
            "epoch [22/200] batch [1/2] time 0.748 (0.748) data 0.435 (0.435) loss 4.3126 (4.3126) acc 53.1250 (53.1250) lr 1.9511e+01 eta 0:04:26\n",
            "epoch [22/200] batch [2/2] time 0.317 (0.532) data 0.001 (0.218) loss 3.8280 (4.0703) acc 25.0000 (39.0625) lr 1.9461e+01 eta 0:03:09\n",
            "epoch [23/200] batch [1/2] time 0.760 (0.760) data 0.448 (0.448) loss 4.8286 (4.8286) acc 31.2500 (31.2500) lr 1.9461e+01 eta 0:04:29\n",
            "epoch [23/200] batch [2/2] time 0.304 (0.532) data 0.001 (0.224) loss 3.3045 (4.0665) acc 43.7500 (37.5000) lr 1.9409e+01 eta 0:03:08\n",
            "epoch [24/200] batch [1/2] time 0.735 (0.735) data 0.425 (0.425) loss 2.4289 (2.4289) acc 46.8750 (46.8750) lr 1.9409e+01 eta 0:04:19\n",
            "epoch [24/200] batch [2/2] time 0.318 (0.526) data 0.000 (0.213) loss 2.5184 (2.4737) acc 40.6250 (43.7500) lr 1.9354e+01 eta 0:03:05\n",
            "epoch [25/200] batch [1/2] time 0.757 (0.757) data 0.445 (0.445) loss 1.2018 (1.2018) acc 75.0000 (75.0000) lr 1.9354e+01 eta 0:04:25\n",
            "epoch [25/200] batch [2/2] time 0.318 (0.538) data 0.000 (0.222) loss 1.7307 (1.4663) acc 56.2500 (65.6250) lr 1.9298e+01 eta 0:03:08\n",
            "epoch [26/200] batch [1/2] time 0.775 (0.775) data 0.464 (0.464) loss 1.4602 (1.4602) acc 65.6250 (65.6250) lr 1.9298e+01 eta 0:04:30\n",
            "epoch [26/200] batch [2/2] time 0.320 (0.547) data 0.000 (0.232) loss 1.1396 (1.2999) acc 71.8750 (68.7500) lr 1.9239e+01 eta 0:03:10\n",
            "epoch [27/200] batch [1/2] time 0.782 (0.782) data 0.467 (0.467) loss 1.5690 (1.5690) acc 65.6250 (65.6250) lr 1.9239e+01 eta 0:04:31\n",
            "epoch [27/200] batch [2/2] time 0.319 (0.550) data 0.001 (0.234) loss 0.9358 (1.2524) acc 84.3750 (75.0000) lr 1.9178e+01 eta 0:03:10\n",
            "epoch [28/200] batch [1/2] time 0.768 (0.768) data 0.456 (0.456) loss 1.1810 (1.1810) acc 81.2500 (81.2500) lr 1.9178e+01 eta 0:04:25\n",
            "epoch [28/200] batch [2/2] time 0.319 (0.544) data 0.001 (0.228) loss 0.9015 (1.0413) acc 87.5000 (84.3750) lr 1.9114e+01 eta 0:03:07\n",
            "epoch [29/200] batch [1/2] time 0.764 (0.764) data 0.451 (0.451) loss 0.9667 (0.9667) acc 78.1250 (78.1250) lr 1.9114e+01 eta 0:04:21\n",
            "epoch [29/200] batch [2/2] time 0.317 (0.540) data 0.000 (0.226) loss 1.0857 (1.0262) acc 78.1250 (78.1250) lr 1.9048e+01 eta 0:03:04\n",
            "epoch [30/200] batch [1/2] time 0.756 (0.756) data 0.442 (0.442) loss 0.7791 (0.7791) acc 87.5000 (87.5000) lr 1.9048e+01 eta 0:04:17\n",
            "epoch [30/200] batch [2/2] time 0.321 (0.538) data 0.001 (0.222) loss 0.6186 (0.6989) acc 90.6250 (89.0625) lr 1.8980e+01 eta 0:03:02\n",
            "epoch [31/200] batch [1/2] time 0.752 (0.752) data 0.439 (0.439) loss 0.6278 (0.6278) acc 93.7500 (93.7500) lr 1.8980e+01 eta 0:04:14\n",
            "epoch [31/200] batch [2/2] time 0.319 (0.536) data 0.000 (0.220) loss 0.9795 (0.8037) acc 78.1250 (85.9375) lr 1.8910e+01 eta 0:03:01\n",
            "epoch [32/200] batch [1/2] time 0.740 (0.740) data 0.428 (0.428) loss 0.6061 (0.6061) acc 93.7500 (93.7500) lr 1.8910e+01 eta 0:04:09\n",
            "epoch [32/200] batch [2/2] time 0.320 (0.530) data 0.000 (0.214) loss 0.8390 (0.7226) acc 87.5000 (90.6250) lr 1.8838e+01 eta 0:02:58\n",
            "epoch [33/200] batch [1/2] time 0.768 (0.768) data 0.452 (0.452) loss 0.7297 (0.7297) acc 84.3750 (84.3750) lr 1.8838e+01 eta 0:04:17\n",
            "epoch [33/200] batch [2/2] time 0.320 (0.544) data 0.001 (0.227) loss 0.7027 (0.7162) acc 87.5000 (85.9375) lr 1.8763e+01 eta 0:03:01\n",
            "epoch [34/200] batch [1/2] time 0.757 (0.757) data 0.445 (0.445) loss 0.8071 (0.8071) acc 84.3750 (84.3750) lr 1.8763e+01 eta 0:04:12\n",
            "epoch [34/200] batch [2/2] time 0.322 (0.540) data 0.000 (0.223) loss 0.9496 (0.8784) acc 78.1250 (81.2500) lr 1.8686e+01 eta 0:02:59\n",
            "epoch [35/200] batch [1/2] time 0.757 (0.757) data 0.440 (0.440) loss 1.1963 (1.1963) acc 75.0000 (75.0000) lr 1.8686e+01 eta 0:04:10\n",
            "epoch [35/200] batch [2/2] time 0.322 (0.539) data 0.000 (0.220) loss 1.2103 (1.2033) acc 78.1250 (76.5625) lr 1.8607e+01 eta 0:02:58\n",
            "epoch [36/200] batch [1/2] time 0.783 (0.783) data 0.467 (0.467) loss 1.7548 (1.7548) acc 62.5000 (62.5000) lr 1.8607e+01 eta 0:04:17\n",
            "epoch [36/200] batch [2/2] time 0.321 (0.552) data 0.000 (0.234) loss 3.7663 (2.7606) acc 37.5000 (50.0000) lr 1.8526e+01 eta 0:03:01\n",
            "epoch [37/200] batch [1/2] time 0.746 (0.746) data 0.430 (0.430) loss 2.9627 (2.9627) acc 34.3750 (34.3750) lr 1.8526e+01 eta 0:04:04\n",
            "epoch [37/200] batch [2/2] time 0.323 (0.535) data 0.000 (0.215) loss 2.8844 (2.9236) acc 56.2500 (45.3125) lr 1.8443e+01 eta 0:02:54\n",
            "epoch [38/200] batch [1/2] time 0.793 (0.793) data 0.478 (0.478) loss 5.0102 (5.0102) acc 25.0000 (25.0000) lr 1.8443e+01 eta 0:04:17\n",
            "epoch [38/200] batch [2/2] time 0.322 (0.557) data 0.001 (0.239) loss 3.5109 (4.2606) acc 40.6250 (32.8125) lr 1.8358e+01 eta 0:03:00\n",
            "epoch [39/200] batch [1/2] time 0.768 (0.768) data 0.454 (0.454) loss 3.2510 (3.2510) acc 43.7500 (43.7500) lr 1.8358e+01 eta 0:04:08\n",
            "epoch [39/200] batch [2/2] time 0.321 (0.545) data 0.001 (0.227) loss 3.2066 (3.2288) acc 59.3750 (51.5625) lr 1.8271e+01 eta 0:02:55\n",
            "epoch [40/200] batch [1/2] time 0.755 (0.755) data 0.439 (0.439) loss 1.8173 (1.8173) acc 59.3750 (59.3750) lr 1.8271e+01 eta 0:04:02\n",
            "epoch [40/200] batch [2/2] time 0.320 (0.537) data 0.001 (0.220) loss 1.8691 (1.8432) acc 59.3750 (59.3750) lr 1.8181e+01 eta 0:02:51\n",
            "epoch [41/200] batch [1/2] time 0.735 (0.735) data 0.423 (0.423) loss 1.2280 (1.2280) acc 84.3750 (84.3750) lr 1.8181e+01 eta 0:03:54\n",
            "epoch [41/200] batch [2/2] time 0.317 (0.526) data 0.000 (0.212) loss 1.0052 (1.1166) acc 78.1250 (81.2500) lr 1.8090e+01 eta 0:02:47\n",
            "epoch [42/200] batch [1/2] time 0.757 (0.757) data 0.445 (0.445) loss 0.8799 (0.8799) acc 87.5000 (87.5000) lr 1.8090e+01 eta 0:04:00\n",
            "epoch [42/200] batch [2/2] time 0.317 (0.537) data 0.001 (0.223) loss 1.1500 (1.0149) acc 75.0000 (81.2500) lr 1.7997e+01 eta 0:02:49\n",
            "epoch [43/200] batch [1/2] time 0.752 (0.752) data 0.440 (0.440) loss 1.6393 (1.6393) acc 65.6250 (65.6250) lr 1.7997e+01 eta 0:03:57\n",
            "epoch [43/200] batch [2/2] time 0.315 (0.534) data 0.001 (0.220) loss 1.0968 (1.3680) acc 75.0000 (70.3125) lr 1.7902e+01 eta 0:02:47\n",
            "epoch [44/200] batch [1/2] time 0.722 (0.722) data 0.413 (0.413) loss 1.0109 (1.0109) acc 75.0000 (75.0000) lr 1.7902e+01 eta 0:03:45\n",
            "epoch [44/200] batch [2/2] time 0.318 (0.520) data 0.001 (0.207) loss 1.1111 (1.0610) acc 75.0000 (75.0000) lr 1.7804e+01 eta 0:02:42\n",
            "epoch [45/200] batch [1/2] time 0.726 (0.726) data 0.415 (0.415) loss 0.9477 (0.9477) acc 81.2500 (81.2500) lr 1.7804e+01 eta 0:03:45\n",
            "epoch [45/200] batch [2/2] time 0.317 (0.522) data 0.001 (0.208) loss 0.5999 (0.7738) acc 93.7500 (87.5000) lr 1.7705e+01 eta 0:02:41\n",
            "epoch [46/200] batch [1/2] time 0.737 (0.737) data 0.430 (0.430) loss 0.8806 (0.8806) acc 78.1250 (78.1250) lr 1.7705e+01 eta 0:03:47\n",
            "epoch [46/200] batch [2/2] time 0.312 (0.525) data 0.000 (0.215) loss 0.4441 (0.6624) acc 96.8750 (87.5000) lr 1.7604e+01 eta 0:02:41\n",
            "epoch [47/200] batch [1/2] time 0.742 (0.742) data 0.434 (0.434) loss 0.6483 (0.6483) acc 93.7500 (93.7500) lr 1.7604e+01 eta 0:03:47\n",
            "epoch [47/200] batch [2/2] time 0.314 (0.528) data 0.000 (0.217) loss 0.4104 (0.5294) acc 100.0000 (96.8750) lr 1.7501e+01 eta 0:02:41\n",
            "epoch [48/200] batch [1/2] time 0.740 (0.740) data 0.432 (0.432) loss 0.5348 (0.5348) acc 93.7500 (93.7500) lr 1.7501e+01 eta 0:03:45\n",
            "epoch [48/200] batch [2/2] time 0.312 (0.526) data 0.000 (0.216) loss 0.4558 (0.4953) acc 93.7500 (93.7500) lr 1.7396e+01 eta 0:02:39\n",
            "epoch [49/200] batch [1/2] time 0.767 (0.767) data 0.457 (0.457) loss 0.4426 (0.4426) acc 96.8750 (96.8750) lr 1.7396e+01 eta 0:03:52\n",
            "epoch [49/200] batch [2/2] time 0.314 (0.541) data 0.001 (0.229) loss 0.7363 (0.5894) acc 84.3750 (90.6250) lr 1.7290e+01 eta 0:02:43\n",
            "epoch [50/200] batch [1/2] time 0.740 (0.740) data 0.435 (0.435) loss 0.5190 (0.5190) acc 87.5000 (87.5000) lr 1.7290e+01 eta 0:03:42\n",
            "epoch [50/200] batch [2/2] time 0.314 (0.527) data 0.001 (0.218) loss 0.7102 (0.6146) acc 87.5000 (87.5000) lr 1.7181e+01 eta 0:02:38\n",
            "epoch [51/200] batch [1/2] time 0.769 (0.769) data 0.460 (0.460) loss 0.6236 (0.6236) acc 84.3750 (84.3750) lr 1.7181e+01 eta 0:03:49\n",
            "epoch [51/200] batch [2/2] time 0.313 (0.541) data 0.000 (0.230) loss 0.8195 (0.7216) acc 81.2500 (82.8125) lr 1.7071e+01 eta 0:02:41\n",
            "epoch [52/200] batch [1/2] time 0.747 (0.747) data 0.442 (0.442) loss 0.5137 (0.5137) acc 93.7500 (93.7500) lr 1.7071e+01 eta 0:03:41\n",
            "epoch [52/200] batch [2/2] time 0.311 (0.529) data 0.000 (0.221) loss 1.6102 (1.0620) acc 71.8750 (82.8125) lr 1.6959e+01 eta 0:02:36\n",
            "epoch [53/200] batch [1/2] time 0.724 (0.724) data 0.415 (0.415) loss 0.8051 (0.8051) acc 81.2500 (81.2500) lr 1.6959e+01 eta 0:03:33\n",
            "epoch [53/200] batch [2/2] time 0.313 (0.518) data 0.000 (0.208) loss 2.1211 (1.4631) acc 59.3750 (70.3125) lr 1.6845e+01 eta 0:02:32\n",
            "epoch [54/200] batch [1/2] time 0.735 (0.735) data 0.428 (0.428) loss 1.6148 (1.6148) acc 71.8750 (71.8750) lr 1.6845e+01 eta 0:03:35\n",
            "epoch [54/200] batch [2/2] time 0.311 (0.523) data 0.000 (0.214) loss 2.4384 (2.0266) acc 59.3750 (65.6250) lr 1.6730e+01 eta 0:02:32\n",
            "epoch [55/200] batch [1/2] time 0.725 (0.725) data 0.420 (0.420) loss 2.3230 (2.3230) acc 71.8750 (71.8750) lr 1.6730e+01 eta 0:03:31\n",
            "epoch [55/200] batch [2/2] time 0.314 (0.519) data 0.000 (0.210) loss 2.3709 (2.3470) acc 46.8750 (59.3750) lr 1.6613e+01 eta 0:02:30\n",
            "epoch [56/200] batch [1/2] time 0.752 (0.752) data 0.445 (0.445) loss 3.2340 (3.2340) acc 65.6250 (65.6250) lr 1.6613e+01 eta 0:03:37\n",
            "epoch [56/200] batch [2/2] time 0.313 (0.533) data 0.000 (0.223) loss 4.4411 (3.8375) acc 31.2500 (48.4375) lr 1.6494e+01 eta 0:02:33\n",
            "epoch [57/200] batch [1/2] time 0.715 (0.715) data 0.407 (0.407) loss 8.7231 (8.7231) acc 31.2500 (31.2500) lr 1.6494e+01 eta 0:03:25\n",
            "epoch [57/200] batch [2/2] time 0.312 (0.513) data 0.000 (0.204) loss 5.7117 (7.2174) acc 15.6250 (23.4375) lr 1.6374e+01 eta 0:02:26\n",
            "epoch [58/200] batch [1/2] time 0.752 (0.752) data 0.446 (0.446) loss 3.3200 (3.3200) acc 53.1250 (53.1250) lr 1.6374e+01 eta 0:03:34\n",
            "epoch [58/200] batch [2/2] time 0.312 (0.532) data 0.000 (0.223) loss 3.1221 (3.2211) acc 40.6250 (46.8750) lr 1.6252e+01 eta 0:02:31\n",
            "epoch [59/200] batch [1/2] time 0.738 (0.738) data 0.431 (0.431) loss 3.2850 (3.2850) acc 59.3750 (59.3750) lr 1.6252e+01 eta 0:03:28\n",
            "epoch [59/200] batch [2/2] time 0.309 (0.524) data 0.001 (0.216) loss 2.1445 (2.7147) acc 56.2500 (57.8125) lr 1.6129e+01 eta 0:02:27\n",
            "epoch [60/200] batch [1/2] time 0.721 (0.721) data 0.442 (0.442) loss 1.7930 (1.7930) acc 65.6250 (65.6250) lr 1.6129e+01 eta 0:03:22\n",
            "epoch [60/200] batch [2/2] time 0.312 (0.516) data 0.000 (0.221) loss 1.4887 (1.6408) acc 59.3750 (62.5000) lr 1.6004e+01 eta 0:02:24\n",
            "epoch [61/200] batch [1/2] time 0.773 (0.773) data 0.469 (0.469) loss 1.8351 (1.8351) acc 59.3750 (59.3750) lr 1.6004e+01 eta 0:03:35\n",
            "epoch [61/200] batch [2/2] time 0.312 (0.543) data 0.000 (0.235) loss 1.6837 (1.7594) acc 56.2500 (57.8125) lr 1.5878e+01 eta 0:02:30\n",
            "epoch [62/200] batch [1/2] time 0.755 (0.755) data 0.449 (0.449) loss 1.3801 (1.3801) acc 71.8750 (71.8750) lr 1.5878e+01 eta 0:03:29\n",
            "epoch [62/200] batch [2/2] time 0.309 (0.532) data 0.000 (0.225) loss 1.1915 (1.2858) acc 81.2500 (76.5625) lr 1.5750e+01 eta 0:02:26\n",
            "epoch [63/200] batch [1/2] time 0.701 (0.701) data 0.395 (0.395) loss 1.3968 (1.3968) acc 68.7500 (68.7500) lr 1.5750e+01 eta 0:03:12\n",
            "epoch [63/200] batch [2/2] time 0.308 (0.504) data 0.000 (0.198) loss 0.8095 (1.1031) acc 87.5000 (78.1250) lr 1.5621e+01 eta 0:02:18\n",
            "epoch [64/200] batch [1/2] time 0.739 (0.739) data 0.435 (0.435) loss 1.4078 (1.4078) acc 68.7500 (68.7500) lr 1.5621e+01 eta 0:03:21\n",
            "epoch [64/200] batch [2/2] time 0.311 (0.525) data 0.000 (0.218) loss 0.7246 (1.0662) acc 90.6250 (79.6875) lr 1.5490e+01 eta 0:02:22\n",
            "epoch [65/200] batch [1/2] time 0.746 (0.746) data 0.443 (0.443) loss 0.8609 (0.8609) acc 81.2500 (81.2500) lr 1.5490e+01 eta 0:03:22\n",
            "epoch [65/200] batch [2/2] time 0.310 (0.528) data 0.000 (0.222) loss 0.9166 (0.8888) acc 75.0000 (78.1250) lr 1.5358e+01 eta 0:02:22\n",
            "epoch [66/200] batch [1/2] time 0.748 (0.748) data 0.444 (0.444) loss 0.6381 (0.6381) acc 87.5000 (87.5000) lr 1.5358e+01 eta 0:03:21\n",
            "epoch [66/200] batch [2/2] time 0.310 (0.529) data 0.001 (0.222) loss 0.9012 (0.7696) acc 90.6250 (89.0625) lr 1.5225e+01 eta 0:02:21\n",
            "epoch [67/200] batch [1/2] time 0.760 (0.760) data 0.457 (0.457) loss 0.7057 (0.7057) acc 90.6250 (90.6250) lr 1.5225e+01 eta 0:03:23\n",
            "epoch [67/200] batch [2/2] time 0.310 (0.535) data 0.000 (0.229) loss 0.7706 (0.7382) acc 84.3750 (87.5000) lr 1.5090e+01 eta 0:02:22\n",
            "epoch [68/200] batch [1/2] time 0.734 (0.734) data 0.430 (0.430) loss 0.8426 (0.8426) acc 90.6250 (90.6250) lr 1.5090e+01 eta 0:03:14\n",
            "epoch [68/200] batch [2/2] time 0.309 (0.521) data 0.001 (0.215) loss 0.8180 (0.8303) acc 84.3750 (87.5000) lr 1.4955e+01 eta 0:02:17\n",
            "epoch [69/200] batch [1/2] time 0.726 (0.726) data 0.423 (0.423) loss 0.7233 (0.7233) acc 93.7500 (93.7500) lr 1.4955e+01 eta 0:03:11\n",
            "epoch [69/200] batch [2/2] time 0.308 (0.517) data 0.000 (0.212) loss 0.7154 (0.7193) acc 84.3750 (89.0625) lr 1.4818e+01 eta 0:02:15\n",
            "epoch [70/200] batch [1/2] time 0.759 (0.759) data 0.454 (0.454) loss 0.6872 (0.6872) acc 84.3750 (84.3750) lr 1.4818e+01 eta 0:03:18\n",
            "epoch [70/200] batch [2/2] time 0.309 (0.534) data 0.000 (0.227) loss 0.9632 (0.8252) acc 78.1250 (81.2500) lr 1.4679e+01 eta 0:02:18\n",
            "epoch [71/200] batch [1/2] time 0.757 (0.757) data 0.453 (0.453) loss 1.1936 (1.1936) acc 81.2500 (81.2500) lr 1.4679e+01 eta 0:03:16\n",
            "epoch [71/200] batch [2/2] time 0.310 (0.533) data 0.000 (0.227) loss 1.0606 (1.1271) acc 78.1250 (79.6875) lr 1.4540e+01 eta 0:02:17\n",
            "epoch [72/200] batch [1/2] time 0.757 (0.757) data 0.452 (0.452) loss 1.2675 (1.2675) acc 68.7500 (68.7500) lr 1.4540e+01 eta 0:03:14\n",
            "epoch [72/200] batch [2/2] time 0.309 (0.533) data 0.001 (0.226) loss 2.2475 (1.7575) acc 43.7500 (56.2500) lr 1.4399e+01 eta 0:02:16\n",
            "epoch [73/200] batch [1/2] time 0.765 (0.765) data 0.461 (0.461) loss 5.9356 (5.9356) acc 37.5000 (37.5000) lr 1.4399e+01 eta 0:03:15\n",
            "epoch [73/200] batch [2/2] time 0.309 (0.537) data 0.001 (0.231) loss 6.5609 (6.2483) acc 43.7500 (40.6250) lr 1.4258e+01 eta 0:02:16\n",
            "epoch [74/200] batch [1/2] time 0.752 (0.752) data 0.444 (0.444) loss 3.2383 (3.2383) acc 43.7500 (43.7500) lr 1.4258e+01 eta 0:03:10\n",
            "epoch [74/200] batch [2/2] time 0.309 (0.530) data 0.001 (0.222) loss 4.0473 (3.6428) acc 34.3750 (39.0625) lr 1.4115e+01 eta 0:02:13\n",
            "epoch [75/200] batch [1/2] time 0.761 (0.761) data 0.456 (0.456) loss 7.1650 (7.1650) acc 9.3750 (9.3750) lr 1.4115e+01 eta 0:03:11\n",
            "epoch [75/200] batch [2/2] time 0.309 (0.535) data 0.000 (0.228) loss 10.0796 (8.6223) acc 15.6250 (12.5000) lr 1.3971e+01 eta 0:02:13\n",
            "epoch [76/200] batch [1/2] time 0.751 (0.751) data 0.447 (0.447) loss 13.1434 (13.1434) acc 0.0000 (0.0000) lr 1.3971e+01 eta 0:03:07\n",
            "epoch [76/200] batch [2/2] time 0.310 (0.530) data 0.001 (0.224) loss 7.0653 (10.1043) acc 25.0000 (12.5000) lr 1.3827e+01 eta 0:02:11\n",
            "epoch [77/200] batch [1/2] time 0.746 (0.746) data 0.440 (0.440) loss 4.8883 (4.8883) acc 28.1250 (28.1250) lr 1.3827e+01 eta 0:03:04\n",
            "epoch [77/200] batch [2/2] time 0.310 (0.528) data 0.001 (0.220) loss 5.8381 (5.3632) acc 28.1250 (28.1250) lr 1.3681e+01 eta 0:02:09\n",
            "epoch [78/200] batch [1/2] time 0.743 (0.743) data 0.439 (0.439) loss 2.9306 (2.9306) acc 37.5000 (37.5000) lr 1.3681e+01 eta 0:03:02\n",
            "epoch [78/200] batch [2/2] time 0.309 (0.526) data 0.000 (0.220) loss 3.5417 (3.2361) acc 31.2500 (34.3750) lr 1.3535e+01 eta 0:02:08\n",
            "epoch [79/200] batch [1/2] time 0.771 (0.771) data 0.465 (0.465) loss 1.9611 (1.9611) acc 56.2500 (56.2500) lr 1.3535e+01 eta 0:03:07\n",
            "epoch [79/200] batch [2/2] time 0.310 (0.541) data 0.000 (0.233) loss 1.5799 (1.7705) acc 53.1250 (54.6875) lr 1.3387e+01 eta 0:02:10\n",
            "epoch [80/200] batch [1/2] time 0.724 (0.724) data 0.420 (0.420) loss 1.2272 (1.2272) acc 75.0000 (75.0000) lr 1.3387e+01 eta 0:02:54\n",
            "epoch [80/200] batch [2/2] time 0.310 (0.517) data 0.000 (0.210) loss 1.3193 (1.2732) acc 71.8750 (73.4375) lr 1.3239e+01 eta 0:02:04\n",
            "epoch [81/200] batch [1/2] time 0.725 (0.725) data 0.422 (0.422) loss 1.1901 (1.1901) acc 78.1250 (78.1250) lr 1.3239e+01 eta 0:02:53\n",
            "epoch [81/200] batch [2/2] time 0.311 (0.518) data 0.000 (0.211) loss 0.9999 (1.0950) acc 81.2500 (79.6875) lr 1.3090e+01 eta 0:02:03\n",
            "epoch [82/200] batch [1/2] time 0.798 (0.798) data 0.493 (0.493) loss 1.3863 (1.3863) acc 75.0000 (75.0000) lr 1.3090e+01 eta 0:03:09\n",
            "epoch [82/200] batch [2/2] time 0.309 (0.554) data 0.001 (0.247) loss 1.1445 (1.2654) acc 68.7500 (71.8750) lr 1.2940e+01 eta 0:02:10\n",
            "epoch [83/200] batch [1/2] time 0.797 (0.797) data 0.494 (0.494) loss 1.0059 (1.0059) acc 78.1250 (78.1250) lr 1.2940e+01 eta 0:03:07\n",
            "epoch [83/200] batch [2/2] time 0.313 (0.555) data 0.001 (0.247) loss 1.1065 (1.0562) acc 78.1250 (78.1250) lr 1.2790e+01 eta 0:02:09\n",
            "epoch [84/200] batch [1/2] time 0.815 (0.815) data 0.510 (0.510) loss 0.9843 (0.9843) acc 75.0000 (75.0000) lr 1.2790e+01 eta 0:03:09\n",
            "epoch [84/200] batch [2/2] time 0.309 (0.562) data 0.000 (0.255) loss 0.7855 (0.8849) acc 81.2500 (78.1250) lr 1.2639e+01 eta 0:02:10\n",
            "epoch [85/200] batch [1/2] time 0.760 (0.760) data 0.456 (0.456) loss 0.6027 (0.6027) acc 93.7500 (93.7500) lr 1.2639e+01 eta 0:02:55\n",
            "epoch [85/200] batch [2/2] time 0.309 (0.534) data 0.000 (0.228) loss 1.1421 (0.8724) acc 68.7500 (81.2500) lr 1.2487e+01 eta 0:02:02\n",
            "epoch [86/200] batch [1/2] time 0.733 (0.733) data 0.428 (0.428) loss 0.7196 (0.7196) acc 84.3750 (84.3750) lr 1.2487e+01 eta 0:02:47\n",
            "epoch [86/200] batch [2/2] time 0.309 (0.521) data 0.001 (0.214) loss 0.6196 (0.6696) acc 87.5000 (85.9375) lr 1.2334e+01 eta 0:01:58\n",
            "epoch [87/200] batch [1/2] time 0.752 (0.752) data 0.448 (0.448) loss 0.8575 (0.8575) acc 78.1250 (78.1250) lr 1.2334e+01 eta 0:02:50\n",
            "epoch [87/200] batch [2/2] time 0.311 (0.532) data 0.000 (0.224) loss 0.5226 (0.6901) acc 96.8750 (87.5000) lr 1.2181e+01 eta 0:02:00\n",
            "epoch [88/200] batch [1/2] time 0.724 (0.724) data 0.420 (0.420) loss 1.1345 (1.1345) acc 78.1250 (78.1250) lr 1.2181e+01 eta 0:02:43\n",
            "epoch [88/200] batch [2/2] time 0.308 (0.516) data 0.000 (0.210) loss 0.5623 (0.8484) acc 90.6250 (84.3750) lr 1.2028e+01 eta 0:01:55\n",
            "epoch [89/200] batch [1/2] time 0.758 (0.758) data 0.454 (0.454) loss 1.1796 (1.1796) acc 75.0000 (75.0000) lr 1.2028e+01 eta 0:02:49\n",
            "epoch [89/200] batch [2/2] time 0.311 (0.535) data 0.000 (0.227) loss 0.7325 (0.9561) acc 84.3750 (79.6875) lr 1.1874e+01 eta 0:01:58\n",
            "epoch [90/200] batch [1/2] time 0.729 (0.729) data 0.422 (0.422) loss 0.8508 (0.8508) acc 75.0000 (75.0000) lr 1.1874e+01 eta 0:02:41\n",
            "epoch [90/200] batch [2/2] time 0.311 (0.520) data 0.001 (0.212) loss 2.0287 (1.4397) acc 62.5000 (68.7500) lr 1.1719e+01 eta 0:01:54\n",
            "epoch [91/200] batch [1/2] time 0.730 (0.730) data 0.424 (0.424) loss 0.7114 (0.7114) acc 87.5000 (87.5000) lr 1.1719e+01 eta 0:02:39\n",
            "epoch [91/200] batch [2/2] time 0.311 (0.520) data 0.001 (0.213) loss 1.1846 (0.9480) acc 81.2500 (84.3750) lr 1.1564e+01 eta 0:01:53\n",
            "epoch [92/200] batch [1/2] time 0.726 (0.726) data 0.421 (0.421) loss 1.1563 (1.1563) acc 78.1250 (78.1250) lr 1.1564e+01 eta 0:02:37\n",
            "epoch [92/200] batch [2/2] time 0.310 (0.518) data 0.000 (0.210) loss 0.7006 (0.9284) acc 87.5000 (82.8125) lr 1.1409e+01 eta 0:01:51\n",
            "epoch [93/200] batch [1/2] time 0.730 (0.730) data 0.423 (0.423) loss 0.6329 (0.6329) acc 90.6250 (90.6250) lr 1.1409e+01 eta 0:02:37\n",
            "epoch [93/200] batch [2/2] time 0.311 (0.521) data 0.001 (0.212) loss 0.8816 (0.7572) acc 84.3750 (87.5000) lr 1.1253e+01 eta 0:01:51\n",
            "epoch [94/200] batch [1/2] time 0.769 (0.769) data 0.464 (0.464) loss 0.6688 (0.6688) acc 90.6250 (90.6250) lr 1.1253e+01 eta 0:02:43\n",
            "epoch [94/200] batch [2/2] time 0.313 (0.541) data 0.000 (0.232) loss 0.9012 (0.7850) acc 75.0000 (82.8125) lr 1.1097e+01 eta 0:01:54\n",
            "epoch [95/200] batch [1/2] time 0.774 (0.774) data 0.469 (0.469) loss 1.1142 (1.1142) acc 78.1250 (78.1250) lr 1.1097e+01 eta 0:02:43\n",
            "epoch [95/200] batch [2/2] time 0.310 (0.542) data 0.001 (0.235) loss 1.5762 (1.3452) acc 68.7500 (73.4375) lr 1.0941e+01 eta 0:01:53\n",
            "epoch [96/200] batch [1/2] time 0.735 (0.735) data 0.431 (0.431) loss 1.4270 (1.4270) acc 81.2500 (81.2500) lr 1.0941e+01 eta 0:02:33\n",
            "epoch [96/200] batch [2/2] time 0.314 (0.525) data 0.000 (0.216) loss 3.5829 (2.5049) acc 37.5000 (59.3750) lr 1.0785e+01 eta 0:01:49\n",
            "epoch [97/200] batch [1/2] time 0.742 (0.742) data 0.435 (0.435) loss 2.8713 (2.8713) acc 59.3750 (59.3750) lr 1.0785e+01 eta 0:02:33\n",
            "epoch [97/200] batch [2/2] time 0.314 (0.528) data 0.000 (0.218) loss 2.0019 (2.4366) acc 68.7500 (64.0625) lr 1.0628e+01 eta 0:01:48\n",
            "epoch [98/200] batch [1/2] time 0.729 (0.729) data 0.422 (0.422) loss 1.4422 (1.4422) acc 81.2500 (81.2500) lr 1.0628e+01 eta 0:02:29\n",
            "epoch [98/200] batch [2/2] time 0.313 (0.521) data 0.000 (0.211) loss 1.3431 (1.3926) acc 56.2500 (68.7500) lr 1.0471e+01 eta 0:01:46\n",
            "epoch [99/200] batch [1/2] time 0.727 (0.727) data 0.419 (0.419) loss 2.3047 (2.3047) acc 62.5000 (62.5000) lr 1.0471e+01 eta 0:02:27\n",
            "epoch [99/200] batch [2/2] time 0.312 (0.520) data 0.000 (0.210) loss 1.1784 (1.7416) acc 71.8750 (67.1875) lr 1.0314e+01 eta 0:01:44\n",
            "epoch [100/200] batch [1/2] time 0.740 (0.740) data 0.434 (0.434) loss 1.0678 (1.0678) acc 71.8750 (71.8750) lr 1.0314e+01 eta 0:02:28\n",
            "epoch [100/200] batch [2/2] time 0.311 (0.526) data 0.000 (0.217) loss 2.9078 (1.9878) acc 56.2500 (64.0625) lr 1.0157e+01 eta 0:01:45\n",
            "epoch [101/200] batch [1/2] time 0.750 (0.750) data 0.443 (0.443) loss 1.0286 (1.0286) acc 75.0000 (75.0000) lr 1.0157e+01 eta 0:02:29\n",
            "epoch [101/200] batch [2/2] time 0.316 (0.533) data 0.000 (0.222) loss 1.2329 (1.1308) acc 65.6250 (70.3125) lr 1.0000e+01 eta 0:01:45\n",
            "epoch [102/200] batch [1/2] time 0.753 (0.753) data 0.446 (0.446) loss 1.3734 (1.3734) acc 71.8750 (71.8750) lr 1.0000e+01 eta 0:02:28\n",
            "epoch [102/200] batch [2/2] time 0.313 (0.533) data 0.000 (0.223) loss 1.5403 (1.4568) acc 62.5000 (67.1875) lr 9.8429e+00 eta 0:01:44\n",
            "epoch [103/200] batch [1/2] time 0.740 (0.740) data 0.433 (0.433) loss 1.7432 (1.7432) acc 68.7500 (68.7500) lr 9.8429e+00 eta 0:02:24\n",
            "epoch [103/200] batch [2/2] time 0.312 (0.526) data 0.000 (0.217) loss 1.5229 (1.6331) acc 75.0000 (71.8750) lr 9.6859e+00 eta 0:01:42\n",
            "epoch [104/200] batch [1/2] time 0.741 (0.741) data 0.432 (0.432) loss 0.6619 (0.6619) acc 93.7500 (93.7500) lr 9.6859e+00 eta 0:02:22\n",
            "epoch [104/200] batch [2/2] time 0.311 (0.526) data 0.000 (0.216) loss 0.6744 (0.6681) acc 93.7500 (93.7500) lr 9.5289e+00 eta 0:01:40\n",
            "epoch [105/200] batch [1/2] time 0.765 (0.765) data 0.458 (0.458) loss 0.9613 (0.9613) acc 90.6250 (90.6250) lr 9.5289e+00 eta 0:02:26\n",
            "epoch [105/200] batch [2/2] time 0.315 (0.540) data 0.001 (0.229) loss 1.6300 (1.2956) acc 84.3750 (87.5000) lr 9.3721e+00 eta 0:01:42\n",
            "epoch [106/200] batch [1/2] time 0.775 (0.775) data 0.469 (0.469) loss 0.4042 (0.4042) acc 100.0000 (100.0000) lr 9.3721e+00 eta 0:02:26\n",
            "epoch [106/200] batch [2/2] time 0.312 (0.543) data 0.000 (0.235) loss 0.8639 (0.6341) acc 87.5000 (93.7500) lr 9.2154e+00 eta 0:01:42\n",
            "epoch [107/200] batch [1/2] time 0.749 (0.749) data 0.443 (0.443) loss 0.4422 (0.4422) acc 96.8750 (96.8750) lr 9.2154e+00 eta 0:02:20\n",
            "epoch [107/200] batch [2/2] time 0.313 (0.531) data 0.000 (0.222) loss 1.3085 (0.8754) acc 71.8750 (84.3750) lr 9.0589e+00 eta 0:01:38\n",
            "epoch [108/200] batch [1/2] time 0.739 (0.739) data 0.430 (0.430) loss 1.0035 (1.0035) acc 71.8750 (71.8750) lr 9.0589e+00 eta 0:02:16\n",
            "epoch [108/200] batch [2/2] time 0.313 (0.526) data 0.000 (0.215) loss 0.7008 (0.8521) acc 81.2500 (76.5625) lr 8.9027e+00 eta 0:01:36\n",
            "epoch [109/200] batch [1/2] time 0.760 (0.760) data 0.454 (0.454) loss 0.6761 (0.6761) acc 90.6250 (90.6250) lr 8.9027e+00 eta 0:02:19\n",
            "epoch [109/200] batch [2/2] time 0.312 (0.536) data 0.000 (0.227) loss 0.8025 (0.7393) acc 84.3750 (87.5000) lr 8.7467e+00 eta 0:01:37\n",
            "epoch [110/200] batch [1/2] time 0.758 (0.758) data 0.450 (0.450) loss 1.0597 (1.0597) acc 78.1250 (78.1250) lr 8.7467e+00 eta 0:02:17\n",
            "epoch [110/200] batch [2/2] time 0.312 (0.535) data 0.000 (0.225) loss 0.4721 (0.7659) acc 90.6250 (84.3750) lr 8.5910e+00 eta 0:01:36\n",
            "epoch [111/200] batch [1/2] time 0.754 (0.754) data 0.449 (0.449) loss 0.6224 (0.6224) acc 90.6250 (90.6250) lr 8.5910e+00 eta 0:02:15\n",
            "epoch [111/200] batch [2/2] time 0.311 (0.533) data 0.001 (0.225) loss 1.2181 (0.9203) acc 84.3750 (87.5000) lr 8.4357e+00 eta 0:01:34\n",
            "epoch [112/200] batch [1/2] time 0.738 (0.738) data 0.429 (0.429) loss 0.6622 (0.6622) acc 90.6250 (90.6250) lr 8.4357e+00 eta 0:02:10\n",
            "epoch [112/200] batch [2/2] time 0.312 (0.525) data 0.000 (0.215) loss 0.7260 (0.6941) acc 90.6250 (90.6250) lr 8.2807e+00 eta 0:01:32\n",
            "epoch [113/200] batch [1/2] time 0.762 (0.762) data 0.454 (0.454) loss 0.6819 (0.6819) acc 90.6250 (90.6250) lr 8.2807e+00 eta 0:02:13\n",
            "epoch [113/200] batch [2/2] time 0.313 (0.537) data 0.001 (0.227) loss 1.2896 (0.9857) acc 71.8750 (81.2500) lr 8.1262e+00 eta 0:01:33\n",
            "epoch [114/200] batch [1/2] time 0.748 (0.748) data 0.441 (0.441) loss 0.7748 (0.7748) acc 81.2500 (81.2500) lr 8.1262e+00 eta 0:02:09\n",
            "epoch [114/200] batch [2/2] time 0.311 (0.530) data 0.000 (0.221) loss 0.7223 (0.7485) acc 87.5000 (84.3750) lr 7.9721e+00 eta 0:01:31\n",
            "epoch [115/200] batch [1/2] time 0.744 (0.744) data 0.437 (0.437) loss 0.5871 (0.5871) acc 90.6250 (90.6250) lr 7.9721e+00 eta 0:02:07\n",
            "epoch [115/200] batch [2/2] time 0.312 (0.528) data 0.001 (0.219) loss 0.4755 (0.5313) acc 93.7500 (92.1875) lr 7.8186e+00 eta 0:01:29\n",
            "epoch [116/200] batch [1/2] time 0.759 (0.759) data 0.452 (0.452) loss 0.7752 (0.7752) acc 84.3750 (84.3750) lr 7.8186e+00 eta 0:02:08\n",
            "epoch [116/200] batch [2/2] time 0.314 (0.537) data 0.000 (0.226) loss 0.6983 (0.7368) acc 84.3750 (84.3750) lr 7.6655e+00 eta 0:01:30\n",
            "epoch [117/200] batch [1/2] time 0.785 (0.785) data 0.478 (0.478) loss 0.6486 (0.6486) acc 90.6250 (90.6250) lr 7.6655e+00 eta 0:02:11\n",
            "epoch [117/200] batch [2/2] time 0.313 (0.549) data 0.000 (0.239) loss 0.5169 (0.5828) acc 90.6250 (90.6250) lr 7.5131e+00 eta 0:01:31\n",
            "epoch [118/200] batch [1/2] time 0.762 (0.762) data 0.455 (0.455) loss 1.0712 (1.0712) acc 84.3750 (84.3750) lr 7.5131e+00 eta 0:02:05\n",
            "epoch [118/200] batch [2/2] time 0.315 (0.538) data 0.000 (0.227) loss 0.5209 (0.7960) acc 96.8750 (90.6250) lr 7.3613e+00 eta 0:01:28\n",
            "epoch [119/200] batch [1/2] time 0.757 (0.757) data 0.449 (0.449) loss 0.7582 (0.7582) acc 90.6250 (90.6250) lr 7.3613e+00 eta 0:02:03\n",
            "epoch [119/200] batch [2/2] time 0.311 (0.534) data 0.000 (0.225) loss 0.7022 (0.7302) acc 87.5000 (89.0625) lr 7.2101e+00 eta 0:01:26\n",
            "epoch [120/200] batch [1/2] time 0.718 (0.718) data 0.409 (0.409) loss 0.7048 (0.7048) acc 90.6250 (90.6250) lr 7.2101e+00 eta 0:01:55\n",
            "epoch [120/200] batch [2/2] time 0.314 (0.516) data 0.001 (0.205) loss 0.5175 (0.6112) acc 90.6250 (90.6250) lr 7.0596e+00 eta 0:01:22\n",
            "epoch [121/200] batch [1/2] time 0.741 (0.741) data 0.435 (0.435) loss 0.6733 (0.6733) acc 90.6250 (90.6250) lr 7.0596e+00 eta 0:01:57\n",
            "epoch [121/200] batch [2/2] time 0.315 (0.528) data 0.001 (0.218) loss 0.5572 (0.6152) acc 90.6250 (90.6250) lr 6.9098e+00 eta 0:01:23\n",
            "epoch [122/200] batch [1/2] time 0.714 (0.714) data 0.406 (0.406) loss 0.4811 (0.4811) acc 90.6250 (90.6250) lr 6.9098e+00 eta 0:01:52\n",
            "epoch [122/200] batch [2/2] time 0.312 (0.513) data 0.000 (0.203) loss 1.0555 (0.7683) acc 81.2500 (85.9375) lr 6.7608e+00 eta 0:01:20\n",
            "epoch [123/200] batch [1/2] time 0.720 (0.720) data 0.412 (0.412) loss 0.5713 (0.5713) acc 93.7500 (93.7500) lr 6.7608e+00 eta 0:01:51\n",
            "epoch [123/200] batch [2/2] time 0.311 (0.516) data 0.001 (0.206) loss 0.3944 (0.4829) acc 100.0000 (96.8750) lr 6.6126e+00 eta 0:01:19\n",
            "epoch [124/200] batch [1/2] time 0.739 (0.739) data 0.432 (0.432) loss 0.6412 (0.6412) acc 93.7500 (93.7500) lr 6.6126e+00 eta 0:01:53\n",
            "epoch [124/200] batch [2/2] time 0.312 (0.526) data 0.000 (0.216) loss 0.5441 (0.5927) acc 90.6250 (92.1875) lr 6.4653e+00 eta 0:01:19\n",
            "epoch [125/200] batch [1/2] time 0.731 (0.731) data 0.424 (0.424) loss 0.4061 (0.4061) acc 96.8750 (96.8750) lr 6.4653e+00 eta 0:01:50\n",
            "epoch [125/200] batch [2/2] time 0.314 (0.522) data 0.000 (0.212) loss 1.1684 (0.7872) acc 75.0000 (85.9375) lr 6.3188e+00 eta 0:01:18\n",
            "epoch [126/200] batch [1/2] time 0.740 (0.740) data 0.432 (0.432) loss 0.5073 (0.5073) acc 90.6250 (90.6250) lr 6.3188e+00 eta 0:01:50\n",
            "epoch [126/200] batch [2/2] time 0.311 (0.526) data 0.001 (0.216) loss 0.7500 (0.6286) acc 75.0000 (82.8125) lr 6.1732e+00 eta 0:01:17\n",
            "epoch [127/200] batch [1/2] time 0.794 (0.794) data 0.487 (0.487) loss 0.6671 (0.6671) acc 84.3750 (84.3750) lr 6.1732e+00 eta 0:01:56\n",
            "epoch [127/200] batch [2/2] time 0.311 (0.552) data 0.001 (0.244) loss 0.5103 (0.5887) acc 93.7500 (89.0625) lr 6.0285e+00 eta 0:01:20\n",
            "epoch [128/200] batch [1/2] time 0.757 (0.757) data 0.453 (0.453) loss 0.5006 (0.5006) acc 90.6250 (90.6250) lr 6.0285e+00 eta 0:01:49\n",
            "epoch [128/200] batch [2/2] time 0.313 (0.535) data 0.000 (0.227) loss 1.0361 (0.7684) acc 81.2500 (85.9375) lr 5.8849e+00 eta 0:01:16\n",
            "epoch [129/200] batch [1/2] time 0.736 (0.736) data 0.430 (0.430) loss 0.4851 (0.4851) acc 90.6250 (90.6250) lr 5.8849e+00 eta 0:01:45\n",
            "epoch [129/200] batch [2/2] time 0.312 (0.524) data 0.000 (0.215) loss 0.9523 (0.7187) acc 87.5000 (89.0625) lr 5.7422e+00 eta 0:01:14\n",
            "epoch [130/200] batch [1/2] time 0.724 (0.724) data 0.417 (0.417) loss 0.4200 (0.4200) acc 93.7500 (93.7500) lr 5.7422e+00 eta 0:01:42\n",
            "epoch [130/200] batch [2/2] time 0.313 (0.519) data 0.000 (0.209) loss 0.6059 (0.5129) acc 84.3750 (89.0625) lr 5.6006e+00 eta 0:01:12\n",
            "epoch [131/200] batch [1/2] time 0.722 (0.722) data 0.418 (0.418) loss 0.5683 (0.5683) acc 93.7500 (93.7500) lr 5.6006e+00 eta 0:01:40\n",
            "epoch [131/200] batch [2/2] time 0.311 (0.516) data 0.000 (0.209) loss 0.3957 (0.4820) acc 93.7500 (93.7500) lr 5.4601e+00 eta 0:01:11\n",
            "epoch [132/200] batch [1/2] time 0.709 (0.709) data 0.398 (0.398) loss 0.8448 (0.8448) acc 81.2500 (81.2500) lr 5.4601e+00 eta 0:01:37\n",
            "epoch [132/200] batch [2/2] time 0.315 (0.512) data 0.000 (0.199) loss 0.7094 (0.7771) acc 90.6250 (85.9375) lr 5.3207e+00 eta 0:01:09\n",
            "epoch [133/200] batch [1/2] time 0.756 (0.756) data 0.449 (0.449) loss 0.4336 (0.4336) acc 93.7500 (93.7500) lr 5.3207e+00 eta 0:01:42\n",
            "epoch [133/200] batch [2/2] time 0.314 (0.535) data 0.000 (0.225) loss 0.5701 (0.5018) acc 93.7500 (93.7500) lr 5.1825e+00 eta 0:01:11\n",
            "epoch [134/200] batch [1/2] time 0.745 (0.745) data 0.436 (0.436) loss 1.2993 (1.2993) acc 59.3750 (59.3750) lr 5.1825e+00 eta 0:01:39\n",
            "epoch [134/200] batch [2/2] time 0.311 (0.528) data 0.001 (0.218) loss 0.4638 (0.8816) acc 93.7500 (76.5625) lr 5.0454e+00 eta 0:01:09\n",
            "epoch [135/200] batch [1/2] time 0.741 (0.741) data 0.438 (0.438) loss 0.5390 (0.5390) acc 90.6250 (90.6250) lr 5.0454e+00 eta 0:01:37\n",
            "epoch [135/200] batch [2/2] time 0.312 (0.526) data 0.000 (0.219) loss 0.4767 (0.5079) acc 96.8750 (93.7500) lr 4.9096e+00 eta 0:01:08\n",
            "epoch [136/200] batch [1/2] time 0.753 (0.753) data 0.445 (0.445) loss 0.8145 (0.8145) acc 81.2500 (81.2500) lr 4.9096e+00 eta 0:01:37\n",
            "epoch [136/200] batch [2/2] time 0.311 (0.532) data 0.000 (0.223) loss 0.6938 (0.7542) acc 90.6250 (85.9375) lr 4.7750e+00 eta 0:01:08\n",
            "epoch [137/200] batch [1/2] time 0.768 (0.768) data 0.462 (0.462) loss 0.5263 (0.5263) acc 87.5000 (87.5000) lr 4.7750e+00 eta 0:01:37\n",
            "epoch [137/200] batch [2/2] time 0.312 (0.540) data 0.000 (0.231) loss 0.7383 (0.6323) acc 84.3750 (85.9375) lr 4.6417e+00 eta 0:01:08\n",
            "epoch [138/200] batch [1/2] time 0.755 (0.755) data 0.449 (0.449) loss 1.1652 (1.1652) acc 87.5000 (87.5000) lr 4.6417e+00 eta 0:01:34\n",
            "epoch [138/200] batch [2/2] time 0.314 (0.535) data 0.000 (0.225) loss 0.5977 (0.8814) acc 87.5000 (87.5000) lr 4.5098e+00 eta 0:01:06\n",
            "epoch [139/200] batch [1/2] time 0.758 (0.758) data 0.452 (0.452) loss 0.6005 (0.6005) acc 87.5000 (87.5000) lr 4.5098e+00 eta 0:01:33\n",
            "epoch [139/200] batch [2/2] time 0.313 (0.535) data 0.001 (0.226) loss 0.8368 (0.7186) acc 87.5000 (87.5000) lr 4.3792e+00 eta 0:01:05\n",
            "epoch [140/200] batch [1/2] time 0.734 (0.734) data 0.427 (0.427) loss 0.5991 (0.5991) acc 90.6250 (90.6250) lr 4.3792e+00 eta 0:01:28\n",
            "epoch [140/200] batch [2/2] time 0.310 (0.522) data 0.000 (0.214) loss 0.3610 (0.4801) acc 96.8750 (93.7500) lr 4.2499e+00 eta 0:01:02\n",
            "epoch [141/200] batch [1/2] time 0.733 (0.733) data 0.428 (0.428) loss 0.9432 (0.9432) acc 87.5000 (87.5000) lr 4.2499e+00 eta 0:01:27\n",
            "epoch [141/200] batch [2/2] time 0.312 (0.523) data 0.000 (0.214) loss 0.4529 (0.6980) acc 93.7500 (90.6250) lr 4.1221e+00 eta 0:01:01\n",
            "epoch [142/200] batch [1/2] time 0.745 (0.745) data 0.440 (0.440) loss 0.4538 (0.4538) acc 90.6250 (90.6250) lr 4.1221e+00 eta 0:01:27\n",
            "epoch [142/200] batch [2/2] time 0.310 (0.528) data 0.000 (0.220) loss 0.7406 (0.5972) acc 93.7500 (92.1875) lr 3.9958e+00 eta 0:01:01\n",
            "epoch [143/200] batch [1/2] time 0.738 (0.738) data 0.434 (0.434) loss 0.3490 (0.3490) acc 96.8750 (96.8750) lr 3.9958e+00 eta 0:01:24\n",
            "epoch [143/200] batch [2/2] time 0.311 (0.524) data 0.000 (0.217) loss 0.6118 (0.4804) acc 90.6250 (93.7500) lr 3.8709e+00 eta 0:00:59\n",
            "epoch [144/200] batch [1/2] time 0.764 (0.764) data 0.460 (0.460) loss 0.3049 (0.3049) acc 100.0000 (100.0000) lr 3.8709e+00 eta 0:01:26\n",
            "epoch [144/200] batch [2/2] time 0.313 (0.538) data 0.001 (0.230) loss 0.4532 (0.3790) acc 93.7500 (96.8750) lr 3.7476e+00 eta 0:01:00\n",
            "epoch [145/200] batch [1/2] time 0.788 (0.788) data 0.482 (0.482) loss 0.3960 (0.3960) acc 93.7500 (93.7500) lr 3.7476e+00 eta 0:01:27\n",
            "epoch [145/200] batch [2/2] time 0.311 (0.549) data 0.001 (0.241) loss 0.8040 (0.6000) acc 87.5000 (90.6250) lr 3.6258e+00 eta 0:01:00\n",
            "epoch [146/200] batch [1/2] time 0.789 (0.789) data 0.485 (0.485) loss 0.4900 (0.4900) acc 93.7500 (93.7500) lr 3.6258e+00 eta 0:01:26\n",
            "epoch [146/200] batch [2/2] time 0.312 (0.551) data 0.000 (0.243) loss 0.2942 (0.3921) acc 100.0000 (96.8750) lr 3.5055e+00 eta 0:00:59\n",
            "epoch [147/200] batch [1/2] time 0.722 (0.722) data 0.420 (0.420) loss 0.4517 (0.4517) acc 93.7500 (93.7500) lr 3.5055e+00 eta 0:01:17\n",
            "epoch [147/200] batch [2/2] time 0.311 (0.517) data 0.001 (0.210) loss 0.2926 (0.3721) acc 100.0000 (96.8750) lr 3.3869e+00 eta 0:00:54\n",
            "epoch [148/200] batch [1/2] time 0.778 (0.778) data 0.473 (0.473) loss 0.2991 (0.2991) acc 100.0000 (100.0000) lr 3.3869e+00 eta 0:01:21\n",
            "epoch [148/200] batch [2/2] time 0.308 (0.543) data 0.001 (0.237) loss 0.5742 (0.4367) acc 90.6250 (95.3125) lr 3.2699e+00 eta 0:00:56\n",
            "epoch [149/200] batch [1/2] time 0.779 (0.779) data 0.469 (0.469) loss 0.4387 (0.4387) acc 90.6250 (90.6250) lr 3.2699e+00 eta 0:01:20\n",
            "epoch [149/200] batch [2/2] time 0.311 (0.545) data 0.001 (0.235) loss 0.3317 (0.3852) acc 93.7500 (92.1875) lr 3.1545e+00 eta 0:00:55\n",
            "epoch [150/200] batch [1/2] time 0.815 (0.815) data 0.509 (0.509) loss 0.7089 (0.7089) acc 87.5000 (87.5000) lr 3.1545e+00 eta 0:01:22\n",
            "epoch [150/200] batch [2/2] time 0.312 (0.563) data 0.001 (0.255) loss 0.3781 (0.5435) acc 96.8750 (92.1875) lr 3.0409e+00 eta 0:00:56\n",
            "epoch [151/200] batch [1/2] time 0.779 (0.779) data 0.476 (0.476) loss 0.2839 (0.2839) acc 100.0000 (100.0000) lr 3.0409e+00 eta 0:01:17\n",
            "epoch [151/200] batch [2/2] time 0.311 (0.545) data 0.001 (0.238) loss 0.3073 (0.2956) acc 100.0000 (100.0000) lr 2.9289e+00 eta 0:00:53\n",
            "epoch [152/200] batch [1/2] time 0.763 (0.763) data 0.456 (0.456) loss 0.3105 (0.3105) acc 96.8750 (96.8750) lr 2.9289e+00 eta 0:01:13\n",
            "epoch [152/200] batch [2/2] time 0.308 (0.536) data 0.001 (0.229) loss 0.4838 (0.3971) acc 96.8750 (96.8750) lr 2.8187e+00 eta 0:00:51\n",
            "epoch [153/200] batch [1/2] time 0.781 (0.781) data 0.475 (0.475) loss 0.4144 (0.4144) acc 96.8750 (96.8750) lr 2.8187e+00 eta 0:01:14\n",
            "epoch [153/200] batch [2/2] time 0.311 (0.546) data 0.001 (0.238) loss 0.4898 (0.4521) acc 87.5000 (92.1875) lr 2.7103e+00 eta 0:00:51\n",
            "epoch [154/200] batch [1/2] time 0.804 (0.804) data 0.498 (0.498) loss 0.4262 (0.4262) acc 93.7500 (93.7500) lr 2.7103e+00 eta 0:01:14\n",
            "epoch [154/200] batch [2/2] time 0.308 (0.556) data 0.001 (0.249) loss 0.4518 (0.4390) acc 96.8750 (95.3125) lr 2.6037e+00 eta 0:00:51\n",
            "epoch [155/200] batch [1/2] time 0.814 (0.814) data 0.509 (0.509) loss 0.5079 (0.5079) acc 87.5000 (87.5000) lr 2.6037e+00 eta 0:01:14\n",
            "epoch [155/200] batch [2/2] time 0.311 (0.563) data 0.001 (0.255) loss 0.3412 (0.4245) acc 100.0000 (93.7500) lr 2.4989e+00 eta 0:00:50\n",
            "epoch [156/200] batch [1/2] time 0.807 (0.807) data 0.502 (0.502) loss 0.3871 (0.3871) acc 93.7500 (93.7500) lr 2.4989e+00 eta 0:01:11\n",
            "epoch [156/200] batch [2/2] time 0.308 (0.558) data 0.001 (0.251) loss 0.2868 (0.3370) acc 100.0000 (96.8750) lr 2.3959e+00 eta 0:00:49\n",
            "epoch [157/200] batch [1/2] time 0.804 (0.804) data 0.497 (0.497) loss 0.4197 (0.4197) acc 93.7500 (93.7500) lr 2.3959e+00 eta 0:01:09\n",
            "epoch [157/200] batch [2/2] time 0.309 (0.557) data 0.000 (0.249) loss 0.2598 (0.3397) acc 100.0000 (96.8750) lr 2.2949e+00 eta 0:00:47\n",
            "epoch [158/200] batch [1/2] time 0.826 (0.826) data 0.521 (0.521) loss 0.3339 (0.3339) acc 96.8750 (96.8750) lr 2.2949e+00 eta 0:01:10\n",
            "epoch [158/200] batch [2/2] time 0.309 (0.567) data 0.001 (0.261) loss 0.2647 (0.2993) acc 100.0000 (98.4375) lr 2.1957e+00 eta 0:00:47\n",
            "epoch [159/200] batch [1/2] time 0.796 (0.796) data 0.491 (0.491) loss 0.3971 (0.3971) acc 93.7500 (93.7500) lr 2.1957e+00 eta 0:01:06\n",
            "epoch [159/200] batch [2/2] time 0.310 (0.553) data 0.001 (0.246) loss 0.6184 (0.5077) acc 90.6250 (92.1875) lr 2.0984e+00 eta 0:00:45\n",
            "epoch [160/200] batch [1/2] time 0.823 (0.823) data 0.517 (0.517) loss 0.4584 (0.4584) acc 96.8750 (96.8750) lr 2.0984e+00 eta 0:01:06\n",
            "epoch [160/200] batch [2/2] time 0.310 (0.566) data 0.001 (0.259) loss 0.3195 (0.3889) acc 96.8750 (96.8750) lr 2.0032e+00 eta 0:00:45\n",
            "epoch [161/200] batch [1/2] time 0.797 (0.797) data 0.491 (0.491) loss 0.5731 (0.5731) acc 93.7500 (93.7500) lr 2.0032e+00 eta 0:01:02\n",
            "epoch [161/200] batch [2/2] time 0.310 (0.553) data 0.001 (0.246) loss 0.4776 (0.5254) acc 93.7500 (93.7500) lr 1.9098e+00 eta 0:00:43\n",
            "epoch [162/200] batch [1/2] time 0.790 (0.790) data 0.485 (0.485) loss 0.3788 (0.3788) acc 96.8750 (96.8750) lr 1.9098e+00 eta 0:01:00\n",
            "epoch [162/200] batch [2/2] time 0.310 (0.550) data 0.001 (0.243) loss 0.6561 (0.5175) acc 93.7500 (95.3125) lr 1.8185e+00 eta 0:00:41\n",
            "epoch [163/200] batch [1/2] time 0.801 (0.801) data 0.497 (0.497) loss 0.5012 (0.5012) acc 93.7500 (93.7500) lr 1.8185e+00 eta 0:01:00\n",
            "epoch [163/200] batch [2/2] time 0.310 (0.556) data 0.001 (0.249) loss 0.2943 (0.3977) acc 96.8750 (95.3125) lr 1.7292e+00 eta 0:00:41\n",
            "epoch [164/200] batch [1/2] time 0.755 (0.755) data 0.448 (0.448) loss 0.5031 (0.5031) acc 93.7500 (93.7500) lr 1.7292e+00 eta 0:00:55\n",
            "epoch [164/200] batch [2/2] time 0.312 (0.534) data 0.001 (0.224) loss 0.2862 (0.3946) acc 100.0000 (96.8750) lr 1.6419e+00 eta 0:00:38\n",
            "epoch [165/200] batch [1/2] time 0.763 (0.763) data 0.457 (0.457) loss 0.5122 (0.5122) acc 90.6250 (90.6250) lr 1.6419e+00 eta 0:00:54\n",
            "epoch [165/200] batch [2/2] time 0.312 (0.537) data 0.001 (0.229) loss 0.4199 (0.4661) acc 90.6250 (90.6250) lr 1.5567e+00 eta 0:00:37\n",
            "epoch [166/200] batch [1/2] time 0.770 (0.770) data 0.466 (0.466) loss 0.2469 (0.2469) acc 100.0000 (100.0000) lr 1.5567e+00 eta 0:00:53\n",
            "epoch [166/200] batch [2/2] time 0.310 (0.540) data 0.000 (0.233) loss 0.5533 (0.4001) acc 90.6250 (95.3125) lr 1.4736e+00 eta 0:00:36\n",
            "epoch [167/200] batch [1/2] time 0.763 (0.763) data 0.457 (0.457) loss 0.3917 (0.3917) acc 96.8750 (96.8750) lr 1.4736e+00 eta 0:00:51\n",
            "epoch [167/200] batch [2/2] time 0.310 (0.536) data 0.001 (0.229) loss 0.3430 (0.3673) acc 96.8750 (96.8750) lr 1.3926e+00 eta 0:00:35\n",
            "epoch [168/200] batch [1/2] time 0.757 (0.757) data 0.448 (0.448) loss 0.5490 (0.5490) acc 93.7500 (93.7500) lr 1.3926e+00 eta 0:00:49\n",
            "epoch [168/200] batch [2/2] time 0.310 (0.533) data 0.000 (0.224) loss 0.2917 (0.4204) acc 100.0000 (96.8750) lr 1.3137e+00 eta 0:00:34\n",
            "epoch [169/200] batch [1/2] time 0.759 (0.759) data 0.453 (0.453) loss 0.5591 (0.5591) acc 87.5000 (87.5000) lr 1.3137e+00 eta 0:00:47\n",
            "epoch [169/200] batch [2/2] time 0.311 (0.535) data 0.001 (0.227) loss 0.4196 (0.4894) acc 93.7500 (90.6250) lr 1.2369e+00 eta 0:00:33\n",
            "epoch [170/200] batch [1/2] time 0.773 (0.773) data 0.466 (0.466) loss 0.3572 (0.3572) acc 93.7500 (93.7500) lr 1.2369e+00 eta 0:00:47\n",
            "epoch [170/200] batch [2/2] time 0.313 (0.543) data 0.001 (0.233) loss 0.3353 (0.3463) acc 93.7500 (93.7500) lr 1.1623e+00 eta 0:00:32\n",
            "epoch [171/200] batch [1/2] time 0.779 (0.779) data 0.475 (0.475) loss 0.3049 (0.3049) acc 96.8750 (96.8750) lr 1.1623e+00 eta 0:00:45\n",
            "epoch [171/200] batch [2/2] time 0.309 (0.544) data 0.001 (0.238) loss 0.2975 (0.3012) acc 96.8750 (96.8750) lr 1.0899e+00 eta 0:00:31\n",
            "epoch [172/200] batch [1/2] time 0.770 (0.770) data 0.464 (0.464) loss 0.3265 (0.3265) acc 96.8750 (96.8750) lr 1.0899e+00 eta 0:00:43\n",
            "epoch [172/200] batch [2/2] time 0.311 (0.541) data 0.001 (0.232) loss 0.4872 (0.4069) acc 93.7500 (95.3125) lr 1.0197e+00 eta 0:00:30\n",
            "epoch [173/200] batch [1/2] time 0.772 (0.772) data 0.467 (0.467) loss 0.6171 (0.6171) acc 90.6250 (90.6250) lr 1.0197e+00 eta 0:00:42\n",
            "epoch [173/200] batch [2/2] time 0.311 (0.542) data 0.000 (0.234) loss 0.2763 (0.4467) acc 96.8750 (93.7500) lr 9.5173e-01 eta 0:00:29\n",
            "epoch [174/200] batch [1/2] time 0.721 (0.721) data 0.418 (0.418) loss 0.2646 (0.2646) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:38\n",
            "epoch [174/200] batch [2/2] time 0.311 (0.516) data 0.000 (0.209) loss 0.2885 (0.2766) acc 100.0000 (100.0000) lr 8.8597e-01 eta 0:00:26\n",
            "epoch [175/200] batch [1/2] time 0.754 (0.754) data 0.446 (0.446) loss 0.2760 (0.2760) acc 96.8750 (96.8750) lr 8.8597e-01 eta 0:00:38\n",
            "epoch [175/200] batch [2/2] time 0.310 (0.532) data 0.001 (0.223) loss 0.3379 (0.3070) acc 96.8750 (96.8750) lr 8.2245e-01 eta 0:00:26\n",
            "epoch [176/200] batch [1/2] time 0.733 (0.733) data 0.428 (0.428) loss 0.4578 (0.4578) acc 93.7500 (93.7500) lr 8.2245e-01 eta 0:00:35\n",
            "epoch [176/200] batch [2/2] time 0.313 (0.523) data 0.000 (0.214) loss 0.4419 (0.4499) acc 93.7500 (93.7500) lr 7.6120e-01 eta 0:00:25\n",
            "epoch [177/200] batch [1/2] time 0.726 (0.726) data 0.422 (0.422) loss 0.4507 (0.4507) acc 90.6250 (90.6250) lr 7.6120e-01 eta 0:00:34\n",
            "epoch [177/200] batch [2/2] time 0.312 (0.519) data 0.000 (0.211) loss 0.4355 (0.4431) acc 90.6250 (90.6250) lr 7.0224e-01 eta 0:00:23\n",
            "epoch [178/200] batch [1/2] time 0.730 (0.730) data 0.424 (0.424) loss 0.3801 (0.3801) acc 93.7500 (93.7500) lr 7.0224e-01 eta 0:00:32\n",
            "epoch [178/200] batch [2/2] time 0.311 (0.521) data 0.000 (0.212) loss 0.3329 (0.3565) acc 96.8750 (95.3125) lr 6.4556e-01 eta 0:00:22\n",
            "epoch [179/200] batch [1/2] time 0.737 (0.737) data 0.429 (0.429) loss 0.2384 (0.2384) acc 100.0000 (100.0000) lr 6.4556e-01 eta 0:00:31\n",
            "epoch [179/200] batch [2/2] time 0.313 (0.525) data 0.001 (0.215) loss 0.2805 (0.2595) acc 96.8750 (98.4375) lr 5.9119e-01 eta 0:00:22\n",
            "epoch [180/200] batch [1/2] time 0.738 (0.738) data 0.433 (0.433) loss 0.3502 (0.3502) acc 93.7500 (93.7500) lr 5.9119e-01 eta 0:00:30\n",
            "epoch [180/200] batch [2/2] time 0.312 (0.525) data 0.000 (0.217) loss 0.3106 (0.3304) acc 93.7500 (93.7500) lr 5.3915e-01 eta 0:00:21\n",
            "epoch [181/200] batch [1/2] time 0.776 (0.776) data 0.470 (0.470) loss 0.4711 (0.4711) acc 93.7500 (93.7500) lr 5.3915e-01 eta 0:00:30\n",
            "epoch [181/200] batch [2/2] time 0.313 (0.545) data 0.000 (0.235) loss 0.3455 (0.4083) acc 96.8750 (95.3125) lr 4.8943e-01 eta 0:00:20\n",
            "epoch [182/200] batch [1/2] time 0.746 (0.746) data 0.442 (0.442) loss 0.2788 (0.2788) acc 96.8750 (96.8750) lr 4.8943e-01 eta 0:00:27\n",
            "epoch [182/200] batch [2/2] time 0.310 (0.528) data 0.001 (0.221) loss 0.2400 (0.2594) acc 100.0000 (98.4375) lr 4.4207e-01 eta 0:00:19\n",
            "epoch [183/200] batch [1/2] time 0.760 (0.760) data 0.456 (0.456) loss 0.2625 (0.2625) acc 96.8750 (96.8750) lr 4.4207e-01 eta 0:00:26\n",
            "epoch [183/200] batch [2/2] time 0.310 (0.535) data 0.000 (0.228) loss 0.5271 (0.3948) acc 90.6250 (93.7500) lr 3.9706e-01 eta 0:00:18\n",
            "epoch [184/200] batch [1/2] time 0.750 (0.750) data 0.444 (0.444) loss 0.3509 (0.3509) acc 96.8750 (96.8750) lr 3.9706e-01 eta 0:00:24\n",
            "epoch [184/200] batch [2/2] time 0.311 (0.530) data 0.000 (0.222) loss 0.5154 (0.4331) acc 93.7500 (95.3125) lr 3.5443e-01 eta 0:00:16\n",
            "epoch [185/200] batch [1/2] time 0.739 (0.739) data 0.435 (0.435) loss 0.2828 (0.2828) acc 100.0000 (100.0000) lr 3.5443e-01 eta 0:00:22\n",
            "epoch [185/200] batch [2/2] time 0.311 (0.525) data 0.000 (0.218) loss 0.2612 (0.2720) acc 100.0000 (100.0000) lr 3.1417e-01 eta 0:00:15\n",
            "epoch [186/200] batch [1/2] time 0.751 (0.751) data 0.448 (0.448) loss 0.4324 (0.4324) acc 90.6250 (90.6250) lr 3.1417e-01 eta 0:00:21\n",
            "epoch [186/200] batch [2/2] time 0.311 (0.531) data 0.000 (0.224) loss 0.3919 (0.4121) acc 96.8750 (93.7500) lr 2.7630e-01 eta 0:00:14\n",
            "epoch [187/200] batch [1/2] time 0.742 (0.742) data 0.436 (0.436) loss 0.2725 (0.2725) acc 96.8750 (96.8750) lr 2.7630e-01 eta 0:00:20\n",
            "epoch [187/200] batch [2/2] time 0.313 (0.527) data 0.000 (0.218) loss 0.3038 (0.2881) acc 96.8750 (96.8750) lr 2.4083e-01 eta 0:00:13\n",
            "epoch [188/200] batch [1/2] time 0.757 (0.757) data 0.450 (0.450) loss 0.2458 (0.2458) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:18\n",
            "epoch [188/200] batch [2/2] time 0.312 (0.535) data 0.001 (0.225) loss 0.4202 (0.3330) acc 96.8750 (98.4375) lr 2.0777e-01 eta 0:00:12\n",
            "epoch [189/200] batch [1/2] time 0.759 (0.759) data 0.454 (0.454) loss 0.2522 (0.2522) acc 100.0000 (100.0000) lr 2.0777e-01 eta 0:00:17\n",
            "epoch [189/200] batch [2/2] time 0.310 (0.535) data 0.000 (0.227) loss 0.2378 (0.2450) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:11\n",
            "epoch [190/200] batch [1/2] time 0.750 (0.750) data 0.444 (0.444) loss 0.2558 (0.2558) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:15\n",
            "epoch [190/200] batch [2/2] time 0.310 (0.530) data 0.000 (0.222) loss 0.3102 (0.2830) acc 96.8750 (98.4375) lr 1.4891e-01 eta 0:00:10\n",
            "epoch [191/200] batch [1/2] time 0.779 (0.779) data 0.475 (0.475) loss 0.2554 (0.2554) acc 100.0000 (100.0000) lr 1.4891e-01 eta 0:00:14\n",
            "epoch [191/200] batch [2/2] time 0.313 (0.546) data 0.001 (0.238) loss 0.3259 (0.2907) acc 93.7500 (96.8750) lr 1.2312e-01 eta 0:00:09\n",
            "epoch [192/200] batch [1/2] time 0.785 (0.785) data 0.481 (0.481) loss 0.2444 (0.2444) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:13\n",
            "epoch [192/200] batch [2/2] time 0.312 (0.549) data 0.001 (0.241) loss 0.2849 (0.2646) acc 100.0000 (100.0000) lr 9.9763e-02 eta 0:00:08\n",
            "epoch [193/200] batch [1/2] time 0.758 (0.758) data 0.455 (0.455) loss 0.4547 (0.4547) acc 93.7500 (93.7500) lr 9.9763e-02 eta 0:00:11\n",
            "epoch [193/200] batch [2/2] time 0.312 (0.535) data 0.001 (0.228) loss 0.2580 (0.3564) acc 100.0000 (96.8750) lr 7.8853e-02 eta 0:00:07\n",
            "epoch [194/200] batch [1/2] time 0.747 (0.747) data 0.441 (0.441) loss 0.2582 (0.2582) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:09\n",
            "epoch [194/200] batch [2/2] time 0.311 (0.529) data 0.000 (0.221) loss 0.2794 (0.2688) acc 96.8750 (98.4375) lr 6.0390e-02 eta 0:00:06\n",
            "epoch [195/200] batch [1/2] time 0.740 (0.740) data 0.433 (0.433) loss 0.2586 (0.2586) acc 96.8750 (96.8750) lr 6.0390e-02 eta 0:00:08\n",
            "epoch [195/200] batch [2/2] time 0.312 (0.526) data 0.000 (0.217) loss 0.4402 (0.3494) acc 96.8750 (96.8750) lr 4.4380e-02 eta 0:00:05\n",
            "epoch [196/200] batch [1/2] time 0.719 (0.719) data 0.412 (0.412) loss 0.2340 (0.2340) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:06\n",
            "epoch [196/200] batch [2/2] time 0.309 (0.514) data 0.000 (0.206) loss 0.2672 (0.2506) acc 100.0000 (100.0000) lr 3.0827e-02 eta 0:00:04\n",
            "epoch [197/200] batch [1/2] time 0.727 (0.727) data 0.423 (0.423) loss 0.2753 (0.2753) acc 100.0000 (100.0000) lr 3.0827e-02 eta 0:00:05\n",
            "epoch [197/200] batch [2/2] time 0.310 (0.519) data 0.000 (0.212) loss 0.2527 (0.2640) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [1/2] time 0.740 (0.740) data 0.439 (0.439) loss 0.2376 (0.2376) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [2/2] time 0.310 (0.525) data 0.000 (0.220) loss 0.2379 (0.2377) acc 100.0000 (100.0000) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [1/2] time 0.736 (0.736) data 0.432 (0.432) loss 0.2574 (0.2574) acc 96.8750 (96.8750) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [2/2] time 0.308 (0.522) data 0.000 (0.216) loss 0.2350 (0.2462) acc 100.0000 (98.4375) lr 4.9344e-03 eta 0:00:01\n",
            "epoch [200/200] batch [1/2] time 0.781 (0.781) data 0.474 (0.474) loss 0.2371 (0.2371) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:00\n",
            "epoch [200/200] batch [2/2] time 0.311 (0.546) data 0.000 (0.237) loss 0.2977 (0.2674) acc 93.7500 (96.8750) lr 1.2337e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed3/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:33<00:00,  2.43it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 6,948\n",
            "* accuracy: 85.8%\n",
            "* error: 14.2%\n",
            "* macro_f1: 85.7%\n",
            "Elapsed: 0:04:21\n"
          ]
        }
      ],
      "source": [
        "#eurosat-8shots-seed3\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 3 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_init/eurosat/DAPT/vit_b16_8shots/seed3 \\\n",
        "        DATASET.NUM_SHOTS 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qn_EbwoAv1nw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7c9cb0-4a09-40a4-ed76-160c4a16e358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:52:20.102507: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:52:20.122130: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:52:20.128029: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:52:20.142204: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:52:21.154928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4']\n",
            "output_dir: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  40\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 1.763 (1.763) data 0.528 (0.528) loss 8.6787 (8.6787) acc 28.1250 (28.1250) lr 2.0000e+01 eta 0:02:54\n",
            "epoch [2/100] batch [1/1] time 0.714 (0.714) data 0.415 (0.415) loss 8.7939 (8.7939) acc 18.7500 (18.7500) lr 1.9995e+01 eta 0:01:10\n",
            "epoch [3/100] batch [1/1] time 0.716 (0.716) data 0.419 (0.419) loss 7.7816 (7.7816) acc 18.7500 (18.7500) lr 1.9980e+01 eta 0:01:09\n",
            "epoch [4/100] batch [1/1] time 0.741 (0.741) data 0.441 (0.441) loss 6.0907 (6.0907) acc 18.7500 (18.7500) lr 1.9956e+01 eta 0:01:11\n",
            "epoch [5/100] batch [1/1] time 0.722 (0.722) data 0.421 (0.421) loss 7.0115 (7.0115) acc 18.7500 (18.7500) lr 1.9921e+01 eta 0:01:08\n",
            "epoch [6/100] batch [1/1] time 0.731 (0.731) data 0.429 (0.429) loss 7.0626 (7.0626) acc 12.5000 (12.5000) lr 1.9877e+01 eta 0:01:08\n",
            "epoch [7/100] batch [1/1] time 0.719 (0.719) data 0.416 (0.416) loss 7.4973 (7.4973) acc 18.7500 (18.7500) lr 1.9823e+01 eta 0:01:06\n",
            "epoch [8/100] batch [1/1] time 0.751 (0.751) data 0.449 (0.449) loss 6.2458 (6.2458) acc 12.5000 (12.5000) lr 1.9759e+01 eta 0:01:09\n",
            "epoch [9/100] batch [1/1] time 0.724 (0.724) data 0.421 (0.421) loss 4.7445 (4.7445) acc 18.7500 (18.7500) lr 1.9686e+01 eta 0:01:05\n",
            "epoch [10/100] batch [1/1] time 0.733 (0.733) data 0.429 (0.429) loss 5.7691 (5.7691) acc 15.6250 (15.6250) lr 1.9603e+01 eta 0:01:05\n",
            "epoch [11/100] batch [1/1] time 0.742 (0.742) data 0.434 (0.434) loss 4.3428 (4.3428) acc 12.5000 (12.5000) lr 1.9511e+01 eta 0:01:06\n",
            "epoch [12/100] batch [1/1] time 0.729 (0.729) data 0.425 (0.425) loss 3.7711 (3.7711) acc 18.7500 (18.7500) lr 1.9409e+01 eta 0:01:04\n",
            "epoch [13/100] batch [1/1] time 0.722 (0.722) data 0.421 (0.421) loss 3.6382 (3.6382) acc 6.2500 (6.2500) lr 1.9298e+01 eta 0:01:02\n",
            "epoch [14/100] batch [1/1] time 0.697 (0.697) data 0.396 (0.396) loss 3.2014 (3.2014) acc 25.0000 (25.0000) lr 1.9178e+01 eta 0:00:59\n",
            "epoch [15/100] batch [1/1] time 0.716 (0.716) data 0.413 (0.413) loss 3.1826 (3.1826) acc 28.1250 (28.1250) lr 1.9048e+01 eta 0:01:00\n",
            "epoch [16/100] batch [1/1] time 0.715 (0.715) data 0.412 (0.412) loss 3.1905 (3.1905) acc 15.6250 (15.6250) lr 1.8910e+01 eta 0:01:00\n",
            "epoch [17/100] batch [1/1] time 0.722 (0.722) data 0.419 (0.419) loss 2.8681 (2.8681) acc 37.5000 (37.5000) lr 1.8763e+01 eta 0:00:59\n",
            "epoch [18/100] batch [1/1] time 0.727 (0.727) data 0.422 (0.422) loss 2.7203 (2.7203) acc 28.1250 (28.1250) lr 1.8607e+01 eta 0:00:59\n",
            "epoch [19/100] batch [1/1] time 0.733 (0.733) data 0.428 (0.428) loss 2.6142 (2.6142) acc 31.2500 (31.2500) lr 1.8443e+01 eta 0:00:59\n",
            "epoch [20/100] batch [1/1] time 0.712 (0.712) data 0.408 (0.408) loss 2.5176 (2.5176) acc 40.6250 (40.6250) lr 1.8271e+01 eta 0:00:56\n",
            "epoch [21/100] batch [1/1] time 0.752 (0.752) data 0.445 (0.445) loss 2.2771 (2.2771) acc 50.0000 (50.0000) lr 1.8090e+01 eta 0:00:59\n",
            "epoch [22/100] batch [1/1] time 0.732 (0.732) data 0.427 (0.427) loss 2.2667 (2.2667) acc 37.5000 (37.5000) lr 1.7902e+01 eta 0:00:57\n",
            "epoch [23/100] batch [1/1] time 0.718 (0.718) data 0.414 (0.414) loss 2.3078 (2.3078) acc 43.7500 (43.7500) lr 1.7705e+01 eta 0:00:55\n",
            "epoch [24/100] batch [1/1] time 0.709 (0.709) data 0.402 (0.402) loss 2.1667 (2.1667) acc 43.7500 (43.7500) lr 1.7501e+01 eta 0:00:53\n",
            "epoch [25/100] batch [1/1] time 0.716 (0.716) data 0.411 (0.411) loss 2.1673 (2.1673) acc 37.5000 (37.5000) lr 1.7290e+01 eta 0:00:53\n",
            "epoch [26/100] batch [1/1] time 0.733 (0.733) data 0.427 (0.427) loss 2.1041 (2.1041) acc 50.0000 (50.0000) lr 1.7071e+01 eta 0:00:54\n",
            "epoch [27/100] batch [1/1] time 0.711 (0.711) data 0.403 (0.403) loss 2.2868 (2.2868) acc 40.6250 (40.6250) lr 1.6845e+01 eta 0:00:51\n",
            "epoch [28/100] batch [1/1] time 0.714 (0.714) data 0.406 (0.406) loss 2.1224 (2.1224) acc 50.0000 (50.0000) lr 1.6613e+01 eta 0:00:51\n",
            "epoch [29/100] batch [1/1] time 0.715 (0.715) data 0.406 (0.406) loss 2.5617 (2.5617) acc 56.2500 (56.2500) lr 1.6374e+01 eta 0:00:50\n",
            "epoch [30/100] batch [1/1] time 0.733 (0.733) data 0.424 (0.424) loss 1.5268 (1.5268) acc 56.2500 (56.2500) lr 1.6129e+01 eta 0:00:51\n",
            "epoch [31/100] batch [1/1] time 0.710 (0.710) data 0.402 (0.402) loss 1.4843 (1.4843) acc 62.5000 (62.5000) lr 1.5878e+01 eta 0:00:48\n",
            "epoch [32/100] batch [1/1] time 0.664 (0.664) data 0.376 (0.376) loss 1.9053 (1.9053) acc 46.8750 (46.8750) lr 1.5621e+01 eta 0:00:45\n",
            "epoch [33/100] batch [1/1] time 0.723 (0.723) data 0.412 (0.412) loss 2.0586 (2.0586) acc 56.2500 (56.2500) lr 1.5358e+01 eta 0:00:48\n",
            "epoch [34/100] batch [1/1] time 0.718 (0.718) data 0.408 (0.408) loss 2.7325 (2.7325) acc 53.1250 (53.1250) lr 1.5090e+01 eta 0:00:47\n",
            "epoch [35/100] batch [1/1] time 0.715 (0.715) data 0.407 (0.407) loss 4.2838 (4.2838) acc 40.6250 (40.6250) lr 1.4818e+01 eta 0:00:46\n",
            "epoch [36/100] batch [1/1] time 0.728 (0.728) data 0.418 (0.418) loss 5.0807 (5.0807) acc 31.2500 (31.2500) lr 1.4540e+01 eta 0:00:46\n",
            "epoch [37/100] batch [1/1] time 0.703 (0.703) data 0.394 (0.394) loss 3.7149 (3.7149) acc 46.8750 (46.8750) lr 1.4258e+01 eta 0:00:44\n",
            "epoch [38/100] batch [1/1] time 0.703 (0.703) data 0.391 (0.391) loss 2.4923 (2.4923) acc 50.0000 (50.0000) lr 1.3971e+01 eta 0:00:43\n",
            "epoch [39/100] batch [1/1] time 0.721 (0.721) data 0.408 (0.408) loss 5.6135 (5.6135) acc 40.6250 (40.6250) lr 1.3681e+01 eta 0:00:43\n",
            "epoch [40/100] batch [1/1] time 0.720 (0.720) data 0.406 (0.406) loss 7.0206 (7.0206) acc 15.6250 (15.6250) lr 1.3387e+01 eta 0:00:43\n",
            "epoch [41/100] batch [1/1] time 0.704 (0.704) data 0.394 (0.394) loss 10.9534 (10.9534) acc 12.5000 (12.5000) lr 1.3090e+01 eta 0:00:41\n",
            "epoch [42/100] batch [1/1] time 0.711 (0.711) data 0.400 (0.400) loss 12.7088 (12.7088) acc 9.3750 (9.3750) lr 1.2790e+01 eta 0:00:41\n",
            "epoch [43/100] batch [1/1] time 0.712 (0.712) data 0.398 (0.398) loss 10.0042 (10.0042) acc 18.7500 (18.7500) lr 1.2487e+01 eta 0:00:40\n",
            "epoch [44/100] batch [1/1] time 0.697 (0.697) data 0.386 (0.386) loss 13.6660 (13.6660) acc 12.5000 (12.5000) lr 1.2181e+01 eta 0:00:39\n",
            "epoch [45/100] batch [1/1] time 0.711 (0.711) data 0.397 (0.397) loss 10.6852 (10.6852) acc 9.3750 (9.3750) lr 1.1874e+01 eta 0:00:39\n",
            "epoch [46/100] batch [1/1] time 0.708 (0.708) data 0.393 (0.393) loss 8.0933 (8.0933) acc 9.3750 (9.3750) lr 1.1564e+01 eta 0:00:38\n",
            "epoch [47/100] batch [1/1] time 0.707 (0.707) data 0.392 (0.392) loss 5.9693 (5.9693) acc 12.5000 (12.5000) lr 1.1253e+01 eta 0:00:37\n",
            "epoch [48/100] batch [1/1] time 0.731 (0.731) data 0.415 (0.415) loss 8.3565 (8.3565) acc 6.2500 (6.2500) lr 1.0941e+01 eta 0:00:38\n",
            "epoch [49/100] batch [1/1] time 0.691 (0.691) data 0.379 (0.379) loss 6.0911 (6.0911) acc 18.7500 (18.7500) lr 1.0628e+01 eta 0:00:35\n",
            "epoch [50/100] batch [1/1] time 0.704 (0.704) data 0.392 (0.392) loss 5.0713 (5.0713) acc 9.3750 (9.3750) lr 1.0314e+01 eta 0:00:35\n",
            "epoch [51/100] batch [1/1] time 0.747 (0.747) data 0.432 (0.432) loss 6.0460 (6.0460) acc 12.5000 (12.5000) lr 1.0000e+01 eta 0:00:36\n",
            "epoch [52/100] batch [1/1] time 0.753 (0.753) data 0.438 (0.438) loss 4.5965 (4.5965) acc 12.5000 (12.5000) lr 9.6859e+00 eta 0:00:36\n",
            "epoch [53/100] batch [1/1] time 0.758 (0.758) data 0.441 (0.441) loss 3.5969 (3.5969) acc 21.8750 (21.8750) lr 9.3721e+00 eta 0:00:35\n",
            "epoch [54/100] batch [1/1] time 0.729 (0.729) data 0.417 (0.417) loss 3.2419 (3.2419) acc 21.8750 (21.8750) lr 9.0589e+00 eta 0:00:33\n",
            "epoch [55/100] batch [1/1] time 0.736 (0.736) data 0.421 (0.421) loss 3.1062 (3.1062) acc 15.6250 (15.6250) lr 8.7467e+00 eta 0:00:33\n",
            "epoch [56/100] batch [1/1] time 0.725 (0.725) data 0.410 (0.410) loss 3.0713 (3.0713) acc 25.0000 (25.0000) lr 8.4357e+00 eta 0:00:31\n",
            "epoch [57/100] batch [1/1] time 0.705 (0.705) data 0.391 (0.391) loss 2.7291 (2.7291) acc 21.8750 (21.8750) lr 8.1262e+00 eta 0:00:30\n",
            "epoch [58/100] batch [1/1] time 0.708 (0.708) data 0.395 (0.395) loss 2.6930 (2.6930) acc 25.0000 (25.0000) lr 7.8186e+00 eta 0:00:29\n",
            "epoch [59/100] batch [1/1] time 0.717 (0.717) data 0.403 (0.403) loss 2.7297 (2.7297) acc 25.0000 (25.0000) lr 7.5131e+00 eta 0:00:29\n",
            "epoch [60/100] batch [1/1] time 0.724 (0.724) data 0.409 (0.409) loss 2.6913 (2.6913) acc 25.0000 (25.0000) lr 7.2101e+00 eta 0:00:28\n",
            "epoch [61/100] batch [1/1] time 0.707 (0.707) data 0.394 (0.394) loss 2.2982 (2.2982) acc 37.5000 (37.5000) lr 6.9098e+00 eta 0:00:27\n",
            "epoch [62/100] batch [1/1] time 0.699 (0.699) data 0.389 (0.389) loss 2.1511 (2.1511) acc 43.7500 (43.7500) lr 6.6126e+00 eta 0:00:26\n",
            "epoch [63/100] batch [1/1] time 0.704 (0.704) data 0.393 (0.393) loss 2.2211 (2.2211) acc 50.0000 (50.0000) lr 6.3188e+00 eta 0:00:26\n",
            "epoch [64/100] batch [1/1] time 0.711 (0.711) data 0.400 (0.400) loss 2.2714 (2.2714) acc 34.3750 (34.3750) lr 6.0285e+00 eta 0:00:25\n",
            "epoch [65/100] batch [1/1] time 0.710 (0.710) data 0.400 (0.400) loss 2.1872 (2.1872) acc 43.7500 (43.7500) lr 5.7422e+00 eta 0:00:24\n",
            "epoch [66/100] batch [1/1] time 0.702 (0.702) data 0.389 (0.389) loss 1.9665 (1.9665) acc 46.8750 (46.8750) lr 5.4601e+00 eta 0:00:23\n",
            "epoch [67/100] batch [1/1] time 0.725 (0.725) data 0.415 (0.415) loss 1.7703 (1.7703) acc 56.2500 (56.2500) lr 5.1825e+00 eta 0:00:23\n",
            "epoch [68/100] batch [1/1] time 0.761 (0.761) data 0.451 (0.451) loss 1.7341 (1.7341) acc 56.2500 (56.2500) lr 4.9096e+00 eta 0:00:24\n",
            "epoch [69/100] batch [1/1] time 0.727 (0.727) data 0.417 (0.417) loss 1.6048 (1.6048) acc 62.5000 (62.5000) lr 4.6417e+00 eta 0:00:22\n",
            "epoch [70/100] batch [1/1] time 0.758 (0.758) data 0.449 (0.449) loss 1.4903 (1.4903) acc 65.6250 (65.6250) lr 4.3792e+00 eta 0:00:22\n",
            "epoch [71/100] batch [1/1] time 0.740 (0.740) data 0.428 (0.428) loss 1.4710 (1.4710) acc 65.6250 (65.6250) lr 4.1221e+00 eta 0:00:21\n",
            "epoch [72/100] batch [1/1] time 0.710 (0.710) data 0.402 (0.402) loss 1.4802 (1.4802) acc 65.6250 (65.6250) lr 3.8709e+00 eta 0:00:19\n",
            "epoch [73/100] batch [1/1] time 0.733 (0.733) data 0.425 (0.425) loss 1.3729 (1.3729) acc 65.6250 (65.6250) lr 3.6258e+00 eta 0:00:19\n",
            "epoch [74/100] batch [1/1] time 0.699 (0.699) data 0.393 (0.393) loss 1.5133 (1.5133) acc 68.7500 (68.7500) lr 3.3869e+00 eta 0:00:18\n",
            "epoch [75/100] batch [1/1] time 0.699 (0.699) data 0.390 (0.390) loss 1.3408 (1.3408) acc 75.0000 (75.0000) lr 3.1545e+00 eta 0:00:17\n",
            "epoch [76/100] batch [1/1] time 0.698 (0.698) data 0.390 (0.390) loss 1.2476 (1.2476) acc 65.6250 (65.6250) lr 2.9289e+00 eta 0:00:16\n",
            "epoch [77/100] batch [1/1] time 0.738 (0.738) data 0.431 (0.431) loss 1.4430 (1.4430) acc 62.5000 (62.5000) lr 2.7103e+00 eta 0:00:16\n",
            "epoch [78/100] batch [1/1] time 0.746 (0.746) data 0.436 (0.436) loss 1.1075 (1.1075) acc 81.2500 (81.2500) lr 2.4989e+00 eta 0:00:16\n",
            "epoch [79/100] batch [1/1] time 0.706 (0.706) data 0.398 (0.398) loss 0.9366 (0.9366) acc 84.3750 (84.3750) lr 2.2949e+00 eta 0:00:14\n",
            "epoch [80/100] batch [1/1] time 0.721 (0.721) data 0.413 (0.413) loss 1.1886 (1.1886) acc 71.8750 (71.8750) lr 2.0984e+00 eta 0:00:14\n",
            "epoch [81/100] batch [1/1] time 0.685 (0.685) data 0.379 (0.379) loss 1.1426 (1.1426) acc 78.1250 (78.1250) lr 1.9098e+00 eta 0:00:13\n",
            "epoch [82/100] batch [1/1] time 0.721 (0.721) data 0.417 (0.417) loss 1.1171 (1.1171) acc 75.0000 (75.0000) lr 1.7292e+00 eta 0:00:12\n",
            "epoch [83/100] batch [1/1] time 0.736 (0.736) data 0.429 (0.429) loss 0.9998 (0.9998) acc 81.2500 (81.2500) lr 1.5567e+00 eta 0:00:12\n",
            "epoch [84/100] batch [1/1] time 0.715 (0.715) data 0.409 (0.409) loss 1.0065 (1.0065) acc 90.6250 (90.6250) lr 1.3926e+00 eta 0:00:11\n",
            "epoch [85/100] batch [1/1] time 0.762 (0.762) data 0.456 (0.456) loss 1.0278 (1.0278) acc 78.1250 (78.1250) lr 1.2369e+00 eta 0:00:11\n",
            "epoch [86/100] batch [1/1] time 0.720 (0.720) data 0.417 (0.417) loss 1.0245 (1.0245) acc 75.0000 (75.0000) lr 1.0899e+00 eta 0:00:10\n",
            "epoch [87/100] batch [1/1] time 0.722 (0.722) data 0.415 (0.415) loss 1.0950 (1.0950) acc 78.1250 (78.1250) lr 9.5173e-01 eta 0:00:09\n",
            "epoch [88/100] batch [1/1] time 0.693 (0.693) data 0.388 (0.388) loss 0.8931 (0.8931) acc 84.3750 (84.3750) lr 8.2245e-01 eta 0:00:08\n",
            "epoch [89/100] batch [1/1] time 0.695 (0.695) data 0.392 (0.392) loss 1.0143 (1.0143) acc 81.2500 (81.2500) lr 7.0224e-01 eta 0:00:07\n",
            "epoch [90/100] batch [1/1] time 0.711 (0.711) data 0.405 (0.405) loss 0.7312 (0.7312) acc 90.6250 (90.6250) lr 5.9119e-01 eta 0:00:07\n",
            "epoch [91/100] batch [1/1] time 0.696 (0.696) data 0.393 (0.393) loss 1.0202 (1.0202) acc 78.1250 (78.1250) lr 4.8943e-01 eta 0:00:06\n",
            "epoch [92/100] batch [1/1] time 0.700 (0.700) data 0.395 (0.395) loss 0.8651 (0.8651) acc 90.6250 (90.6250) lr 3.9706e-01 eta 0:00:05\n",
            "epoch [93/100] batch [1/1] time 0.685 (0.685) data 0.381 (0.381) loss 0.9514 (0.9514) acc 81.2500 (81.2500) lr 3.1417e-01 eta 0:00:04\n",
            "epoch [94/100] batch [1/1] time 0.719 (0.719) data 0.413 (0.413) loss 0.6920 (0.6920) acc 90.6250 (90.6250) lr 2.4083e-01 eta 0:00:04\n",
            "epoch [95/100] batch [1/1] time 0.737 (0.737) data 0.430 (0.430) loss 0.8123 (0.8123) acc 81.2500 (81.2500) lr 1.7713e-01 eta 0:00:03\n",
            "epoch [96/100] batch [1/1] time 0.699 (0.699) data 0.395 (0.395) loss 0.7301 (0.7301) acc 90.6250 (90.6250) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.702 (0.702) data 0.397 (0.397) loss 0.9859 (0.9859) acc 68.7500 (68.7500) lr 7.8853e-02 eta 0:00:02\n",
            "epoch [98/100] batch [1/1] time 0.695 (0.695) data 0.389 (0.389) loss 0.7261 (0.7261) acc 87.5000 (87.5000) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.720 (0.720) data 0.417 (0.417) loss 0.7666 (0.7666) acc 87.5000 (87.5000) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.721 (0.721) data 0.418 (0.418) loss 0.9509 (0.9509) acc 75.0000 (75.0000) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:33<00:00,  2.44it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 4,550\n",
            "* accuracy: 56.2%\n",
            "* error: 43.8%\n",
            "* macro_f1: 55.4%\n",
            "Elapsed: 0:01:53\n"
          ]
        }
      ],
      "source": [
        "#eurosat-4shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "slxfKB7OKD73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e56a499-02e8-493a-f6db-1fe818a9c5f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:54:33.334506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:54:33.354546: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:54:33.360586: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:54:33.374975: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:54:34.386161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4']\n",
            "output_dir: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed2\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  40\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0173, -0.5723,  0.1752,  ...,  0.4941,  0.4187,  0.0375],\n",
            "         [ 0.0214, -0.5706,  0.1699,  ...,  0.5021,  0.4206,  0.0425],\n",
            "         [ 0.0149, -0.5713,  0.1720,  ...,  0.4970,  0.4210,  0.0462],\n",
            "         ...,\n",
            "         [ 0.0324, -0.5680,  0.1748,  ...,  0.5016,  0.4188,  0.0404],\n",
            "         [ 0.0201, -0.5696,  0.1726,  ...,  0.4989,  0.4190,  0.0354],\n",
            "         [ 0.0228, -0.5696,  0.1684,  ...,  0.5053,  0.4212,  0.0412]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5609\n",
            "  Max: 1.0111\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0400\n",
            "  Min: -0.4519\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed2/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 1.748 (1.748) data 0.513 (0.513) loss 8.8014 (8.8014) acc 18.7500 (18.7500) lr 2.0000e+01 eta 0:02:53\n",
            "epoch [2/100] batch [1/1] time 0.728 (0.728) data 0.426 (0.426) loss 8.7075 (8.7075) acc 34.3750 (34.3750) lr 1.9995e+01 eta 0:01:11\n",
            "epoch [3/100] batch [1/1] time 0.721 (0.721) data 0.420 (0.420) loss 7.0128 (7.0128) acc 15.6250 (15.6250) lr 1.9980e+01 eta 0:01:09\n",
            "epoch [4/100] batch [1/1] time 0.713 (0.713) data 0.410 (0.410) loss 5.0312 (5.0312) acc 12.5000 (12.5000) lr 1.9956e+01 eta 0:01:08\n",
            "epoch [5/100] batch [1/1] time 0.702 (0.702) data 0.399 (0.399) loss 6.2001 (6.2001) acc 12.5000 (12.5000) lr 1.9921e+01 eta 0:01:06\n",
            "epoch [6/100] batch [1/1] time 0.702 (0.702) data 0.397 (0.397) loss 5.8668 (5.8668) acc 18.7500 (18.7500) lr 1.9877e+01 eta 0:01:05\n",
            "epoch [7/100] batch [1/1] time 0.726 (0.726) data 0.413 (0.413) loss 6.8887 (6.8887) acc 25.0000 (25.0000) lr 1.9823e+01 eta 0:01:07\n",
            "epoch [8/100] batch [1/1] time 0.681 (0.681) data 0.378 (0.378) loss 5.7765 (5.7765) acc 21.8750 (21.8750) lr 1.9759e+01 eta 0:01:02\n",
            "epoch [9/100] batch [1/1] time 0.708 (0.708) data 0.405 (0.405) loss 4.1931 (4.1931) acc 18.7500 (18.7500) lr 1.9686e+01 eta 0:01:04\n",
            "epoch [10/100] batch [1/1] time 0.675 (0.675) data 0.373 (0.373) loss 3.4955 (3.4955) acc 21.8750 (21.8750) lr 1.9603e+01 eta 0:01:00\n",
            "epoch [11/100] batch [1/1] time 0.685 (0.685) data 0.408 (0.408) loss 2.9618 (2.9618) acc 37.5000 (37.5000) lr 1.9511e+01 eta 0:01:00\n",
            "epoch [12/100] batch [1/1] time 0.708 (0.708) data 0.404 (0.404) loss 2.9574 (2.9574) acc 40.6250 (40.6250) lr 1.9409e+01 eta 0:01:02\n",
            "epoch [13/100] batch [1/1] time 0.678 (0.678) data 0.376 (0.376) loss 2.7203 (2.7203) acc 37.5000 (37.5000) lr 1.9298e+01 eta 0:00:58\n",
            "epoch [14/100] batch [1/1] time 0.704 (0.704) data 0.399 (0.399) loss 2.5109 (2.5109) acc 37.5000 (37.5000) lr 1.9178e+01 eta 0:01:00\n",
            "epoch [15/100] batch [1/1] time 0.719 (0.719) data 0.412 (0.412) loss 2.2133 (2.2133) acc 59.3750 (59.3750) lr 1.9048e+01 eta 0:01:01\n",
            "epoch [16/100] batch [1/1] time 0.722 (0.722) data 0.417 (0.417) loss 2.1307 (2.1307) acc 62.5000 (62.5000) lr 1.8910e+01 eta 0:01:00\n",
            "epoch [17/100] batch [1/1] time 0.738 (0.738) data 0.431 (0.431) loss 2.0038 (2.0038) acc 62.5000 (62.5000) lr 1.8763e+01 eta 0:01:01\n",
            "epoch [18/100] batch [1/1] time 0.743 (0.743) data 0.439 (0.439) loss 1.9049 (1.9049) acc 71.8750 (71.8750) lr 1.8607e+01 eta 0:01:00\n",
            "epoch [19/100] batch [1/1] time 0.753 (0.753) data 0.446 (0.446) loss 1.6883 (1.6883) acc 62.5000 (62.5000) lr 1.8443e+01 eta 0:01:00\n",
            "epoch [20/100] batch [1/1] time 0.728 (0.728) data 0.422 (0.422) loss 1.9203 (1.9203) acc 56.2500 (56.2500) lr 1.8271e+01 eta 0:00:58\n",
            "epoch [21/100] batch [1/1] time 0.686 (0.686) data 0.381 (0.381) loss 1.6465 (1.6465) acc 59.3750 (59.3750) lr 1.8090e+01 eta 0:00:54\n",
            "epoch [22/100] batch [1/1] time 0.707 (0.707) data 0.400 (0.400) loss 1.4696 (1.4696) acc 75.0000 (75.0000) lr 1.7902e+01 eta 0:00:55\n",
            "epoch [23/100] batch [1/1] time 0.716 (0.716) data 0.409 (0.409) loss 1.4675 (1.4675) acc 71.8750 (71.8750) lr 1.7705e+01 eta 0:00:55\n",
            "epoch [24/100] batch [1/1] time 0.721 (0.721) data 0.414 (0.414) loss 1.4925 (1.4925) acc 62.5000 (62.5000) lr 1.7501e+01 eta 0:00:54\n",
            "epoch [25/100] batch [1/1] time 0.710 (0.710) data 0.402 (0.402) loss 1.3606 (1.3606) acc 71.8750 (71.8750) lr 1.7290e+01 eta 0:00:53\n",
            "epoch [26/100] batch [1/1] time 0.692 (0.692) data 0.384 (0.384) loss 1.1136 (1.1136) acc 81.2500 (81.2500) lr 1.7071e+01 eta 0:00:51\n",
            "epoch [27/100] batch [1/1] time 0.739 (0.739) data 0.431 (0.431) loss 1.1443 (1.1443) acc 87.5000 (87.5000) lr 1.6845e+01 eta 0:00:53\n",
            "epoch [28/100] batch [1/1] time 0.689 (0.689) data 0.381 (0.381) loss 1.2480 (1.2480) acc 75.0000 (75.0000) lr 1.6613e+01 eta 0:00:49\n",
            "epoch [29/100] batch [1/1] time 0.709 (0.709) data 0.402 (0.402) loss 1.0787 (1.0787) acc 78.1250 (78.1250) lr 1.6374e+01 eta 0:00:50\n",
            "epoch [30/100] batch [1/1] time 0.736 (0.736) data 0.428 (0.428) loss 1.0703 (1.0703) acc 75.0000 (75.0000) lr 1.6129e+01 eta 0:00:51\n",
            "epoch [31/100] batch [1/1] time 0.708 (0.708) data 0.397 (0.397) loss 1.3820 (1.3820) acc 75.0000 (75.0000) lr 1.5878e+01 eta 0:00:48\n",
            "epoch [32/100] batch [1/1] time 0.713 (0.713) data 0.404 (0.404) loss 1.2610 (1.2610) acc 68.7500 (68.7500) lr 1.5621e+01 eta 0:00:48\n",
            "epoch [33/100] batch [1/1] time 0.713 (0.713) data 0.400 (0.400) loss 1.1320 (1.1320) acc 71.8750 (71.8750) lr 1.5358e+01 eta 0:00:47\n",
            "epoch [34/100] batch [1/1] time 0.741 (0.741) data 0.433 (0.433) loss 0.7599 (0.7599) acc 90.6250 (90.6250) lr 1.5090e+01 eta 0:00:48\n",
            "epoch [35/100] batch [1/1] time 0.733 (0.733) data 0.423 (0.423) loss 0.7915 (0.7915) acc 84.3750 (84.3750) lr 1.4818e+01 eta 0:00:47\n",
            "epoch [36/100] batch [1/1] time 0.712 (0.712) data 0.401 (0.401) loss 0.6312 (0.6312) acc 93.7500 (93.7500) lr 1.4540e+01 eta 0:00:45\n",
            "epoch [37/100] batch [1/1] time 0.755 (0.755) data 0.445 (0.445) loss 1.2099 (1.2099) acc 68.7500 (68.7500) lr 1.4258e+01 eta 0:00:47\n",
            "epoch [38/100] batch [1/1] time 0.703 (0.703) data 0.392 (0.392) loss 1.3062 (1.3062) acc 71.8750 (71.8750) lr 1.3971e+01 eta 0:00:43\n",
            "epoch [39/100] batch [1/1] time 0.714 (0.714) data 0.403 (0.403) loss 1.4889 (1.4889) acc 75.0000 (75.0000) lr 1.3681e+01 eta 0:00:43\n",
            "epoch [40/100] batch [1/1] time 0.704 (0.704) data 0.391 (0.391) loss 2.3065 (2.3065) acc 59.3750 (59.3750) lr 1.3387e+01 eta 0:00:42\n",
            "epoch [41/100] batch [1/1] time 0.708 (0.708) data 0.397 (0.397) loss 3.9654 (3.9654) acc 37.5000 (37.5000) lr 1.3090e+01 eta 0:00:41\n",
            "epoch [42/100] batch [1/1] time 0.723 (0.723) data 0.407 (0.407) loss 4.3649 (4.3649) acc 46.8750 (46.8750) lr 1.2790e+01 eta 0:00:41\n",
            "epoch [43/100] batch [1/1] time 0.703 (0.703) data 0.390 (0.390) loss 7.9200 (7.9200) acc 40.6250 (40.6250) lr 1.2487e+01 eta 0:00:40\n",
            "epoch [44/100] batch [1/1] time 0.716 (0.716) data 0.401 (0.401) loss 4.8675 (4.8675) acc 34.3750 (34.3750) lr 1.2181e+01 eta 0:00:40\n",
            "epoch [45/100] batch [1/1] time 0.730 (0.730) data 0.415 (0.415) loss 4.8376 (4.8376) acc 31.2500 (31.2500) lr 1.1874e+01 eta 0:00:40\n",
            "epoch [46/100] batch [1/1] time 0.704 (0.704) data 0.391 (0.391) loss 5.0885 (5.0885) acc 28.1250 (28.1250) lr 1.1564e+01 eta 0:00:38\n",
            "epoch [47/100] batch [1/1] time 0.719 (0.719) data 0.404 (0.404) loss 4.6261 (4.6261) acc 15.6250 (15.6250) lr 1.1253e+01 eta 0:00:38\n",
            "epoch [48/100] batch [1/1] time 0.707 (0.707) data 0.393 (0.393) loss 5.4020 (5.4020) acc 34.3750 (34.3750) lr 1.0941e+01 eta 0:00:36\n",
            "epoch [49/100] batch [1/1] time 0.735 (0.735) data 0.422 (0.422) loss 3.4916 (3.4916) acc 40.6250 (40.6250) lr 1.0628e+01 eta 0:00:37\n",
            "epoch [50/100] batch [1/1] time 0.742 (0.742) data 0.428 (0.428) loss 2.8652 (2.8652) acc 37.5000 (37.5000) lr 1.0314e+01 eta 0:00:37\n",
            "epoch [51/100] batch [1/1] time 0.731 (0.731) data 0.418 (0.418) loss 2.9034 (2.9034) acc 40.6250 (40.6250) lr 1.0000e+01 eta 0:00:35\n",
            "epoch [52/100] batch [1/1] time 0.753 (0.753) data 0.436 (0.436) loss 2.3052 (2.3052) acc 50.0000 (50.0000) lr 9.6859e+00 eta 0:00:36\n",
            "epoch [53/100] batch [1/1] time 0.695 (0.695) data 0.379 (0.379) loss 2.0836 (2.0836) acc 59.3750 (59.3750) lr 9.3721e+00 eta 0:00:32\n",
            "epoch [54/100] batch [1/1] time 0.712 (0.712) data 0.397 (0.397) loss 2.2846 (2.2846) acc 65.6250 (65.6250) lr 9.0589e+00 eta 0:00:32\n",
            "epoch [55/100] batch [1/1] time 0.724 (0.724) data 0.406 (0.406) loss 2.0589 (2.0589) acc 62.5000 (62.5000) lr 8.7467e+00 eta 0:00:32\n",
            "epoch [56/100] batch [1/1] time 0.703 (0.703) data 0.388 (0.388) loss 1.6289 (1.6289) acc 56.2500 (56.2500) lr 8.4357e+00 eta 0:00:30\n",
            "epoch [57/100] batch [1/1] time 0.713 (0.713) data 0.402 (0.402) loss 1.6967 (1.6967) acc 59.3750 (59.3750) lr 8.1262e+00 eta 0:00:30\n",
            "epoch [58/100] batch [1/1] time 0.692 (0.692) data 0.380 (0.380) loss 1.1385 (1.1385) acc 71.8750 (71.8750) lr 7.8186e+00 eta 0:00:29\n",
            "epoch [59/100] batch [1/1] time 0.721 (0.721) data 0.408 (0.408) loss 1.0331 (1.0331) acc 84.3750 (84.3750) lr 7.5131e+00 eta 0:00:29\n",
            "epoch [60/100] batch [1/1] time 0.728 (0.728) data 0.416 (0.416) loss 0.9520 (0.9520) acc 78.1250 (78.1250) lr 7.2101e+00 eta 0:00:29\n",
            "epoch [61/100] batch [1/1] time 0.708 (0.708) data 0.397 (0.397) loss 0.9479 (0.9479) acc 84.3750 (84.3750) lr 6.9098e+00 eta 0:00:27\n",
            "epoch [62/100] batch [1/1] time 0.713 (0.713) data 0.401 (0.401) loss 1.0725 (1.0725) acc 71.8750 (71.8750) lr 6.6126e+00 eta 0:00:27\n",
            "epoch [63/100] batch [1/1] time 0.689 (0.689) data 0.378 (0.378) loss 0.8675 (0.8675) acc 84.3750 (84.3750) lr 6.3188e+00 eta 0:00:25\n",
            "epoch [64/100] batch [1/1] time 0.714 (0.714) data 0.403 (0.403) loss 0.7615 (0.7615) acc 87.5000 (87.5000) lr 6.0285e+00 eta 0:00:25\n",
            "epoch [65/100] batch [1/1] time 0.738 (0.738) data 0.429 (0.429) loss 0.8075 (0.8075) acc 87.5000 (87.5000) lr 5.7422e+00 eta 0:00:25\n",
            "epoch [66/100] batch [1/1] time 0.727 (0.727) data 0.419 (0.419) loss 0.6554 (0.6554) acc 87.5000 (87.5000) lr 5.4601e+00 eta 0:00:24\n",
            "epoch [67/100] batch [1/1] time 0.725 (0.725) data 0.416 (0.416) loss 0.8408 (0.8408) acc 84.3750 (84.3750) lr 5.1825e+00 eta 0:00:23\n",
            "epoch [68/100] batch [1/1] time 0.705 (0.705) data 0.396 (0.396) loss 0.6074 (0.6074) acc 96.8750 (96.8750) lr 4.9096e+00 eta 0:00:22\n",
            "epoch [69/100] batch [1/1] time 0.705 (0.705) data 0.396 (0.396) loss 0.5908 (0.5908) acc 90.6250 (90.6250) lr 4.6417e+00 eta 0:00:21\n",
            "epoch [70/100] batch [1/1] time 0.698 (0.698) data 0.389 (0.389) loss 0.6166 (0.6166) acc 93.7500 (93.7500) lr 4.3792e+00 eta 0:00:20\n",
            "epoch [71/100] batch [1/1] time 0.692 (0.692) data 0.383 (0.383) loss 0.4718 (0.4718) acc 96.8750 (96.8750) lr 4.1221e+00 eta 0:00:20\n",
            "epoch [72/100] batch [1/1] time 0.722 (0.722) data 0.413 (0.413) loss 0.5418 (0.5418) acc 90.6250 (90.6250) lr 3.8709e+00 eta 0:00:20\n",
            "epoch [73/100] batch [1/1] time 0.722 (0.722) data 0.413 (0.413) loss 0.5708 (0.5708) acc 90.6250 (90.6250) lr 3.6258e+00 eta 0:00:19\n",
            "epoch [74/100] batch [1/1] time 0.721 (0.721) data 0.413 (0.413) loss 0.6526 (0.6526) acc 93.7500 (93.7500) lr 3.3869e+00 eta 0:00:18\n",
            "epoch [75/100] batch [1/1] time 0.706 (0.706) data 0.397 (0.397) loss 0.5014 (0.5014) acc 96.8750 (96.8750) lr 3.1545e+00 eta 0:00:17\n",
            "epoch [76/100] batch [1/1] time 0.684 (0.684) data 0.376 (0.376) loss 0.7267 (0.7267) acc 90.6250 (90.6250) lr 2.9289e+00 eta 0:00:16\n",
            "epoch [77/100] batch [1/1] time 0.704 (0.704) data 0.395 (0.395) loss 0.8754 (0.8754) acc 87.5000 (87.5000) lr 2.7103e+00 eta 0:00:16\n",
            "epoch [78/100] batch [1/1] time 0.706 (0.706) data 0.399 (0.399) loss 0.5824 (0.5824) acc 93.7500 (93.7500) lr 2.4989e+00 eta 0:00:15\n",
            "epoch [79/100] batch [1/1] time 0.694 (0.694) data 0.388 (0.388) loss 0.6473 (0.6473) acc 93.7500 (93.7500) lr 2.2949e+00 eta 0:00:14\n",
            "epoch [80/100] batch [1/1] time 0.703 (0.703) data 0.397 (0.397) loss 0.5487 (0.5487) acc 96.8750 (96.8750) lr 2.0984e+00 eta 0:00:14\n",
            "epoch [81/100] batch [1/1] time 0.711 (0.711) data 0.407 (0.407) loss 0.4498 (0.4498) acc 96.8750 (96.8750) lr 1.9098e+00 eta 0:00:13\n",
            "epoch [82/100] batch [1/1] time 0.746 (0.746) data 0.441 (0.441) loss 0.5828 (0.5828) acc 84.3750 (84.3750) lr 1.7292e+00 eta 0:00:13\n",
            "epoch [83/100] batch [1/1] time 0.717 (0.717) data 0.411 (0.411) loss 0.8357 (0.8357) acc 87.5000 (87.5000) lr 1.5567e+00 eta 0:00:12\n",
            "epoch [84/100] batch [1/1] time 0.723 (0.723) data 0.417 (0.417) loss 0.5799 (0.5799) acc 93.7500 (93.7500) lr 1.3926e+00 eta 0:00:11\n",
            "epoch [85/100] batch [1/1] time 0.702 (0.702) data 0.397 (0.397) loss 0.4312 (0.4312) acc 93.7500 (93.7500) lr 1.2369e+00 eta 0:00:10\n",
            "epoch [86/100] batch [1/1] time 0.694 (0.694) data 0.391 (0.391) loss 0.4554 (0.4554) acc 93.7500 (93.7500) lr 1.0899e+00 eta 0:00:09\n",
            "epoch [87/100] batch [1/1] time 0.737 (0.737) data 0.432 (0.432) loss 0.4795 (0.4795) acc 93.7500 (93.7500) lr 9.5173e-01 eta 0:00:09\n",
            "epoch [88/100] batch [1/1] time 0.692 (0.692) data 0.387 (0.387) loss 0.4267 (0.4267) acc 96.8750 (96.8750) lr 8.2245e-01 eta 0:00:08\n",
            "epoch [89/100] batch [1/1] time 0.720 (0.720) data 0.416 (0.416) loss 0.6594 (0.6594) acc 90.6250 (90.6250) lr 7.0224e-01 eta 0:00:07\n",
            "epoch [90/100] batch [1/1] time 0.699 (0.699) data 0.394 (0.394) loss 0.4852 (0.4852) acc 93.7500 (93.7500) lr 5.9119e-01 eta 0:00:06\n",
            "epoch [91/100] batch [1/1] time 0.698 (0.698) data 0.394 (0.394) loss 0.4004 (0.4004) acc 96.8750 (96.8750) lr 4.8943e-01 eta 0:00:06\n",
            "epoch [92/100] batch [1/1] time 0.708 (0.708) data 0.404 (0.404) loss 0.3380 (0.3380) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:05\n",
            "epoch [93/100] batch [1/1] time 0.694 (0.694) data 0.388 (0.388) loss 0.4880 (0.4880) acc 93.7500 (93.7500) lr 3.1417e-01 eta 0:00:04\n",
            "epoch [94/100] batch [1/1] time 0.690 (0.690) data 0.387 (0.387) loss 0.3908 (0.3908) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:04\n",
            "epoch [95/100] batch [1/1] time 0.743 (0.743) data 0.439 (0.439) loss 0.4621 (0.4621) acc 93.7500 (93.7500) lr 1.7713e-01 eta 0:00:03\n",
            "epoch [96/100] batch [1/1] time 0.692 (0.692) data 0.386 (0.386) loss 0.5402 (0.5402) acc 87.5000 (87.5000) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.714 (0.714) data 0.410 (0.410) loss 0.3518 (0.3518) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:02\n",
            "epoch [98/100] batch [1/1] time 0.709 (0.709) data 0.404 (0.404) loss 0.4214 (0.4214) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.720 (0.720) data 0.417 (0.417) loss 0.3786 (0.3786) acc 96.8750 (96.8750) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.721 (0.721) data 0.417 (0.417) loss 0.3888 (0.3888) acc 96.8750 (96.8750) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed2/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:33<00:00,  2.45it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 5,687\n",
            "* accuracy: 70.2%\n",
            "* error: 29.8%\n",
            "* macro_f1: 69.6%\n",
            "Elapsed: 0:01:52\n"
          ]
        }
      ],
      "source": [
        "#eurosat-4shots-seed2\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 2 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed2 \\\n",
        "        DATASET.NUM_SHOTS 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xzeUz5WBKI2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab71ee6-b8c5-47cb-ad0d-92d02fad0680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:56:45.260312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:56:45.280171: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:56:45.286065: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:56:45.300411: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:56:46.287601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4']\n",
            "output_dir: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed3\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  40\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0187, -0.5727,  0.1683,  ...,  0.4947,  0.4188,  0.0344],\n",
            "         [ 0.0173, -0.5677,  0.1739,  ...,  0.4990,  0.4190,  0.0388],\n",
            "         [ 0.0094, -0.5649,  0.1758,  ...,  0.4946,  0.4157,  0.0405],\n",
            "         ...,\n",
            "         [ 0.0227, -0.5676,  0.1747,  ...,  0.5071,  0.4204,  0.0413],\n",
            "         [ 0.0083, -0.5735,  0.1744,  ...,  0.5016,  0.4249,  0.0364],\n",
            "         [ 0.0210, -0.5687,  0.1704,  ...,  0.5030,  0.4152,  0.0323]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2340\n",
            "  Min: -1.5598\n",
            "  Max: 1.0156\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4500\n",
            "  Max: 0.4708\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed3/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 1.743 (1.743) data 0.485 (0.485) loss 8.8697 (8.8697) acc 18.7500 (18.7500) lr 2.0000e+01 eta 0:02:52\n",
            "epoch [2/100] batch [1/1] time 0.730 (0.730) data 0.429 (0.429) loss 8.8481 (8.8481) acc 21.8750 (21.8750) lr 1.9995e+01 eta 0:01:11\n",
            "epoch [3/100] batch [1/1] time 0.703 (0.703) data 0.402 (0.402) loss 7.9474 (7.9474) acc 12.5000 (12.5000) lr 1.9980e+01 eta 0:01:08\n",
            "epoch [4/100] batch [1/1] time 0.707 (0.707) data 0.405 (0.405) loss 5.9248 (5.9248) acc 18.7500 (18.7500) lr 1.9956e+01 eta 0:01:07\n",
            "epoch [5/100] batch [1/1] time 0.726 (0.726) data 0.424 (0.424) loss 6.2788 (6.2788) acc 15.6250 (15.6250) lr 1.9921e+01 eta 0:01:08\n",
            "epoch [6/100] batch [1/1] time 0.694 (0.694) data 0.390 (0.390) loss 5.9358 (5.9358) acc 18.7500 (18.7500) lr 1.9877e+01 eta 0:01:05\n",
            "epoch [7/100] batch [1/1] time 0.707 (0.707) data 0.406 (0.406) loss 4.9143 (4.9143) acc 18.7500 (18.7500) lr 1.9823e+01 eta 0:01:05\n",
            "epoch [8/100] batch [1/1] time 0.695 (0.695) data 0.392 (0.392) loss 4.6865 (4.6865) acc 12.5000 (12.5000) lr 1.9759e+01 eta 0:01:03\n",
            "epoch [9/100] batch [1/1] time 0.679 (0.679) data 0.402 (0.402) loss 3.6072 (3.6072) acc 25.0000 (25.0000) lr 1.9686e+01 eta 0:01:01\n",
            "epoch [10/100] batch [1/1] time 0.684 (0.684) data 0.381 (0.381) loss 3.2943 (3.2943) acc 31.2500 (31.2500) lr 1.9603e+01 eta 0:01:01\n",
            "epoch [11/100] batch [1/1] time 0.682 (0.682) data 0.379 (0.379) loss 3.1792 (3.1792) acc 21.8750 (21.8750) lr 1.9511e+01 eta 0:01:00\n",
            "epoch [12/100] batch [1/1] time 0.737 (0.737) data 0.434 (0.434) loss 2.4814 (2.4814) acc 37.5000 (37.5000) lr 1.9409e+01 eta 0:01:04\n",
            "epoch [13/100] batch [1/1] time 0.737 (0.737) data 0.435 (0.435) loss 2.5003 (2.5003) acc 40.6250 (40.6250) lr 1.9298e+01 eta 0:01:04\n",
            "epoch [14/100] batch [1/1] time 0.741 (0.741) data 0.439 (0.439) loss 2.4376 (2.4376) acc 43.7500 (43.7500) lr 1.9178e+01 eta 0:01:03\n",
            "epoch [15/100] batch [1/1] time 0.703 (0.703) data 0.398 (0.398) loss 2.2989 (2.2989) acc 50.0000 (50.0000) lr 1.9048e+01 eta 0:00:59\n",
            "epoch [16/100] batch [1/1] time 0.722 (0.722) data 0.420 (0.420) loss 1.9114 (1.9114) acc 62.5000 (62.5000) lr 1.8910e+01 eta 0:01:00\n",
            "epoch [17/100] batch [1/1] time 0.740 (0.740) data 0.435 (0.435) loss 2.1946 (2.1946) acc 43.7500 (43.7500) lr 1.8763e+01 eta 0:01:01\n",
            "epoch [18/100] batch [1/1] time 0.706 (0.706) data 0.402 (0.402) loss 1.7817 (1.7817) acc 59.3750 (59.3750) lr 1.8607e+01 eta 0:00:57\n",
            "epoch [19/100] batch [1/1] time 0.703 (0.703) data 0.396 (0.396) loss 1.8665 (1.8665) acc 59.3750 (59.3750) lr 1.8443e+01 eta 0:00:56\n",
            "epoch [20/100] batch [1/1] time 0.709 (0.709) data 0.404 (0.404) loss 1.5532 (1.5532) acc 68.7500 (68.7500) lr 1.8271e+01 eta 0:00:56\n",
            "epoch [21/100] batch [1/1] time 0.725 (0.725) data 0.418 (0.418) loss 2.0220 (2.0220) acc 53.1250 (53.1250) lr 1.8090e+01 eta 0:00:57\n",
            "epoch [22/100] batch [1/1] time 0.732 (0.732) data 0.427 (0.427) loss 1.4967 (1.4967) acc 68.7500 (68.7500) lr 1.7902e+01 eta 0:00:57\n",
            "epoch [23/100] batch [1/1] time 0.706 (0.706) data 0.399 (0.399) loss 1.3882 (1.3882) acc 71.8750 (71.8750) lr 1.7705e+01 eta 0:00:54\n",
            "epoch [24/100] batch [1/1] time 0.702 (0.702) data 0.395 (0.395) loss 1.4844 (1.4844) acc 71.8750 (71.8750) lr 1.7501e+01 eta 0:00:53\n",
            "epoch [25/100] batch [1/1] time 0.701 (0.701) data 0.394 (0.394) loss 1.5888 (1.5888) acc 65.6250 (65.6250) lr 1.7290e+01 eta 0:00:52\n",
            "epoch [26/100] batch [1/1] time 0.726 (0.726) data 0.418 (0.418) loss 1.3706 (1.3706) acc 68.7500 (68.7500) lr 1.7071e+01 eta 0:00:53\n",
            "epoch [27/100] batch [1/1] time 0.733 (0.733) data 0.425 (0.425) loss 1.2383 (1.2383) acc 78.1250 (78.1250) lr 1.6845e+01 eta 0:00:53\n",
            "epoch [28/100] batch [1/1] time 0.730 (0.730) data 0.422 (0.422) loss 1.3511 (1.3511) acc 71.8750 (71.8750) lr 1.6613e+01 eta 0:00:52\n",
            "epoch [29/100] batch [1/1] time 0.736 (0.736) data 0.426 (0.426) loss 1.2201 (1.2201) acc 78.1250 (78.1250) lr 1.6374e+01 eta 0:00:52\n",
            "epoch [30/100] batch [1/1] time 0.742 (0.742) data 0.433 (0.433) loss 1.5431 (1.5431) acc 59.3750 (59.3750) lr 1.6129e+01 eta 0:00:51\n",
            "epoch [31/100] batch [1/1] time 0.731 (0.731) data 0.421 (0.421) loss 1.9784 (1.9784) acc 56.2500 (56.2500) lr 1.5878e+01 eta 0:00:50\n",
            "epoch [32/100] batch [1/1] time 0.728 (0.728) data 0.419 (0.419) loss 1.1644 (1.1644) acc 81.2500 (81.2500) lr 1.5621e+01 eta 0:00:49\n",
            "epoch [33/100] batch [1/1] time 0.690 (0.690) data 0.382 (0.382) loss 1.0102 (1.0102) acc 78.1250 (78.1250) lr 1.5358e+01 eta 0:00:46\n",
            "epoch [34/100] batch [1/1] time 0.718 (0.718) data 0.407 (0.407) loss 1.1460 (1.1460) acc 78.1250 (78.1250) lr 1.5090e+01 eta 0:00:47\n",
            "epoch [35/100] batch [1/1] time 0.726 (0.726) data 0.417 (0.417) loss 1.2928 (1.2928) acc 62.5000 (62.5000) lr 1.4818e+01 eta 0:00:47\n",
            "epoch [36/100] batch [1/1] time 0.715 (0.715) data 0.403 (0.403) loss 2.1997 (2.1997) acc 59.3750 (59.3750) lr 1.4540e+01 eta 0:00:45\n",
            "epoch [37/100] batch [1/1] time 0.726 (0.726) data 0.417 (0.417) loss 1.9001 (1.9001) acc 62.5000 (62.5000) lr 1.4258e+01 eta 0:00:45\n",
            "epoch [38/100] batch [1/1] time 0.713 (0.713) data 0.401 (0.401) loss 2.6436 (2.6436) acc 50.0000 (50.0000) lr 1.3971e+01 eta 0:00:44\n",
            "epoch [39/100] batch [1/1] time 0.728 (0.728) data 0.415 (0.415) loss 3.7483 (3.7483) acc 34.3750 (34.3750) lr 1.3681e+01 eta 0:00:44\n",
            "epoch [40/100] batch [1/1] time 0.712 (0.712) data 0.400 (0.400) loss 8.9433 (8.9433) acc 31.2500 (31.2500) lr 1.3387e+01 eta 0:00:42\n",
            "epoch [41/100] batch [1/1] time 0.752 (0.752) data 0.438 (0.438) loss 4.4606 (4.4606) acc 53.1250 (53.1250) lr 1.3090e+01 eta 0:00:44\n",
            "epoch [42/100] batch [1/1] time 0.725 (0.725) data 0.409 (0.409) loss 6.0138 (6.0138) acc 28.1250 (28.1250) lr 1.2790e+01 eta 0:00:42\n",
            "epoch [43/100] batch [1/1] time 0.701 (0.701) data 0.387 (0.387) loss 3.1668 (3.1668) acc 25.0000 (25.0000) lr 1.2487e+01 eta 0:00:39\n",
            "epoch [44/100] batch [1/1] time 0.783 (0.783) data 0.469 (0.469) loss 4.8705 (4.8705) acc 53.1250 (53.1250) lr 1.2181e+01 eta 0:00:43\n",
            "epoch [45/100] batch [1/1] time 0.734 (0.734) data 0.419 (0.419) loss 4.3512 (4.3512) acc 34.3750 (34.3750) lr 1.1874e+01 eta 0:00:40\n",
            "epoch [46/100] batch [1/1] time 0.769 (0.769) data 0.454 (0.454) loss 2.5021 (2.5021) acc 34.3750 (34.3750) lr 1.1564e+01 eta 0:00:41\n",
            "epoch [47/100] batch [1/1] time 0.730 (0.730) data 0.416 (0.416) loss 1.6959 (1.6959) acc 53.1250 (53.1250) lr 1.1253e+01 eta 0:00:38\n",
            "epoch [48/100] batch [1/1] time 0.719 (0.719) data 0.404 (0.404) loss 1.3214 (1.3214) acc 62.5000 (62.5000) lr 1.0941e+01 eta 0:00:37\n",
            "epoch [49/100] batch [1/1] time 0.742 (0.742) data 0.428 (0.428) loss 0.9318 (0.9318) acc 78.1250 (78.1250) lr 1.0628e+01 eta 0:00:37\n",
            "epoch [50/100] batch [1/1] time 0.726 (0.726) data 0.414 (0.414) loss 1.1405 (1.1405) acc 75.0000 (75.0000) lr 1.0314e+01 eta 0:00:36\n",
            "epoch [51/100] batch [1/1] time 0.737 (0.737) data 0.424 (0.424) loss 1.3605 (1.3605) acc 68.7500 (68.7500) lr 1.0000e+01 eta 0:00:36\n",
            "epoch [52/100] batch [1/1] time 0.723 (0.723) data 0.410 (0.410) loss 1.6233 (1.6233) acc 53.1250 (53.1250) lr 9.6859e+00 eta 0:00:34\n",
            "epoch [53/100] batch [1/1] time 0.729 (0.729) data 0.418 (0.418) loss 1.2420 (1.2420) acc 71.8750 (71.8750) lr 9.3721e+00 eta 0:00:34\n",
            "epoch [54/100] batch [1/1] time 0.728 (0.728) data 0.413 (0.413) loss 1.0312 (1.0312) acc 81.2500 (81.2500) lr 9.0589e+00 eta 0:00:33\n",
            "epoch [55/100] batch [1/1] time 0.718 (0.718) data 0.405 (0.405) loss 0.9688 (0.9688) acc 75.0000 (75.0000) lr 8.7467e+00 eta 0:00:32\n",
            "epoch [56/100] batch [1/1] time 0.734 (0.734) data 0.425 (0.425) loss 0.5853 (0.5853) acc 87.5000 (87.5000) lr 8.4357e+00 eta 0:00:32\n",
            "epoch [57/100] batch [1/1] time 0.718 (0.718) data 0.405 (0.405) loss 0.8862 (0.8862) acc 84.3750 (84.3750) lr 8.1262e+00 eta 0:00:30\n",
            "epoch [58/100] batch [1/1] time 0.730 (0.730) data 0.417 (0.417) loss 0.8404 (0.8404) acc 84.3750 (84.3750) lr 7.8186e+00 eta 0:00:30\n",
            "epoch [59/100] batch [1/1] time 0.738 (0.738) data 0.424 (0.424) loss 0.6650 (0.6650) acc 90.6250 (90.6250) lr 7.5131e+00 eta 0:00:30\n",
            "epoch [60/100] batch [1/1] time 0.730 (0.730) data 0.419 (0.419) loss 0.7041 (0.7041) acc 87.5000 (87.5000) lr 7.2101e+00 eta 0:00:29\n",
            "epoch [61/100] batch [1/1] time 0.720 (0.720) data 0.410 (0.410) loss 0.5877 (0.5877) acc 93.7500 (93.7500) lr 6.9098e+00 eta 0:00:28\n",
            "epoch [62/100] batch [1/1] time 0.733 (0.733) data 0.423 (0.423) loss 0.9981 (0.9981) acc 81.2500 (81.2500) lr 6.6126e+00 eta 0:00:27\n",
            "epoch [63/100] batch [1/1] time 0.740 (0.740) data 0.429 (0.429) loss 0.7288 (0.7288) acc 87.5000 (87.5000) lr 6.3188e+00 eta 0:00:27\n",
            "epoch [64/100] batch [1/1] time 0.711 (0.711) data 0.402 (0.402) loss 0.6379 (0.6379) acc 87.5000 (87.5000) lr 6.0285e+00 eta 0:00:25\n",
            "epoch [65/100] batch [1/1] time 0.712 (0.712) data 0.402 (0.402) loss 0.6301 (0.6301) acc 90.6250 (90.6250) lr 5.7422e+00 eta 0:00:24\n",
            "epoch [66/100] batch [1/1] time 0.705 (0.705) data 0.396 (0.396) loss 0.5247 (0.5247) acc 90.6250 (90.6250) lr 5.4601e+00 eta 0:00:23\n",
            "epoch [67/100] batch [1/1] time 0.725 (0.725) data 0.415 (0.415) loss 0.5529 (0.5529) acc 87.5000 (87.5000) lr 5.1825e+00 eta 0:00:23\n",
            "epoch [68/100] batch [1/1] time 0.712 (0.712) data 0.406 (0.406) loss 0.4263 (0.4263) acc 100.0000 (100.0000) lr 4.9096e+00 eta 0:00:22\n",
            "epoch [69/100] batch [1/1] time 0.712 (0.712) data 0.403 (0.403) loss 0.4432 (0.4432) acc 96.8750 (96.8750) lr 4.6417e+00 eta 0:00:22\n",
            "epoch [70/100] batch [1/1] time 0.718 (0.718) data 0.409 (0.409) loss 0.5334 (0.5334) acc 93.7500 (93.7500) lr 4.3792e+00 eta 0:00:21\n",
            "epoch [71/100] batch [1/1] time 0.717 (0.717) data 0.408 (0.408) loss 0.5776 (0.5776) acc 93.7500 (93.7500) lr 4.1221e+00 eta 0:00:20\n",
            "epoch [72/100] batch [1/1] time 0.701 (0.701) data 0.395 (0.395) loss 0.6261 (0.6261) acc 90.6250 (90.6250) lr 3.8709e+00 eta 0:00:19\n",
            "epoch [73/100] batch [1/1] time 0.727 (0.727) data 0.419 (0.419) loss 0.4326 (0.4326) acc 96.8750 (96.8750) lr 3.6258e+00 eta 0:00:19\n",
            "epoch [74/100] batch [1/1] time 0.713 (0.713) data 0.407 (0.407) loss 0.4686 (0.4686) acc 96.8750 (96.8750) lr 3.3869e+00 eta 0:00:18\n",
            "epoch [75/100] batch [1/1] time 0.720 (0.720) data 0.412 (0.412) loss 0.5320 (0.5320) acc 84.3750 (84.3750) lr 3.1545e+00 eta 0:00:17\n",
            "epoch [76/100] batch [1/1] time 0.746 (0.746) data 0.438 (0.438) loss 0.4693 (0.4693) acc 93.7500 (93.7500) lr 2.9289e+00 eta 0:00:17\n",
            "epoch [77/100] batch [1/1] time 0.722 (0.722) data 0.416 (0.416) loss 0.5127 (0.5127) acc 90.6250 (90.6250) lr 2.7103e+00 eta 0:00:16\n",
            "epoch [78/100] batch [1/1] time 0.738 (0.738) data 0.433 (0.433) loss 0.3893 (0.3893) acc 96.8750 (96.8750) lr 2.4989e+00 eta 0:00:16\n",
            "epoch [79/100] batch [1/1] time 0.706 (0.706) data 0.399 (0.399) loss 0.5882 (0.5882) acc 90.6250 (90.6250) lr 2.2949e+00 eta 0:00:14\n",
            "epoch [80/100] batch [1/1] time 0.708 (0.708) data 0.403 (0.403) loss 0.5364 (0.5364) acc 90.6250 (90.6250) lr 2.0984e+00 eta 0:00:14\n",
            "epoch [81/100] batch [1/1] time 0.724 (0.724) data 0.420 (0.420) loss 0.4257 (0.4257) acc 93.7500 (93.7500) lr 1.9098e+00 eta 0:00:13\n",
            "epoch [82/100] batch [1/1] time 0.692 (0.692) data 0.387 (0.387) loss 0.5874 (0.5874) acc 90.6250 (90.6250) lr 1.7292e+00 eta 0:00:12\n",
            "epoch [83/100] batch [1/1] time 0.713 (0.713) data 0.407 (0.407) loss 0.4934 (0.4934) acc 96.8750 (96.8750) lr 1.5567e+00 eta 0:00:12\n",
            "epoch [84/100] batch [1/1] time 0.703 (0.703) data 0.398 (0.398) loss 0.3351 (0.3351) acc 100.0000 (100.0000) lr 1.3926e+00 eta 0:00:11\n",
            "epoch [85/100] batch [1/1] time 0.680 (0.680) data 0.376 (0.376) loss 0.4182 (0.4182) acc 96.8750 (96.8750) lr 1.2369e+00 eta 0:00:10\n",
            "epoch [86/100] batch [1/1] time 0.704 (0.704) data 0.399 (0.399) loss 0.3854 (0.3854) acc 93.7500 (93.7500) lr 1.0899e+00 eta 0:00:09\n",
            "epoch [87/100] batch [1/1] time 0.688 (0.688) data 0.383 (0.383) loss 0.3356 (0.3356) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:08\n",
            "epoch [88/100] batch [1/1] time 0.717 (0.717) data 0.414 (0.414) loss 0.5672 (0.5672) acc 87.5000 (87.5000) lr 8.2245e-01 eta 0:00:08\n",
            "epoch [89/100] batch [1/1] time 0.708 (0.708) data 0.404 (0.404) loss 0.5065 (0.5065) acc 90.6250 (90.6250) lr 7.0224e-01 eta 0:00:07\n",
            "epoch [90/100] batch [1/1] time 0.701 (0.701) data 0.397 (0.397) loss 0.5158 (0.5158) acc 93.7500 (93.7500) lr 5.9119e-01 eta 0:00:07\n",
            "epoch [91/100] batch [1/1] time 0.695 (0.695) data 0.391 (0.391) loss 0.3984 (0.3984) acc 96.8750 (96.8750) lr 4.8943e-01 eta 0:00:06\n",
            "epoch [92/100] batch [1/1] time 0.716 (0.716) data 0.411 (0.411) loss 0.5801 (0.5801) acc 90.6250 (90.6250) lr 3.9706e-01 eta 0:00:05\n",
            "epoch [93/100] batch [1/1] time 0.743 (0.743) data 0.439 (0.439) loss 0.4196 (0.4196) acc 96.8750 (96.8750) lr 3.1417e-01 eta 0:00:05\n",
            "epoch [94/100] batch [1/1] time 0.737 (0.737) data 0.432 (0.432) loss 0.5358 (0.5358) acc 90.6250 (90.6250) lr 2.4083e-01 eta 0:00:04\n",
            "epoch [95/100] batch [1/1] time 0.696 (0.696) data 0.393 (0.393) loss 0.4066 (0.4066) acc 93.7500 (93.7500) lr 1.7713e-01 eta 0:00:03\n",
            "epoch [96/100] batch [1/1] time 0.709 (0.709) data 0.403 (0.403) loss 0.4197 (0.4197) acc 96.8750 (96.8750) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.697 (0.697) data 0.393 (0.393) loss 0.5517 (0.5517) acc 90.6250 (90.6250) lr 7.8853e-02 eta 0:00:02\n",
            "epoch [98/100] batch [1/1] time 0.714 (0.714) data 0.409 (0.409) loss 0.3658 (0.3658) acc 93.7500 (93.7500) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.706 (0.706) data 0.403 (0.403) loss 0.3327 (0.3327) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.698 (0.698) data 0.392 (0.392) loss 0.4668 (0.4668) acc 93.7500 (93.7500) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed3/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:33<00:00,  2.45it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 6,379\n",
            "* accuracy: 78.8%\n",
            "* error: 21.2%\n",
            "* macro_f1: 78.7%\n",
            "Elapsed: 0:01:53\n"
          ]
        }
      ],
      "source": [
        "#eurosat-4shots-seed3\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 3 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_init/eurosat/DAPT/vit_b16_ep100_4shots/seed3 \\\n",
        "        DATASET.NUM_SHOTS 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "RgW5NPS-v9KQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38eaee3b-21d6-4578-a52e-ce4d9042fab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 15:58:57.805611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 15:58:57.826162: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 15:58:57.832144: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 15:58:57.846915: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 15:58:58.891856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2']\n",
            "output_dir: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  20\n",
            "# val      20\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 1.461 (1.461) data 0.433 (0.433) loss 8.7315 (8.7315) acc 20.0000 (20.0000) lr 2.0000e+01 eta 0:02:24\n",
            "epoch [2/100] batch [1/1] time 0.548 (0.548) data 0.342 (0.342) loss 8.7490 (8.7490) acc 30.0000 (30.0000) lr 1.9995e+01 eta 0:00:53\n",
            "epoch [3/100] batch [1/1] time 0.537 (0.537) data 0.329 (0.329) loss 7.7860 (7.7860) acc 20.0000 (20.0000) lr 1.9980e+01 eta 0:00:52\n",
            "epoch [4/100] batch [1/1] time 0.561 (0.561) data 0.354 (0.354) loss 6.8719 (6.8719) acc 15.0000 (15.0000) lr 1.9956e+01 eta 0:00:53\n",
            "epoch [5/100] batch [1/1] time 0.558 (0.558) data 0.351 (0.351) loss 9.0434 (9.0434) acc 10.0000 (10.0000) lr 1.9921e+01 eta 0:00:53\n",
            "epoch [6/100] batch [1/1] time 0.547 (0.547) data 0.338 (0.338) loss 7.4913 (7.4913) acc 10.0000 (10.0000) lr 1.9877e+01 eta 0:00:51\n",
            "epoch [7/100] batch [1/1] time 0.552 (0.552) data 0.348 (0.348) loss 8.3489 (8.3489) acc 10.0000 (10.0000) lr 1.9823e+01 eta 0:00:51\n",
            "epoch [8/100] batch [1/1] time 0.537 (0.537) data 0.331 (0.331) loss 8.0165 (8.0165) acc 10.0000 (10.0000) lr 1.9759e+01 eta 0:00:49\n",
            "epoch [9/100] batch [1/1] time 0.551 (0.551) data 0.342 (0.342) loss 7.3294 (7.3294) acc 25.0000 (25.0000) lr 1.9686e+01 eta 0:00:50\n",
            "epoch [10/100] batch [1/1] time 0.553 (0.553) data 0.345 (0.345) loss 6.2627 (6.2627) acc 20.0000 (20.0000) lr 1.9603e+01 eta 0:00:49\n",
            "epoch [11/100] batch [1/1] time 0.561 (0.561) data 0.352 (0.352) loss 5.1215 (5.1215) acc 15.0000 (15.0000) lr 1.9511e+01 eta 0:00:49\n",
            "epoch [12/100] batch [1/1] time 0.581 (0.581) data 0.371 (0.371) loss 4.3324 (4.3324) acc 15.0000 (15.0000) lr 1.9409e+01 eta 0:00:51\n",
            "epoch [13/100] batch [1/1] time 0.587 (0.587) data 0.380 (0.380) loss 3.4138 (3.4138) acc 25.0000 (25.0000) lr 1.9298e+01 eta 0:00:51\n",
            "epoch [14/100] batch [1/1] time 0.544 (0.544) data 0.335 (0.335) loss 2.9130 (2.9130) acc 20.0000 (20.0000) lr 1.9178e+01 eta 0:00:46\n",
            "epoch [15/100] batch [1/1] time 0.536 (0.536) data 0.329 (0.329) loss 3.4810 (3.4810) acc 20.0000 (20.0000) lr 1.9048e+01 eta 0:00:45\n",
            "epoch [16/100] batch [1/1] time 0.541 (0.541) data 0.332 (0.332) loss 3.4976 (3.4976) acc 35.0000 (35.0000) lr 1.8910e+01 eta 0:00:45\n",
            "epoch [17/100] batch [1/1] time 0.547 (0.547) data 0.338 (0.338) loss 3.2212 (3.2212) acc 35.0000 (35.0000) lr 1.8763e+01 eta 0:00:45\n",
            "epoch [18/100] batch [1/1] time 0.551 (0.551) data 0.343 (0.343) loss 2.7983 (2.7983) acc 40.0000 (40.0000) lr 1.8607e+01 eta 0:00:45\n",
            "epoch [19/100] batch [1/1] time 0.537 (0.537) data 0.331 (0.331) loss 2.3762 (2.3762) acc 60.0000 (60.0000) lr 1.8443e+01 eta 0:00:43\n",
            "epoch [20/100] batch [1/1] time 0.541 (0.541) data 0.332 (0.332) loss 2.5890 (2.5890) acc 50.0000 (50.0000) lr 1.8271e+01 eta 0:00:43\n",
            "epoch [21/100] batch [1/1] time 0.540 (0.540) data 0.330 (0.330) loss 2.3517 (2.3517) acc 55.0000 (55.0000) lr 1.8090e+01 eta 0:00:42\n",
            "epoch [22/100] batch [1/1] time 0.542 (0.542) data 0.334 (0.334) loss 1.9678 (1.9678) acc 70.0000 (70.0000) lr 1.7902e+01 eta 0:00:42\n",
            "epoch [23/100] batch [1/1] time 0.573 (0.573) data 0.360 (0.360) loss 1.6101 (1.6101) acc 75.0000 (75.0000) lr 1.7705e+01 eta 0:00:44\n",
            "epoch [24/100] batch [1/1] time 0.567 (0.567) data 0.361 (0.361) loss 1.7265 (1.7265) acc 75.0000 (75.0000) lr 1.7501e+01 eta 0:00:43\n",
            "epoch [25/100] batch [1/1] time 0.557 (0.557) data 0.346 (0.346) loss 1.6822 (1.6822) acc 65.0000 (65.0000) lr 1.7290e+01 eta 0:00:41\n",
            "epoch [26/100] batch [1/1] time 0.565 (0.565) data 0.357 (0.357) loss 1.8126 (1.8126) acc 50.0000 (50.0000) lr 1.7071e+01 eta 0:00:41\n",
            "epoch [27/100] batch [1/1] time 0.545 (0.545) data 0.336 (0.336) loss 1.5021 (1.5021) acc 70.0000 (70.0000) lr 1.6845e+01 eta 0:00:39\n",
            "epoch [28/100] batch [1/1] time 0.561 (0.561) data 0.351 (0.351) loss 1.2238 (1.2238) acc 85.0000 (85.0000) lr 1.6613e+01 eta 0:00:40\n",
            "epoch [29/100] batch [1/1] time 0.539 (0.539) data 0.328 (0.328) loss 1.2103 (1.2103) acc 70.0000 (70.0000) lr 1.6374e+01 eta 0:00:38\n",
            "epoch [30/100] batch [1/1] time 0.566 (0.566) data 0.357 (0.357) loss 1.4042 (1.4042) acc 65.0000 (65.0000) lr 1.6129e+01 eta 0:00:39\n",
            "epoch [31/100] batch [1/1] time 0.583 (0.583) data 0.375 (0.375) loss 1.0765 (1.0765) acc 80.0000 (80.0000) lr 1.5878e+01 eta 0:00:40\n",
            "epoch [32/100] batch [1/1] time 0.562 (0.562) data 0.349 (0.349) loss 1.3584 (1.3584) acc 70.0000 (70.0000) lr 1.5621e+01 eta 0:00:38\n",
            "epoch [33/100] batch [1/1] time 0.597 (0.597) data 0.385 (0.385) loss 1.2992 (1.2992) acc 70.0000 (70.0000) lr 1.5358e+01 eta 0:00:40\n",
            "epoch [34/100] batch [1/1] time 0.537 (0.537) data 0.327 (0.327) loss 1.3197 (1.3197) acc 75.0000 (75.0000) lr 1.5090e+01 eta 0:00:35\n",
            "epoch [35/100] batch [1/1] time 0.547 (0.547) data 0.334 (0.334) loss 1.1515 (1.1515) acc 80.0000 (80.0000) lr 1.4818e+01 eta 0:00:35\n",
            "epoch [36/100] batch [1/1] time 0.544 (0.544) data 0.333 (0.333) loss 2.0084 (2.0084) acc 55.0000 (55.0000) lr 1.4540e+01 eta 0:00:34\n",
            "epoch [37/100] batch [1/1] time 0.548 (0.548) data 0.336 (0.336) loss 1.1865 (1.1865) acc 75.0000 (75.0000) lr 1.4258e+01 eta 0:00:34\n",
            "epoch [38/100] batch [1/1] time 0.549 (0.549) data 0.337 (0.337) loss 3.3973 (3.3973) acc 55.0000 (55.0000) lr 1.3971e+01 eta 0:00:34\n",
            "epoch [39/100] batch [1/1] time 0.554 (0.554) data 0.343 (0.343) loss 4.3229 (4.3229) acc 35.0000 (35.0000) lr 1.3681e+01 eta 0:00:33\n",
            "epoch [40/100] batch [1/1] time 0.541 (0.541) data 0.329 (0.329) loss 3.3248 (3.3248) acc 40.0000 (40.0000) lr 1.3387e+01 eta 0:00:32\n",
            "epoch [41/100] batch [1/1] time 0.571 (0.571) data 0.359 (0.359) loss 3.8761 (3.8761) acc 40.0000 (40.0000) lr 1.3090e+01 eta 0:00:33\n",
            "epoch [42/100] batch [1/1] time 0.548 (0.548) data 0.337 (0.337) loss 4.4561 (4.4561) acc 50.0000 (50.0000) lr 1.2790e+01 eta 0:00:31\n",
            "epoch [43/100] batch [1/1] time 0.574 (0.574) data 0.359 (0.359) loss 4.5127 (4.5127) acc 30.0000 (30.0000) lr 1.2487e+01 eta 0:00:32\n",
            "epoch [44/100] batch [1/1] time 0.547 (0.547) data 0.332 (0.332) loss 4.5067 (4.5067) acc 30.0000 (30.0000) lr 1.2181e+01 eta 0:00:30\n",
            "epoch [45/100] batch [1/1] time 0.552 (0.552) data 0.339 (0.339) loss 3.1237 (3.1237) acc 50.0000 (50.0000) lr 1.1874e+01 eta 0:00:30\n",
            "epoch [46/100] batch [1/1] time 0.556 (0.556) data 0.344 (0.344) loss 2.4922 (2.4922) acc 45.0000 (45.0000) lr 1.1564e+01 eta 0:00:30\n",
            "epoch [47/100] batch [1/1] time 0.548 (0.548) data 0.337 (0.337) loss 2.7284 (2.7284) acc 45.0000 (45.0000) lr 1.1253e+01 eta 0:00:29\n",
            "epoch [48/100] batch [1/1] time 0.540 (0.540) data 0.325 (0.325) loss 1.4404 (1.4404) acc 70.0000 (70.0000) lr 1.0941e+01 eta 0:00:28\n",
            "epoch [49/100] batch [1/1] time 0.544 (0.544) data 0.330 (0.330) loss 1.2663 (1.2663) acc 75.0000 (75.0000) lr 1.0628e+01 eta 0:00:27\n",
            "epoch [50/100] batch [1/1] time 0.553 (0.553) data 0.339 (0.339) loss 0.7040 (0.7040) acc 95.0000 (95.0000) lr 1.0314e+01 eta 0:00:27\n",
            "epoch [51/100] batch [1/1] time 0.572 (0.572) data 0.360 (0.360) loss 1.0085 (1.0085) acc 75.0000 (75.0000) lr 1.0000e+01 eta 0:00:28\n",
            "epoch [52/100] batch [1/1] time 0.557 (0.557) data 0.353 (0.353) loss 1.1301 (1.1301) acc 80.0000 (80.0000) lr 9.6859e+00 eta 0:00:26\n",
            "epoch [53/100] batch [1/1] time 0.562 (0.562) data 0.349 (0.349) loss 0.8648 (0.8648) acc 85.0000 (85.0000) lr 9.3721e+00 eta 0:00:26\n",
            "epoch [54/100] batch [1/1] time 0.550 (0.550) data 0.336 (0.336) loss 0.9232 (0.9232) acc 85.0000 (85.0000) lr 9.0589e+00 eta 0:00:25\n",
            "epoch [55/100] batch [1/1] time 0.572 (0.572) data 0.356 (0.356) loss 0.9484 (0.9484) acc 80.0000 (80.0000) lr 8.7467e+00 eta 0:00:25\n",
            "epoch [56/100] batch [1/1] time 0.563 (0.563) data 0.347 (0.347) loss 0.7787 (0.7787) acc 80.0000 (80.0000) lr 8.4357e+00 eta 0:00:24\n",
            "epoch [57/100] batch [1/1] time 0.546 (0.546) data 0.332 (0.332) loss 0.8611 (0.8611) acc 85.0000 (85.0000) lr 8.1262e+00 eta 0:00:23\n",
            "epoch [58/100] batch [1/1] time 0.573 (0.573) data 0.360 (0.360) loss 0.5388 (0.5388) acc 95.0000 (95.0000) lr 7.8186e+00 eta 0:00:24\n",
            "epoch [59/100] batch [1/1] time 0.551 (0.551) data 0.334 (0.334) loss 0.5016 (0.5016) acc 95.0000 (95.0000) lr 7.5131e+00 eta 0:00:22\n",
            "epoch [60/100] batch [1/1] time 0.553 (0.553) data 0.340 (0.340) loss 0.5225 (0.5225) acc 95.0000 (95.0000) lr 7.2101e+00 eta 0:00:22\n",
            "epoch [61/100] batch [1/1] time 0.559 (0.559) data 0.343 (0.343) loss 0.6613 (0.6613) acc 90.0000 (90.0000) lr 6.9098e+00 eta 0:00:21\n",
            "epoch [62/100] batch [1/1] time 0.560 (0.560) data 0.344 (0.344) loss 0.5561 (0.5561) acc 90.0000 (90.0000) lr 6.6126e+00 eta 0:00:21\n",
            "epoch [63/100] batch [1/1] time 0.574 (0.574) data 0.360 (0.360) loss 0.6548 (0.6548) acc 90.0000 (90.0000) lr 6.3188e+00 eta 0:00:21\n",
            "epoch [64/100] batch [1/1] time 0.538 (0.538) data 0.324 (0.324) loss 0.4019 (0.4019) acc 95.0000 (95.0000) lr 6.0285e+00 eta 0:00:19\n",
            "epoch [65/100] batch [1/1] time 0.555 (0.555) data 0.339 (0.339) loss 0.4876 (0.4876) acc 95.0000 (95.0000) lr 5.7422e+00 eta 0:00:19\n",
            "epoch [66/100] batch [1/1] time 0.543 (0.543) data 0.329 (0.329) loss 0.4577 (0.4577) acc 95.0000 (95.0000) lr 5.4601e+00 eta 0:00:18\n",
            "epoch [67/100] batch [1/1] time 0.551 (0.551) data 0.338 (0.338) loss 0.3353 (0.3353) acc 100.0000 (100.0000) lr 5.1825e+00 eta 0:00:18\n",
            "epoch [68/100] batch [1/1] time 0.567 (0.567) data 0.354 (0.354) loss 0.4445 (0.4445) acc 95.0000 (95.0000) lr 4.9096e+00 eta 0:00:18\n",
            "epoch [69/100] batch [1/1] time 0.557 (0.557) data 0.342 (0.342) loss 0.3951 (0.3951) acc 100.0000 (100.0000) lr 4.6417e+00 eta 0:00:17\n",
            "epoch [70/100] batch [1/1] time 0.588 (0.588) data 0.373 (0.373) loss 0.5668 (0.5668) acc 85.0000 (85.0000) lr 4.3792e+00 eta 0:00:17\n",
            "epoch [71/100] batch [1/1] time 0.558 (0.558) data 0.343 (0.343) loss 0.6462 (0.6462) acc 90.0000 (90.0000) lr 4.1221e+00 eta 0:00:16\n",
            "epoch [72/100] batch [1/1] time 0.559 (0.559) data 0.347 (0.347) loss 0.3645 (0.3645) acc 100.0000 (100.0000) lr 3.8709e+00 eta 0:00:15\n",
            "epoch [73/100] batch [1/1] time 0.567 (0.567) data 0.354 (0.354) loss 0.3098 (0.3098) acc 100.0000 (100.0000) lr 3.6258e+00 eta 0:00:15\n",
            "epoch [74/100] batch [1/1] time 0.548 (0.548) data 0.332 (0.332) loss 0.3805 (0.3805) acc 100.0000 (100.0000) lr 3.3869e+00 eta 0:00:14\n",
            "epoch [75/100] batch [1/1] time 0.553 (0.553) data 0.340 (0.340) loss 0.3244 (0.3244) acc 100.0000 (100.0000) lr 3.1545e+00 eta 0:00:13\n",
            "epoch [76/100] batch [1/1] time 0.549 (0.549) data 0.334 (0.334) loss 0.3395 (0.3395) acc 100.0000 (100.0000) lr 2.9289e+00 eta 0:00:13\n",
            "epoch [77/100] batch [1/1] time 0.546 (0.546) data 0.332 (0.332) loss 0.4365 (0.4365) acc 90.0000 (90.0000) lr 2.7103e+00 eta 0:00:12\n",
            "epoch [78/100] batch [1/1] time 0.567 (0.567) data 0.353 (0.353) loss 0.4188 (0.4188) acc 95.0000 (95.0000) lr 2.4989e+00 eta 0:00:12\n",
            "epoch [79/100] batch [1/1] time 0.548 (0.548) data 0.336 (0.336) loss 0.3128 (0.3128) acc 100.0000 (100.0000) lr 2.2949e+00 eta 0:00:11\n",
            "epoch [80/100] batch [1/1] time 0.560 (0.560) data 0.346 (0.346) loss 0.3084 (0.3084) acc 100.0000 (100.0000) lr 2.0984e+00 eta 0:00:11\n",
            "epoch [81/100] batch [1/1] time 0.542 (0.542) data 0.328 (0.328) loss 0.3970 (0.3970) acc 95.0000 (95.0000) lr 1.9098e+00 eta 0:00:10\n",
            "epoch [82/100] batch [1/1] time 0.555 (0.555) data 0.342 (0.342) loss 0.2956 (0.2956) acc 100.0000 (100.0000) lr 1.7292e+00 eta 0:00:09\n",
            "epoch [83/100] batch [1/1] time 0.556 (0.556) data 0.340 (0.340) loss 0.4847 (0.4847) acc 95.0000 (95.0000) lr 1.5567e+00 eta 0:00:09\n",
            "epoch [84/100] batch [1/1] time 0.554 (0.554) data 0.341 (0.341) loss 0.5832 (0.5832) acc 95.0000 (95.0000) lr 1.3926e+00 eta 0:00:08\n",
            "epoch [85/100] batch [1/1] time 0.543 (0.543) data 0.327 (0.327) loss 0.3680 (0.3680) acc 95.0000 (95.0000) lr 1.2369e+00 eta 0:00:08\n",
            "epoch [86/100] batch [1/1] time 0.563 (0.563) data 0.351 (0.351) loss 0.3035 (0.3035) acc 100.0000 (100.0000) lr 1.0899e+00 eta 0:00:07\n",
            "epoch [87/100] batch [1/1] time 0.558 (0.558) data 0.347 (0.347) loss 0.2952 (0.2952) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:07\n",
            "epoch [88/100] batch [1/1] time 0.558 (0.558) data 0.345 (0.345) loss 0.4167 (0.4167) acc 95.0000 (95.0000) lr 8.2245e-01 eta 0:00:06\n",
            "epoch [89/100] batch [1/1] time 0.576 (0.576) data 0.363 (0.363) loss 0.2917 (0.2917) acc 100.0000 (100.0000) lr 7.0224e-01 eta 0:00:06\n",
            "epoch [90/100] batch [1/1] time 0.567 (0.567) data 0.355 (0.355) loss 0.3091 (0.3091) acc 100.0000 (100.0000) lr 5.9119e-01 eta 0:00:05\n",
            "epoch [91/100] batch [1/1] time 0.562 (0.562) data 0.351 (0.351) loss 0.5178 (0.5178) acc 85.0000 (85.0000) lr 4.8943e-01 eta 0:00:05\n",
            "epoch [92/100] batch [1/1] time 0.595 (0.595) data 0.383 (0.383) loss 0.2912 (0.2912) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:04\n",
            "epoch [93/100] batch [1/1] time 0.565 (0.565) data 0.353 (0.353) loss 0.2925 (0.2925) acc 100.0000 (100.0000) lr 3.1417e-01 eta 0:00:03\n",
            "epoch [94/100] batch [1/1] time 0.572 (0.572) data 0.359 (0.359) loss 0.3007 (0.3007) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:03\n",
            "epoch [95/100] batch [1/1] time 0.563 (0.563) data 0.351 (0.351) loss 0.3479 (0.3479) acc 95.0000 (95.0000) lr 1.7713e-01 eta 0:00:02\n",
            "epoch [96/100] batch [1/1] time 0.557 (0.557) data 0.343 (0.343) loss 0.2797 (0.2797) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.571 (0.571) data 0.360 (0.360) loss 0.2863 (0.2863) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:01\n",
            "epoch [98/100] batch [1/1] time 0.558 (0.558) data 0.346 (0.346) loss 0.2975 (0.2975) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.582 (0.582) data 0.369 (0.369) loss 0.3172 (0.3172) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.569 (0.569) data 0.359 (0.359) loss 0.3101 (0.3101) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.46it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 4,389\n",
            "* accuracy: 54.2%\n",
            "* error: 45.8%\n",
            "* macro_f1: 52.4%\n",
            "Elapsed: 0:01:36\n"
          ]
        }
      ],
      "source": [
        "#eurosat-2shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "z2wTiCjlLXd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6dba9d-3e38-4970-df53-53e9d0174adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 16:00:54.163164: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 16:00:54.182724: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 16:00:54.188614: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 16:00:54.202733: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 16:00:55.214360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2']\n",
            "output_dir: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed2\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  20\n",
            "# val      20\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0173, -0.5723,  0.1752,  ...,  0.4941,  0.4187,  0.0375],\n",
            "         [ 0.0214, -0.5706,  0.1699,  ...,  0.5021,  0.4206,  0.0425],\n",
            "         [ 0.0149, -0.5713,  0.1720,  ...,  0.4970,  0.4210,  0.0462],\n",
            "         ...,\n",
            "         [ 0.0324, -0.5680,  0.1748,  ...,  0.5016,  0.4188,  0.0404],\n",
            "         [ 0.0201, -0.5696,  0.1726,  ...,  0.4989,  0.4190,  0.0354],\n",
            "         [ 0.0228, -0.5696,  0.1684,  ...,  0.5053,  0.4212,  0.0412]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5609\n",
            "  Max: 1.0111\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]],\n",
            "\n",
            "        [[ 0.0119, -0.0026, -0.0250,  ..., -0.0004,  0.0113,  0.0125],\n",
            "         [ 0.0055, -0.0176, -0.0244,  ..., -0.0013,  0.0060,  0.0016],\n",
            "         [ 0.0094,  0.0010, -0.0152,  ..., -0.0031,  0.0068,  0.0049],\n",
            "         ...,\n",
            "         [ 0.0002, -0.0079, -0.0239,  ..., -0.0015,  0.0120, -0.0039],\n",
            "         [ 0.0110, -0.0074, -0.0237,  ...,  0.0004,  0.0057, -0.0025],\n",
            "         [-0.0026, -0.0114, -0.0208,  ..., -0.0027,  0.0040, -0.0072]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0400\n",
            "  Min: -0.4519\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed2/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 1.468 (1.468) data 0.424 (0.424) loss 8.5885 (8.5885) acc 35.0000 (35.0000) lr 2.0000e+01 eta 0:02:25\n",
            "epoch [2/100] batch [1/1] time 0.535 (0.535) data 0.326 (0.326) loss 8.6489 (8.6489) acc 35.0000 (35.0000) lr 1.9995e+01 eta 0:00:52\n",
            "epoch [3/100] batch [1/1] time 0.552 (0.552) data 0.345 (0.345) loss 7.2881 (7.2881) acc 10.0000 (10.0000) lr 1.9980e+01 eta 0:00:53\n",
            "epoch [4/100] batch [1/1] time 0.555 (0.555) data 0.350 (0.350) loss 5.5827 (5.5827) acc 15.0000 (15.0000) lr 1.9956e+01 eta 0:00:53\n",
            "epoch [5/100] batch [1/1] time 0.540 (0.540) data 0.337 (0.337) loss 7.3549 (7.3549) acc 10.0000 (10.0000) lr 1.9921e+01 eta 0:00:51\n",
            "epoch [6/100] batch [1/1] time 0.541 (0.541) data 0.336 (0.336) loss 6.6850 (6.6850) acc 10.0000 (10.0000) lr 1.9877e+01 eta 0:00:50\n",
            "epoch [7/100] batch [1/1] time 0.559 (0.559) data 0.351 (0.351) loss 6.2346 (6.2346) acc 10.0000 (10.0000) lr 1.9823e+01 eta 0:00:51\n",
            "epoch [8/100] batch [1/1] time 0.542 (0.542) data 0.335 (0.335) loss 5.1961 (5.1961) acc 15.0000 (15.0000) lr 1.9759e+01 eta 0:00:49\n",
            "epoch [9/100] batch [1/1] time 0.579 (0.579) data 0.371 (0.371) loss 4.3650 (4.3650) acc 10.0000 (10.0000) lr 1.9686e+01 eta 0:00:52\n",
            "epoch [10/100] batch [1/1] time 0.567 (0.567) data 0.360 (0.360) loss 3.9956 (3.9956) acc 15.0000 (15.0000) lr 1.9603e+01 eta 0:00:51\n",
            "epoch [11/100] batch [1/1] time 0.577 (0.577) data 0.369 (0.369) loss 3.8283 (3.8283) acc 10.0000 (10.0000) lr 1.9511e+01 eta 0:00:51\n",
            "epoch [12/100] batch [1/1] time 0.580 (0.580) data 0.374 (0.374) loss 3.8584 (3.8584) acc 15.0000 (15.0000) lr 1.9409e+01 eta 0:00:51\n",
            "epoch [13/100] batch [1/1] time 0.568 (0.568) data 0.359 (0.359) loss 3.6020 (3.6020) acc 15.0000 (15.0000) lr 1.9298e+01 eta 0:00:49\n",
            "epoch [14/100] batch [1/1] time 0.550 (0.550) data 0.341 (0.341) loss 3.3939 (3.3939) acc 25.0000 (25.0000) lr 1.9178e+01 eta 0:00:47\n",
            "epoch [15/100] batch [1/1] time 0.556 (0.556) data 0.351 (0.351) loss 3.2554 (3.2554) acc 30.0000 (30.0000) lr 1.9048e+01 eta 0:00:47\n",
            "epoch [16/100] batch [1/1] time 0.563 (0.563) data 0.354 (0.354) loss 3.0837 (3.0837) acc 25.0000 (25.0000) lr 1.8910e+01 eta 0:00:47\n",
            "epoch [17/100] batch [1/1] time 0.590 (0.590) data 0.392 (0.392) loss 2.8466 (2.8466) acc 25.0000 (25.0000) lr 1.8763e+01 eta 0:00:48\n",
            "epoch [18/100] batch [1/1] time 0.569 (0.569) data 0.360 (0.360) loss 2.7212 (2.7212) acc 40.0000 (40.0000) lr 1.8607e+01 eta 0:00:46\n",
            "epoch [19/100] batch [1/1] time 0.562 (0.562) data 0.353 (0.353) loss 2.6515 (2.6515) acc 45.0000 (45.0000) lr 1.8443e+01 eta 0:00:45\n",
            "epoch [20/100] batch [1/1] time 0.566 (0.566) data 0.356 (0.356) loss 2.5103 (2.5103) acc 55.0000 (55.0000) lr 1.8271e+01 eta 0:00:45\n",
            "epoch [21/100] batch [1/1] time 0.555 (0.555) data 0.346 (0.346) loss 2.5099 (2.5099) acc 35.0000 (35.0000) lr 1.8090e+01 eta 0:00:43\n",
            "epoch [22/100] batch [1/1] time 0.569 (0.569) data 0.359 (0.359) loss 2.2596 (2.2596) acc 55.0000 (55.0000) lr 1.7902e+01 eta 0:00:44\n",
            "epoch [23/100] batch [1/1] time 0.553 (0.553) data 0.343 (0.343) loss 1.9986 (1.9986) acc 60.0000 (60.0000) lr 1.7705e+01 eta 0:00:42\n",
            "epoch [24/100] batch [1/1] time 0.545 (0.545) data 0.336 (0.336) loss 1.8887 (1.8887) acc 80.0000 (80.0000) lr 1.7501e+01 eta 0:00:41\n",
            "epoch [25/100] batch [1/1] time 0.555 (0.555) data 0.339 (0.339) loss 1.8311 (1.8311) acc 55.0000 (55.0000) lr 1.7290e+01 eta 0:00:41\n",
            "epoch [26/100] batch [1/1] time 0.547 (0.547) data 0.337 (0.337) loss 1.7279 (1.7279) acc 50.0000 (50.0000) lr 1.7071e+01 eta 0:00:40\n",
            "epoch [27/100] batch [1/1] time 0.547 (0.547) data 0.338 (0.338) loss 1.4925 (1.4925) acc 65.0000 (65.0000) lr 1.6845e+01 eta 0:00:39\n",
            "epoch [28/100] batch [1/1] time 0.557 (0.557) data 0.346 (0.346) loss 1.2842 (1.2842) acc 80.0000 (80.0000) lr 1.6613e+01 eta 0:00:40\n",
            "epoch [29/100] batch [1/1] time 0.554 (0.554) data 0.344 (0.344) loss 1.5698 (1.5698) acc 70.0000 (70.0000) lr 1.6374e+01 eta 0:00:39\n",
            "epoch [30/100] batch [1/1] time 0.566 (0.566) data 0.357 (0.357) loss 1.6431 (1.6431) acc 70.0000 (70.0000) lr 1.6129e+01 eta 0:00:39\n",
            "epoch [31/100] batch [1/1] time 0.577 (0.577) data 0.367 (0.367) loss 1.5766 (1.5766) acc 70.0000 (70.0000) lr 1.5878e+01 eta 0:00:39\n",
            "epoch [32/100] batch [1/1] time 0.573 (0.573) data 0.364 (0.364) loss 1.9496 (1.9496) acc 75.0000 (75.0000) lr 1.5621e+01 eta 0:00:38\n",
            "epoch [33/100] batch [1/1] time 0.544 (0.544) data 0.333 (0.333) loss 1.8753 (1.8753) acc 75.0000 (75.0000) lr 1.5358e+01 eta 0:00:36\n",
            "epoch [34/100] batch [1/1] time 0.546 (0.546) data 0.336 (0.336) loss 1.5246 (1.5246) acc 60.0000 (60.0000) lr 1.5090e+01 eta 0:00:36\n",
            "epoch [35/100] batch [1/1] time 0.538 (0.538) data 0.326 (0.326) loss 3.5308 (3.5308) acc 50.0000 (50.0000) lr 1.4818e+01 eta 0:00:34\n",
            "epoch [36/100] batch [1/1] time 0.563 (0.563) data 0.352 (0.352) loss 5.0462 (5.0462) acc 30.0000 (30.0000) lr 1.4540e+01 eta 0:00:36\n",
            "epoch [37/100] batch [1/1] time 0.547 (0.547) data 0.337 (0.337) loss 5.9596 (5.9596) acc 35.0000 (35.0000) lr 1.4258e+01 eta 0:00:34\n",
            "epoch [38/100] batch [1/1] time 0.559 (0.559) data 0.349 (0.349) loss 4.4119 (4.4119) acc 40.0000 (40.0000) lr 1.3971e+01 eta 0:00:34\n",
            "epoch [39/100] batch [1/1] time 0.535 (0.535) data 0.321 (0.321) loss 3.2673 (3.2673) acc 30.0000 (30.0000) lr 1.3681e+01 eta 0:00:32\n",
            "epoch [40/100] batch [1/1] time 0.566 (0.566) data 0.354 (0.354) loss 4.9349 (4.9349) acc 50.0000 (50.0000) lr 1.3387e+01 eta 0:00:33\n",
            "epoch [41/100] batch [1/1] time 0.554 (0.554) data 0.344 (0.344) loss 5.1266 (5.1266) acc 40.0000 (40.0000) lr 1.3090e+01 eta 0:00:32\n",
            "epoch [42/100] batch [1/1] time 0.555 (0.555) data 0.345 (0.345) loss 1.6406 (1.6406) acc 60.0000 (60.0000) lr 1.2790e+01 eta 0:00:32\n",
            "epoch [43/100] batch [1/1] time 0.578 (0.578) data 0.365 (0.365) loss 1.7177 (1.7177) acc 60.0000 (60.0000) lr 1.2487e+01 eta 0:00:32\n",
            "epoch [44/100] batch [1/1] time 0.575 (0.575) data 0.362 (0.362) loss 2.8385 (2.8385) acc 55.0000 (55.0000) lr 1.2181e+01 eta 0:00:32\n",
            "epoch [45/100] batch [1/1] time 0.550 (0.550) data 0.340 (0.340) loss 1.6577 (1.6577) acc 75.0000 (75.0000) lr 1.1874e+01 eta 0:00:30\n",
            "epoch [46/100] batch [1/1] time 0.570 (0.570) data 0.356 (0.356) loss 1.9131 (1.9131) acc 60.0000 (60.0000) lr 1.1564e+01 eta 0:00:30\n",
            "epoch [47/100] batch [1/1] time 0.546 (0.546) data 0.332 (0.332) loss 0.9995 (0.9995) acc 70.0000 (70.0000) lr 1.1253e+01 eta 0:00:28\n",
            "epoch [48/100] batch [1/1] time 0.584 (0.584) data 0.371 (0.371) loss 0.6973 (0.6973) acc 95.0000 (95.0000) lr 1.0941e+01 eta 0:00:30\n",
            "epoch [49/100] batch [1/1] time 0.568 (0.568) data 0.355 (0.355) loss 0.6381 (0.6381) acc 95.0000 (95.0000) lr 1.0628e+01 eta 0:00:28\n",
            "epoch [50/100] batch [1/1] time 0.563 (0.563) data 0.348 (0.348) loss 0.6451 (0.6451) acc 85.0000 (85.0000) lr 1.0314e+01 eta 0:00:28\n",
            "epoch [51/100] batch [1/1] time 0.587 (0.587) data 0.371 (0.371) loss 0.5836 (0.5836) acc 95.0000 (95.0000) lr 1.0000e+01 eta 0:00:28\n",
            "epoch [52/100] batch [1/1] time 0.587 (0.587) data 0.361 (0.361) loss 0.8521 (0.8521) acc 90.0000 (90.0000) lr 9.6859e+00 eta 0:00:28\n",
            "epoch [53/100] batch [1/1] time 0.558 (0.558) data 0.345 (0.345) loss 0.4889 (0.4889) acc 100.0000 (100.0000) lr 9.3721e+00 eta 0:00:26\n",
            "epoch [54/100] batch [1/1] time 0.549 (0.549) data 0.336 (0.336) loss 0.5177 (0.5177) acc 95.0000 (95.0000) lr 9.0589e+00 eta 0:00:25\n",
            "epoch [55/100] batch [1/1] time 0.526 (0.526) data 0.339 (0.339) loss 0.5417 (0.5417) acc 95.0000 (95.0000) lr 8.7467e+00 eta 0:00:23\n",
            "epoch [56/100] batch [1/1] time 0.565 (0.565) data 0.352 (0.352) loss 0.7213 (0.7213) acc 90.0000 (90.0000) lr 8.4357e+00 eta 0:00:24\n",
            "epoch [57/100] batch [1/1] time 0.561 (0.561) data 0.347 (0.347) loss 1.1324 (1.1324) acc 90.0000 (90.0000) lr 8.1262e+00 eta 0:00:24\n",
            "epoch [58/100] batch [1/1] time 0.573 (0.573) data 0.360 (0.360) loss 0.4711 (0.4711) acc 95.0000 (95.0000) lr 7.8186e+00 eta 0:00:24\n",
            "epoch [59/100] batch [1/1] time 0.596 (0.596) data 0.383 (0.383) loss 0.7285 (0.7285) acc 85.0000 (85.0000) lr 7.5131e+00 eta 0:00:24\n",
            "epoch [60/100] batch [1/1] time 0.577 (0.577) data 0.361 (0.361) loss 0.4071 (0.4071) acc 100.0000 (100.0000) lr 7.2101e+00 eta 0:00:23\n",
            "epoch [61/100] batch [1/1] time 0.603 (0.603) data 0.389 (0.389) loss 0.6716 (0.6716) acc 90.0000 (90.0000) lr 6.9098e+00 eta 0:00:23\n",
            "epoch [62/100] batch [1/1] time 0.554 (0.554) data 0.339 (0.339) loss 0.9482 (0.9482) acc 70.0000 (70.0000) lr 6.6126e+00 eta 0:00:21\n",
            "epoch [63/100] batch [1/1] time 0.555 (0.555) data 0.342 (0.342) loss 0.7527 (0.7527) acc 80.0000 (80.0000) lr 6.3188e+00 eta 0:00:20\n",
            "epoch [64/100] batch [1/1] time 0.577 (0.577) data 0.362 (0.362) loss 0.5021 (0.5021) acc 90.0000 (90.0000) lr 6.0285e+00 eta 0:00:20\n",
            "epoch [65/100] batch [1/1] time 0.583 (0.583) data 0.366 (0.366) loss 0.5615 (0.5615) acc 95.0000 (95.0000) lr 5.7422e+00 eta 0:00:20\n",
            "epoch [66/100] batch [1/1] time 0.552 (0.552) data 0.337 (0.337) loss 0.7410 (0.7410) acc 80.0000 (80.0000) lr 5.4601e+00 eta 0:00:18\n",
            "epoch [67/100] batch [1/1] time 0.552 (0.552) data 0.334 (0.334) loss 0.6122 (0.6122) acc 90.0000 (90.0000) lr 5.1825e+00 eta 0:00:18\n",
            "epoch [68/100] batch [1/1] time 0.576 (0.576) data 0.360 (0.360) loss 0.6245 (0.6245) acc 95.0000 (95.0000) lr 4.9096e+00 eta 0:00:18\n",
            "epoch [69/100] batch [1/1] time 0.574 (0.574) data 0.359 (0.359) loss 0.4519 (0.4519) acc 95.0000 (95.0000) lr 4.6417e+00 eta 0:00:17\n",
            "epoch [70/100] batch [1/1] time 0.579 (0.579) data 0.365 (0.365) loss 0.3835 (0.3835) acc 100.0000 (100.0000) lr 4.3792e+00 eta 0:00:17\n",
            "epoch [71/100] batch [1/1] time 0.560 (0.560) data 0.346 (0.346) loss 0.5654 (0.5654) acc 90.0000 (90.0000) lr 4.1221e+00 eta 0:00:16\n",
            "epoch [72/100] batch [1/1] time 0.590 (0.590) data 0.375 (0.375) loss 0.4014 (0.4014) acc 95.0000 (95.0000) lr 3.8709e+00 eta 0:00:16\n",
            "epoch [73/100] batch [1/1] time 0.552 (0.552) data 0.337 (0.337) loss 0.5624 (0.5624) acc 90.0000 (90.0000) lr 3.6258e+00 eta 0:00:14\n",
            "epoch [74/100] batch [1/1] time 0.551 (0.551) data 0.337 (0.337) loss 0.3444 (0.3444) acc 100.0000 (100.0000) lr 3.3869e+00 eta 0:00:14\n",
            "epoch [75/100] batch [1/1] time 0.553 (0.553) data 0.340 (0.340) loss 0.3580 (0.3580) acc 100.0000 (100.0000) lr 3.1545e+00 eta 0:00:13\n",
            "epoch [76/100] batch [1/1] time 0.588 (0.588) data 0.373 (0.373) loss 0.3637 (0.3637) acc 100.0000 (100.0000) lr 2.9289e+00 eta 0:00:14\n",
            "epoch [77/100] batch [1/1] time 0.570 (0.570) data 0.356 (0.356) loss 0.3716 (0.3716) acc 100.0000 (100.0000) lr 2.7103e+00 eta 0:00:13\n",
            "epoch [78/100] batch [1/1] time 0.565 (0.565) data 0.351 (0.351) loss 0.3765 (0.3765) acc 100.0000 (100.0000) lr 2.4989e+00 eta 0:00:12\n",
            "epoch [79/100] batch [1/1] time 0.555 (0.555) data 0.340 (0.340) loss 0.4996 (0.4996) acc 90.0000 (90.0000) lr 2.2949e+00 eta 0:00:11\n",
            "epoch [80/100] batch [1/1] time 0.562 (0.562) data 0.349 (0.349) loss 0.3450 (0.3450) acc 100.0000 (100.0000) lr 2.0984e+00 eta 0:00:11\n",
            "epoch [81/100] batch [1/1] time 0.556 (0.556) data 0.343 (0.343) loss 0.4150 (0.4150) acc 95.0000 (95.0000) lr 1.9098e+00 eta 0:00:10\n",
            "epoch [82/100] batch [1/1] time 0.543 (0.543) data 0.330 (0.330) loss 0.4308 (0.4308) acc 95.0000 (95.0000) lr 1.7292e+00 eta 0:00:09\n",
            "epoch [83/100] batch [1/1] time 0.564 (0.564) data 0.349 (0.349) loss 0.3256 (0.3256) acc 100.0000 (100.0000) lr 1.5567e+00 eta 0:00:09\n",
            "epoch [84/100] batch [1/1] time 0.541 (0.541) data 0.328 (0.328) loss 0.4013 (0.4013) acc 95.0000 (95.0000) lr 1.3926e+00 eta 0:00:08\n",
            "epoch [85/100] batch [1/1] time 0.573 (0.573) data 0.359 (0.359) loss 0.3213 (0.3213) acc 100.0000 (100.0000) lr 1.2369e+00 eta 0:00:08\n",
            "epoch [86/100] batch [1/1] time 0.540 (0.540) data 0.329 (0.329) loss 0.3393 (0.3393) acc 100.0000 (100.0000) lr 1.0899e+00 eta 0:00:07\n",
            "epoch [87/100] batch [1/1] time 0.563 (0.563) data 0.353 (0.353) loss 0.7708 (0.7708) acc 90.0000 (90.0000) lr 9.5173e-01 eta 0:00:07\n",
            "epoch [88/100] batch [1/1] time 0.576 (0.576) data 0.362 (0.362) loss 0.3499 (0.3499) acc 100.0000 (100.0000) lr 8.2245e-01 eta 0:00:06\n",
            "epoch [89/100] batch [1/1] time 0.555 (0.555) data 0.344 (0.344) loss 0.3195 (0.3195) acc 100.0000 (100.0000) lr 7.0224e-01 eta 0:00:06\n",
            "epoch [90/100] batch [1/1] time 0.587 (0.587) data 0.374 (0.374) loss 0.3561 (0.3561) acc 100.0000 (100.0000) lr 5.9119e-01 eta 0:00:05\n",
            "epoch [91/100] batch [1/1] time 0.578 (0.578) data 0.365 (0.365) loss 0.3733 (0.3733) acc 95.0000 (95.0000) lr 4.8943e-01 eta 0:00:05\n",
            "epoch [92/100] batch [1/1] time 0.583 (0.583) data 0.370 (0.370) loss 0.3300 (0.3300) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:04\n",
            "epoch [93/100] batch [1/1] time 0.584 (0.584) data 0.371 (0.371) loss 0.4080 (0.4080) acc 95.0000 (95.0000) lr 3.1417e-01 eta 0:00:04\n",
            "epoch [94/100] batch [1/1] time 0.575 (0.575) data 0.365 (0.365) loss 0.3073 (0.3073) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:03\n",
            "epoch [95/100] batch [1/1] time 0.548 (0.548) data 0.335 (0.335) loss 0.4855 (0.4855) acc 95.0000 (95.0000) lr 1.7713e-01 eta 0:00:02\n",
            "epoch [96/100] batch [1/1] time 0.560 (0.560) data 0.348 (0.348) loss 0.3210 (0.3210) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.542 (0.542) data 0.329 (0.329) loss 0.3220 (0.3220) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:01\n",
            "epoch [98/100] batch [1/1] time 0.570 (0.570) data 0.360 (0.360) loss 0.3393 (0.3393) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.549 (0.549) data 0.338 (0.338) loss 0.3775 (0.3775) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.571 (0.571) data 0.359 (0.359) loss 0.3360 (0.3360) acc 95.0000 (95.0000) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed2/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:33<00:00,  2.45it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 4,585\n",
            "* accuracy: 56.6%\n",
            "* error: 43.4%\n",
            "* macro_f1: 55.9%\n",
            "Elapsed: 0:01:37\n"
          ]
        }
      ],
      "source": [
        "#eurosat-2shots-seed2\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 2 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed2 \\\n",
        "        DATASET.NUM_SHOTS 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rwkB-R1dLbYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430f6ea9-c150-48b3-dd7d-b413cd89ee8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 16:02:51.367838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 16:02:51.387730: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 16:02:51.393716: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 16:02:51.407980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 16:02:52.408713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2']\n",
            "output_dir: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed3\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  20\n",
            "# val      20\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0187, -0.5727,  0.1683,  ...,  0.4947,  0.4188,  0.0344],\n",
            "         [ 0.0173, -0.5677,  0.1739,  ...,  0.4990,  0.4190,  0.0388],\n",
            "         [ 0.0094, -0.5649,  0.1758,  ...,  0.4946,  0.4157,  0.0405],\n",
            "         ...,\n",
            "         [ 0.0227, -0.5676,  0.1747,  ...,  0.5071,  0.4204,  0.0413],\n",
            "         [ 0.0083, -0.5735,  0.1744,  ...,  0.5016,  0.4249,  0.0364],\n",
            "         [ 0.0210, -0.5687,  0.1704,  ...,  0.5030,  0.4152,  0.0323]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2340\n",
            "  Min: -1.5598\n",
            "  Max: 1.0156\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]],\n",
            "\n",
            "        [[-0.0001, -0.0094, -0.0214,  ..., -0.0101, -0.0007,  0.0008],\n",
            "         [ 0.0089,  0.0017, -0.0101,  ..., -0.0021,  0.0051,  0.0022],\n",
            "         [ 0.0008,  0.0003, -0.0110,  ...,  0.0020,  0.0018,  0.0065],\n",
            "         ...,\n",
            "         [ 0.0050, -0.0156, -0.0200,  ..., -0.0023,  0.0032,  0.0056],\n",
            "         [ 0.0061, -0.0075, -0.0168,  ...,  0.0021,  0.0036,  0.0033],\n",
            "         [ 0.0012, -0.0061, -0.0182,  ..., -0.0050,  0.0050,  0.0014]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4500\n",
            "  Max: 0.4708\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed3/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 1.547 (1.547) data 0.455 (0.455) loss 8.5279 (8.5279) acc 30.0000 (30.0000) lr 2.0000e+01 eta 0:02:33\n",
            "epoch [2/100] batch [1/1] time 0.547 (0.547) data 0.340 (0.340) loss 8.4497 (8.4497) acc 40.0000 (40.0000) lr 1.9995e+01 eta 0:00:53\n",
            "epoch [3/100] batch [1/1] time 0.554 (0.554) data 0.348 (0.348) loss 6.9827 (6.9827) acc 20.0000 (20.0000) lr 1.9980e+01 eta 0:00:53\n",
            "epoch [4/100] batch [1/1] time 0.537 (0.537) data 0.334 (0.334) loss 5.8026 (5.8026) acc 15.0000 (15.0000) lr 1.9956e+01 eta 0:00:51\n",
            "epoch [5/100] batch [1/1] time 0.544 (0.544) data 0.339 (0.339) loss 5.7814 (5.7814) acc 15.0000 (15.0000) lr 1.9921e+01 eta 0:00:51\n",
            "epoch [6/100] batch [1/1] time 0.556 (0.556) data 0.349 (0.349) loss 7.0589 (7.0589) acc 10.0000 (10.0000) lr 1.9877e+01 eta 0:00:52\n",
            "epoch [7/100] batch [1/1] time 0.540 (0.540) data 0.335 (0.335) loss 9.6826 (9.6826) acc 10.0000 (10.0000) lr 1.9823e+01 eta 0:00:50\n",
            "epoch [8/100] batch [1/1] time 0.534 (0.534) data 0.327 (0.327) loss 7.4804 (7.4804) acc 15.0000 (15.0000) lr 1.9759e+01 eta 0:00:49\n",
            "epoch [9/100] batch [1/1] time 0.543 (0.543) data 0.335 (0.335) loss 6.3055 (6.3055) acc 20.0000 (20.0000) lr 1.9686e+01 eta 0:00:49\n",
            "epoch [10/100] batch [1/1] time 0.572 (0.572) data 0.365 (0.365) loss 5.1545 (5.1545) acc 35.0000 (35.0000) lr 1.9603e+01 eta 0:00:51\n",
            "epoch [11/100] batch [1/1] time 0.570 (0.570) data 0.363 (0.363) loss 4.2594 (4.2594) acc 20.0000 (20.0000) lr 1.9511e+01 eta 0:00:50\n",
            "epoch [12/100] batch [1/1] time 0.556 (0.556) data 0.348 (0.348) loss 4.1403 (4.1403) acc 10.0000 (10.0000) lr 1.9409e+01 eta 0:00:48\n",
            "epoch [13/100] batch [1/1] time 0.542 (0.542) data 0.337 (0.337) loss 3.1840 (3.1840) acc 40.0000 (40.0000) lr 1.9298e+01 eta 0:00:47\n",
            "epoch [14/100] batch [1/1] time 0.565 (0.565) data 0.357 (0.357) loss 3.2288 (3.2288) acc 30.0000 (30.0000) lr 1.9178e+01 eta 0:00:48\n",
            "epoch [15/100] batch [1/1] time 0.553 (0.553) data 0.347 (0.347) loss 2.9955 (2.9955) acc 35.0000 (35.0000) lr 1.9048e+01 eta 0:00:47\n",
            "epoch [16/100] batch [1/1] time 0.540 (0.540) data 0.334 (0.334) loss 2.8204 (2.8204) acc 45.0000 (45.0000) lr 1.8910e+01 eta 0:00:45\n",
            "epoch [17/100] batch [1/1] time 0.535 (0.535) data 0.325 (0.325) loss 2.5696 (2.5696) acc 50.0000 (50.0000) lr 1.8763e+01 eta 0:00:44\n",
            "epoch [18/100] batch [1/1] time 0.565 (0.565) data 0.357 (0.357) loss 2.3434 (2.3434) acc 55.0000 (55.0000) lr 1.8607e+01 eta 0:00:46\n",
            "epoch [19/100] batch [1/1] time 0.551 (0.551) data 0.343 (0.343) loss 2.3810 (2.3810) acc 40.0000 (40.0000) lr 1.8443e+01 eta 0:00:44\n",
            "epoch [20/100] batch [1/1] time 0.534 (0.534) data 0.326 (0.326) loss 2.1305 (2.1305) acc 55.0000 (55.0000) lr 1.8271e+01 eta 0:00:42\n",
            "epoch [21/100] batch [1/1] time 0.578 (0.578) data 0.369 (0.369) loss 1.8580 (1.8580) acc 70.0000 (70.0000) lr 1.8090e+01 eta 0:00:45\n",
            "epoch [22/100] batch [1/1] time 0.537 (0.537) data 0.329 (0.329) loss 1.8594 (1.8594) acc 65.0000 (65.0000) lr 1.7902e+01 eta 0:00:41\n",
            "epoch [23/100] batch [1/1] time 0.541 (0.541) data 0.335 (0.335) loss 1.9821 (1.9821) acc 70.0000 (70.0000) lr 1.7705e+01 eta 0:00:41\n",
            "epoch [24/100] batch [1/1] time 0.556 (0.556) data 0.345 (0.345) loss 1.7158 (1.7158) acc 65.0000 (65.0000) lr 1.7501e+01 eta 0:00:42\n",
            "epoch [25/100] batch [1/1] time 0.547 (0.547) data 0.338 (0.338) loss 1.7927 (1.7927) acc 50.0000 (50.0000) lr 1.7290e+01 eta 0:00:41\n",
            "epoch [26/100] batch [1/1] time 0.577 (0.577) data 0.367 (0.367) loss 1.2217 (1.2217) acc 85.0000 (85.0000) lr 1.7071e+01 eta 0:00:42\n",
            "epoch [27/100] batch [1/1] time 0.559 (0.559) data 0.347 (0.347) loss 1.7150 (1.7150) acc 60.0000 (60.0000) lr 1.6845e+01 eta 0:00:40\n",
            "epoch [28/100] batch [1/1] time 0.549 (0.549) data 0.341 (0.341) loss 1.4165 (1.4165) acc 70.0000 (70.0000) lr 1.6613e+01 eta 0:00:39\n",
            "epoch [29/100] batch [1/1] time 0.561 (0.561) data 0.351 (0.351) loss 1.1407 (1.1407) acc 85.0000 (85.0000) lr 1.6374e+01 eta 0:00:39\n",
            "epoch [30/100] batch [1/1] time 0.545 (0.545) data 0.335 (0.335) loss 1.1836 (1.1836) acc 80.0000 (80.0000) lr 1.6129e+01 eta 0:00:38\n",
            "epoch [31/100] batch [1/1] time 0.574 (0.574) data 0.366 (0.366) loss 0.7802 (0.7802) acc 90.0000 (90.0000) lr 1.5878e+01 eta 0:00:39\n",
            "epoch [32/100] batch [1/1] time 0.569 (0.569) data 0.358 (0.358) loss 1.0126 (1.0126) acc 75.0000 (75.0000) lr 1.5621e+01 eta 0:00:38\n",
            "epoch [33/100] batch [1/1] time 0.534 (0.534) data 0.324 (0.324) loss 0.6963 (0.6963) acc 90.0000 (90.0000) lr 1.5358e+01 eta 0:00:35\n",
            "epoch [34/100] batch [1/1] time 0.550 (0.550) data 0.340 (0.340) loss 0.6964 (0.6964) acc 85.0000 (85.0000) lr 1.5090e+01 eta 0:00:36\n",
            "epoch [35/100] batch [1/1] time 0.547 (0.547) data 0.336 (0.336) loss 0.9283 (0.9283) acc 85.0000 (85.0000) lr 1.4818e+01 eta 0:00:35\n",
            "epoch [36/100] batch [1/1] time 0.557 (0.557) data 0.345 (0.345) loss 0.7749 (0.7749) acc 80.0000 (80.0000) lr 1.4540e+01 eta 0:00:35\n",
            "epoch [37/100] batch [1/1] time 0.535 (0.535) data 0.324 (0.324) loss 1.0741 (1.0741) acc 75.0000 (75.0000) lr 1.4258e+01 eta 0:00:33\n",
            "epoch [38/100] batch [1/1] time 0.564 (0.564) data 0.354 (0.354) loss 2.8415 (2.8415) acc 65.0000 (65.0000) lr 1.3971e+01 eta 0:00:34\n",
            "epoch [39/100] batch [1/1] time 0.572 (0.572) data 0.360 (0.360) loss 2.3425 (2.3425) acc 50.0000 (50.0000) lr 1.3681e+01 eta 0:00:34\n",
            "epoch [40/100] batch [1/1] time 0.546 (0.546) data 0.337 (0.337) loss 2.2170 (2.2170) acc 50.0000 (50.0000) lr 1.3387e+01 eta 0:00:32\n",
            "epoch [41/100] batch [1/1] time 0.538 (0.538) data 0.328 (0.328) loss 4.7332 (4.7332) acc 65.0000 (65.0000) lr 1.3090e+01 eta 0:00:31\n",
            "epoch [42/100] batch [1/1] time 0.556 (0.556) data 0.343 (0.343) loss 5.0755 (5.0755) acc 45.0000 (45.0000) lr 1.2790e+01 eta 0:00:32\n",
            "epoch [43/100] batch [1/1] time 0.563 (0.563) data 0.351 (0.351) loss 3.8716 (3.8716) acc 55.0000 (55.0000) lr 1.2487e+01 eta 0:00:32\n",
            "epoch [44/100] batch [1/1] time 0.548 (0.548) data 0.337 (0.337) loss 4.4488 (4.4488) acc 45.0000 (45.0000) lr 1.2181e+01 eta 0:00:30\n",
            "epoch [45/100] batch [1/1] time 0.574 (0.574) data 0.361 (0.361) loss 6.8307 (6.8307) acc 45.0000 (45.0000) lr 1.1874e+01 eta 0:00:31\n",
            "epoch [46/100] batch [1/1] time 0.548 (0.548) data 0.336 (0.336) loss 3.0978 (3.0978) acc 30.0000 (30.0000) lr 1.1564e+01 eta 0:00:29\n",
            "epoch [47/100] batch [1/1] time 0.543 (0.543) data 0.331 (0.331) loss 1.5234 (1.5234) acc 60.0000 (60.0000) lr 1.1253e+01 eta 0:00:28\n",
            "epoch [48/100] batch [1/1] time 0.588 (0.588) data 0.375 (0.375) loss 0.8385 (0.8385) acc 90.0000 (90.0000) lr 1.0941e+01 eta 0:00:30\n",
            "epoch [49/100] batch [1/1] time 0.561 (0.561) data 0.348 (0.348) loss 0.9440 (0.9440) acc 85.0000 (85.0000) lr 1.0628e+01 eta 0:00:28\n",
            "epoch [50/100] batch [1/1] time 0.563 (0.563) data 0.340 (0.340) loss 0.7352 (0.7352) acc 85.0000 (85.0000) lr 1.0314e+01 eta 0:00:28\n",
            "epoch [51/100] batch [1/1] time 0.577 (0.577) data 0.365 (0.365) loss 0.8628 (0.8628) acc 80.0000 (80.0000) lr 1.0000e+01 eta 0:00:28\n",
            "epoch [52/100] batch [1/1] time 0.564 (0.564) data 0.349 (0.349) loss 0.6785 (0.6785) acc 95.0000 (95.0000) lr 9.6859e+00 eta 0:00:27\n",
            "epoch [53/100] batch [1/1] time 0.560 (0.560) data 0.345 (0.345) loss 0.7801 (0.7801) acc 90.0000 (90.0000) lr 9.3721e+00 eta 0:00:26\n",
            "epoch [54/100] batch [1/1] time 0.553 (0.553) data 0.338 (0.338) loss 0.7096 (0.7096) acc 90.0000 (90.0000) lr 9.0589e+00 eta 0:00:25\n",
            "epoch [55/100] batch [1/1] time 0.559 (0.559) data 0.345 (0.345) loss 0.4465 (0.4465) acc 100.0000 (100.0000) lr 8.7467e+00 eta 0:00:25\n",
            "epoch [56/100] batch [1/1] time 0.552 (0.552) data 0.337 (0.337) loss 1.0793 (1.0793) acc 80.0000 (80.0000) lr 8.4357e+00 eta 0:00:24\n",
            "epoch [57/100] batch [1/1] time 0.563 (0.563) data 0.350 (0.350) loss 0.5015 (0.5015) acc 95.0000 (95.0000) lr 8.1262e+00 eta 0:00:24\n",
            "epoch [58/100] batch [1/1] time 0.566 (0.566) data 0.373 (0.373) loss 0.6050 (0.6050) acc 95.0000 (95.0000) lr 7.8186e+00 eta 0:00:23\n",
            "epoch [59/100] batch [1/1] time 0.533 (0.533) data 0.322 (0.322) loss 0.4929 (0.4929) acc 95.0000 (95.0000) lr 7.5131e+00 eta 0:00:21\n",
            "epoch [60/100] batch [1/1] time 0.556 (0.556) data 0.343 (0.343) loss 0.5143 (0.5143) acc 95.0000 (95.0000) lr 7.2101e+00 eta 0:00:22\n",
            "epoch [61/100] batch [1/1] time 0.563 (0.563) data 0.348 (0.348) loss 0.4570 (0.4570) acc 100.0000 (100.0000) lr 6.9098e+00 eta 0:00:21\n",
            "epoch [62/100] batch [1/1] time 0.550 (0.550) data 0.337 (0.337) loss 0.9910 (0.9910) acc 80.0000 (80.0000) lr 6.6126e+00 eta 0:00:20\n",
            "epoch [63/100] batch [1/1] time 0.536 (0.536) data 0.320 (0.320) loss 0.5785 (0.5785) acc 90.0000 (90.0000) lr 6.3188e+00 eta 0:00:19\n",
            "epoch [64/100] batch [1/1] time 0.540 (0.540) data 0.325 (0.325) loss 0.5406 (0.5406) acc 95.0000 (95.0000) lr 6.0285e+00 eta 0:00:19\n",
            "epoch [65/100] batch [1/1] time 0.565 (0.565) data 0.349 (0.349) loss 0.5943 (0.5943) acc 95.0000 (95.0000) lr 5.7422e+00 eta 0:00:19\n",
            "epoch [66/100] batch [1/1] time 0.556 (0.556) data 0.341 (0.341) loss 0.3686 (0.3686) acc 100.0000 (100.0000) lr 5.4601e+00 eta 0:00:18\n",
            "epoch [67/100] batch [1/1] time 0.535 (0.535) data 0.322 (0.322) loss 0.4520 (0.4520) acc 100.0000 (100.0000) lr 5.1825e+00 eta 0:00:17\n",
            "epoch [68/100] batch [1/1] time 0.553 (0.553) data 0.339 (0.339) loss 0.4209 (0.4209) acc 95.0000 (95.0000) lr 4.9096e+00 eta 0:00:17\n",
            "epoch [69/100] batch [1/1] time 0.552 (0.552) data 0.337 (0.337) loss 0.3780 (0.3780) acc 100.0000 (100.0000) lr 4.6417e+00 eta 0:00:17\n",
            "epoch [70/100] batch [1/1] time 0.572 (0.572) data 0.357 (0.357) loss 0.3904 (0.3904) acc 95.0000 (95.0000) lr 4.3792e+00 eta 0:00:17\n",
            "epoch [71/100] batch [1/1] time 0.583 (0.583) data 0.367 (0.367) loss 0.6674 (0.6674) acc 90.0000 (90.0000) lr 4.1221e+00 eta 0:00:16\n",
            "epoch [72/100] batch [1/1] time 0.573 (0.573) data 0.360 (0.360) loss 0.3304 (0.3304) acc 100.0000 (100.0000) lr 3.8709e+00 eta 0:00:16\n",
            "epoch [73/100] batch [1/1] time 0.559 (0.559) data 0.344 (0.344) loss 0.5022 (0.5022) acc 90.0000 (90.0000) lr 3.6258e+00 eta 0:00:15\n",
            "epoch [74/100] batch [1/1] time 0.541 (0.541) data 0.328 (0.328) loss 0.6194 (0.6194) acc 95.0000 (95.0000) lr 3.3869e+00 eta 0:00:14\n",
            "epoch [75/100] batch [1/1] time 0.570 (0.570) data 0.354 (0.354) loss 0.3846 (0.3846) acc 100.0000 (100.0000) lr 3.1545e+00 eta 0:00:14\n",
            "epoch [76/100] batch [1/1] time 0.571 (0.571) data 0.355 (0.355) loss 0.3615 (0.3615) acc 100.0000 (100.0000) lr 2.9289e+00 eta 0:00:13\n",
            "epoch [77/100] batch [1/1] time 0.576 (0.576) data 0.363 (0.363) loss 0.3827 (0.3827) acc 95.0000 (95.0000) lr 2.7103e+00 eta 0:00:13\n",
            "epoch [78/100] batch [1/1] time 0.530 (0.530) data 0.314 (0.314) loss 0.4300 (0.4300) acc 95.0000 (95.0000) lr 2.4989e+00 eta 0:00:11\n",
            "epoch [79/100] batch [1/1] time 0.535 (0.535) data 0.321 (0.321) loss 0.4268 (0.4268) acc 95.0000 (95.0000) lr 2.2949e+00 eta 0:00:11\n",
            "epoch [80/100] batch [1/1] time 0.573 (0.573) data 0.360 (0.360) loss 0.3589 (0.3589) acc 100.0000 (100.0000) lr 2.0984e+00 eta 0:00:11\n",
            "epoch [81/100] batch [1/1] time 0.541 (0.541) data 0.325 (0.325) loss 0.4267 (0.4267) acc 95.0000 (95.0000) lr 1.9098e+00 eta 0:00:10\n",
            "epoch [82/100] batch [1/1] time 0.578 (0.578) data 0.365 (0.365) loss 0.3532 (0.3532) acc 100.0000 (100.0000) lr 1.7292e+00 eta 0:00:10\n",
            "epoch [83/100] batch [1/1] time 0.535 (0.535) data 0.322 (0.322) loss 0.3087 (0.3087) acc 100.0000 (100.0000) lr 1.5567e+00 eta 0:00:09\n",
            "epoch [84/100] batch [1/1] time 0.558 (0.558) data 0.345 (0.345) loss 0.3263 (0.3263) acc 100.0000 (100.0000) lr 1.3926e+00 eta 0:00:08\n",
            "epoch [85/100] batch [1/1] time 0.563 (0.563) data 0.348 (0.348) loss 0.5680 (0.5680) acc 95.0000 (95.0000) lr 1.2369e+00 eta 0:00:08\n",
            "epoch [86/100] batch [1/1] time 0.529 (0.529) data 0.316 (0.316) loss 0.4461 (0.4461) acc 95.0000 (95.0000) lr 1.0899e+00 eta 0:00:07\n",
            "epoch [87/100] batch [1/1] time 0.562 (0.562) data 0.348 (0.348) loss 0.4655 (0.4655) acc 90.0000 (90.0000) lr 9.5173e-01 eta 0:00:07\n",
            "epoch [88/100] batch [1/1] time 0.566 (0.566) data 0.353 (0.353) loss 0.4423 (0.4423) acc 90.0000 (90.0000) lr 8.2245e-01 eta 0:00:06\n",
            "epoch [89/100] batch [1/1] time 0.560 (0.560) data 0.347 (0.347) loss 0.3547 (0.3547) acc 100.0000 (100.0000) lr 7.0224e-01 eta 0:00:06\n",
            "epoch [90/100] batch [1/1] time 0.570 (0.570) data 0.357 (0.357) loss 0.3670 (0.3670) acc 95.0000 (95.0000) lr 5.9119e-01 eta 0:00:05\n",
            "epoch [91/100] batch [1/1] time 0.569 (0.569) data 0.358 (0.358) loss 0.3082 (0.3082) acc 100.0000 (100.0000) lr 4.8943e-01 eta 0:00:05\n",
            "epoch [92/100] batch [1/1] time 0.575 (0.575) data 0.363 (0.363) loss 0.4346 (0.4346) acc 95.0000 (95.0000) lr 3.9706e-01 eta 0:00:04\n",
            "epoch [93/100] batch [1/1] time 0.553 (0.553) data 0.339 (0.339) loss 0.3678 (0.3678) acc 95.0000 (95.0000) lr 3.1417e-01 eta 0:00:03\n",
            "epoch [94/100] batch [1/1] time 0.547 (0.547) data 0.337 (0.337) loss 0.3879 (0.3879) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:03\n",
            "epoch [95/100] batch [1/1] time 0.562 (0.562) data 0.350 (0.350) loss 0.6901 (0.6901) acc 90.0000 (90.0000) lr 1.7713e-01 eta 0:00:02\n",
            "epoch [96/100] batch [1/1] time 0.548 (0.548) data 0.337 (0.337) loss 0.3270 (0.3270) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.571 (0.571) data 0.360 (0.360) loss 0.3382 (0.3382) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:01\n",
            "epoch [98/100] batch [1/1] time 0.576 (0.576) data 0.363 (0.363) loss 0.3185 (0.3185) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.552 (0.552) data 0.343 (0.343) loss 0.3381 (0.3381) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.560 (0.560) data 0.349 (0.349) loss 0.6048 (0.6048) acc 90.0000 (90.0000) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed3/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.46it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 5,322\n",
            "* accuracy: 65.7%\n",
            "* error: 34.3%\n",
            "* macro_f1: 65.2%\n",
            "Elapsed: 0:01:36\n"
          ]
        }
      ],
      "source": [
        "#eurosat-2shots-seed3\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 3 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_init/eurosat/DAPT/vit_b16_ep100_2shots/seed3 \\\n",
        "        DATASET.NUM_SHOTS 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#eurosat-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/1207_new_init/eurosat/DAPT/vit_b16_ep50_2shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrmYJ2dU41eP",
        "outputId": "27dc5212-b656-4ba7-8d9b-626ded36e3f3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 09:19:42.067146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 09:19:42.086666: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 09:19:42.092491: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 09:19:42.106492: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 09:19:43.122454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/1207_new_init/eurosat/DAPT/vit_b16_ep50_2shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/eurosat/DAPT/vit_b16_ep50_2shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  10\n",
            "# val      10\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([10, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]],\n",
            "\n",
            "        [[ 0.0151, -0.5700,  0.1741,  ...,  0.5022,  0.4193,  0.0408],\n",
            "         [ 0.0128, -0.5710,  0.1663,  ...,  0.5062,  0.4116,  0.0396],\n",
            "         [ 0.0188, -0.5721,  0.1755,  ...,  0.5068,  0.4251,  0.0375],\n",
            "         ...,\n",
            "         [ 0.0225, -0.5663,  0.1784,  ...,  0.5024,  0.4130,  0.0371],\n",
            "         [ 0.0176, -0.5749,  0.1742,  ...,  0.5011,  0.4176,  0.0432],\n",
            "         [ 0.0184, -0.5741,  0.1727,  ...,  0.5055,  0.4152,  0.0365]],\n",
            "\n",
            "        [[ 0.0178, -0.5687,  0.1718,  ...,  0.5026,  0.4129,  0.0371],\n",
            "         [ 0.0130, -0.5685,  0.1724,  ...,  0.4969,  0.4214,  0.0369],\n",
            "         [ 0.0146, -0.5725,  0.1763,  ...,  0.4998,  0.4262,  0.0355],\n",
            "         ...,\n",
            "         [ 0.0194, -0.5768,  0.1708,  ...,  0.5000,  0.4200,  0.0375],\n",
            "         [ 0.0145, -0.5713,  0.1709,  ...,  0.4966,  0.4180,  0.0364],\n",
            "         [ 0.0148, -0.5677,  0.1643,  ...,  0.4962,  0.4130,  0.0407]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0201, -0.5710,  0.1785,  ...,  0.5011,  0.4186,  0.0394],\n",
            "         [ 0.0196, -0.5696,  0.1757,  ...,  0.4984,  0.4160,  0.0351],\n",
            "         [ 0.0162, -0.5716,  0.1726,  ...,  0.5025,  0.4189,  0.0342],\n",
            "         ...,\n",
            "         [ 0.0186, -0.5715,  0.1754,  ...,  0.5031,  0.4218,  0.0419],\n",
            "         [ 0.0150, -0.5709,  0.1767,  ...,  0.5029,  0.4240,  0.0374],\n",
            "         [ 0.0171, -0.5648,  0.1729,  ...,  0.4970,  0.4216,  0.0409]],\n",
            "\n",
            "        [[ 0.0101, -0.5685,  0.1691,  ...,  0.4948,  0.4184,  0.0445],\n",
            "         [ 0.0212, -0.5652,  0.1708,  ...,  0.5029,  0.4143,  0.0357],\n",
            "         [ 0.0176, -0.5651,  0.1770,  ...,  0.4984,  0.4227,  0.0433],\n",
            "         ...,\n",
            "         [ 0.0120, -0.5720,  0.1763,  ...,  0.5011,  0.4164,  0.0315],\n",
            "         [ 0.0148, -0.5691,  0.1683,  ...,  0.4919,  0.4181,  0.0394],\n",
            "         [ 0.0140, -0.5704,  0.1717,  ...,  0.4993,  0.4230,  0.0265]],\n",
            "\n",
            "        [[ 0.0209, -0.5730,  0.1765,  ...,  0.5037,  0.4166,  0.0307],\n",
            "         [ 0.0209, -0.5755,  0.1680,  ...,  0.4951,  0.4112,  0.0348],\n",
            "         [ 0.0181, -0.5683,  0.1647,  ...,  0.5047,  0.4196,  0.0401],\n",
            "         ...,\n",
            "         [ 0.0149, -0.5652,  0.1702,  ...,  0.4987,  0.4142,  0.0270],\n",
            "         [ 0.0141, -0.5684,  0.1736,  ...,  0.4999,  0.4222,  0.0395],\n",
            "         [ 0.0190, -0.5701,  0.1723,  ...,  0.5047,  0.4247,  0.0420]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5650\n",
            "  Max: 1.0122\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-5.2644e-04, -1.1168e-02, -2.1661e-02,  ..., -4.8596e-03,\n",
            "           5.8912e-03,  6.7493e-03],\n",
            "         [ 5.8841e-04, -1.8584e-02, -2.5060e-02,  ..., -3.4987e-03,\n",
            "           5.4105e-03, -1.7861e-03],\n",
            "         [ 1.7309e-03, -7.2606e-03, -2.3481e-02,  ..., -2.1107e-03,\n",
            "           4.9070e-03,  6.0702e-03],\n",
            "         ...,\n",
            "         [ 1.0223e-03, -1.2080e-02, -1.5868e-02,  ..., -1.2882e-02,\n",
            "          -2.0567e-03,  4.8362e-03],\n",
            "         [ 7.8468e-03, -1.1038e-02, -1.0770e-02,  ...,  7.4988e-04,\n",
            "           1.5215e-03, -1.6774e-03],\n",
            "         [ 2.3028e-03, -5.9798e-03, -1.7329e-02,  ...,  5.5724e-04,\n",
            "           1.5857e-02,  4.3269e-03]],\n",
            "\n",
            "        [[ 6.0997e-03,  1.7106e-03, -2.1188e-02,  ...,  2.5523e-03,\n",
            "           2.1776e-03,  7.7525e-03],\n",
            "         [ 6.9275e-03, -6.8095e-03, -1.3799e-02,  ..., -3.6580e-03,\n",
            "           5.6513e-03,  1.9980e-03],\n",
            "         [ 6.2942e-04, -7.5124e-03, -9.4807e-03,  ...,  9.2128e-03,\n",
            "           1.0227e-02,  3.9989e-03],\n",
            "         ...,\n",
            "         [-8.9265e-04, -1.5108e-02, -1.9400e-02,  ..., -3.9164e-03,\n",
            "           4.4435e-03, -3.3369e-04],\n",
            "         [ 9.0713e-03, -1.0043e-02, -2.8737e-02,  ..., -4.9359e-03,\n",
            "           9.1699e-03,  5.4980e-03],\n",
            "         [-8.9653e-05, -7.9620e-03, -2.2422e-02,  ..., -2.5861e-03,\n",
            "           2.1928e-03,  6.9095e-03]],\n",
            "\n",
            "        [[ 2.2506e-04, -1.2583e-02, -1.8306e-02,  ..., -1.1051e-02,\n",
            "           9.2462e-03,  8.1454e-03],\n",
            "         [-9.4987e-04, -1.2930e-02, -2.0111e-02,  ..., -9.7730e-03,\n",
            "           5.3495e-03,  1.8893e-03],\n",
            "         [-2.7428e-03, -8.8699e-03, -1.4280e-02,  ...,  5.0471e-03,\n",
            "           5.3924e-03,  1.0839e-02],\n",
            "         ...,\n",
            "         [ 7.2136e-03, -9.1107e-03, -1.5424e-02,  ...,  1.9115e-03,\n",
            "           6.7415e-03, -8.7823e-04],\n",
            "         [-4.8866e-03, -9.1675e-03, -1.5351e-02,  ..., -5.0427e-03,\n",
            "           8.2830e-03,  1.1029e-02],\n",
            "         [-4.4785e-03, -1.2034e-02, -1.6046e-02,  ..., -1.3892e-03,\n",
            "           1.1470e-02, -4.6686e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.1645e-03,  2.2294e-03, -2.3786e-02,  ..., -2.2859e-04,\n",
            "           6.9243e-03,  8.3819e-03],\n",
            "         [ 4.7722e-03, -5.7014e-03, -1.7818e-02,  ..., -5.6092e-03,\n",
            "           1.0013e-02,  1.3503e-03],\n",
            "         [-2.0599e-03, -4.0076e-03, -2.7204e-02,  ...,  2.8766e-03,\n",
            "           1.4182e-02, -4.1284e-04],\n",
            "         ...,\n",
            "         [-2.6131e-03, -1.2682e-02, -1.9781e-02,  ...,  2.6591e-03,\n",
            "           5.9818e-03,  1.0518e-02],\n",
            "         [ 5.3825e-03, -7.2549e-03, -1.3387e-02,  ..., -5.7954e-04,\n",
            "           5.6404e-03,  2.1621e-03],\n",
            "         [ 1.1158e-03,  5.8145e-04, -2.5354e-02,  ...,  1.2668e-03,\n",
            "           9.6658e-03,  5.6201e-03]],\n",
            "\n",
            "        [[ 5.7640e-03, -5.7109e-03, -1.9671e-02,  ..., -6.6449e-03,\n",
            "          -2.2398e-03,  3.5010e-04],\n",
            "         [ 4.0493e-03, -1.3674e-02, -2.2840e-02,  ..., -2.0630e-03,\n",
            "           1.2584e-02, -4.8818e-03],\n",
            "         [ 1.1436e-05, -3.8550e-03, -2.1955e-02,  ..., -3.6313e-03,\n",
            "           3.4708e-03,  2.7534e-03],\n",
            "         ...,\n",
            "         [ 7.8735e-03, -1.2430e-02, -2.0399e-02,  ..., -1.3462e-02,\n",
            "           1.2340e-02, -4.7330e-03],\n",
            "         [ 2.9120e-03, -1.2139e-02, -6.7417e-03,  ...,  4.5043e-04,\n",
            "           7.1866e-05,  9.1229e-04],\n",
            "         [-3.6163e-03, -4.2251e-03, -2.5861e-02,  ..., -6.7784e-03,\n",
            "           5.3829e-03, -2.0913e-03]],\n",
            "\n",
            "        [[ 1.0143e-02, -1.1706e-02, -2.0403e-02,  ..., -5.1286e-03,\n",
            "           9.2310e-03,  1.6630e-03],\n",
            "         [ 5.0354e-03, -6.9526e-03, -1.5557e-02,  ..., -3.4320e-03,\n",
            "           9.9329e-03,  5.0002e-03],\n",
            "         [ 3.1037e-03, -1.5677e-02, -2.5472e-02,  ..., -1.3558e-03,\n",
            "           2.2081e-03,  4.0732e-03],\n",
            "         ...,\n",
            "         [ 3.1457e-03, -4.8411e-03, -1.6359e-02,  ...,  3.0177e-03,\n",
            "           9.1890e-03, -4.2676e-03],\n",
            "         [ 5.7907e-03, -1.0981e-02, -2.1062e-02,  ..., -9.6700e-03,\n",
            "           4.3233e-03,  1.2432e-03],\n",
            "         [-5.3413e-05, -5.7033e-03, -2.5506e-02,  ..., -3.7009e-03,\n",
            "           7.3665e-03,  1.6910e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4741\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/eurosat/DAPT/vit_b16_ep50_2shots/seed1/tensorboard)\n",
            "epoch [1/50] batch [1/1] time 7.467 (7.467) data 6.503 (6.503) loss 9.1516 (9.1516) acc 20.0000 (20.0000) lr 2.0000e+01 eta 0:06:05\n",
            "epoch [2/50] batch [1/1] time 0.425 (0.425) data 0.275 (0.275) loss 8.8763 (8.8763) acc 20.0000 (20.0000) lr 1.9980e+01 eta 0:00:20\n",
            "epoch [3/50] batch [1/1] time 0.387 (0.387) data 0.265 (0.265) loss 7.5790 (7.5790) acc 0.0000 (0.0000) lr 1.9921e+01 eta 0:00:18\n",
            "epoch [4/50] batch [1/1] time 0.377 (0.377) data 0.257 (0.257) loss 4.8146 (4.8146) acc 20.0000 (20.0000) lr 1.9823e+01 eta 0:00:17\n",
            "epoch [5/50] batch [1/1] time 0.377 (0.377) data 0.254 (0.254) loss 4.9346 (4.9346) acc 20.0000 (20.0000) lr 1.9686e+01 eta 0:00:16\n",
            "epoch [6/50] batch [1/1] time 0.378 (0.378) data 0.257 (0.257) loss 4.4832 (4.4832) acc 10.0000 (10.0000) lr 1.9511e+01 eta 0:00:16\n",
            "epoch [7/50] batch [1/1] time 0.378 (0.378) data 0.255 (0.255) loss 5.6139 (5.6139) acc 20.0000 (20.0000) lr 1.9298e+01 eta 0:00:16\n",
            "epoch [8/50] batch [1/1] time 0.390 (0.390) data 0.270 (0.270) loss 5.1683 (5.1683) acc 30.0000 (30.0000) lr 1.9048e+01 eta 0:00:16\n",
            "epoch [9/50] batch [1/1] time 0.378 (0.378) data 0.256 (0.256) loss 4.5618 (4.5618) acc 20.0000 (20.0000) lr 1.8763e+01 eta 0:00:15\n",
            "epoch [10/50] batch [1/1] time 0.380 (0.380) data 0.258 (0.258) loss 3.5388 (3.5388) acc 20.0000 (20.0000) lr 1.8443e+01 eta 0:00:15\n",
            "epoch [11/50] batch [1/1] time 0.385 (0.385) data 0.263 (0.263) loss 6.1196 (6.1196) acc 20.0000 (20.0000) lr 1.8090e+01 eta 0:00:15\n",
            "epoch [12/50] batch [1/1] time 0.374 (0.374) data 0.254 (0.254) loss 5.3866 (5.3866) acc 0.0000 (0.0000) lr 1.7705e+01 eta 0:00:14\n",
            "epoch [13/50] batch [1/1] time 0.376 (0.376) data 0.255 (0.255) loss 3.7431 (3.7431) acc 30.0000 (30.0000) lr 1.7290e+01 eta 0:00:13\n",
            "epoch [14/50] batch [1/1] time 0.376 (0.376) data 0.255 (0.255) loss 3.6113 (3.6113) acc 20.0000 (20.0000) lr 1.6845e+01 eta 0:00:13\n",
            "epoch [15/50] batch [1/1] time 0.394 (0.394) data 0.271 (0.271) loss 3.3568 (3.3568) acc 40.0000 (40.0000) lr 1.6374e+01 eta 0:00:13\n",
            "epoch [16/50] batch [1/1] time 0.385 (0.385) data 0.262 (0.262) loss 2.8793 (2.8793) acc 30.0000 (30.0000) lr 1.5878e+01 eta 0:00:13\n",
            "epoch [17/50] batch [1/1] time 0.390 (0.390) data 0.269 (0.269) loss 3.3243 (3.3243) acc 40.0000 (40.0000) lr 1.5358e+01 eta 0:00:12\n",
            "epoch [18/50] batch [1/1] time 0.389 (0.389) data 0.268 (0.268) loss 3.7479 (3.7479) acc 20.0000 (20.0000) lr 1.4818e+01 eta 0:00:12\n",
            "epoch [19/50] batch [1/1] time 0.391 (0.391) data 0.270 (0.270) loss 2.7271 (2.7271) acc 40.0000 (40.0000) lr 1.4258e+01 eta 0:00:12\n",
            "epoch [20/50] batch [1/1] time 0.400 (0.400) data 0.278 (0.278) loss 3.6036 (3.6036) acc 50.0000 (50.0000) lr 1.3681e+01 eta 0:00:12\n",
            "epoch [21/50] batch [1/1] time 0.382 (0.382) data 0.259 (0.259) loss 2.8453 (2.8453) acc 40.0000 (40.0000) lr 1.3090e+01 eta 0:00:11\n",
            "epoch [22/50] batch [1/1] time 0.401 (0.401) data 0.279 (0.279) loss 2.5772 (2.5772) acc 40.0000 (40.0000) lr 1.2487e+01 eta 0:00:11\n",
            "epoch [23/50] batch [1/1] time 0.384 (0.384) data 0.261 (0.261) loss 2.3523 (2.3523) acc 40.0000 (40.0000) lr 1.1874e+01 eta 0:00:10\n",
            "epoch [24/50] batch [1/1] time 0.374 (0.374) data 0.252 (0.252) loss 2.5579 (2.5579) acc 40.0000 (40.0000) lr 1.1253e+01 eta 0:00:09\n",
            "epoch [25/50] batch [1/1] time 0.407 (0.407) data 0.286 (0.286) loss 2.7454 (2.7454) acc 40.0000 (40.0000) lr 1.0628e+01 eta 0:00:10\n",
            "epoch [26/50] batch [1/1] time 0.388 (0.388) data 0.265 (0.265) loss 2.0916 (2.0916) acc 50.0000 (50.0000) lr 1.0000e+01 eta 0:00:09\n",
            "epoch [27/50] batch [1/1] time 0.412 (0.412) data 0.290 (0.290) loss 1.7946 (1.7946) acc 60.0000 (60.0000) lr 9.3721e+00 eta 0:00:09\n",
            "epoch [28/50] batch [1/1] time 0.411 (0.411) data 0.290 (0.290) loss 1.8216 (1.8216) acc 60.0000 (60.0000) lr 8.7467e+00 eta 0:00:09\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 211, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 151, in main\n",
            "    trainer.train()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 385, in train\n",
            "    super().train(self.start_epoch, self.max_epoch)\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 249, in train\n",
            "    self.run_epoch()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 593, in run_epoch\n",
            "    loss_summary = self.forward_backward(batch)\n",
            "  File \"/content/drive/MyDrive/DAPT/trainers/dapt.py\", line 552, in forward_backward\n",
            "    accuracy = compute_accuracy(output, label)[0].item()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Diffrent_Init"
      ],
      "metadata": {
        "id": "g4WzxLKUiSF6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3218f8eb-d7f7-4cda-c57d-1dfa95dd6fd3",
        "id": "1Eu_OeoHiUYd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 09:27:03.426932: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 09:27:03.446739: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 09:27:03.452695: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 09:27:03.466918: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 09:27:04.454629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16']\n",
            "output_dir: output/1207_new_diff_init/eurosat/DAPT/vit_b16_16shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/eurosat/DAPT/vit_b16_16shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_16-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  160\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "        [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "        [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "        ...,\n",
            "        [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "        [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "        [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-5.2644e-04, -1.1168e-02, -2.1661e-02,  ..., -4.8596e-03,\n",
            "           5.8912e-03,  6.7493e-03],\n",
            "         [ 5.8841e-04, -1.8584e-02, -2.5060e-02,  ..., -3.4987e-03,\n",
            "           5.4105e-03, -1.7861e-03],\n",
            "         [ 1.7309e-03, -7.2606e-03, -2.3481e-02,  ..., -2.1107e-03,\n",
            "           4.9070e-03,  6.0702e-03],\n",
            "         ...,\n",
            "         [ 1.0223e-03, -1.2080e-02, -1.5868e-02,  ..., -1.2882e-02,\n",
            "          -2.0567e-03,  4.8362e-03],\n",
            "         [ 7.8468e-03, -1.1038e-02, -1.0770e-02,  ...,  7.4988e-04,\n",
            "           1.5215e-03, -1.6774e-03],\n",
            "         [ 2.3028e-03, -5.9798e-03, -1.7329e-02,  ...,  5.5724e-04,\n",
            "           1.5857e-02,  4.3269e-03]],\n",
            "\n",
            "        [[ 6.0997e-03,  1.7106e-03, -2.1188e-02,  ...,  2.5523e-03,\n",
            "           2.1776e-03,  7.7525e-03],\n",
            "         [ 6.9275e-03, -6.8095e-03, -1.3799e-02,  ..., -3.6580e-03,\n",
            "           5.6513e-03,  1.9980e-03],\n",
            "         [ 6.2942e-04, -7.5124e-03, -9.4807e-03,  ...,  9.2128e-03,\n",
            "           1.0227e-02,  3.9989e-03],\n",
            "         ...,\n",
            "         [-8.9265e-04, -1.5108e-02, -1.9400e-02,  ..., -3.9164e-03,\n",
            "           4.4435e-03, -3.3369e-04],\n",
            "         [ 9.0713e-03, -1.0043e-02, -2.8737e-02,  ..., -4.9359e-03,\n",
            "           9.1699e-03,  5.4980e-03],\n",
            "         [-8.9653e-05, -7.9620e-03, -2.2422e-02,  ..., -2.5861e-03,\n",
            "           2.1928e-03,  6.9095e-03]],\n",
            "\n",
            "        [[ 2.2506e-04, -1.2583e-02, -1.8306e-02,  ..., -1.1051e-02,\n",
            "           9.2462e-03,  8.1454e-03],\n",
            "         [-9.4987e-04, -1.2930e-02, -2.0111e-02,  ..., -9.7730e-03,\n",
            "           5.3495e-03,  1.8893e-03],\n",
            "         [-2.7428e-03, -8.8699e-03, -1.4280e-02,  ...,  5.0471e-03,\n",
            "           5.3924e-03,  1.0839e-02],\n",
            "         ...,\n",
            "         [ 7.2136e-03, -9.1107e-03, -1.5424e-02,  ...,  1.9115e-03,\n",
            "           6.7415e-03, -8.7823e-04],\n",
            "         [-4.8866e-03, -9.1675e-03, -1.5351e-02,  ..., -5.0427e-03,\n",
            "           8.2830e-03,  1.1029e-02],\n",
            "         [-4.4785e-03, -1.2034e-02, -1.6046e-02,  ..., -1.3892e-03,\n",
            "           1.1470e-02, -4.6686e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.1645e-03,  2.2294e-03, -2.3786e-02,  ..., -2.2859e-04,\n",
            "           6.9243e-03,  8.3819e-03],\n",
            "         [ 4.7722e-03, -5.7014e-03, -1.7818e-02,  ..., -5.6092e-03,\n",
            "           1.0013e-02,  1.3503e-03],\n",
            "         [-2.0599e-03, -4.0076e-03, -2.7204e-02,  ...,  2.8766e-03,\n",
            "           1.4182e-02, -4.1284e-04],\n",
            "         ...,\n",
            "         [-2.6131e-03, -1.2682e-02, -1.9781e-02,  ...,  2.6591e-03,\n",
            "           5.9818e-03,  1.0518e-02],\n",
            "         [ 5.3825e-03, -7.2549e-03, -1.3387e-02,  ..., -5.7954e-04,\n",
            "           5.6404e-03,  2.1621e-03],\n",
            "         [ 1.1158e-03,  5.8145e-04, -2.5354e-02,  ...,  1.2668e-03,\n",
            "           9.6658e-03,  5.6201e-03]],\n",
            "\n",
            "        [[ 5.7640e-03, -5.7109e-03, -1.9671e-02,  ..., -6.6449e-03,\n",
            "          -2.2398e-03,  3.5010e-04],\n",
            "         [ 4.0493e-03, -1.3674e-02, -2.2840e-02,  ..., -2.0630e-03,\n",
            "           1.2584e-02, -4.8818e-03],\n",
            "         [ 1.1436e-05, -3.8550e-03, -2.1955e-02,  ..., -3.6313e-03,\n",
            "           3.4708e-03,  2.7534e-03],\n",
            "         ...,\n",
            "         [ 7.8735e-03, -1.2430e-02, -2.0399e-02,  ..., -1.3462e-02,\n",
            "           1.2340e-02, -4.7330e-03],\n",
            "         [ 2.9120e-03, -1.2139e-02, -6.7417e-03,  ...,  4.5043e-04,\n",
            "           7.1866e-05,  9.1229e-04],\n",
            "         [-3.6163e-03, -4.2251e-03, -2.5861e-02,  ..., -6.7784e-03,\n",
            "           5.3829e-03, -2.0913e-03]],\n",
            "\n",
            "        [[ 1.0143e-02, -1.1706e-02, -2.0403e-02,  ..., -5.1286e-03,\n",
            "           9.2310e-03,  1.6630e-03],\n",
            "         [ 5.0354e-03, -6.9526e-03, -1.5557e-02,  ..., -3.4320e-03,\n",
            "           9.9329e-03,  5.0002e-03],\n",
            "         [ 3.1037e-03, -1.5677e-02, -2.5472e-02,  ..., -1.3558e-03,\n",
            "           2.2081e-03,  4.0732e-03],\n",
            "         ...,\n",
            "         [ 3.1457e-03, -4.8411e-03, -1.6359e-02,  ...,  3.0177e-03,\n",
            "           9.1890e-03, -4.2676e-03],\n",
            "         [ 5.7907e-03, -1.0981e-02, -2.1062e-02,  ..., -9.6700e-03,\n",
            "           4.3233e-03,  1.2432e-03],\n",
            "         [-5.3413e-05, -5.7033e-03, -2.5506e-02,  ..., -3.7009e-03,\n",
            "           7.3665e-03,  1.6910e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4741\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/eurosat/DAPT/vit_b16_16shots/seed1/tensorboard)\n",
            "epoch [1/200] batch [5/5] time 0.289 (0.629) data 0.000 (0.148) loss 8.6019 (8.5046) acc 28.1250 (31.2500) lr 2.0000e+01 eta 0:10:26\n",
            "epoch [2/200] batch [5/5] time 0.289 (0.411) data 0.000 (0.122) loss 5.6463 (6.7479) acc 15.6250 (14.3750) lr 1.9999e+01 eta 0:06:47\n",
            "epoch [3/200] batch [5/5] time 0.289 (0.414) data 0.000 (0.125) loss 4.7020 (7.2953) acc 9.3750 (15.6250) lr 1.9995e+01 eta 0:06:47\n",
            "epoch [4/200] batch [5/5] time 0.289 (0.415) data 0.000 (0.125) loss 4.1519 (4.1577) acc 12.5000 (25.6250) lr 1.9989e+01 eta 0:06:46\n",
            "epoch [5/200] batch [5/5] time 0.290 (0.412) data 0.000 (0.123) loss 3.4875 (3.6199) acc 28.1250 (28.1250) lr 1.9980e+01 eta 0:06:41\n",
            "epoch [6/200] batch [5/5] time 0.288 (0.418) data 0.000 (0.128) loss 2.6002 (2.8717) acc 37.5000 (36.2500) lr 1.9969e+01 eta 0:06:45\n",
            "epoch [7/200] batch [5/5] time 0.291 (0.414) data 0.000 (0.122) loss 2.1542 (2.1710) acc 43.7500 (58.1250) lr 1.9956e+01 eta 0:06:39\n",
            "epoch [8/200] batch [5/5] time 0.292 (0.418) data 0.000 (0.127) loss 5.4577 (4.3382) acc 28.1250 (32.5000) lr 1.9940e+01 eta 0:06:41\n",
            "epoch [9/200] batch [5/5] time 0.292 (0.423) data 0.000 (0.131) loss 10.1549 (7.5879) acc 18.7500 (18.7500) lr 1.9921e+01 eta 0:06:43\n",
            "epoch [10/200] batch [5/5] time 0.294 (0.415) data 0.000 (0.124) loss 3.9715 (6.8528) acc 9.3750 (22.5000) lr 1.9900e+01 eta 0:06:34\n",
            "epoch [11/200] batch [5/5] time 0.294 (0.415) data 0.000 (0.123) loss 3.0786 (3.4025) acc 31.2500 (24.3750) lr 1.9877e+01 eta 0:06:32\n",
            "epoch [12/200] batch [5/5] time 0.290 (0.417) data 0.001 (0.126) loss 1.8868 (2.1118) acc 53.1250 (44.3750) lr 1.9851e+01 eta 0:06:32\n",
            "epoch [13/200] batch [5/5] time 0.291 (0.412) data 0.000 (0.120) loss 1.4571 (1.9435) acc 59.3750 (50.6250) lr 1.9823e+01 eta 0:06:24\n",
            "epoch [14/200] batch [5/5] time 0.293 (0.415) data 0.000 (0.122) loss 2.4157 (1.9455) acc 43.7500 (55.6250) lr 1.9792e+01 eta 0:06:25\n",
            "epoch [15/200] batch [5/5] time 0.294 (0.421) data 0.000 (0.128) loss 5.3201 (2.9281) acc 21.8750 (43.1250) lr 1.9759e+01 eta 0:06:29\n",
            "epoch [16/200] batch [5/5] time 0.294 (0.423) data 0.000 (0.130) loss 4.2458 (5.5591) acc 37.5000 (29.3750) lr 1.9724e+01 eta 0:06:29\n",
            "epoch [17/200] batch [5/5] time 0.294 (0.409) data 0.000 (0.115) loss 2.4901 (3.2462) acc 56.2500 (45.6250) lr 1.9686e+01 eta 0:06:14\n",
            "epoch [18/200] batch [5/5] time 0.298 (0.419) data 0.000 (0.125) loss 3.2132 (2.9561) acc 46.8750 (47.5000) lr 1.9646e+01 eta 0:06:21\n",
            "epoch [19/200] batch [5/5] time 0.293 (0.415) data 0.000 (0.120) loss 2.1643 (2.3156) acc 59.3750 (50.6250) lr 1.9603e+01 eta 0:06:15\n",
            "epoch [20/200] batch [5/5] time 0.295 (0.414) data 0.000 (0.117) loss 1.6640 (1.8040) acc 59.3750 (59.3750) lr 1.9558e+01 eta 0:06:12\n",
            "epoch [21/200] batch [5/5] time 0.295 (0.423) data 0.000 (0.127) loss 1.6729 (1.9249) acc 68.7500 (57.5000) lr 1.9511e+01 eta 0:06:18\n",
            "epoch [22/200] batch [5/5] time 0.298 (0.422) data 0.000 (0.124) loss 2.2840 (2.3055) acc 46.8750 (58.1250) lr 1.9461e+01 eta 0:06:15\n",
            "epoch [23/200] batch [5/5] time 0.300 (0.423) data 0.000 (0.124) loss 6.2130 (4.0090) acc 28.1250 (39.3750) lr 1.9409e+01 eta 0:06:14\n",
            "epoch [24/200] batch [5/5] time 0.298 (0.423) data 0.000 (0.125) loss 11.3911 (10.0375) acc 15.6250 (14.3750) lr 1.9354e+01 eta 0:06:12\n",
            "epoch [25/200] batch [5/5] time 0.302 (0.422) data 0.000 (0.121) loss 4.1768 (5.7061) acc 15.6250 (16.8750) lr 1.9298e+01 eta 0:06:09\n",
            "epoch [26/200] batch [5/5] time 0.303 (0.417) data 0.000 (0.115) loss 2.8057 (3.6110) acc 28.1250 (28.7500) lr 1.9239e+01 eta 0:06:02\n",
            "epoch [27/200] batch [5/5] time 0.304 (0.434) data 0.000 (0.133) loss 2.7755 (2.2222) acc 34.3750 (45.0000) lr 1.9178e+01 eta 0:06:15\n",
            "epoch [28/200] batch [5/5] time 0.302 (0.429) data 0.000 (0.127) loss 1.7056 (1.7413) acc 53.1250 (53.7500) lr 1.9114e+01 eta 0:06:08\n",
            "epoch [29/200] batch [5/5] time 0.303 (0.429) data 0.000 (0.126) loss 2.2585 (1.5601) acc 56.2500 (63.7500) lr 1.9048e+01 eta 0:06:06\n",
            "epoch [30/200] batch [5/5] time 0.299 (0.429) data 0.000 (0.128) loss 2.2958 (2.2038) acc 56.2500 (58.1250) lr 1.8980e+01 eta 0:06:04\n",
            "epoch [31/200] batch [5/5] time 0.301 (0.423) data 0.000 (0.123) loss 6.7691 (6.9576) acc 21.8750 (21.8750) lr 1.8910e+01 eta 0:05:57\n",
            "epoch [32/200] batch [5/5] time 0.301 (0.419) data 0.000 (0.119) loss 2.2752 (5.0486) acc 40.6250 (24.3750) lr 1.8838e+01 eta 0:05:51\n",
            "epoch [33/200] batch [5/5] time 0.300 (0.432) data 0.000 (0.133) loss 1.7683 (2.1209) acc 53.1250 (48.7500) lr 1.8763e+01 eta 0:06:00\n",
            "epoch [34/200] batch [5/5] time 0.303 (0.421) data 0.000 (0.123) loss 1.8436 (2.0240) acc 53.1250 (51.2500) lr 1.8686e+01 eta 0:05:49\n",
            "epoch [35/200] batch [5/5] time 0.296 (0.415) data 0.000 (0.119) loss 1.6181 (2.2065) acc 65.6250 (52.5000) lr 1.8607e+01 eta 0:05:42\n",
            "epoch [36/200] batch [5/5] time 0.299 (0.417) data 0.000 (0.120) loss 1.4803 (1.5767) acc 59.3750 (56.8750) lr 1.8526e+01 eta 0:05:41\n",
            "epoch [37/200] batch [5/5] time 0.295 (0.419) data 0.000 (0.123) loss 2.0994 (2.0621) acc 59.3750 (55.6250) lr 1.8443e+01 eta 0:05:41\n",
            "epoch [38/200] batch [5/5] time 0.295 (0.418) data 0.000 (0.122) loss 3.5180 (4.6382) acc 28.1250 (33.7500) lr 1.8358e+01 eta 0:05:38\n",
            "epoch [39/200] batch [5/5] time 0.298 (0.423) data 0.000 (0.126) loss 4.5195 (5.7760) acc 31.2500 (23.7500) lr 1.8271e+01 eta 0:05:40\n",
            "epoch [40/200] batch [5/5] time 0.297 (0.421) data 0.000 (0.125) loss 1.4166 (1.8700) acc 68.7500 (55.6250) lr 1.8181e+01 eta 0:05:36\n",
            "epoch [41/200] batch [5/5] time 0.294 (0.416) data 0.000 (0.121) loss 1.4254 (1.5688) acc 62.5000 (59.3750) lr 1.8090e+01 eta 0:05:30\n",
            "epoch [42/200] batch [5/5] time 0.295 (0.415) data 0.000 (0.119) loss 2.8656 (1.7514) acc 46.8750 (57.5000) lr 1.7997e+01 eta 0:05:27\n",
            "epoch [43/200] batch [5/5] time 0.292 (0.413) data 0.000 (0.119) loss 1.0046 (1.6126) acc 78.1250 (60.6250) lr 1.7902e+01 eta 0:05:24\n",
            "epoch [44/200] batch [5/5] time 0.296 (0.414) data 0.000 (0.121) loss 3.0328 (1.6122) acc 46.8750 (67.5000) lr 1.7804e+01 eta 0:05:23\n",
            "epoch [45/200] batch [5/5] time 0.294 (0.420) data 0.000 (0.127) loss 3.5170 (1.9181) acc 37.5000 (58.1250) lr 1.7705e+01 eta 0:05:25\n",
            "epoch [46/200] batch [5/5] time 0.292 (0.422) data 0.000 (0.129) loss 9.3597 (7.4149) acc 6.2500 (11.2500) lr 1.7604e+01 eta 0:05:24\n",
            "epoch [47/200] batch [5/5] time 0.293 (0.412) data 0.000 (0.120) loss 4.3627 (6.5365) acc 15.6250 (18.7500) lr 1.7501e+01 eta 0:05:15\n",
            "epoch [48/200] batch [5/5] time 0.296 (0.415) data 0.000 (0.121) loss 1.9117 (2.5429) acc 50.0000 (39.3750) lr 1.7396e+01 eta 0:05:15\n",
            "epoch [49/200] batch [5/5] time 0.294 (0.415) data 0.000 (0.121) loss 1.7338 (1.8516) acc 53.1250 (48.7500) lr 1.7290e+01 eta 0:05:13\n",
            "epoch [50/200] batch [5/5] time 0.295 (0.417) data 0.000 (0.122) loss 1.3382 (1.4228) acc 71.8750 (68.1250) lr 1.7181e+01 eta 0:05:12\n",
            "epoch [51/200] batch [5/5] time 0.296 (0.420) data 0.000 (0.125) loss 1.1575 (1.4001) acc 78.1250 (66.2500) lr 1.7071e+01 eta 0:05:12\n",
            "epoch [52/200] batch [5/5] time 0.297 (0.424) data 0.000 (0.129) loss 1.9157 (2.2243) acc 46.8750 (52.5000) lr 1.6959e+01 eta 0:05:14\n",
            "epoch [53/200] batch [5/5] time 0.295 (0.412) data 0.000 (0.116) loss 4.8802 (3.4424) acc 31.2500 (47.5000) lr 1.6845e+01 eta 0:05:02\n",
            "epoch [54/200] batch [5/5] time 0.296 (0.412) data 0.000 (0.116) loss 3.5760 (5.3328) acc 53.1250 (41.8750) lr 1.6730e+01 eta 0:05:01\n",
            "epoch [55/200] batch [5/5] time 0.300 (0.420) data 0.000 (0.122) loss 2.8800 (3.2682) acc 50.0000 (38.1250) lr 1.6613e+01 eta 0:05:04\n",
            "epoch [56/200] batch [5/5] time 0.298 (0.420) data 0.000 (0.122) loss 1.4758 (2.1030) acc 56.2500 (57.5000) lr 1.6494e+01 eta 0:05:02\n",
            "epoch [57/200] batch [5/5] time 0.302 (0.429) data 0.000 (0.127) loss 1.2553 (1.3668) acc 68.7500 (65.6250) lr 1.6374e+01 eta 0:05:06\n",
            "epoch [58/200] batch [5/5] time 0.300 (0.426) data 0.000 (0.126) loss 1.7536 (1.2698) acc 62.5000 (70.0000) lr 1.6252e+01 eta 0:05:02\n",
            "epoch [59/200] batch [5/5] time 0.300 (0.427) data 0.000 (0.127) loss 1.3898 (1.8467) acc 65.6250 (65.6250) lr 1.6129e+01 eta 0:05:01\n",
            "epoch [60/200] batch [5/5] time 0.300 (0.420) data 0.000 (0.118) loss 1.5567 (1.4986) acc 81.2500 (69.3750) lr 1.6004e+01 eta 0:04:54\n",
            "epoch [61/200] batch [5/5] time 0.301 (0.420) data 0.000 (0.119) loss 1.6172 (1.6108) acc 65.6250 (65.6250) lr 1.5878e+01 eta 0:04:52\n",
            "epoch [62/200] batch [5/5] time 0.303 (0.428) data 0.000 (0.125) loss 2.1182 (1.3968) acc 62.5000 (70.0000) lr 1.5750e+01 eta 0:04:55\n",
            "epoch [63/200] batch [5/5] time 0.302 (0.431) data 0.000 (0.128) loss 3.2474 (2.2877) acc 34.3750 (54.3750) lr 1.5621e+01 eta 0:04:55\n",
            "epoch [64/200] batch [5/5] time 0.302 (0.433) data 0.000 (0.130) loss 1.0780 (1.2465) acc 81.2500 (73.7500) lr 1.5490e+01 eta 0:04:54\n",
            "epoch [65/200] batch [5/5] time 0.302 (0.428) data 0.000 (0.125) loss 0.7244 (1.2872) acc 84.3750 (76.2500) lr 1.5358e+01 eta 0:04:48\n",
            "epoch [66/200] batch [5/5] time 0.304 (0.424) data 0.000 (0.120) loss 0.9601 (1.2607) acc 78.1250 (72.5000) lr 1.5225e+01 eta 0:04:44\n",
            "epoch [67/200] batch [5/5] time 0.307 (0.426) data 0.000 (0.119) loss 1.3146 (1.1807) acc 68.7500 (72.5000) lr 1.5090e+01 eta 0:04:43\n",
            "epoch [68/200] batch [5/5] time 0.306 (0.427) data 0.000 (0.123) loss 1.5232 (1.6181) acc 68.7500 (72.5000) lr 1.4955e+01 eta 0:04:41\n",
            "epoch [69/200] batch [5/5] time 0.309 (0.437) data 0.000 (0.128) loss 3.0293 (1.5409) acc 75.0000 (75.0000) lr 1.4818e+01 eta 0:04:46\n",
            "epoch [70/200] batch [5/5] time 0.309 (0.429) data 0.000 (0.121) loss 2.4731 (1.7798) acc 40.6250 (63.7500) lr 1.4679e+01 eta 0:04:38\n",
            "epoch [71/200] batch [5/5] time 0.309 (0.429) data 0.000 (0.120) loss 1.3552 (1.7514) acc 71.8750 (65.0000) lr 1.4540e+01 eta 0:04:36\n",
            "epoch [72/200] batch [5/5] time 0.318 (0.429) data 0.000 (0.117) loss 1.8814 (1.1830) acc 59.3750 (73.7500) lr 1.4399e+01 eta 0:04:34\n",
            "epoch [73/200] batch [5/5] time 0.311 (0.440) data 0.000 (0.127) loss 1.5252 (1.5223) acc 68.7500 (66.2500) lr 1.4258e+01 eta 0:04:39\n",
            "epoch [74/200] batch [5/5] time 0.313 (0.436) data 0.000 (0.124) loss 0.6400 (1.2321) acc 87.5000 (76.8750) lr 1.4115e+01 eta 0:04:34\n",
            "epoch [75/200] batch [5/5] time 0.312 (0.438) data 0.000 (0.125) loss 0.4975 (0.8580) acc 93.7500 (79.3750) lr 1.3971e+01 eta 0:04:33\n",
            "epoch [76/200] batch [5/5] time 0.316 (0.441) data 0.000 (0.127) loss 0.5481 (0.9919) acc 93.7500 (77.5000) lr 1.3827e+01 eta 0:04:33\n",
            "epoch [77/200] batch [5/5] time 0.314 (0.442) data 0.000 (0.126) loss 1.5830 (1.3838) acc 65.6250 (71.8750) lr 1.3681e+01 eta 0:04:31\n",
            "epoch [78/200] batch [5/5] time 0.316 (0.436) data 0.001 (0.120) loss 2.0351 (2.0364) acc 56.2500 (61.8750) lr 1.3535e+01 eta 0:04:26\n",
            "epoch [79/200] batch [5/5] time 0.320 (0.438) data 0.000 (0.120) loss 1.4173 (1.7401) acc 75.0000 (63.1250) lr 1.3387e+01 eta 0:04:24\n",
            "epoch [80/200] batch [5/5] time 0.318 (0.439) data 0.000 (0.122) loss 1.3095 (1.2838) acc 75.0000 (73.7500) lr 1.3239e+01 eta 0:04:23\n",
            "epoch [81/200] batch [5/5] time 0.318 (0.441) data 0.000 (0.123) loss 1.9293 (1.3294) acc 65.6250 (71.8750) lr 1.3090e+01 eta 0:04:22\n",
            "epoch [82/200] batch [5/5] time 0.312 (0.444) data 0.000 (0.129) loss 1.7185 (1.1687) acc 65.6250 (77.5000) lr 1.2940e+01 eta 0:04:22\n",
            "epoch [83/200] batch [5/5] time 0.310 (0.434) data 0.000 (0.121) loss 1.8110 (1.1799) acc 56.2500 (75.0000) lr 1.2790e+01 eta 0:04:14\n",
            "epoch [84/200] batch [5/5] time 0.317 (0.438) data 0.000 (0.124) loss 0.9585 (1.2019) acc 78.1250 (78.1250) lr 1.2639e+01 eta 0:04:13\n",
            "epoch [85/200] batch [5/5] time 0.308 (0.436) data 0.000 (0.126) loss 1.0326 (0.9584) acc 78.1250 (80.6250) lr 1.2487e+01 eta 0:04:10\n",
            "epoch [86/200] batch [5/5] time 0.312 (0.431) data 0.000 (0.120) loss 2.0504 (1.5879) acc 65.6250 (70.0000) lr 1.2334e+01 eta 0:04:05\n",
            "epoch [87/200] batch [5/5] time 0.307 (0.440) data 0.000 (0.131) loss 0.9840 (1.1847) acc 78.1250 (75.6250) lr 1.2181e+01 eta 0:04:08\n",
            "epoch [88/200] batch [5/5] time 0.307 (0.432) data 0.000 (0.124) loss 1.0255 (1.5244) acc 71.8750 (66.8750) lr 1.2028e+01 eta 0:04:01\n",
            "epoch [89/200] batch [5/5] time 0.307 (0.428) data 0.000 (0.120) loss 1.5538 (1.1855) acc 75.0000 (75.6250) lr 1.1874e+01 eta 0:03:57\n",
            "epoch [90/200] batch [5/5] time 0.305 (0.427) data 0.000 (0.121) loss 1.2735 (1.7800) acc 71.8750 (66.2500) lr 1.1719e+01 eta 0:03:54\n",
            "epoch [91/200] batch [5/5] time 0.304 (0.425) data 0.000 (0.119) loss 1.4969 (1.4391) acc 78.1250 (71.2500) lr 1.1564e+01 eta 0:03:51\n",
            "epoch [92/200] batch [5/5] time 0.302 (0.433) data 0.000 (0.131) loss 0.7960 (1.0804) acc 84.3750 (77.5000) lr 1.1409e+01 eta 0:03:53\n",
            "epoch [93/200] batch [5/5] time 0.301 (0.431) data 0.000 (0.129) loss 1.3659 (1.4302) acc 75.0000 (69.3750) lr 1.1253e+01 eta 0:03:50\n",
            "epoch [94/200] batch [5/5] time 0.303 (0.422) data 0.000 (0.122) loss 2.0818 (1.3780) acc 46.8750 (68.7500) lr 1.1097e+01 eta 0:03:43\n",
            "epoch [95/200] batch [5/5] time 0.300 (0.420) data 0.000 (0.120) loss 0.7829 (1.6478) acc 78.1250 (61.2500) lr 1.0941e+01 eta 0:03:40\n",
            "epoch [96/200] batch [5/5] time 0.305 (0.419) data 0.000 (0.119) loss 1.2536 (1.0539) acc 75.0000 (77.5000) lr 1.0785e+01 eta 0:03:38\n",
            "epoch [97/200] batch [5/5] time 0.299 (0.426) data 0.000 (0.125) loss 1.0361 (1.5467) acc 78.1250 (73.1250) lr 1.0628e+01 eta 0:03:39\n",
            "epoch [98/200] batch [5/5] time 0.302 (0.422) data 0.000 (0.121) loss 1.2341 (1.3764) acc 62.5000 (70.0000) lr 1.0471e+01 eta 0:03:35\n",
            "epoch [99/200] batch [5/5] time 0.302 (0.430) data 0.000 (0.129) loss 1.5533 (1.4729) acc 56.2500 (63.7500) lr 1.0314e+01 eta 0:03:37\n",
            "epoch [100/200] batch [5/5] time 0.301 (0.423) data 0.000 (0.123) loss 1.0655 (0.8579) acc 81.2500 (79.3750) lr 1.0157e+01 eta 0:03:31\n",
            "epoch [101/200] batch [5/5] time 0.300 (0.419) data 0.000 (0.119) loss 1.6197 (1.0810) acc 59.3750 (76.2500) lr 1.0000e+01 eta 0:03:27\n",
            "epoch [102/200] batch [5/5] time 0.303 (0.420) data 0.000 (0.118) loss 0.7553 (1.0342) acc 87.5000 (79.3750) lr 9.8429e+00 eta 0:03:25\n",
            "epoch [103/200] batch [5/5] time 0.304 (0.425) data 0.000 (0.121) loss 2.3414 (1.5318) acc 62.5000 (68.1250) lr 9.6859e+00 eta 0:03:25\n",
            "epoch [104/200] batch [5/5] time 0.305 (0.423) data 0.000 (0.120) loss 0.8320 (0.8856) acc 78.1250 (80.6250) lr 9.5289e+00 eta 0:03:22\n",
            "epoch [105/200] batch [5/5] time 0.305 (0.431) data 0.000 (0.126) loss 0.6444 (0.9999) acc 81.2500 (74.3750) lr 9.3721e+00 eta 0:03:24\n",
            "epoch [106/200] batch [5/5] time 0.307 (0.429) data 0.000 (0.123) loss 1.6277 (0.9569) acc 71.8750 (78.7500) lr 9.2154e+00 eta 0:03:21\n",
            "epoch [107/200] batch [5/5] time 0.306 (0.424) data 0.000 (0.116) loss 0.7971 (0.9040) acc 81.2500 (78.1250) lr 9.0589e+00 eta 0:03:17\n",
            "epoch [108/200] batch [5/5] time 0.309 (0.430) data 0.000 (0.120) loss 0.9349 (0.7791) acc 75.0000 (83.7500) lr 8.9027e+00 eta 0:03:17\n",
            "epoch [109/200] batch [5/5] time 0.308 (0.431) data 0.000 (0.123) loss 1.2316 (1.2851) acc 75.0000 (70.0000) lr 8.7467e+00 eta 0:03:16\n",
            "epoch [110/200] batch [5/5] time 0.312 (0.433) data 0.000 (0.122) loss 1.2718 (0.9409) acc 81.2500 (83.1250) lr 8.5910e+00 eta 0:03:14\n",
            "epoch [111/200] batch [5/5] time 0.310 (0.437) data 0.000 (0.126) loss 1.0287 (0.9516) acc 81.2500 (78.7500) lr 8.4357e+00 eta 0:03:14\n",
            "epoch [112/200] batch [5/5] time 0.315 (0.437) data 0.000 (0.125) loss 1.1341 (0.9878) acc 75.0000 (81.2500) lr 8.2807e+00 eta 0:03:12\n",
            "epoch [113/200] batch [5/5] time 0.315 (0.437) data 0.000 (0.124) loss 0.6786 (0.8648) acc 90.6250 (83.1250) lr 8.1262e+00 eta 0:03:10\n",
            "epoch [114/200] batch [5/5] time 0.321 (0.441) data 0.000 (0.126) loss 0.8440 (0.8752) acc 81.2500 (81.8750) lr 7.9721e+00 eta 0:03:09\n",
            "epoch [115/200] batch [5/5] time 0.313 (0.429) data 0.000 (0.115) loss 0.9536 (0.7729) acc 75.0000 (79.3750) lr 7.8186e+00 eta 0:03:02\n",
            "epoch [116/200] batch [5/5] time 0.318 (0.439) data 0.000 (0.121) loss 0.9750 (0.9021) acc 71.8750 (76.2500) lr 7.6655e+00 eta 0:03:04\n",
            "epoch [117/200] batch [5/5] time 0.319 (0.438) data 0.000 (0.122) loss 0.8405 (0.6186) acc 71.8750 (84.3750) lr 7.5131e+00 eta 0:03:01\n",
            "epoch [118/200] batch [5/5] time 0.320 (0.446) data 0.000 (0.129) loss 0.6091 (0.6948) acc 93.7500 (88.1250) lr 7.3613e+00 eta 0:03:02\n",
            "epoch [119/200] batch [5/5] time 0.318 (0.438) data 0.000 (0.120) loss 0.4662 (0.7100) acc 90.6250 (84.3750) lr 7.2101e+00 eta 0:02:57\n",
            "epoch [120/200] batch [5/5] time 0.320 (0.442) data 0.000 (0.124) loss 1.7628 (0.9371) acc 62.5000 (80.6250) lr 7.0596e+00 eta 0:02:56\n",
            "epoch [121/200] batch [5/5] time 0.317 (0.444) data 0.000 (0.126) loss 0.5925 (1.0031) acc 90.6250 (78.7500) lr 6.9098e+00 eta 0:02:55\n",
            "epoch [122/200] batch [5/5] time 0.314 (0.439) data 0.000 (0.122) loss 0.9088 (0.9801) acc 71.8750 (78.7500) lr 6.7608e+00 eta 0:02:51\n",
            "epoch [123/200] batch [5/5] time 0.316 (0.442) data 0.000 (0.127) loss 0.5863 (0.8252) acc 87.5000 (81.8750) lr 6.6126e+00 eta 0:02:50\n",
            "epoch [124/200] batch [5/5] time 0.312 (0.445) data 0.000 (0.131) loss 0.8357 (0.9927) acc 75.0000 (78.7500) lr 6.4653e+00 eta 0:02:49\n",
            "epoch [125/200] batch [5/5] time 0.314 (0.436) data 0.000 (0.124) loss 1.2788 (0.8852) acc 81.2500 (87.5000) lr 6.3188e+00 eta 0:02:43\n",
            "epoch [126/200] batch [5/5] time 0.313 (0.436) data 0.000 (0.122) loss 1.1278 (1.0961) acc 78.1250 (77.5000) lr 6.1732e+00 eta 0:02:41\n",
            "epoch [127/200] batch [5/5] time 0.311 (0.433) data 0.000 (0.122) loss 0.7012 (0.7849) acc 84.3750 (83.7500) lr 6.0285e+00 eta 0:02:38\n",
            "epoch [128/200] batch [5/5] time 0.310 (0.428) data 0.000 (0.119) loss 0.4908 (0.6541) acc 93.7500 (88.7500) lr 5.8849e+00 eta 0:02:34\n",
            "epoch [129/200] batch [5/5] time 0.312 (0.436) data 0.000 (0.127) loss 0.7471 (0.7388) acc 78.1250 (83.1250) lr 5.7422e+00 eta 0:02:34\n",
            "epoch [130/200] batch [5/5] time 0.306 (0.432) data 0.000 (0.125) loss 0.6019 (0.6833) acc 90.6250 (86.8750) lr 5.6006e+00 eta 0:02:31\n",
            "epoch [131/200] batch [5/5] time 0.308 (0.427) data 0.000 (0.120) loss 0.7927 (0.6298) acc 78.1250 (88.7500) lr 5.4601e+00 eta 0:02:27\n",
            "epoch [132/200] batch [5/5] time 0.308 (0.431) data 0.000 (0.123) loss 0.5338 (0.7784) acc 90.6250 (83.7500) lr 5.3207e+00 eta 0:02:26\n",
            "epoch [133/200] batch [5/5] time 0.305 (0.426) data 0.000 (0.120) loss 0.8418 (0.7762) acc 84.3750 (86.2500) lr 5.1825e+00 eta 0:02:22\n",
            "epoch [134/200] batch [5/5] time 0.312 (0.433) data 0.000 (0.127) loss 1.2329 (0.8606) acc 75.0000 (81.8750) lr 5.0454e+00 eta 0:02:22\n",
            "epoch [135/200] batch [5/5] time 0.305 (0.432) data 0.000 (0.126) loss 0.6205 (0.7970) acc 84.3750 (84.3750) lr 4.9096e+00 eta 0:02:20\n",
            "epoch [136/200] batch [5/5] time 0.302 (0.433) data 0.000 (0.128) loss 0.6319 (0.7871) acc 87.5000 (82.5000) lr 4.7750e+00 eta 0:02:18\n",
            "epoch [137/200] batch [5/5] time 0.304 (0.423) data 0.000 (0.120) loss 0.5627 (0.7340) acc 90.6250 (86.8750) lr 4.6417e+00 eta 0:02:13\n",
            "epoch [138/200] batch [5/5] time 0.303 (0.424) data 0.000 (0.120) loss 0.8001 (0.6115) acc 84.3750 (86.8750) lr 4.5098e+00 eta 0:02:11\n",
            "epoch [139/200] batch [5/5] time 0.302 (0.423) data 0.000 (0.121) loss 0.6690 (0.5807) acc 90.6250 (89.3750) lr 4.3792e+00 eta 0:02:09\n",
            "epoch [140/200] batch [5/5] time 0.303 (0.422) data 0.000 (0.119) loss 0.5898 (0.6782) acc 87.5000 (86.2500) lr 4.2499e+00 eta 0:02:06\n",
            "epoch [141/200] batch [5/5] time 0.303 (0.429) data 0.000 (0.125) loss 0.7019 (0.5120) acc 87.5000 (90.0000) lr 4.1221e+00 eta 0:02:06\n",
            "epoch [142/200] batch [5/5] time 0.303 (0.428) data 0.000 (0.123) loss 0.4666 (0.5625) acc 93.7500 (90.0000) lr 3.9958e+00 eta 0:02:03\n",
            "epoch [143/200] batch [5/5] time 0.303 (0.421) data 0.000 (0.118) loss 0.6425 (0.6121) acc 87.5000 (86.8750) lr 3.8709e+00 eta 0:02:00\n",
            "epoch [144/200] batch [5/5] time 0.302 (0.425) data 0.000 (0.120) loss 0.2776 (0.4929) acc 100.0000 (91.8750) lr 3.7476e+00 eta 0:01:58\n",
            "epoch [145/200] batch [5/5] time 0.303 (0.423) data 0.000 (0.118) loss 0.4065 (0.4641) acc 93.7500 (93.1250) lr 3.6258e+00 eta 0:01:56\n",
            "epoch [146/200] batch [5/5] time 0.306 (0.427) data 0.000 (0.121) loss 0.4432 (0.6157) acc 93.7500 (91.8750) lr 3.5055e+00 eta 0:01:55\n",
            "epoch [147/200] batch [5/5] time 0.308 (0.435) data 0.000 (0.128) loss 0.8405 (0.5159) acc 84.3750 (91.8750) lr 3.3869e+00 eta 0:01:55\n",
            "epoch [148/200] batch [5/5] time 0.307 (0.430) data 0.000 (0.121) loss 0.3739 (0.5694) acc 93.7500 (86.2500) lr 3.2699e+00 eta 0:01:51\n",
            "epoch [149/200] batch [5/5] time 0.309 (0.430) data 0.000 (0.120) loss 0.7240 (0.5113) acc 81.2500 (90.6250) lr 3.1545e+00 eta 0:01:49\n",
            "epoch [150/200] batch [5/5] time 0.311 (0.428) data 0.000 (0.118) loss 0.5574 (0.5321) acc 87.5000 (88.1250) lr 3.0409e+00 eta 0:01:46\n",
            "epoch [151/200] batch [5/5] time 0.313 (0.430) data 0.000 (0.117) loss 1.0277 (0.6932) acc 71.8750 (89.3750) lr 2.9289e+00 eta 0:01:45\n",
            "epoch [152/200] batch [5/5] time 0.311 (0.437) data 0.000 (0.124) loss 0.3642 (0.5526) acc 93.7500 (88.7500) lr 2.8187e+00 eta 0:01:44\n",
            "epoch [153/200] batch [5/5] time 0.312 (0.440) data 0.000 (0.127) loss 0.6263 (0.5770) acc 87.5000 (88.7500) lr 2.7103e+00 eta 0:01:43\n",
            "epoch [154/200] batch [5/5] time 0.315 (0.438) data 0.000 (0.125) loss 0.5565 (0.5097) acc 90.6250 (91.2500) lr 2.6037e+00 eta 0:01:40\n",
            "epoch [155/200] batch [5/5] time 0.313 (0.436) data 0.000 (0.121) loss 0.4779 (0.4723) acc 93.7500 (93.1250) lr 2.4989e+00 eta 0:01:38\n",
            "epoch [156/200] batch [5/5] time 0.314 (0.437) data 0.000 (0.123) loss 0.5965 (0.4979) acc 90.6250 (91.8750) lr 2.3959e+00 eta 0:01:36\n",
            "epoch [157/200] batch [5/5] time 0.313 (0.439) data 0.000 (0.128) loss 0.4088 (0.4223) acc 93.7500 (91.2500) lr 2.2949e+00 eta 0:01:34\n",
            "epoch [158/200] batch [5/5] time 0.317 (0.435) data 0.000 (0.122) loss 0.3839 (0.4954) acc 93.7500 (92.5000) lr 2.1957e+00 eta 0:01:31\n",
            "epoch [159/200] batch [5/5] time 0.317 (0.444) data 0.000 (0.132) loss 0.6343 (0.4981) acc 87.5000 (91.8750) lr 2.0984e+00 eta 0:01:31\n",
            "epoch [160/200] batch [5/5] time 0.308 (0.433) data 0.000 (0.124) loss 0.6130 (0.4852) acc 90.6250 (90.6250) lr 2.0032e+00 eta 0:01:26\n",
            "epoch [161/200] batch [5/5] time 0.311 (0.428) data 0.000 (0.117) loss 0.6512 (0.5441) acc 84.3750 (89.3750) lr 1.9098e+00 eta 0:01:23\n",
            "epoch [162/200] batch [5/5] time 0.313 (0.428) data 0.000 (0.116) loss 0.4442 (0.4831) acc 93.7500 (92.5000) lr 1.8185e+00 eta 0:01:21\n",
            "epoch [163/200] batch [5/5] time 0.312 (0.434) data 0.000 (0.123) loss 0.3577 (0.4625) acc 96.8750 (93.7500) lr 1.7292e+00 eta 0:01:20\n",
            "epoch [164/200] batch [5/5] time 0.310 (0.434) data 0.000 (0.124) loss 0.6056 (0.4816) acc 90.6250 (91.2500) lr 1.6419e+00 eta 0:01:18\n",
            "epoch [165/200] batch [5/5] time 0.307 (0.440) data 0.000 (0.131) loss 0.3935 (0.4320) acc 93.7500 (93.7500) lr 1.5567e+00 eta 0:01:17\n",
            "epoch [166/200] batch [5/5] time 0.309 (0.433) data 0.000 (0.124) loss 0.3511 (0.3891) acc 96.8750 (95.6250) lr 1.4736e+00 eta 0:01:13\n",
            "epoch [167/200] batch [5/5] time 0.312 (0.434) data 0.000 (0.125) loss 0.4499 (0.4112) acc 90.6250 (94.3750) lr 1.3926e+00 eta 0:01:11\n",
            "epoch [168/200] batch [5/5] time 0.311 (0.428) data 0.000 (0.118) loss 0.5755 (0.4397) acc 90.6250 (94.3750) lr 1.3137e+00 eta 0:01:08\n",
            "epoch [169/200] batch [5/5] time 0.307 (0.427) data 0.000 (0.119) loss 0.2973 (0.3302) acc 96.8750 (97.5000) lr 1.2369e+00 eta 0:01:06\n",
            "epoch [170/200] batch [5/5] time 0.305 (0.434) data 0.000 (0.126) loss 0.2702 (0.3508) acc 100.0000 (96.2500) lr 1.1623e+00 eta 0:01:05\n",
            "epoch [171/200] batch [5/5] time 0.310 (0.436) data 0.000 (0.127) loss 0.3575 (0.4335) acc 96.8750 (94.3750) lr 1.0899e+00 eta 0:01:03\n",
            "epoch [172/200] batch [5/5] time 0.308 (0.429) data 0.000 (0.125) loss 0.2635 (0.2957) acc 96.8750 (96.8750) lr 1.0197e+00 eta 0:01:00\n",
            "epoch [173/200] batch [5/5] time 0.306 (0.427) data 0.000 (0.121) loss 0.2281 (0.2993) acc 100.0000 (96.2500) lr 9.5173e-01 eta 0:00:57\n",
            "epoch [174/200] batch [5/5] time 0.309 (0.427) data 0.000 (0.119) loss 0.4120 (0.3475) acc 90.6250 (95.0000) lr 8.8597e-01 eta 0:00:55\n",
            "epoch [175/200] batch [5/5] time 0.311 (0.433) data 0.000 (0.125) loss 0.2386 (0.3066) acc 100.0000 (96.8750) lr 8.2245e-01 eta 0:00:54\n",
            "epoch [176/200] batch [5/5] time 0.310 (0.421) data 0.000 (0.113) loss 0.3316 (0.3969) acc 96.8750 (93.1250) lr 7.6120e-01 eta 0:00:50\n",
            "epoch [177/200] batch [5/5] time 0.308 (0.436) data 0.000 (0.128) loss 0.2281 (0.3073) acc 100.0000 (96.8750) lr 7.0224e-01 eta 0:00:50\n",
            "epoch [178/200] batch [5/5] time 0.311 (0.431) data 0.000 (0.123) loss 0.5362 (0.3615) acc 90.6250 (95.0000) lr 6.4556e-01 eta 0:00:47\n",
            "epoch [179/200] batch [5/5] time 0.307 (0.427) data 0.000 (0.121) loss 0.2706 (0.3032) acc 100.0000 (96.2500) lr 5.9119e-01 eta 0:00:44\n",
            "epoch [180/200] batch [5/5] time 0.302 (0.426) data 0.000 (0.120) loss 0.3195 (0.3566) acc 93.7500 (95.6250) lr 5.3915e-01 eta 0:00:42\n",
            "epoch [181/200] batch [5/5] time 0.307 (0.430) data 0.000 (0.124) loss 0.2263 (0.2823) acc 100.0000 (96.8750) lr 4.8943e-01 eta 0:00:40\n",
            "epoch [182/200] batch [5/5] time 0.310 (0.435) data 0.000 (0.127) loss 0.2598 (0.3439) acc 96.8750 (95.6250) lr 4.4207e-01 eta 0:00:39\n",
            "epoch [183/200] batch [5/5] time 0.308 (0.429) data 0.000 (0.121) loss 0.4235 (0.3351) acc 93.7500 (95.6250) lr 3.9706e-01 eta 0:00:36\n",
            "epoch [184/200] batch [5/5] time 0.309 (0.432) data 0.000 (0.124) loss 0.4430 (0.3849) acc 90.6250 (93.1250) lr 3.5443e-01 eta 0:00:34\n",
            "epoch [185/200] batch [5/5] time 0.310 (0.431) data 0.000 (0.122) loss 0.2149 (0.3092) acc 100.0000 (96.2500) lr 3.1417e-01 eta 0:00:32\n",
            "epoch [186/200] batch [5/5] time 0.313 (0.425) data 0.000 (0.118) loss 0.3091 (0.2721) acc 93.7500 (96.8750) lr 2.7630e-01 eta 0:00:29\n",
            "epoch [187/200] batch [5/5] time 0.306 (0.424) data 0.000 (0.117) loss 0.3068 (0.2753) acc 96.8750 (97.5000) lr 2.4083e-01 eta 0:00:27\n",
            "epoch [188/200] batch [5/5] time 0.309 (0.431) data 0.000 (0.123) loss 0.2324 (0.2591) acc 100.0000 (98.1250) lr 2.0777e-01 eta 0:00:25\n",
            "epoch [189/200] batch [5/5] time 0.307 (0.431) data 0.000 (0.124) loss 0.2360 (0.2413) acc 100.0000 (99.3750) lr 1.7713e-01 eta 0:00:23\n",
            "epoch [190/200] batch [5/5] time 0.307 (0.428) data 0.000 (0.119) loss 0.2519 (0.3145) acc 96.8750 (96.2500) lr 1.4891e-01 eta 0:00:21\n",
            "epoch [191/200] batch [5/5] time 0.303 (0.423) data 0.000 (0.116) loss 0.4569 (0.3630) acc 93.7500 (95.6250) lr 1.2312e-01 eta 0:00:19\n",
            "epoch [192/200] batch [5/5] time 0.309 (0.424) data 0.000 (0.115) loss 0.2317 (0.2570) acc 100.0000 (97.5000) lr 9.9763e-02 eta 0:00:16\n",
            "epoch [193/200] batch [5/5] time 0.309 (0.436) data 0.000 (0.128) loss 0.5844 (0.3227) acc 93.7500 (97.5000) lr 7.8853e-02 eta 0:00:15\n",
            "epoch [194/200] batch [5/5] time 0.310 (0.432) data 0.000 (0.123) loss 0.3406 (0.2619) acc 93.7500 (97.5000) lr 6.0390e-02 eta 0:00:12\n",
            "epoch [195/200] batch [5/5] time 0.308 (0.433) data 0.000 (0.124) loss 0.2347 (0.2669) acc 96.8750 (96.8750) lr 4.4380e-02 eta 0:00:10\n",
            "epoch [196/200] batch [5/5] time 0.309 (0.433) data 0.000 (0.125) loss 0.2533 (0.2744) acc 100.0000 (98.1250) lr 3.0827e-02 eta 0:00:08\n",
            "epoch [197/200] batch [5/5] time 0.309 (0.431) data 0.000 (0.122) loss 0.3421 (0.2881) acc 96.8750 (96.8750) lr 1.9733e-02 eta 0:00:06\n",
            "epoch [198/200] batch [5/5] time 0.313 (0.431) data 0.000 (0.121) loss 0.1998 (0.2162) acc 100.0000 (100.0000) lr 1.1101e-02 eta 0:00:04\n",
            "epoch [199/200] batch [5/5] time 0.308 (0.433) data 0.000 (0.124) loss 0.2178 (0.2695) acc 100.0000 (97.5000) lr 4.9344e-03 eta 0:00:02\n",
            "epoch [200/200] batch [5/5] time 0.309 (0.428) data 0.000 (0.118) loss 0.2740 (0.2506) acc 93.7500 (96.8750) lr 1.2337e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/eurosat/DAPT/vit_b16_16shots/seed1/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [12:56<00:00,  9.59s/it]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 7,325\n",
            "* accuracy: 90.4%\n",
            "* error: 9.6%\n",
            "* macro_f1: 90.2%\n",
            "Elapsed: 0:20:19\n"
          ]
        }
      ],
      "source": [
        "#eurosat-16shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/eurosat/DAPT/vit_b16_16shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91cc0988-52af-4d9c-e0ff-bd94727bb827",
        "id": "h6NSk8vqiUYe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 09:47:41.636124: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 09:47:41.656040: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 09:47:41.661989: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 09:47:41.676391: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 09:47:42.682395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8']\n",
            "output_dir: output/1207_new_diff_init/eurosat/DAPT/vit_b16_8shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/eurosat/DAPT/vit_b16_8shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  80\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "        [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "        [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "        ...,\n",
            "        [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "        [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "        [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-5.2644e-04, -1.1168e-02, -2.1661e-02,  ..., -4.8596e-03,\n",
            "           5.8912e-03,  6.7493e-03],\n",
            "         [ 5.8841e-04, -1.8584e-02, -2.5060e-02,  ..., -3.4987e-03,\n",
            "           5.4105e-03, -1.7861e-03],\n",
            "         [ 1.7309e-03, -7.2606e-03, -2.3481e-02,  ..., -2.1107e-03,\n",
            "           4.9070e-03,  6.0702e-03],\n",
            "         ...,\n",
            "         [ 1.0223e-03, -1.2080e-02, -1.5868e-02,  ..., -1.2882e-02,\n",
            "          -2.0567e-03,  4.8362e-03],\n",
            "         [ 7.8468e-03, -1.1038e-02, -1.0770e-02,  ...,  7.4988e-04,\n",
            "           1.5215e-03, -1.6774e-03],\n",
            "         [ 2.3028e-03, -5.9798e-03, -1.7329e-02,  ...,  5.5724e-04,\n",
            "           1.5857e-02,  4.3269e-03]],\n",
            "\n",
            "        [[ 6.0997e-03,  1.7106e-03, -2.1188e-02,  ...,  2.5523e-03,\n",
            "           2.1776e-03,  7.7525e-03],\n",
            "         [ 6.9275e-03, -6.8095e-03, -1.3799e-02,  ..., -3.6580e-03,\n",
            "           5.6513e-03,  1.9980e-03],\n",
            "         [ 6.2942e-04, -7.5124e-03, -9.4807e-03,  ...,  9.2128e-03,\n",
            "           1.0227e-02,  3.9989e-03],\n",
            "         ...,\n",
            "         [-8.9265e-04, -1.5108e-02, -1.9400e-02,  ..., -3.9164e-03,\n",
            "           4.4435e-03, -3.3369e-04],\n",
            "         [ 9.0713e-03, -1.0043e-02, -2.8737e-02,  ..., -4.9359e-03,\n",
            "           9.1699e-03,  5.4980e-03],\n",
            "         [-8.9653e-05, -7.9620e-03, -2.2422e-02,  ..., -2.5861e-03,\n",
            "           2.1928e-03,  6.9095e-03]],\n",
            "\n",
            "        [[ 2.2506e-04, -1.2583e-02, -1.8306e-02,  ..., -1.1051e-02,\n",
            "           9.2462e-03,  8.1454e-03],\n",
            "         [-9.4987e-04, -1.2930e-02, -2.0111e-02,  ..., -9.7730e-03,\n",
            "           5.3495e-03,  1.8893e-03],\n",
            "         [-2.7428e-03, -8.8699e-03, -1.4280e-02,  ...,  5.0471e-03,\n",
            "           5.3924e-03,  1.0839e-02],\n",
            "         ...,\n",
            "         [ 7.2136e-03, -9.1107e-03, -1.5424e-02,  ...,  1.9115e-03,\n",
            "           6.7415e-03, -8.7823e-04],\n",
            "         [-4.8866e-03, -9.1675e-03, -1.5351e-02,  ..., -5.0427e-03,\n",
            "           8.2830e-03,  1.1029e-02],\n",
            "         [-4.4785e-03, -1.2034e-02, -1.6046e-02,  ..., -1.3892e-03,\n",
            "           1.1470e-02, -4.6686e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.1645e-03,  2.2294e-03, -2.3786e-02,  ..., -2.2859e-04,\n",
            "           6.9243e-03,  8.3819e-03],\n",
            "         [ 4.7722e-03, -5.7014e-03, -1.7818e-02,  ..., -5.6092e-03,\n",
            "           1.0013e-02,  1.3503e-03],\n",
            "         [-2.0599e-03, -4.0076e-03, -2.7204e-02,  ...,  2.8766e-03,\n",
            "           1.4182e-02, -4.1284e-04],\n",
            "         ...,\n",
            "         [-2.6131e-03, -1.2682e-02, -1.9781e-02,  ...,  2.6591e-03,\n",
            "           5.9818e-03,  1.0518e-02],\n",
            "         [ 5.3825e-03, -7.2549e-03, -1.3387e-02,  ..., -5.7954e-04,\n",
            "           5.6404e-03,  2.1621e-03],\n",
            "         [ 1.1158e-03,  5.8145e-04, -2.5354e-02,  ...,  1.2668e-03,\n",
            "           9.6658e-03,  5.6201e-03]],\n",
            "\n",
            "        [[ 5.7640e-03, -5.7109e-03, -1.9671e-02,  ..., -6.6449e-03,\n",
            "          -2.2398e-03,  3.5010e-04],\n",
            "         [ 4.0493e-03, -1.3674e-02, -2.2840e-02,  ..., -2.0630e-03,\n",
            "           1.2584e-02, -4.8818e-03],\n",
            "         [ 1.1436e-05, -3.8550e-03, -2.1955e-02,  ..., -3.6313e-03,\n",
            "           3.4708e-03,  2.7534e-03],\n",
            "         ...,\n",
            "         [ 7.8735e-03, -1.2430e-02, -2.0399e-02,  ..., -1.3462e-02,\n",
            "           1.2340e-02, -4.7330e-03],\n",
            "         [ 2.9120e-03, -1.2139e-02, -6.7417e-03,  ...,  4.5043e-04,\n",
            "           7.1866e-05,  9.1229e-04],\n",
            "         [-3.6163e-03, -4.2251e-03, -2.5861e-02,  ..., -6.7784e-03,\n",
            "           5.3829e-03, -2.0913e-03]],\n",
            "\n",
            "        [[ 1.0143e-02, -1.1706e-02, -2.0403e-02,  ..., -5.1286e-03,\n",
            "           9.2310e-03,  1.6630e-03],\n",
            "         [ 5.0354e-03, -6.9526e-03, -1.5557e-02,  ..., -3.4320e-03,\n",
            "           9.9329e-03,  5.0002e-03],\n",
            "         [ 3.1037e-03, -1.5677e-02, -2.5472e-02,  ..., -1.3558e-03,\n",
            "           2.2081e-03,  4.0732e-03],\n",
            "         ...,\n",
            "         [ 3.1457e-03, -4.8411e-03, -1.6359e-02,  ...,  3.0177e-03,\n",
            "           9.1890e-03, -4.2676e-03],\n",
            "         [ 5.7907e-03, -1.0981e-02, -2.1062e-02,  ..., -9.6700e-03,\n",
            "           4.3233e-03,  1.2432e-03],\n",
            "         [-5.3413e-05, -5.7033e-03, -2.5506e-02,  ..., -3.7009e-03,\n",
            "           7.3665e-03,  1.6910e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4741\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/eurosat/DAPT/vit_b16_8shots/seed1/tensorboard)\n",
            "epoch [1/200] batch [1/2] time 1.787 (1.787) data 0.544 (0.544) loss 8.6321 (8.6321) acc 25.0000 (25.0000) lr 1.0000e-05 eta 0:11:53\n",
            "epoch [1/200] batch [2/2] time 0.291 (1.039) data 0.000 (0.272) loss 8.6848 (8.6584) acc 31.2500 (28.1250) lr 2.0000e+01 eta 0:06:53\n",
            "epoch [2/200] batch [1/2] time 4.524 (4.524) data 4.183 (4.183) loss 8.4756 (8.4756) acc 28.1250 (28.1250) lr 2.0000e+01 eta 0:29:55\n",
            "epoch [2/200] batch [2/2] time 0.805 (2.664) data 0.517 (2.350) loss 6.8762 (7.6759) acc 15.6250 (21.8750) lr 1.9999e+01 eta 0:17:35\n",
            "epoch [3/200] batch [1/2] time 2.672 (2.672) data 2.297 (2.297) loss 4.8759 (4.8759) acc 25.0000 (25.0000) lr 1.9999e+01 eta 0:17:35\n",
            "epoch [3/200] batch [2/2] time 0.294 (1.483) data 0.001 (1.149) loss 4.6242 (4.7500) acc 34.3750 (29.6875) lr 1.9995e+01 eta 0:09:44\n",
            "epoch [4/200] batch [1/2] time 0.732 (0.732) data 0.442 (0.442) loss 6.4060 (6.4060) acc 15.6250 (15.6250) lr 1.9995e+01 eta 0:04:47\n",
            "epoch [4/200] batch [2/2] time 0.294 (0.513) data 0.000 (0.221) loss 5.7025 (6.0542) acc 6.2500 (10.9375) lr 1.9989e+01 eta 0:03:21\n",
            "epoch [5/200] batch [1/2] time 0.715 (0.715) data 0.422 (0.422) loss 6.9985 (6.9985) acc 21.8750 (21.8750) lr 1.9989e+01 eta 0:04:39\n",
            "epoch [5/200] batch [2/2] time 0.294 (0.504) data 0.000 (0.211) loss 5.3383 (6.1684) acc 9.3750 (15.6250) lr 1.9980e+01 eta 0:03:16\n",
            "epoch [6/200] batch [1/2] time 0.749 (0.749) data 0.456 (0.456) loss 5.7429 (5.7429) acc 12.5000 (12.5000) lr 1.9980e+01 eta 0:04:51\n",
            "epoch [6/200] batch [2/2] time 0.296 (0.523) data 0.000 (0.228) loss 4.5490 (5.1459) acc 12.5000 (12.5000) lr 1.9969e+01 eta 0:03:22\n",
            "epoch [7/200] batch [1/2] time 0.736 (0.736) data 0.465 (0.465) loss 3.4987 (3.4987) acc 31.2500 (31.2500) lr 1.9969e+01 eta 0:04:44\n",
            "epoch [7/200] batch [2/2] time 0.296 (0.516) data 0.000 (0.233) loss 4.0843 (3.7915) acc 18.7500 (25.0000) lr 1.9956e+01 eta 0:03:19\n",
            "epoch [8/200] batch [1/2] time 0.727 (0.727) data 0.433 (0.433) loss 3.7570 (3.7570) acc 18.7500 (18.7500) lr 1.9956e+01 eta 0:04:40\n",
            "epoch [8/200] batch [2/2] time 0.297 (0.512) data 0.000 (0.217) loss 3.1733 (3.4652) acc 28.1250 (23.4375) lr 1.9940e+01 eta 0:03:16\n",
            "epoch [9/200] batch [1/2] time 0.712 (0.712) data 0.415 (0.415) loss 3.3557 (3.3557) acc 28.1250 (28.1250) lr 1.9940e+01 eta 0:04:32\n",
            "epoch [9/200] batch [2/2] time 0.299 (0.506) data 0.000 (0.208) loss 3.2537 (3.3047) acc 25.0000 (26.5625) lr 1.9921e+01 eta 0:03:13\n",
            "epoch [10/200] batch [1/2] time 0.721 (0.721) data 0.429 (0.429) loss 2.8446 (2.8446) acc 28.1250 (28.1250) lr 1.9921e+01 eta 0:04:34\n",
            "epoch [10/200] batch [2/2] time 0.295 (0.508) data 0.000 (0.214) loss 3.0864 (2.9655) acc 31.2500 (29.6875) lr 1.9900e+01 eta 0:03:13\n",
            "epoch [11/200] batch [1/2] time 0.737 (0.737) data 0.444 (0.444) loss 3.1954 (3.1954) acc 34.3750 (34.3750) lr 1.9900e+01 eta 0:04:39\n",
            "epoch [11/200] batch [2/2] time 0.297 (0.517) data 0.000 (0.222) loss 2.6729 (2.9342) acc 34.3750 (34.3750) lr 1.9877e+01 eta 0:03:15\n",
            "epoch [12/200] batch [1/2] time 0.685 (0.685) data 0.390 (0.390) loss 2.0075 (2.0075) acc 62.5000 (62.5000) lr 1.9877e+01 eta 0:04:18\n",
            "epoch [12/200] batch [2/2] time 0.296 (0.491) data 0.000 (0.195) loss 2.0967 (2.0521) acc 56.2500 (59.3750) lr 1.9851e+01 eta 0:03:04\n",
            "epoch [13/200] batch [1/2] time 0.710 (0.710) data 0.415 (0.415) loss 1.9350 (1.9350) acc 50.0000 (50.0000) lr 1.9851e+01 eta 0:04:26\n",
            "epoch [13/200] batch [2/2] time 0.298 (0.504) data 0.000 (0.208) loss 2.2766 (2.1058) acc 50.0000 (50.0000) lr 1.9823e+01 eta 0:03:08\n",
            "epoch [14/200] batch [1/2] time 0.754 (0.754) data 0.461 (0.461) loss 1.7185 (1.7185) acc 56.2500 (56.2500) lr 1.9823e+01 eta 0:04:41\n",
            "epoch [14/200] batch [2/2] time 0.298 (0.526) data 0.000 (0.231) loss 1.7866 (1.7526) acc 56.2500 (56.2500) lr 1.9792e+01 eta 0:03:15\n",
            "epoch [15/200] batch [1/2] time 0.754 (0.754) data 0.461 (0.461) loss 1.7240 (1.7240) acc 56.2500 (56.2500) lr 1.9792e+01 eta 0:04:39\n",
            "epoch [15/200] batch [2/2] time 0.300 (0.527) data 0.000 (0.231) loss 1.2849 (1.5044) acc 65.6250 (60.9375) lr 1.9759e+01 eta 0:03:15\n",
            "epoch [16/200] batch [1/2] time 0.741 (0.741) data 0.448 (0.448) loss 2.0356 (2.0356) acc 46.8750 (46.8750) lr 1.9759e+01 eta 0:04:33\n",
            "epoch [16/200] batch [2/2] time 0.300 (0.521) data 0.000 (0.224) loss 1.1176 (1.5766) acc 78.1250 (62.5000) lr 1.9724e+01 eta 0:03:11\n",
            "epoch [17/200] batch [1/2] time 0.722 (0.722) data 0.427 (0.427) loss 1.1375 (1.1375) acc 81.2500 (81.2500) lr 1.9724e+01 eta 0:04:24\n",
            "epoch [17/200] batch [2/2] time 0.302 (0.512) data 0.000 (0.214) loss 1.3792 (1.2583) acc 71.8750 (76.5625) lr 1.9686e+01 eta 0:03:07\n",
            "epoch [18/200] batch [1/2] time 0.743 (0.743) data 0.447 (0.447) loss 1.1482 (1.1482) acc 78.1250 (78.1250) lr 1.9686e+01 eta 0:04:31\n",
            "epoch [18/200] batch [2/2] time 0.298 (0.521) data 0.001 (0.224) loss 2.1626 (1.6554) acc 62.5000 (70.3125) lr 1.9646e+01 eta 0:03:09\n",
            "epoch [19/200] batch [1/2] time 0.760 (0.760) data 0.466 (0.466) loss 2.5528 (2.5528) acc 50.0000 (50.0000) lr 1.9646e+01 eta 0:04:35\n",
            "epoch [19/200] batch [2/2] time 0.299 (0.530) data 0.000 (0.233) loss 5.3749 (3.9639) acc 40.6250 (45.3125) lr 1.9603e+01 eta 0:03:11\n",
            "epoch [20/200] batch [1/2] time 0.743 (0.743) data 0.448 (0.448) loss 6.8430 (6.8430) acc 37.5000 (37.5000) lr 1.9603e+01 eta 0:04:28\n",
            "epoch [20/200] batch [2/2] time 0.304 (0.524) data 0.000 (0.224) loss 9.1753 (8.0092) acc 15.6250 (26.5625) lr 1.9558e+01 eta 0:03:08\n",
            "epoch [21/200] batch [1/2] time 0.743 (0.743) data 0.448 (0.448) loss 4.1853 (4.1853) acc 43.7500 (43.7500) lr 1.9558e+01 eta 0:04:26\n",
            "epoch [21/200] batch [2/2] time 0.303 (0.523) data 0.000 (0.224) loss 11.6041 (7.8947) acc 12.5000 (28.1250) lr 1.9511e+01 eta 0:03:07\n",
            "epoch [22/200] batch [1/2] time 0.718 (0.718) data 0.419 (0.419) loss 8.1822 (8.1822) acc 18.7500 (18.7500) lr 1.9511e+01 eta 0:04:16\n",
            "epoch [22/200] batch [2/2] time 0.303 (0.510) data 0.000 (0.210) loss 8.0402 (8.1112) acc 12.5000 (15.6250) lr 1.9461e+01 eta 0:03:01\n",
            "epoch [23/200] batch [1/2] time 0.731 (0.731) data 0.430 (0.430) loss 3.2978 (3.2978) acc 34.3750 (34.3750) lr 1.9461e+01 eta 0:04:19\n",
            "epoch [23/200] batch [2/2] time 0.301 (0.516) data 0.000 (0.215) loss 3.5485 (3.4231) acc 25.0000 (29.6875) lr 1.9409e+01 eta 0:03:02\n",
            "epoch [24/200] batch [1/2] time 0.712 (0.712) data 0.414 (0.414) loss 2.4051 (2.4051) acc 31.2500 (31.2500) lr 1.9409e+01 eta 0:04:11\n",
            "epoch [24/200] batch [2/2] time 0.302 (0.507) data 0.000 (0.207) loss 4.6108 (3.5079) acc 15.6250 (23.4375) lr 1.9354e+01 eta 0:02:58\n",
            "epoch [25/200] batch [1/2] time 0.726 (0.726) data 0.427 (0.427) loss 5.3870 (5.3870) acc 9.3750 (9.3750) lr 1.9354e+01 eta 0:04:14\n",
            "epoch [25/200] batch [2/2] time 0.305 (0.516) data 0.001 (0.214) loss 3.3778 (4.3824) acc 25.0000 (17.1875) lr 1.9298e+01 eta 0:03:00\n",
            "epoch [26/200] batch [1/2] time 0.728 (0.728) data 0.448 (0.448) loss 3.0097 (3.0097) acc 31.2500 (31.2500) lr 1.9298e+01 eta 0:04:14\n",
            "epoch [26/200] batch [2/2] time 0.303 (0.516) data 0.001 (0.224) loss 3.6884 (3.3490) acc 28.1250 (29.6875) lr 1.9239e+01 eta 0:02:59\n",
            "epoch [27/200] batch [1/2] time 0.760 (0.760) data 0.462 (0.462) loss 3.4989 (3.4989) acc 37.5000 (37.5000) lr 1.9239e+01 eta 0:04:23\n",
            "epoch [27/200] batch [2/2] time 0.299 (0.530) data 0.000 (0.231) loss 2.3463 (2.9226) acc 43.7500 (40.6250) lr 1.9178e+01 eta 0:03:03\n",
            "epoch [28/200] batch [1/2] time 0.716 (0.716) data 0.417 (0.417) loss 1.9831 (1.9831) acc 53.1250 (53.1250) lr 1.9178e+01 eta 0:04:06\n",
            "epoch [28/200] batch [2/2] time 0.301 (0.508) data 0.000 (0.209) loss 1.4512 (1.7172) acc 62.5000 (57.8125) lr 1.9114e+01 eta 0:02:54\n",
            "epoch [29/200] batch [1/2] time 0.719 (0.719) data 0.421 (0.421) loss 1.5992 (1.5992) acc 56.2500 (56.2500) lr 1.9114e+01 eta 0:04:06\n",
            "epoch [29/200] batch [2/2] time 0.302 (0.510) data 0.000 (0.211) loss 1.0129 (1.3060) acc 68.7500 (62.5000) lr 1.9048e+01 eta 0:02:54\n",
            "epoch [30/200] batch [1/2] time 0.722 (0.722) data 0.422 (0.422) loss 1.5430 (1.5430) acc 56.2500 (56.2500) lr 1.9048e+01 eta 0:04:06\n",
            "epoch [30/200] batch [2/2] time 0.304 (0.513) data 0.001 (0.211) loss 1.1346 (1.3388) acc 71.8750 (64.0625) lr 1.8980e+01 eta 0:02:54\n",
            "epoch [31/200] batch [1/2] time 0.732 (0.732) data 0.434 (0.434) loss 0.9762 (0.9762) acc 87.5000 (87.5000) lr 1.8980e+01 eta 0:04:08\n",
            "epoch [31/200] batch [2/2] time 0.302 (0.517) data 0.000 (0.217) loss 1.5169 (1.2466) acc 59.3750 (73.4375) lr 1.8910e+01 eta 0:02:54\n",
            "epoch [32/200] batch [1/2] time 0.746 (0.746) data 0.445 (0.445) loss 1.1256 (1.1256) acc 71.8750 (71.8750) lr 1.8910e+01 eta 0:04:11\n",
            "epoch [32/200] batch [2/2] time 0.306 (0.526) data 0.000 (0.223) loss 1.5273 (1.3265) acc 53.1250 (62.5000) lr 1.8838e+01 eta 0:02:56\n",
            "epoch [33/200] batch [1/2] time 0.735 (0.735) data 0.436 (0.436) loss 1.5847 (1.5847) acc 56.2500 (56.2500) lr 1.8838e+01 eta 0:04:06\n",
            "epoch [33/200] batch [2/2] time 0.304 (0.519) data 0.001 (0.218) loss 1.3500 (1.4674) acc 68.7500 (62.5000) lr 1.8763e+01 eta 0:02:53\n",
            "epoch [34/200] batch [1/2] time 0.735 (0.735) data 0.433 (0.433) loss 1.5750 (1.5750) acc 59.3750 (59.3750) lr 1.8763e+01 eta 0:04:04\n",
            "epoch [34/200] batch [2/2] time 0.304 (0.519) data 0.000 (0.217) loss 1.6868 (1.6309) acc 75.0000 (67.1875) lr 1.8686e+01 eta 0:02:52\n",
            "epoch [35/200] batch [1/2] time 0.723 (0.723) data 0.421 (0.421) loss 1.7478 (1.7478) acc 56.2500 (56.2500) lr 1.8686e+01 eta 0:03:59\n",
            "epoch [35/200] batch [2/2] time 0.304 (0.514) data 0.000 (0.211) loss 1.7664 (1.7571) acc 43.7500 (50.0000) lr 1.8607e+01 eta 0:02:49\n",
            "epoch [36/200] batch [1/2] time 0.726 (0.726) data 0.425 (0.425) loss 1.9762 (1.9762) acc 59.3750 (59.3750) lr 1.8607e+01 eta 0:03:58\n",
            "epoch [36/200] batch [2/2] time 0.305 (0.515) data 0.000 (0.213) loss 1.5959 (1.7860) acc 62.5000 (60.9375) lr 1.8526e+01 eta 0:02:49\n",
            "epoch [37/200] batch [1/2] time 0.747 (0.747) data 0.445 (0.445) loss 2.5296 (2.5296) acc 53.1250 (53.1250) lr 1.8526e+01 eta 0:04:04\n",
            "epoch [37/200] batch [2/2] time 0.307 (0.527) data 0.000 (0.223) loss 5.3644 (3.9470) acc 34.3750 (43.7500) lr 1.8443e+01 eta 0:02:51\n",
            "epoch [38/200] batch [1/2] time 0.759 (0.759) data 0.455 (0.455) loss 3.3768 (3.3768) acc 56.2500 (56.2500) lr 1.8443e+01 eta 0:04:06\n",
            "epoch [38/200] batch [2/2] time 0.304 (0.531) data 0.001 (0.228) loss 7.7297 (5.5532) acc 15.6250 (35.9375) lr 1.8358e+01 eta 0:02:52\n",
            "epoch [39/200] batch [1/2] time 0.787 (0.787) data 0.486 (0.486) loss 9.4311 (9.4311) acc 18.7500 (18.7500) lr 1.8358e+01 eta 0:04:14\n",
            "epoch [39/200] batch [2/2] time 0.305 (0.546) data 0.000 (0.243) loss 10.5163 (9.9737) acc 6.2500 (12.5000) lr 1.8271e+01 eta 0:02:55\n",
            "epoch [40/200] batch [1/2] time 0.751 (0.751) data 0.450 (0.450) loss 9.0839 (9.0839) acc 21.8750 (21.8750) lr 1.8271e+01 eta 0:04:01\n",
            "epoch [40/200] batch [2/2] time 0.306 (0.529) data 0.000 (0.225) loss 11.3800 (10.2319) acc 21.8750 (21.8750) lr 1.8181e+01 eta 0:02:49\n",
            "epoch [41/200] batch [1/2] time 0.717 (0.717) data 0.415 (0.415) loss 4.9331 (4.9331) acc 12.5000 (12.5000) lr 1.8181e+01 eta 0:03:48\n",
            "epoch [41/200] batch [2/2] time 0.307 (0.512) data 0.000 (0.208) loss 5.7223 (5.3277) acc 18.7500 (15.6250) lr 1.8090e+01 eta 0:02:42\n",
            "epoch [42/200] batch [1/2] time 0.742 (0.742) data 0.443 (0.443) loss 5.2009 (5.2009) acc 18.7500 (18.7500) lr 1.8090e+01 eta 0:03:55\n",
            "epoch [42/200] batch [2/2] time 0.307 (0.525) data 0.001 (0.222) loss 8.0065 (6.6037) acc 6.2500 (12.5000) lr 1.7997e+01 eta 0:02:45\n",
            "epoch [43/200] batch [1/2] time 0.736 (0.736) data 0.431 (0.431) loss 5.9202 (5.9202) acc 31.2500 (31.2500) lr 1.7997e+01 eta 0:03:51\n",
            "epoch [43/200] batch [2/2] time 0.310 (0.523) data 0.000 (0.216) loss 5.8699 (5.8951) acc 15.6250 (23.4375) lr 1.7902e+01 eta 0:02:44\n",
            "epoch [44/200] batch [1/2] time 0.747 (0.747) data 0.443 (0.443) loss 5.6551 (5.6551) acc 31.2500 (31.2500) lr 1.7902e+01 eta 0:03:53\n",
            "epoch [44/200] batch [2/2] time 0.307 (0.527) data 0.000 (0.222) loss 3.8653 (4.7602) acc 43.7500 (37.5000) lr 1.7804e+01 eta 0:02:44\n",
            "epoch [45/200] batch [1/2] time 0.695 (0.695) data 0.389 (0.389) loss 3.4448 (3.4448) acc 31.2500 (31.2500) lr 1.7804e+01 eta 0:03:36\n",
            "epoch [45/200] batch [2/2] time 0.311 (0.503) data 0.000 (0.195) loss 2.6269 (3.0358) acc 40.6250 (35.9375) lr 1.7705e+01 eta 0:02:35\n",
            "epoch [46/200] batch [1/2] time 0.718 (0.718) data 0.415 (0.415) loss 2.1309 (2.1309) acc 43.7500 (43.7500) lr 1.7705e+01 eta 0:03:41\n",
            "epoch [46/200] batch [2/2] time 0.307 (0.512) data 0.000 (0.208) loss 2.4842 (2.3076) acc 46.8750 (45.3125) lr 1.7604e+01 eta 0:02:37\n",
            "epoch [47/200] batch [1/2] time 0.711 (0.711) data 0.407 (0.407) loss 2.2411 (2.2411) acc 56.2500 (56.2500) lr 1.7604e+01 eta 0:03:38\n",
            "epoch [47/200] batch [2/2] time 0.310 (0.510) data 0.000 (0.204) loss 1.9051 (2.0731) acc 62.5000 (59.3750) lr 1.7501e+01 eta 0:02:36\n",
            "epoch [48/200] batch [1/2] time 0.751 (0.751) data 0.447 (0.447) loss 1.9020 (1.9020) acc 50.0000 (50.0000) lr 1.7501e+01 eta 0:03:49\n",
            "epoch [48/200] batch [2/2] time 0.309 (0.530) data 0.000 (0.224) loss 1.4938 (1.6979) acc 59.3750 (54.6875) lr 1.7396e+01 eta 0:02:41\n",
            "epoch [49/200] batch [1/2] time 0.756 (0.756) data 0.453 (0.453) loss 1.6508 (1.6508) acc 56.2500 (56.2500) lr 1.7396e+01 eta 0:03:49\n",
            "epoch [49/200] batch [2/2] time 0.321 (0.539) data 0.000 (0.227) loss 1.2347 (1.4428) acc 71.8750 (64.0625) lr 1.7290e+01 eta 0:02:42\n",
            "epoch [50/200] batch [1/2] time 0.781 (0.781) data 0.476 (0.476) loss 1.6349 (1.6349) acc 59.3750 (59.3750) lr 1.7290e+01 eta 0:03:54\n",
            "epoch [50/200] batch [2/2] time 0.307 (0.544) data 0.000 (0.238) loss 1.7893 (1.7121) acc 56.2500 (57.8125) lr 1.7181e+01 eta 0:02:43\n",
            "epoch [51/200] batch [1/2] time 0.739 (0.739) data 0.432 (0.432) loss 1.4781 (1.4781) acc 59.3750 (59.3750) lr 1.7181e+01 eta 0:03:40\n",
            "epoch [51/200] batch [2/2] time 0.313 (0.526) data 0.000 (0.216) loss 1.3797 (1.4289) acc 68.7500 (64.0625) lr 1.7071e+01 eta 0:02:36\n",
            "epoch [52/200] batch [1/2] time 0.735 (0.735) data 0.429 (0.429) loss 1.2943 (1.2943) acc 68.7500 (68.7500) lr 1.7071e+01 eta 0:03:38\n",
            "epoch [52/200] batch [2/2] time 0.311 (0.523) data 0.000 (0.215) loss 1.4481 (1.3712) acc 71.8750 (70.3125) lr 1.6959e+01 eta 0:02:34\n",
            "epoch [53/200] batch [1/2] time 0.733 (0.733) data 0.424 (0.424) loss 1.3236 (1.3236) acc 75.0000 (75.0000) lr 1.6959e+01 eta 0:03:36\n",
            "epoch [53/200] batch [2/2] time 0.308 (0.521) data 0.000 (0.212) loss 1.3356 (1.3296) acc 59.3750 (67.1875) lr 1.6845e+01 eta 0:02:33\n",
            "epoch [54/200] batch [1/2] time 0.750 (0.750) data 0.442 (0.442) loss 1.4648 (1.4648) acc 65.6250 (65.6250) lr 1.6845e+01 eta 0:03:39\n",
            "epoch [54/200] batch [2/2] time 0.311 (0.530) data 0.000 (0.221) loss 0.9174 (1.1911) acc 78.1250 (71.8750) lr 1.6730e+01 eta 0:02:34\n",
            "epoch [55/200] batch [1/2] time 0.733 (0.733) data 0.426 (0.426) loss 1.3544 (1.3544) acc 59.3750 (59.3750) lr 1.6730e+01 eta 0:03:33\n",
            "epoch [55/200] batch [2/2] time 0.311 (0.522) data 0.000 (0.213) loss 1.1996 (1.2770) acc 68.7500 (64.0625) lr 1.6613e+01 eta 0:02:31\n",
            "epoch [56/200] batch [1/2] time 0.727 (0.727) data 0.418 (0.418) loss 1.6291 (1.6291) acc 62.5000 (62.5000) lr 1.6613e+01 eta 0:03:29\n",
            "epoch [56/200] batch [2/2] time 0.312 (0.519) data 0.000 (0.209) loss 2.9322 (2.2807) acc 46.8750 (54.6875) lr 1.6494e+01 eta 0:02:29\n",
            "epoch [57/200] batch [1/2] time 0.740 (0.740) data 0.432 (0.432) loss 1.8837 (1.8837) acc 68.7500 (68.7500) lr 1.6494e+01 eta 0:03:32\n",
            "epoch [57/200] batch [2/2] time 0.314 (0.527) data 0.000 (0.216) loss 3.3167 (2.6002) acc 46.8750 (57.8125) lr 1.6374e+01 eta 0:02:30\n",
            "epoch [58/200] batch [1/2] time 0.726 (0.726) data 0.418 (0.418) loss 1.9621 (1.9621) acc 65.6250 (65.6250) lr 1.6374e+01 eta 0:03:26\n",
            "epoch [58/200] batch [2/2] time 0.313 (0.519) data 0.000 (0.209) loss 4.0519 (3.0070) acc 37.5000 (51.5625) lr 1.6252e+01 eta 0:02:27\n",
            "epoch [59/200] batch [1/2] time 0.726 (0.726) data 0.416 (0.416) loss 7.8860 (7.8860) acc 21.8750 (21.8750) lr 1.6252e+01 eta 0:03:25\n",
            "epoch [59/200] batch [2/2] time 0.317 (0.522) data 0.001 (0.209) loss 6.9838 (7.4349) acc 25.0000 (23.4375) lr 1.6129e+01 eta 0:02:27\n",
            "epoch [60/200] batch [1/2] time 0.751 (0.751) data 0.443 (0.443) loss 6.8882 (6.8882) acc 25.0000 (25.0000) lr 1.6129e+01 eta 0:03:31\n",
            "epoch [60/200] batch [2/2] time 0.315 (0.533) data 0.001 (0.222) loss 6.8622 (6.8752) acc 31.2500 (28.1250) lr 1.6004e+01 eta 0:02:29\n",
            "epoch [61/200] batch [1/2] time 0.788 (0.788) data 0.480 (0.480) loss 3.3970 (3.3970) acc 50.0000 (50.0000) lr 1.6004e+01 eta 0:03:39\n",
            "epoch [61/200] batch [2/2] time 0.315 (0.552) data 0.000 (0.240) loss 3.7519 (3.5744) acc 28.1250 (39.0625) lr 1.5878e+01 eta 0:02:33\n",
            "epoch [62/200] batch [1/2] time 0.754 (0.754) data 0.446 (0.446) loss 5.8106 (5.8106) acc 12.5000 (12.5000) lr 1.5878e+01 eta 0:03:28\n",
            "epoch [62/200] batch [2/2] time 0.315 (0.534) data 0.000 (0.223) loss 3.8525 (4.8315) acc 46.8750 (29.6875) lr 1.5750e+01 eta 0:02:27\n",
            "epoch [63/200] batch [1/2] time 0.746 (0.746) data 0.439 (0.439) loss 3.0554 (3.0554) acc 56.2500 (56.2500) lr 1.5750e+01 eta 0:03:25\n",
            "epoch [63/200] batch [2/2] time 0.313 (0.529) data 0.000 (0.220) loss 2.7686 (2.9120) acc 62.5000 (59.3750) lr 1.5621e+01 eta 0:02:25\n",
            "epoch [64/200] batch [1/2] time 0.715 (0.715) data 0.408 (0.408) loss 2.7814 (2.7814) acc 56.2500 (56.2500) lr 1.5621e+01 eta 0:03:15\n",
            "epoch [64/200] batch [2/2] time 0.313 (0.514) data 0.000 (0.204) loss 1.6646 (2.2230) acc 65.6250 (60.9375) lr 1.5490e+01 eta 0:02:19\n",
            "epoch [65/200] batch [1/2] time 0.731 (0.731) data 0.424 (0.424) loss 1.5130 (1.5130) acc 62.5000 (62.5000) lr 1.5490e+01 eta 0:03:18\n",
            "epoch [65/200] batch [2/2] time 0.312 (0.521) data 0.000 (0.212) loss 1.3529 (1.4330) acc 68.7500 (65.6250) lr 1.5358e+01 eta 0:02:20\n",
            "epoch [66/200] batch [1/2] time 0.757 (0.757) data 0.447 (0.447) loss 1.2544 (1.2544) acc 68.7500 (68.7500) lr 1.5358e+01 eta 0:03:23\n",
            "epoch [66/200] batch [2/2] time 0.309 (0.533) data 0.000 (0.224) loss 1.3508 (1.3026) acc 65.6250 (67.1875) lr 1.5225e+01 eta 0:02:22\n",
            "epoch [67/200] batch [1/2] time 0.736 (0.736) data 0.429 (0.429) loss 1.0529 (1.0529) acc 81.2500 (81.2500) lr 1.5225e+01 eta 0:03:16\n",
            "epoch [67/200] batch [2/2] time 0.311 (0.523) data 0.000 (0.215) loss 1.3172 (1.1850) acc 71.8750 (76.5625) lr 1.5090e+01 eta 0:02:19\n",
            "epoch [68/200] batch [1/2] time 0.741 (0.741) data 0.436 (0.436) loss 0.9577 (0.9577) acc 84.3750 (84.3750) lr 1.5090e+01 eta 0:03:16\n",
            "epoch [68/200] batch [2/2] time 0.313 (0.527) data 0.000 (0.218) loss 0.7835 (0.8706) acc 81.2500 (82.8125) lr 1.4955e+01 eta 0:02:19\n",
            "epoch [69/200] batch [1/2] time 0.758 (0.758) data 0.452 (0.452) loss 0.9718 (0.9718) acc 84.3750 (84.3750) lr 1.4955e+01 eta 0:03:19\n",
            "epoch [69/200] batch [2/2] time 0.313 (0.535) data 0.000 (0.226) loss 1.1326 (1.0522) acc 71.8750 (78.1250) lr 1.4818e+01 eta 0:02:20\n",
            "epoch [70/200] batch [1/2] time 0.772 (0.772) data 0.468 (0.468) loss 0.7201 (0.7201) acc 84.3750 (84.3750) lr 1.4818e+01 eta 0:03:21\n",
            "epoch [70/200] batch [2/2] time 0.311 (0.542) data 0.000 (0.234) loss 0.7442 (0.7321) acc 81.2500 (82.8125) lr 1.4679e+01 eta 0:02:20\n",
            "epoch [71/200] batch [1/2] time 0.774 (0.774) data 0.470 (0.470) loss 0.6885 (0.6885) acc 87.5000 (87.5000) lr 1.4679e+01 eta 0:03:20\n",
            "epoch [71/200] batch [2/2] time 0.311 (0.542) data 0.000 (0.235) loss 0.7188 (0.7036) acc 84.3750 (85.9375) lr 1.4540e+01 eta 0:02:19\n",
            "epoch [72/200] batch [1/2] time 0.766 (0.766) data 0.458 (0.458) loss 0.7546 (0.7546) acc 90.6250 (90.6250) lr 1.4540e+01 eta 0:03:16\n",
            "epoch [72/200] batch [2/2] time 0.309 (0.537) data 0.000 (0.229) loss 1.3483 (1.0514) acc 75.0000 (82.8125) lr 1.4399e+01 eta 0:02:17\n",
            "epoch [73/200] batch [1/2] time 0.723 (0.723) data 0.418 (0.418) loss 0.5752 (0.5752) acc 90.6250 (90.6250) lr 1.4399e+01 eta 0:03:04\n",
            "epoch [73/200] batch [2/2] time 0.311 (0.517) data 0.000 (0.209) loss 0.8519 (0.7135) acc 84.3750 (87.5000) lr 1.4258e+01 eta 0:02:11\n",
            "epoch [74/200] batch [1/2] time 0.747 (0.747) data 0.441 (0.441) loss 0.7440 (0.7440) acc 81.2500 (81.2500) lr 1.4258e+01 eta 0:03:08\n",
            "epoch [74/200] batch [2/2] time 0.307 (0.527) data 0.000 (0.221) loss 0.5025 (0.6232) acc 96.8750 (89.0625) lr 1.4115e+01 eta 0:02:12\n",
            "epoch [75/200] batch [1/2] time 0.732 (0.732) data 0.430 (0.430) loss 0.9105 (0.9105) acc 81.2500 (81.2500) lr 1.4115e+01 eta 0:03:03\n",
            "epoch [75/200] batch [2/2] time 0.311 (0.522) data 0.000 (0.215) loss 0.6947 (0.8026) acc 84.3750 (82.8125) lr 1.3971e+01 eta 0:02:10\n",
            "epoch [76/200] batch [1/2] time 0.723 (0.723) data 0.418 (0.418) loss 0.7730 (0.7730) acc 81.2500 (81.2500) lr 1.3971e+01 eta 0:03:00\n",
            "epoch [76/200] batch [2/2] time 0.308 (0.516) data 0.000 (0.209) loss 1.0490 (0.9110) acc 81.2500 (81.2500) lr 1.3827e+01 eta 0:02:07\n",
            "epoch [77/200] batch [1/2] time 0.717 (0.717) data 0.415 (0.415) loss 1.6380 (1.6380) acc 71.8750 (71.8750) lr 1.3827e+01 eta 0:02:57\n",
            "epoch [77/200] batch [2/2] time 0.309 (0.513) data 0.000 (0.208) loss 5.2981 (3.4681) acc 46.8750 (59.3750) lr 1.3681e+01 eta 0:02:06\n",
            "epoch [78/200] batch [1/2] time 0.725 (0.725) data 0.423 (0.423) loss 5.2666 (5.2666) acc 15.6250 (15.6250) lr 1.3681e+01 eta 0:02:57\n",
            "epoch [78/200] batch [2/2] time 0.310 (0.517) data 0.000 (0.212) loss 2.9796 (4.1231) acc 71.8750 (43.7500) lr 1.3535e+01 eta 0:02:06\n",
            "epoch [79/200] batch [1/2] time 0.728 (0.728) data 0.423 (0.423) loss 2.7475 (2.7475) acc 43.7500 (43.7500) lr 1.3535e+01 eta 0:02:56\n",
            "epoch [79/200] batch [2/2] time 0.309 (0.519) data 0.000 (0.212) loss 6.2302 (4.4888) acc 31.2500 (37.5000) lr 1.3387e+01 eta 0:02:05\n",
            "epoch [80/200] batch [1/2] time 0.717 (0.717) data 0.411 (0.411) loss 5.3149 (5.3149) acc 25.0000 (25.0000) lr 1.3387e+01 eta 0:02:52\n",
            "epoch [80/200] batch [2/2] time 0.310 (0.513) data 0.001 (0.206) loss 6.1498 (5.7323) acc 9.3750 (17.1875) lr 1.3239e+01 eta 0:02:03\n",
            "epoch [81/200] batch [1/2] time 0.710 (0.710) data 0.404 (0.404) loss 5.0236 (5.0236) acc 31.2500 (31.2500) lr 1.3239e+01 eta 0:02:49\n",
            "epoch [81/200] batch [2/2] time 0.309 (0.509) data 0.000 (0.202) loss 6.8297 (5.9266) acc 12.5000 (21.8750) lr 1.3090e+01 eta 0:02:01\n",
            "epoch [82/200] batch [1/2] time 0.764 (0.764) data 0.460 (0.460) loss 3.9919 (3.9919) acc 28.1250 (28.1250) lr 1.3090e+01 eta 0:03:01\n",
            "epoch [82/200] batch [2/2] time 0.306 (0.535) data 0.000 (0.230) loss 2.2932 (3.1426) acc 65.6250 (46.8750) lr 1.2940e+01 eta 0:02:06\n",
            "epoch [83/200] batch [1/2] time 0.752 (0.752) data 0.451 (0.451) loss 1.6605 (1.6605) acc 56.2500 (56.2500) lr 1.2940e+01 eta 0:02:56\n",
            "epoch [83/200] batch [2/2] time 0.308 (0.530) data 0.001 (0.226) loss 2.2305 (1.9455) acc 62.5000 (59.3750) lr 1.2790e+01 eta 0:02:04\n",
            "epoch [84/200] batch [1/2] time 0.740 (0.740) data 0.438 (0.438) loss 1.2425 (1.2425) acc 71.8750 (71.8750) lr 1.2790e+01 eta 0:02:52\n",
            "epoch [84/200] batch [2/2] time 0.308 (0.524) data 0.000 (0.219) loss 1.3392 (1.2909) acc 65.6250 (68.7500) lr 1.2639e+01 eta 0:02:01\n",
            "epoch [85/200] batch [1/2] time 0.747 (0.747) data 0.447 (0.447) loss 1.0095 (1.0095) acc 71.8750 (71.8750) lr 1.2639e+01 eta 0:02:52\n",
            "epoch [85/200] batch [2/2] time 0.304 (0.526) data 0.000 (0.224) loss 0.9727 (0.9911) acc 65.6250 (68.7500) lr 1.2487e+01 eta 0:02:00\n",
            "epoch [86/200] batch [1/2] time 0.726 (0.726) data 0.423 (0.423) loss 0.9251 (0.9251) acc 84.3750 (84.3750) lr 1.2487e+01 eta 0:02:46\n",
            "epoch [86/200] batch [2/2] time 0.306 (0.516) data 0.000 (0.212) loss 0.7951 (0.8601) acc 84.3750 (84.3750) lr 1.2334e+01 eta 0:01:57\n",
            "epoch [87/200] batch [1/2] time 0.727 (0.727) data 0.423 (0.423) loss 0.7247 (0.7247) acc 87.5000 (87.5000) lr 1.2334e+01 eta 0:02:44\n",
            "epoch [87/200] batch [2/2] time 0.306 (0.516) data 0.000 (0.212) loss 0.8893 (0.8070) acc 81.2500 (84.3750) lr 1.2181e+01 eta 0:01:56\n",
            "epoch [88/200] batch [1/2] time 0.726 (0.726) data 0.424 (0.424) loss 0.9374 (0.9374) acc 78.1250 (78.1250) lr 1.2181e+01 eta 0:02:43\n",
            "epoch [88/200] batch [2/2] time 0.307 (0.516) data 0.000 (0.212) loss 0.8319 (0.8847) acc 84.3750 (81.2500) lr 1.2028e+01 eta 0:01:55\n",
            "epoch [89/200] batch [1/2] time 0.709 (0.709) data 0.405 (0.405) loss 0.7382 (0.7382) acc 84.3750 (84.3750) lr 1.2028e+01 eta 0:02:38\n",
            "epoch [89/200] batch [2/2] time 0.303 (0.506) data 0.000 (0.203) loss 0.5967 (0.6675) acc 96.8750 (90.6250) lr 1.1874e+01 eta 0:01:52\n",
            "epoch [90/200] batch [1/2] time 0.714 (0.714) data 0.415 (0.415) loss 0.6604 (0.6604) acc 87.5000 (87.5000) lr 1.1874e+01 eta 0:02:37\n",
            "epoch [90/200] batch [2/2] time 0.305 (0.510) data 0.000 (0.208) loss 0.4550 (0.5577) acc 93.7500 (90.6250) lr 1.1719e+01 eta 0:01:52\n",
            "epoch [91/200] batch [1/2] time 0.746 (0.746) data 0.446 (0.446) loss 0.7171 (0.7171) acc 84.3750 (84.3750) lr 1.1719e+01 eta 0:02:43\n",
            "epoch [91/200] batch [2/2] time 0.306 (0.526) data 0.000 (0.223) loss 0.5546 (0.6359) acc 90.6250 (87.5000) lr 1.1564e+01 eta 0:01:54\n",
            "epoch [92/200] batch [1/2] time 0.735 (0.735) data 0.434 (0.434) loss 0.4628 (0.4628) acc 96.8750 (96.8750) lr 1.1564e+01 eta 0:02:39\n",
            "epoch [92/200] batch [2/2] time 0.305 (0.520) data 0.000 (0.217) loss 0.4928 (0.4778) acc 90.6250 (93.7500) lr 1.1409e+01 eta 0:01:52\n",
            "epoch [93/200] batch [1/2] time 0.775 (0.775) data 0.474 (0.474) loss 0.4964 (0.4964) acc 93.7500 (93.7500) lr 1.1409e+01 eta 0:02:46\n",
            "epoch [93/200] batch [2/2] time 0.304 (0.540) data 0.000 (0.237) loss 0.5894 (0.5429) acc 84.3750 (89.0625) lr 1.1253e+01 eta 0:01:55\n",
            "epoch [94/200] batch [1/2] time 0.736 (0.736) data 0.435 (0.435) loss 0.5661 (0.5661) acc 84.3750 (84.3750) lr 1.1253e+01 eta 0:02:36\n",
            "epoch [94/200] batch [2/2] time 0.305 (0.520) data 0.000 (0.218) loss 0.6777 (0.6219) acc 90.6250 (87.5000) lr 1.1097e+01 eta 0:01:50\n",
            "epoch [95/200] batch [1/2] time 0.745 (0.745) data 0.443 (0.443) loss 0.7567 (0.7567) acc 87.5000 (87.5000) lr 1.1097e+01 eta 0:02:37\n",
            "epoch [95/200] batch [2/2] time 0.305 (0.525) data 0.000 (0.222) loss 0.5371 (0.6469) acc 87.5000 (87.5000) lr 1.0941e+01 eta 0:01:50\n",
            "epoch [96/200] batch [1/2] time 0.722 (0.722) data 0.420 (0.420) loss 0.4432 (0.4432) acc 93.7500 (93.7500) lr 1.0941e+01 eta 0:02:30\n",
            "epoch [96/200] batch [2/2] time 0.305 (0.513) data 0.000 (0.210) loss 0.6324 (0.5378) acc 84.3750 (89.0625) lr 1.0785e+01 eta 0:01:46\n",
            "epoch [97/200] batch [1/2] time 0.729 (0.729) data 0.429 (0.429) loss 0.6530 (0.6530) acc 84.3750 (84.3750) lr 1.0785e+01 eta 0:02:30\n",
            "epoch [97/200] batch [2/2] time 0.305 (0.517) data 0.000 (0.215) loss 0.4853 (0.5692) acc 90.6250 (87.5000) lr 1.0628e+01 eta 0:01:46\n",
            "epoch [98/200] batch [1/2] time 0.706 (0.706) data 0.403 (0.403) loss 0.4287 (0.4287) acc 90.6250 (90.6250) lr 1.0628e+01 eta 0:02:24\n",
            "epoch [98/200] batch [2/2] time 0.305 (0.505) data 0.000 (0.202) loss 0.9541 (0.6914) acc 84.3750 (87.5000) lr 1.0471e+01 eta 0:01:43\n",
            "epoch [99/200] batch [1/2] time 0.717 (0.717) data 0.418 (0.418) loss 1.0131 (1.0131) acc 75.0000 (75.0000) lr 1.0471e+01 eta 0:02:25\n",
            "epoch [99/200] batch [2/2] time 0.308 (0.512) data 0.000 (0.209) loss 1.4183 (1.2157) acc 68.7500 (71.8750) lr 1.0314e+01 eta 0:01:43\n",
            "epoch [100/200] batch [1/2] time 0.711 (0.711) data 0.408 (0.408) loss 2.5756 (2.5756) acc 46.8750 (46.8750) lr 1.0314e+01 eta 0:02:22\n",
            "epoch [100/200] batch [2/2] time 0.303 (0.507) data 0.001 (0.204) loss 2.0796 (2.3276) acc 75.0000 (60.9375) lr 1.0157e+01 eta 0:01:41\n",
            "epoch [101/200] batch [1/2] time 0.768 (0.768) data 0.466 (0.466) loss 1.2829 (1.2829) acc 62.5000 (62.5000) lr 1.0157e+01 eta 0:02:32\n",
            "epoch [101/200] batch [2/2] time 0.307 (0.538) data 0.001 (0.233) loss 1.3190 (1.3010) acc 75.0000 (68.7500) lr 1.0000e+01 eta 0:01:46\n",
            "epoch [102/200] batch [1/2] time 0.738 (0.738) data 0.435 (0.435) loss 1.0971 (1.0971) acc 81.2500 (81.2500) lr 1.0000e+01 eta 0:02:25\n",
            "epoch [102/200] batch [2/2] time 0.305 (0.522) data 0.001 (0.218) loss 2.6429 (1.8700) acc 50.0000 (65.6250) lr 9.8429e+00 eta 0:01:42\n",
            "epoch [103/200] batch [1/2] time 0.708 (0.708) data 0.404 (0.404) loss 2.4668 (2.4668) acc 50.0000 (50.0000) lr 9.8429e+00 eta 0:02:17\n",
            "epoch [103/200] batch [2/2] time 0.306 (0.507) data 0.000 (0.202) loss 1.4630 (1.9649) acc 71.8750 (60.9375) lr 9.6859e+00 eta 0:01:38\n",
            "epoch [104/200] batch [1/2] time 0.707 (0.707) data 0.401 (0.401) loss 1.4098 (1.4098) acc 71.8750 (71.8750) lr 9.6859e+00 eta 0:02:16\n",
            "epoch [104/200] batch [2/2] time 0.304 (0.505) data 0.001 (0.201) loss 0.9293 (1.1695) acc 84.3750 (78.1250) lr 9.5289e+00 eta 0:01:37\n",
            "epoch [105/200] batch [1/2] time 0.758 (0.758) data 0.456 (0.456) loss 0.8875 (0.8875) acc 68.7500 (68.7500) lr 9.5289e+00 eta 0:02:24\n",
            "epoch [105/200] batch [2/2] time 0.303 (0.531) data 0.000 (0.228) loss 1.5628 (1.2252) acc 68.7500 (68.7500) lr 9.3721e+00 eta 0:01:40\n",
            "epoch [106/200] batch [1/2] time 0.771 (0.771) data 0.471 (0.471) loss 0.6481 (0.6481) acc 84.3750 (84.3750) lr 9.3721e+00 eta 0:02:25\n",
            "epoch [106/200] batch [2/2] time 0.302 (0.536) data 0.000 (0.236) loss 1.3101 (0.9791) acc 75.0000 (79.6875) lr 9.2154e+00 eta 0:01:40\n",
            "epoch [107/200] batch [1/2] time 0.776 (0.776) data 0.476 (0.476) loss 1.0185 (1.0185) acc 84.3750 (84.3750) lr 9.2154e+00 eta 0:02:25\n",
            "epoch [107/200] batch [2/2] time 0.306 (0.541) data 0.000 (0.238) loss 1.0868 (1.0527) acc 81.2500 (82.8125) lr 9.0589e+00 eta 0:01:40\n",
            "epoch [108/200] batch [1/2] time 0.729 (0.729) data 0.428 (0.428) loss 0.6806 (0.6806) acc 87.5000 (87.5000) lr 9.0589e+00 eta 0:02:14\n",
            "epoch [108/200] batch [2/2] time 0.308 (0.519) data 0.000 (0.214) loss 0.9632 (0.8219) acc 81.2500 (84.3750) lr 8.9027e+00 eta 0:01:35\n",
            "epoch [109/200] batch [1/2] time 0.713 (0.713) data 0.412 (0.412) loss 0.7973 (0.7973) acc 84.3750 (84.3750) lr 8.9027e+00 eta 0:02:10\n",
            "epoch [109/200] batch [2/2] time 0.305 (0.509) data 0.001 (0.206) loss 0.9781 (0.8877) acc 81.2500 (82.8125) lr 8.7467e+00 eta 0:01:32\n",
            "epoch [110/200] batch [1/2] time 0.722 (0.722) data 0.422 (0.422) loss 0.9305 (0.9305) acc 84.3750 (84.3750) lr 8.7467e+00 eta 0:02:10\n",
            "epoch [110/200] batch [2/2] time 0.306 (0.514) data 0.000 (0.211) loss 0.7802 (0.8553) acc 90.6250 (87.5000) lr 8.5910e+00 eta 0:01:32\n",
            "epoch [111/200] batch [1/2] time 0.727 (0.727) data 0.427 (0.427) loss 0.8391 (0.8391) acc 84.3750 (84.3750) lr 8.5910e+00 eta 0:02:10\n",
            "epoch [111/200] batch [2/2] time 0.305 (0.516) data 0.000 (0.214) loss 0.8203 (0.8297) acc 84.3750 (84.3750) lr 8.4357e+00 eta 0:01:31\n",
            "epoch [112/200] batch [1/2] time 0.754 (0.754) data 0.451 (0.451) loss 0.4196 (0.4196) acc 100.0000 (100.0000) lr 8.4357e+00 eta 0:02:13\n",
            "epoch [112/200] batch [2/2] time 0.306 (0.530) data 0.000 (0.226) loss 0.5607 (0.4902) acc 90.6250 (95.3125) lr 8.2807e+00 eta 0:01:33\n",
            "epoch [113/200] batch [1/2] time 0.763 (0.763) data 0.461 (0.461) loss 0.5267 (0.5267) acc 87.5000 (87.5000) lr 8.2807e+00 eta 0:02:13\n",
            "epoch [113/200] batch [2/2] time 0.307 (0.535) data 0.000 (0.231) loss 0.4176 (0.4721) acc 93.7500 (90.6250) lr 8.1262e+00 eta 0:01:33\n",
            "epoch [114/200] batch [1/2] time 0.748 (0.748) data 0.446 (0.446) loss 0.3950 (0.3950) acc 96.8750 (96.8750) lr 8.1262e+00 eta 0:02:09\n",
            "epoch [114/200] batch [2/2] time 0.303 (0.526) data 0.000 (0.223) loss 0.5033 (0.4491) acc 96.8750 (96.8750) lr 7.9721e+00 eta 0:01:30\n",
            "epoch [115/200] batch [1/2] time 0.739 (0.739) data 0.438 (0.438) loss 0.5792 (0.5792) acc 93.7500 (93.7500) lr 7.9721e+00 eta 0:02:06\n",
            "epoch [115/200] batch [2/2] time 0.305 (0.522) data 0.000 (0.219) loss 0.4360 (0.5076) acc 96.8750 (95.3125) lr 7.8186e+00 eta 0:01:28\n",
            "epoch [116/200] batch [1/2] time 0.747 (0.747) data 0.444 (0.444) loss 0.5289 (0.5289) acc 93.7500 (93.7500) lr 7.8186e+00 eta 0:02:06\n",
            "epoch [116/200] batch [2/2] time 0.306 (0.526) data 0.001 (0.222) loss 0.4289 (0.4789) acc 96.8750 (95.3125) lr 7.6655e+00 eta 0:01:28\n",
            "epoch [117/200] batch [1/2] time 0.757 (0.757) data 0.455 (0.455) loss 0.4551 (0.4551) acc 96.8750 (96.8750) lr 7.6655e+00 eta 0:02:06\n",
            "epoch [117/200] batch [2/2] time 0.307 (0.532) data 0.000 (0.228) loss 0.7281 (0.5916) acc 90.6250 (93.7500) lr 7.5131e+00 eta 0:01:28\n",
            "epoch [118/200] batch [1/2] time 0.752 (0.752) data 0.451 (0.451) loss 0.6194 (0.6194) acc 96.8750 (96.8750) lr 7.5131e+00 eta 0:02:04\n",
            "epoch [118/200] batch [2/2] time 0.306 (0.529) data 0.000 (0.226) loss 0.9535 (0.7864) acc 78.1250 (87.5000) lr 7.3613e+00 eta 0:01:26\n",
            "epoch [119/200] batch [1/2] time 0.723 (0.723) data 0.423 (0.423) loss 0.4148 (0.4148) acc 96.8750 (96.8750) lr 7.3613e+00 eta 0:01:57\n",
            "epoch [119/200] batch [2/2] time 0.307 (0.515) data 0.000 (0.212) loss 0.8909 (0.6528) acc 81.2500 (89.0625) lr 7.2101e+00 eta 0:01:23\n",
            "epoch [120/200] batch [1/2] time 0.737 (0.737) data 0.434 (0.434) loss 0.5425 (0.5425) acc 87.5000 (87.5000) lr 7.2101e+00 eta 0:01:58\n",
            "epoch [120/200] batch [2/2] time 0.307 (0.522) data 0.000 (0.217) loss 0.6919 (0.6172) acc 84.3750 (85.9375) lr 7.0596e+00 eta 0:01:23\n",
            "epoch [121/200] batch [1/2] time 0.753 (0.753) data 0.450 (0.450) loss 0.8110 (0.8110) acc 84.3750 (84.3750) lr 7.0596e+00 eta 0:01:59\n",
            "epoch [121/200] batch [2/2] time 0.307 (0.530) data 0.000 (0.225) loss 0.9257 (0.8683) acc 81.2500 (82.8125) lr 6.9098e+00 eta 0:01:23\n",
            "epoch [122/200] batch [1/2] time 0.749 (0.749) data 0.444 (0.444) loss 1.0806 (1.0806) acc 75.0000 (75.0000) lr 6.9098e+00 eta 0:01:57\n",
            "epoch [122/200] batch [2/2] time 0.310 (0.529) data 0.000 (0.222) loss 0.5572 (0.8189) acc 90.6250 (82.8125) lr 6.7608e+00 eta 0:01:22\n",
            "epoch [123/200] batch [1/2] time 0.743 (0.743) data 0.439 (0.439) loss 0.5488 (0.5488) acc 93.7500 (93.7500) lr 6.7608e+00 eta 0:01:55\n",
            "epoch [123/200] batch [2/2] time 0.305 (0.524) data 0.000 (0.220) loss 1.1108 (0.8298) acc 81.2500 (87.5000) lr 6.6126e+00 eta 0:01:20\n",
            "epoch [124/200] batch [1/2] time 0.737 (0.737) data 0.431 (0.431) loss 1.3960 (1.3960) acc 75.0000 (75.0000) lr 6.6126e+00 eta 0:01:52\n",
            "epoch [124/200] batch [2/2] time 0.308 (0.523) data 0.001 (0.216) loss 1.0761 (1.2360) acc 75.0000 (75.0000) lr 6.4653e+00 eta 0:01:19\n",
            "epoch [125/200] batch [1/2] time 0.747 (0.747) data 0.443 (0.443) loss 0.5883 (0.5883) acc 90.6250 (90.6250) lr 6.4653e+00 eta 0:01:52\n",
            "epoch [125/200] batch [2/2] time 0.306 (0.526) data 0.000 (0.222) loss 0.8995 (0.7439) acc 87.5000 (89.0625) lr 6.3188e+00 eta 0:01:18\n",
            "epoch [126/200] batch [1/2] time 0.716 (0.716) data 0.413 (0.413) loss 0.8746 (0.8746) acc 87.5000 (87.5000) lr 6.3188e+00 eta 0:01:46\n",
            "epoch [126/200] batch [2/2] time 0.307 (0.511) data 0.000 (0.207) loss 0.9129 (0.8937) acc 75.0000 (81.2500) lr 6.1732e+00 eta 0:01:15\n",
            "epoch [127/200] batch [1/2] time 0.733 (0.733) data 0.424 (0.424) loss 0.7831 (0.7831) acc 84.3750 (84.3750) lr 6.1732e+00 eta 0:01:47\n",
            "epoch [127/200] batch [2/2] time 0.309 (0.521) data 0.000 (0.212) loss 0.5683 (0.6757) acc 90.6250 (87.5000) lr 6.0285e+00 eta 0:01:16\n",
            "epoch [128/200] batch [1/2] time 0.762 (0.762) data 0.460 (0.460) loss 0.4695 (0.4695) acc 93.7500 (93.7500) lr 6.0285e+00 eta 0:01:50\n",
            "epoch [128/200] batch [2/2] time 0.307 (0.535) data 0.000 (0.230) loss 0.9570 (0.7132) acc 78.1250 (85.9375) lr 5.8849e+00 eta 0:01:16\n",
            "epoch [129/200] batch [1/2] time 0.725 (0.725) data 0.422 (0.422) loss 0.5621 (0.5621) acc 87.5000 (87.5000) lr 5.8849e+00 eta 0:01:43\n",
            "epoch [129/200] batch [2/2] time 0.311 (0.518) data 0.000 (0.211) loss 0.7831 (0.6726) acc 87.5000 (87.5000) lr 5.7422e+00 eta 0:01:13\n",
            "epoch [130/200] batch [1/2] time 0.730 (0.730) data 0.426 (0.426) loss 0.7231 (0.7231) acc 87.5000 (87.5000) lr 5.7422e+00 eta 0:01:42\n",
            "epoch [130/200] batch [2/2] time 0.309 (0.519) data 0.001 (0.213) loss 1.5268 (1.1249) acc 75.0000 (81.2500) lr 5.6006e+00 eta 0:01:12\n",
            "epoch [131/200] batch [1/2] time 0.717 (0.717) data 0.413 (0.413) loss 0.8928 (0.8928) acc 84.3750 (84.3750) lr 5.6006e+00 eta 0:01:39\n",
            "epoch [131/200] batch [2/2] time 0.309 (0.513) data 0.000 (0.207) loss 0.5707 (0.7317) acc 93.7500 (89.0625) lr 5.4601e+00 eta 0:01:10\n",
            "epoch [132/200] batch [1/2] time 0.714 (0.714) data 0.409 (0.409) loss 0.7377 (0.7377) acc 81.2500 (81.2500) lr 5.4601e+00 eta 0:01:37\n",
            "epoch [132/200] batch [2/2] time 0.309 (0.512) data 0.000 (0.205) loss 0.7839 (0.7608) acc 87.5000 (84.3750) lr 5.3207e+00 eta 0:01:09\n",
            "epoch [133/200] batch [1/2] time 0.740 (0.740) data 0.436 (0.436) loss 0.6077 (0.6077) acc 87.5000 (87.5000) lr 5.3207e+00 eta 0:01:39\n",
            "epoch [133/200] batch [2/2] time 0.310 (0.525) data 0.001 (0.218) loss 0.7487 (0.6782) acc 78.1250 (82.8125) lr 5.1825e+00 eta 0:01:10\n",
            "epoch [134/200] batch [1/2] time 0.747 (0.747) data 0.443 (0.443) loss 0.4574 (0.4574) acc 96.8750 (96.8750) lr 5.1825e+00 eta 0:01:39\n",
            "epoch [134/200] batch [2/2] time 0.308 (0.528) data 0.000 (0.222) loss 0.7784 (0.6179) acc 87.5000 (92.1875) lr 5.0454e+00 eta 0:01:09\n",
            "epoch [135/200] batch [1/2] time 0.734 (0.734) data 0.428 (0.428) loss 0.6042 (0.6042) acc 87.5000 (87.5000) lr 5.0454e+00 eta 0:01:36\n",
            "epoch [135/200] batch [2/2] time 0.303 (0.518) data 0.000 (0.214) loss 0.4285 (0.5164) acc 93.7500 (90.6250) lr 4.9096e+00 eta 0:01:07\n",
            "epoch [136/200] batch [1/2] time 0.712 (0.712) data 0.410 (0.410) loss 0.4597 (0.4597) acc 96.8750 (96.8750) lr 4.9096e+00 eta 0:01:31\n",
            "epoch [136/200] batch [2/2] time 0.307 (0.510) data 0.000 (0.205) loss 0.4279 (0.4438) acc 93.7500 (95.3125) lr 4.7750e+00 eta 0:01:05\n",
            "epoch [137/200] batch [1/2] time 0.730 (0.730) data 0.424 (0.424) loss 0.4537 (0.4537) acc 90.6250 (90.6250) lr 4.7750e+00 eta 0:01:32\n",
            "epoch [137/200] batch [2/2] time 0.308 (0.519) data 0.001 (0.212) loss 0.3301 (0.3919) acc 100.0000 (95.3125) lr 4.6417e+00 eta 0:01:05\n",
            "epoch [138/200] batch [1/2] time 0.728 (0.728) data 0.427 (0.427) loss 0.3700 (0.3700) acc 96.8750 (96.8750) lr 4.6417e+00 eta 0:01:31\n",
            "epoch [138/200] batch [2/2] time 0.307 (0.518) data 0.001 (0.214) loss 0.4777 (0.4239) acc 96.8750 (96.8750) lr 4.5098e+00 eta 0:01:04\n",
            "epoch [139/200] batch [1/2] time 0.753 (0.753) data 0.452 (0.452) loss 0.3851 (0.3851) acc 96.8750 (96.8750) lr 4.5098e+00 eta 0:01:32\n",
            "epoch [139/200] batch [2/2] time 0.308 (0.531) data 0.000 (0.226) loss 0.3818 (0.3834) acc 96.8750 (96.8750) lr 4.3792e+00 eta 0:01:04\n",
            "epoch [140/200] batch [1/2] time 0.760 (0.760) data 0.457 (0.457) loss 0.6075 (0.6075) acc 93.7500 (93.7500) lr 4.3792e+00 eta 0:01:31\n",
            "epoch [140/200] batch [2/2] time 0.308 (0.534) data 0.001 (0.229) loss 0.5131 (0.5603) acc 87.5000 (90.6250) lr 4.2499e+00 eta 0:01:04\n",
            "epoch [141/200] batch [1/2] time 0.759 (0.759) data 0.456 (0.456) loss 0.3604 (0.3604) acc 100.0000 (100.0000) lr 4.2499e+00 eta 0:01:30\n",
            "epoch [141/200] batch [2/2] time 0.309 (0.534) data 0.000 (0.228) loss 0.5770 (0.4687) acc 90.6250 (95.3125) lr 4.1221e+00 eta 0:01:03\n",
            "epoch [142/200] batch [1/2] time 0.731 (0.731) data 0.427 (0.427) loss 0.3177 (0.3177) acc 100.0000 (100.0000) lr 4.1221e+00 eta 0:01:25\n",
            "epoch [142/200] batch [2/2] time 0.309 (0.520) data 0.000 (0.214) loss 0.2916 (0.3046) acc 100.0000 (100.0000) lr 3.9958e+00 eta 0:01:00\n",
            "epoch [143/200] batch [1/2] time 0.732 (0.732) data 0.427 (0.427) loss 0.3436 (0.3436) acc 96.8750 (96.8750) lr 3.9958e+00 eta 0:01:24\n",
            "epoch [143/200] batch [2/2] time 0.304 (0.518) data 0.000 (0.214) loss 0.3417 (0.3427) acc 96.8750 (96.8750) lr 3.8709e+00 eta 0:00:59\n",
            "epoch [144/200] batch [1/2] time 0.724 (0.724) data 0.421 (0.421) loss 0.4225 (0.4225) acc 93.7500 (93.7500) lr 3.8709e+00 eta 0:01:21\n",
            "epoch [144/200] batch [2/2] time 0.306 (0.515) data 0.000 (0.211) loss 0.3424 (0.3824) acc 96.8750 (95.3125) lr 3.7476e+00 eta 0:00:57\n",
            "epoch [145/200] batch [1/2] time 0.713 (0.713) data 0.410 (0.410) loss 0.4956 (0.4956) acc 93.7500 (93.7500) lr 3.7476e+00 eta 0:01:19\n",
            "epoch [145/200] batch [2/2] time 0.307 (0.510) data 0.000 (0.205) loss 0.4252 (0.4604) acc 96.8750 (95.3125) lr 3.6258e+00 eta 0:00:56\n",
            "epoch [146/200] batch [1/2] time 0.740 (0.740) data 0.438 (0.438) loss 0.4630 (0.4630) acc 96.8750 (96.8750) lr 3.6258e+00 eta 0:01:20\n",
            "epoch [146/200] batch [2/2] time 0.310 (0.525) data 0.000 (0.219) loss 0.6989 (0.5809) acc 90.6250 (93.7500) lr 3.5055e+00 eta 0:00:56\n",
            "epoch [147/200] batch [1/2] time 0.723 (0.723) data 0.420 (0.420) loss 0.5202 (0.5202) acc 90.6250 (90.6250) lr 3.5055e+00 eta 0:01:17\n",
            "epoch [147/200] batch [2/2] time 0.308 (0.516) data 0.000 (0.210) loss 0.5176 (0.5189) acc 90.6250 (90.6250) lr 3.3869e+00 eta 0:00:54\n",
            "epoch [148/200] batch [1/2] time 0.727 (0.727) data 0.426 (0.426) loss 0.4217 (0.4217) acc 93.7500 (93.7500) lr 3.3869e+00 eta 0:01:16\n",
            "epoch [148/200] batch [2/2] time 0.307 (0.517) data 0.000 (0.213) loss 0.6852 (0.5534) acc 87.5000 (90.6250) lr 3.2699e+00 eta 0:00:53\n",
            "epoch [149/200] batch [1/2] time 0.746 (0.746) data 0.443 (0.443) loss 0.3075 (0.3075) acc 96.8750 (96.8750) lr 3.2699e+00 eta 0:01:16\n",
            "epoch [149/200] batch [2/2] time 0.305 (0.526) data 0.000 (0.222) loss 0.7376 (0.5225) acc 84.3750 (90.6250) lr 3.1545e+00 eta 0:00:53\n",
            "epoch [150/200] batch [1/2] time 0.753 (0.753) data 0.450 (0.450) loss 0.3075 (0.3075) acc 96.8750 (96.8750) lr 3.1545e+00 eta 0:01:16\n",
            "epoch [150/200] batch [2/2] time 0.308 (0.530) data 0.000 (0.225) loss 0.4412 (0.3744) acc 90.6250 (93.7500) lr 3.0409e+00 eta 0:00:53\n",
            "epoch [151/200] batch [1/2] time 0.758 (0.758) data 0.454 (0.454) loss 0.3651 (0.3651) acc 96.8750 (96.8750) lr 3.0409e+00 eta 0:01:15\n",
            "epoch [151/200] batch [2/2] time 0.306 (0.532) data 0.000 (0.227) loss 0.5198 (0.4424) acc 90.6250 (93.7500) lr 2.9289e+00 eta 0:00:52\n",
            "epoch [152/200] batch [1/2] time 0.728 (0.728) data 0.425 (0.425) loss 0.4122 (0.4122) acc 93.7500 (93.7500) lr 2.9289e+00 eta 0:01:10\n",
            "epoch [152/200] batch [2/2] time 0.307 (0.518) data 0.000 (0.212) loss 0.4778 (0.4450) acc 93.7500 (93.7500) lr 2.8187e+00 eta 0:00:49\n",
            "epoch [153/200] batch [1/2] time 0.717 (0.717) data 0.414 (0.414) loss 0.4282 (0.4282) acc 96.8750 (96.8750) lr 2.8187e+00 eta 0:01:08\n",
            "epoch [153/200] batch [2/2] time 0.307 (0.512) data 0.000 (0.207) loss 0.4376 (0.4329) acc 96.8750 (96.8750) lr 2.7103e+00 eta 0:00:48\n",
            "epoch [154/200] batch [1/2] time 0.714 (0.714) data 0.410 (0.410) loss 0.6985 (0.6985) acc 90.6250 (90.6250) lr 2.7103e+00 eta 0:01:06\n",
            "epoch [154/200] batch [2/2] time 0.307 (0.511) data 0.001 (0.205) loss 0.5003 (0.5994) acc 93.7500 (92.1875) lr 2.6037e+00 eta 0:00:46\n",
            "epoch [155/200] batch [1/2] time 0.731 (0.731) data 0.429 (0.429) loss 0.3210 (0.3210) acc 96.8750 (96.8750) lr 2.6037e+00 eta 0:01:06\n",
            "epoch [155/200] batch [2/2] time 0.305 (0.518) data 0.000 (0.214) loss 1.2550 (0.7880) acc 81.2500 (89.0625) lr 2.4989e+00 eta 0:00:46\n",
            "epoch [156/200] batch [1/2] time 0.695 (0.695) data 0.392 (0.392) loss 0.3331 (0.3331) acc 96.8750 (96.8750) lr 2.4989e+00 eta 0:01:01\n",
            "epoch [156/200] batch [2/2] time 0.307 (0.501) data 0.000 (0.196) loss 0.5264 (0.4297) acc 90.6250 (93.7500) lr 2.3959e+00 eta 0:00:44\n",
            "epoch [157/200] batch [1/2] time 0.746 (0.746) data 0.444 (0.444) loss 0.5531 (0.5531) acc 90.6250 (90.6250) lr 2.3959e+00 eta 0:01:04\n",
            "epoch [157/200] batch [2/2] time 0.305 (0.526) data 0.000 (0.222) loss 0.3118 (0.4325) acc 96.8750 (93.7500) lr 2.2949e+00 eta 0:00:45\n",
            "epoch [158/200] batch [1/2] time 0.744 (0.744) data 0.443 (0.443) loss 0.4799 (0.4799) acc 90.6250 (90.6250) lr 2.2949e+00 eta 0:01:03\n",
            "epoch [158/200] batch [2/2] time 0.304 (0.524) data 0.000 (0.222) loss 0.9148 (0.6973) acc 90.6250 (90.6250) lr 2.1957e+00 eta 0:00:44\n",
            "epoch [159/200] batch [1/2] time 0.753 (0.753) data 0.451 (0.451) loss 0.3460 (0.3460) acc 100.0000 (100.0000) lr 2.1957e+00 eta 0:01:02\n",
            "epoch [159/200] batch [2/2] time 0.307 (0.530) data 0.001 (0.226) loss 0.5821 (0.4640) acc 84.3750 (92.1875) lr 2.0984e+00 eta 0:00:43\n",
            "epoch [160/200] batch [1/2] time 0.740 (0.740) data 0.438 (0.438) loss 0.2946 (0.2946) acc 96.8750 (96.8750) lr 2.0984e+00 eta 0:00:59\n",
            "epoch [160/200] batch [2/2] time 0.306 (0.523) data 0.000 (0.219) loss 0.4080 (0.3513) acc 96.8750 (96.8750) lr 2.0032e+00 eta 0:00:41\n",
            "epoch [161/200] batch [1/2] time 0.744 (0.744) data 0.440 (0.440) loss 0.5420 (0.5420) acc 87.5000 (87.5000) lr 2.0032e+00 eta 0:00:58\n",
            "epoch [161/200] batch [2/2] time 0.305 (0.524) data 0.000 (0.220) loss 0.3373 (0.4396) acc 100.0000 (93.7500) lr 1.9098e+00 eta 0:00:40\n",
            "epoch [162/200] batch [1/2] time 0.776 (0.776) data 0.475 (0.475) loss 0.4654 (0.4654) acc 90.6250 (90.6250) lr 1.9098e+00 eta 0:00:59\n",
            "epoch [162/200] batch [2/2] time 0.307 (0.542) data 0.000 (0.238) loss 0.4090 (0.4372) acc 96.8750 (93.7500) lr 1.8185e+00 eta 0:00:41\n",
            "epoch [163/200] batch [1/2] time 0.751 (0.751) data 0.451 (0.451) loss 0.2620 (0.2620) acc 100.0000 (100.0000) lr 1.8185e+00 eta 0:00:56\n",
            "epoch [163/200] batch [2/2] time 0.305 (0.528) data 0.000 (0.225) loss 0.7698 (0.5159) acc 90.6250 (95.3125) lr 1.7292e+00 eta 0:00:39\n",
            "epoch [164/200] batch [1/2] time 0.744 (0.744) data 0.443 (0.443) loss 0.2646 (0.2646) acc 100.0000 (100.0000) lr 1.7292e+00 eta 0:00:54\n",
            "epoch [164/200] batch [2/2] time 0.307 (0.526) data 0.000 (0.221) loss 0.3601 (0.3123) acc 96.8750 (98.4375) lr 1.6419e+00 eta 0:00:37\n",
            "epoch [165/200] batch [1/2] time 0.748 (0.748) data 0.448 (0.448) loss 0.3123 (0.3123) acc 96.8750 (96.8750) lr 1.6419e+00 eta 0:00:53\n",
            "epoch [165/200] batch [2/2] time 0.304 (0.526) data 0.001 (0.225) loss 0.4127 (0.3625) acc 93.7500 (95.3125) lr 1.5567e+00 eta 0:00:36\n",
            "epoch [166/200] batch [1/2] time 0.764 (0.764) data 0.463 (0.463) loss 0.6188 (0.6188) acc 93.7500 (93.7500) lr 1.5567e+00 eta 0:00:52\n",
            "epoch [166/200] batch [2/2] time 0.302 (0.533) data 0.000 (0.232) loss 0.4216 (0.5202) acc 90.6250 (92.1875) lr 1.4736e+00 eta 0:00:36\n",
            "epoch [167/200] batch [1/2] time 0.733 (0.733) data 0.433 (0.433) loss 0.2540 (0.2540) acc 100.0000 (100.0000) lr 1.4736e+00 eta 0:00:49\n",
            "epoch [167/200] batch [2/2] time 0.305 (0.519) data 0.001 (0.217) loss 0.2652 (0.2596) acc 100.0000 (100.0000) lr 1.3926e+00 eta 0:00:34\n",
            "epoch [168/200] batch [1/2] time 0.718 (0.718) data 0.419 (0.419) loss 0.6304 (0.6304) acc 96.8750 (96.8750) lr 1.3926e+00 eta 0:00:46\n",
            "epoch [168/200] batch [2/2] time 0.304 (0.511) data 0.000 (0.210) loss 0.3886 (0.5095) acc 96.8750 (96.8750) lr 1.3137e+00 eta 0:00:32\n",
            "epoch [169/200] batch [1/2] time 0.713 (0.713) data 0.412 (0.412) loss 0.3741 (0.3741) acc 96.8750 (96.8750) lr 1.3137e+00 eta 0:00:44\n",
            "epoch [169/200] batch [2/2] time 0.303 (0.508) data 0.000 (0.206) loss 0.6848 (0.5294) acc 87.5000 (92.1875) lr 1.2369e+00 eta 0:00:31\n",
            "epoch [170/200] batch [1/2] time 0.733 (0.733) data 0.433 (0.433) loss 0.3710 (0.3710) acc 93.7500 (93.7500) lr 1.2369e+00 eta 0:00:44\n",
            "epoch [170/200] batch [2/2] time 0.304 (0.518) data 0.000 (0.217) loss 0.2566 (0.3138) acc 100.0000 (96.8750) lr 1.1623e+00 eta 0:00:31\n",
            "epoch [171/200] batch [1/2] time 0.737 (0.737) data 0.434 (0.434) loss 0.5625 (0.5625) acc 93.7500 (93.7500) lr 1.1623e+00 eta 0:00:43\n",
            "epoch [171/200] batch [2/2] time 0.305 (0.521) data 0.000 (0.217) loss 0.2390 (0.4007) acc 100.0000 (96.8750) lr 1.0899e+00 eta 0:00:30\n",
            "epoch [172/200] batch [1/2] time 0.727 (0.727) data 0.427 (0.427) loss 0.3256 (0.3256) acc 96.8750 (96.8750) lr 1.0899e+00 eta 0:00:41\n",
            "epoch [172/200] batch [2/2] time 0.305 (0.516) data 0.001 (0.214) loss 0.4229 (0.3742) acc 96.8750 (96.8750) lr 1.0197e+00 eta 0:00:28\n",
            "epoch [173/200] batch [1/2] time 0.763 (0.763) data 0.462 (0.462) loss 0.2493 (0.2493) acc 100.0000 (100.0000) lr 1.0197e+00 eta 0:00:41\n",
            "epoch [173/200] batch [2/2] time 0.303 (0.533) data 0.000 (0.231) loss 0.4652 (0.3573) acc 90.6250 (95.3125) lr 9.5173e-01 eta 0:00:28\n",
            "epoch [174/200] batch [1/2] time 0.755 (0.755) data 0.454 (0.454) loss 0.2394 (0.2394) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:40\n",
            "epoch [174/200] batch [2/2] time 0.307 (0.531) data 0.000 (0.227) loss 0.4603 (0.3499) acc 96.8750 (98.4375) lr 8.8597e-01 eta 0:00:27\n",
            "epoch [175/200] batch [1/2] time 0.764 (0.764) data 0.465 (0.465) loss 0.2822 (0.2822) acc 100.0000 (100.0000) lr 8.8597e-01 eta 0:00:38\n",
            "epoch [175/200] batch [2/2] time 0.305 (0.534) data 0.000 (0.232) loss 0.4161 (0.3492) acc 90.6250 (95.3125) lr 8.2245e-01 eta 0:00:26\n",
            "epoch [176/200] batch [1/2] time 0.727 (0.727) data 0.427 (0.427) loss 0.3106 (0.3106) acc 96.8750 (96.8750) lr 8.2245e-01 eta 0:00:35\n",
            "epoch [176/200] batch [2/2] time 0.305 (0.516) data 0.000 (0.214) loss 0.3258 (0.3182) acc 100.0000 (98.4375) lr 7.6120e-01 eta 0:00:24\n",
            "epoch [177/200] batch [1/2] time 0.726 (0.726) data 0.424 (0.424) loss 0.3167 (0.3167) acc 96.8750 (96.8750) lr 7.6120e-01 eta 0:00:34\n",
            "epoch [177/200] batch [2/2] time 0.304 (0.515) data 0.000 (0.212) loss 0.2646 (0.2907) acc 100.0000 (98.4375) lr 7.0224e-01 eta 0:00:23\n",
            "epoch [178/200] batch [1/2] time 0.721 (0.721) data 0.419 (0.419) loss 0.2489 (0.2489) acc 100.0000 (100.0000) lr 7.0224e-01 eta 0:00:32\n",
            "epoch [178/200] batch [2/2] time 0.304 (0.512) data 0.000 (0.210) loss 0.2450 (0.2469) acc 100.0000 (100.0000) lr 6.4556e-01 eta 0:00:22\n",
            "epoch [179/200] batch [1/2] time 0.741 (0.741) data 0.440 (0.440) loss 0.3631 (0.3631) acc 93.7500 (93.7500) lr 6.4556e-01 eta 0:00:31\n",
            "epoch [179/200] batch [2/2] time 0.306 (0.523) data 0.000 (0.220) loss 0.3678 (0.3655) acc 96.8750 (95.3125) lr 5.9119e-01 eta 0:00:21\n",
            "epoch [180/200] batch [1/2] time 0.710 (0.710) data 0.409 (0.409) loss 0.2404 (0.2404) acc 100.0000 (100.0000) lr 5.9119e-01 eta 0:00:29\n",
            "epoch [180/200] batch [2/2] time 0.304 (0.507) data 0.000 (0.205) loss 0.3210 (0.2807) acc 93.7500 (96.8750) lr 5.3915e-01 eta 0:00:20\n",
            "epoch [181/200] batch [1/2] time 0.738 (0.738) data 0.438 (0.438) loss 0.2715 (0.2715) acc 100.0000 (100.0000) lr 5.3915e-01 eta 0:00:28\n",
            "epoch [181/200] batch [2/2] time 0.305 (0.521) data 0.000 (0.219) loss 0.3164 (0.2939) acc 96.8750 (98.4375) lr 4.8943e-01 eta 0:00:19\n",
            "epoch [182/200] batch [1/2] time 0.711 (0.711) data 0.410 (0.410) loss 0.3600 (0.3600) acc 96.8750 (96.8750) lr 4.8943e-01 eta 0:00:26\n",
            "epoch [182/200] batch [2/2] time 0.304 (0.508) data 0.000 (0.205) loss 0.2651 (0.3126) acc 96.8750 (96.8750) lr 4.4207e-01 eta 0:00:18\n",
            "epoch [183/200] batch [1/2] time 0.708 (0.708) data 0.406 (0.406) loss 0.2828 (0.2828) acc 96.8750 (96.8750) lr 4.4207e-01 eta 0:00:24\n",
            "epoch [183/200] batch [2/2] time 0.305 (0.506) data 0.000 (0.203) loss 0.5052 (0.3940) acc 90.6250 (93.7500) lr 3.9706e-01 eta 0:00:17\n",
            "epoch [184/200] batch [1/2] time 0.717 (0.717) data 0.416 (0.416) loss 0.3007 (0.3007) acc 96.8750 (96.8750) lr 3.9706e-01 eta 0:00:23\n",
            "epoch [184/200] batch [2/2] time 0.304 (0.510) data 0.000 (0.208) loss 0.2766 (0.2886) acc 96.8750 (96.8750) lr 3.5443e-01 eta 0:00:16\n",
            "epoch [185/200] batch [1/2] time 0.721 (0.721) data 0.418 (0.418) loss 0.2335 (0.2335) acc 100.0000 (100.0000) lr 3.5443e-01 eta 0:00:22\n",
            "epoch [185/200] batch [2/2] time 0.301 (0.511) data 0.000 (0.209) loss 0.2560 (0.2447) acc 100.0000 (100.0000) lr 3.1417e-01 eta 0:00:15\n",
            "epoch [186/200] batch [1/2] time 0.733 (0.733) data 0.431 (0.431) loss 0.5197 (0.5197) acc 93.7500 (93.7500) lr 3.1417e-01 eta 0:00:21\n",
            "epoch [186/200] batch [2/2] time 0.306 (0.519) data 0.000 (0.216) loss 0.4840 (0.5019) acc 93.7500 (93.7500) lr 2.7630e-01 eta 0:00:14\n",
            "epoch [187/200] batch [1/2] time 0.746 (0.746) data 0.449 (0.449) loss 0.2634 (0.2634) acc 96.8750 (96.8750) lr 2.7630e-01 eta 0:00:20\n",
            "epoch [187/200] batch [2/2] time 0.306 (0.526) data 0.000 (0.225) loss 0.2576 (0.2605) acc 100.0000 (98.4375) lr 2.4083e-01 eta 0:00:13\n",
            "epoch [188/200] batch [1/2] time 0.712 (0.712) data 0.413 (0.413) loss 0.2402 (0.2402) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:17\n",
            "epoch [188/200] batch [2/2] time 0.308 (0.510) data 0.000 (0.207) loss 0.3148 (0.2775) acc 96.8750 (98.4375) lr 2.0777e-01 eta 0:00:12\n",
            "epoch [189/200] batch [1/2] time 0.720 (0.720) data 0.418 (0.418) loss 0.2308 (0.2308) acc 100.0000 (100.0000) lr 2.0777e-01 eta 0:00:16\n",
            "epoch [189/200] batch [2/2] time 0.305 (0.512) data 0.000 (0.209) loss 0.3306 (0.2807) acc 96.8750 (98.4375) lr 1.7713e-01 eta 0:00:11\n",
            "epoch [190/200] batch [1/2] time 0.724 (0.724) data 0.422 (0.422) loss 0.3925 (0.3925) acc 96.8750 (96.8750) lr 1.7713e-01 eta 0:00:15\n",
            "epoch [190/200] batch [2/2] time 0.305 (0.515) data 0.000 (0.211) loss 0.3172 (0.3548) acc 96.8750 (96.8750) lr 1.4891e-01 eta 0:00:10\n",
            "epoch [191/200] batch [1/2] time 0.755 (0.755) data 0.455 (0.455) loss 0.2239 (0.2239) acc 100.0000 (100.0000) lr 1.4891e-01 eta 0:00:14\n",
            "epoch [191/200] batch [2/2] time 0.304 (0.530) data 0.000 (0.228) loss 0.2921 (0.2580) acc 96.8750 (98.4375) lr 1.2312e-01 eta 0:00:09\n",
            "epoch [192/200] batch [1/2] time 0.719 (0.719) data 0.418 (0.418) loss 0.5100 (0.5100) acc 90.6250 (90.6250) lr 1.2312e-01 eta 0:00:12\n",
            "epoch [192/200] batch [2/2] time 0.302 (0.511) data 0.000 (0.209) loss 0.2773 (0.3936) acc 96.8750 (93.7500) lr 9.9763e-02 eta 0:00:08\n",
            "epoch [193/200] batch [1/2] time 0.709 (0.709) data 0.408 (0.408) loss 0.5401 (0.5401) acc 90.6250 (90.6250) lr 9.9763e-02 eta 0:00:10\n",
            "epoch [193/200] batch [2/2] time 0.303 (0.506) data 0.000 (0.204) loss 0.2495 (0.3948) acc 96.8750 (93.7500) lr 7.8853e-02 eta 0:00:07\n",
            "epoch [194/200] batch [1/2] time 0.720 (0.720) data 0.420 (0.420) loss 0.3488 (0.3488) acc 93.7500 (93.7500) lr 7.8853e-02 eta 0:00:09\n",
            "epoch [194/200] batch [2/2] time 0.305 (0.512) data 0.000 (0.210) loss 0.2295 (0.2891) acc 100.0000 (96.8750) lr 6.0390e-02 eta 0:00:06\n",
            "epoch [195/200] batch [1/2] time 0.730 (0.730) data 0.427 (0.427) loss 0.4573 (0.4573) acc 96.8750 (96.8750) lr 6.0390e-02 eta 0:00:08\n",
            "epoch [195/200] batch [2/2] time 0.305 (0.517) data 0.000 (0.213) loss 0.3838 (0.4206) acc 90.6250 (93.7500) lr 4.4380e-02 eta 0:00:05\n",
            "epoch [196/200] batch [1/2] time 0.740 (0.740) data 0.436 (0.436) loss 0.2410 (0.2410) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:06\n",
            "epoch [196/200] batch [2/2] time 0.307 (0.523) data 0.000 (0.218) loss 0.6395 (0.4403) acc 87.5000 (93.7500) lr 3.0827e-02 eta 0:00:04\n",
            "epoch [197/200] batch [1/2] time 0.751 (0.751) data 0.450 (0.450) loss 0.2293 (0.2293) acc 100.0000 (100.0000) lr 3.0827e-02 eta 0:00:05\n",
            "epoch [197/200] batch [2/2] time 0.305 (0.528) data 0.000 (0.225) loss 0.2415 (0.2354) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [1/2] time 0.746 (0.746) data 0.444 (0.444) loss 0.2313 (0.2313) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [2/2] time 0.304 (0.525) data 0.000 (0.222) loss 0.2404 (0.2358) acc 100.0000 (100.0000) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [1/2] time 0.728 (0.728) data 0.428 (0.428) loss 0.2272 (0.2272) acc 100.0000 (100.0000) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [2/2] time 0.302 (0.515) data 0.000 (0.214) loss 0.2385 (0.2329) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:01\n",
            "epoch [200/200] batch [1/2] time 0.749 (0.749) data 0.447 (0.447) loss 0.2376 (0.2376) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:00\n",
            "epoch [200/200] batch [2/2] time 0.305 (0.527) data 0.000 (0.224) loss 0.3512 (0.2944) acc 93.7500 (96.8750) lr 1.2337e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/eurosat/DAPT/vit_b16_8shots/seed1/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.49it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 6,792\n",
            "* accuracy: 83.9%\n",
            "* error: 16.1%\n",
            "* macro_f1: 83.9%\n",
            "Elapsed: 0:04:21\n"
          ]
        }
      ],
      "source": [
        "#eurosat-8shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/eurosat/DAPT/vit_b16_8shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8489d7-b1ff-4a58-a47e-6136e03b6c24",
        "id": "bfv0uGdfiUYe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 09:52:21.891252: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 09:52:21.910580: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 09:52:21.916425: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 09:52:21.930396: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 09:52:22.936341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8']\n",
            "output_dir: output/1207_new_diff_init/eurosat/DAPT/vit_b16_8shots/seed2\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/eurosat/DAPT/vit_b16_8shots/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  80\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0173, -0.5723,  0.1752,  ...,  0.4941,  0.4187,  0.0375],\n",
            "        [ 0.0214, -0.5706,  0.1699,  ...,  0.5021,  0.4206,  0.0425],\n",
            "        [ 0.0149, -0.5713,  0.1720,  ...,  0.4970,  0.4210,  0.0462],\n",
            "        ...,\n",
            "        [ 0.0324, -0.5680,  0.1748,  ...,  0.5016,  0.4188,  0.0404],\n",
            "        [ 0.0201, -0.5696,  0.1726,  ...,  0.4989,  0.4190,  0.0354],\n",
            "        [ 0.0228, -0.5696,  0.1684,  ...,  0.5053,  0.4212,  0.0412]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5609\n",
            "  Max: 1.0111\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 1.1860e-02, -2.6458e-03, -2.5006e-02,  ..., -3.9643e-04,\n",
            "           1.1337e-02,  1.2487e-02],\n",
            "         [ 5.5103e-03, -1.7622e-02, -2.4362e-02,  ..., -1.3186e-03,\n",
            "           5.9513e-03,  1.6259e-03],\n",
            "         [ 9.3536e-03,  1.0087e-03, -1.5165e-02,  ..., -3.1478e-03,\n",
            "           6.7614e-03,  4.8820e-03],\n",
            "         ...,\n",
            "         [ 2.1362e-04, -7.9000e-03, -2.3896e-02,  ..., -1.4779e-03,\n",
            "           1.1997e-02, -3.9166e-03],\n",
            "         [ 1.1013e-02, -7.4499e-03, -2.3729e-02,  ...,  3.6269e-04,\n",
            "           5.6833e-03, -2.5281e-03],\n",
            "         [-2.5940e-03, -1.1364e-02, -2.0847e-02,  ..., -2.7323e-03,\n",
            "           4.0220e-03, -7.1897e-03]],\n",
            "\n",
            "        [[ 1.6603e-03, -6.0676e-03, -1.4711e-02,  ..., -2.9127e-03,\n",
            "           1.9296e-03,  9.3089e-03],\n",
            "         [-2.8915e-03, -6.1133e-03, -1.3448e-02,  ..., -6.4027e-03,\n",
            "           4.6552e-03,  9.3776e-03],\n",
            "         [-1.5755e-03, -1.1160e-02, -1.4180e-02,  ..., -1.6028e-03,\n",
            "           3.9724e-03,  3.2044e-03],\n",
            "         ...,\n",
            "         [ 4.7645e-03, -1.1244e-02, -1.5756e-02,  ..., -3.3547e-03,\n",
            "           7.3913e-03,  2.4758e-03],\n",
            "         [ 7.0801e-03, -7.1700e-03, -1.9013e-02,  ..., -1.0163e-03,\n",
            "           2.7002e-03,  5.4027e-03],\n",
            "         [ 5.8899e-03, -6.5368e-03, -2.4068e-02,  ...,  5.2455e-03,\n",
            "           1.2527e-02, -4.9466e-03]],\n",
            "\n",
            "        [[ 1.4686e-04, -8.2129e-03, -1.6530e-02,  ..., -4.4667e-03,\n",
            "           6.4542e-03,  6.4059e-03],\n",
            "         [-4.8981e-03, -1.1071e-02, -2.5045e-02,  ..., -3.8926e-03,\n",
            "           6.6517e-03,  1.2408e-04],\n",
            "         [ 3.3245e-03, -1.0737e-02, -1.9468e-02,  ..., -5.5749e-03,\n",
            "           1.1035e-02,  7.8831e-04],\n",
            "         ...,\n",
            "         [-3.4180e-03, -4.5054e-03, -1.2708e-02,  ..., -2.1145e-03,\n",
            "           3.3296e-03,  1.9134e-03],\n",
            "         [ 4.5376e-03, -8.9677e-03, -2.3088e-02,  ..., -4.5306e-03,\n",
            "           1.3023e-02,  5.9138e-03],\n",
            "         [ 9.3574e-03, -8.1519e-03, -2.2479e-02,  ..., -1.7821e-03,\n",
            "           4.5026e-03,  2.8945e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.0692e-03, -1.3144e-02, -1.6256e-02,  ..., -9.4716e-03,\n",
            "           7.2750e-03,  1.0649e-03],\n",
            "         [ 6.2599e-03, -8.7433e-03, -1.7177e-02,  ..., -3.8430e-03,\n",
            "           1.0294e-03, -2.1371e-03],\n",
            "         [-2.1286e-03, -9.9261e-03, -2.9645e-02,  ..., -7.6253e-03,\n",
            "           8.4852e-03,  8.8320e-04],\n",
            "         ...,\n",
            "         [-4.9095e-03, -3.9542e-03, -2.2611e-02,  ...,  5.4057e-03,\n",
            "           7.7175e-03,  5.5609e-04],\n",
            "         [ 2.0873e-03,  2.1607e-03, -2.0847e-02,  ...,  5.8585e-04,\n",
            "           7.9912e-03,  2.8716e-03],\n",
            "         [-1.7548e-03, -1.4300e-02, -2.3313e-02,  ...,  1.7131e-03,\n",
            "           3.8103e-03,  3.7662e-03]],\n",
            "\n",
            "        [[ 7.1334e-04,  2.7558e-03, -2.0714e-02,  ...,  7.9310e-03,\n",
            "           1.0803e-02,  4.1495e-03],\n",
            "         [ 6.0177e-03, -5.6785e-03, -1.8350e-02,  ..., -5.0325e-04,\n",
            "           6.6672e-03,  8.7787e-03],\n",
            "         [ 8.4114e-03, -8.2713e-03, -2.0122e-02,  ..., -3.4172e-03,\n",
            "           4.1345e-03, -5.8164e-03],\n",
            "         ...,\n",
            "         [ 9.1743e-04, -5.1101e-03, -1.8438e-02,  ...,  2.2155e-04,\n",
            "           3.6560e-04,  6.9476e-03],\n",
            "         [ 7.8316e-03, -6.9430e-03, -2.2275e-02,  ...,  2.8422e-03,\n",
            "           3.2896e-03,  3.4438e-03],\n",
            "         [ 3.4103e-03, -1.1050e-02, -2.0027e-02,  ...,  5.7185e-03,\n",
            "           7.1662e-03, -8.0766e-04]],\n",
            "\n",
            "        [[ 2.7560e-04, -1.0767e-02, -2.1695e-02,  ..., -1.2066e-02,\n",
            "           1.0478e-02, -4.3096e-03],\n",
            "         [ 9.3173e-04,  9.4767e-04, -1.9784e-02,  ..., -4.0280e-03,\n",
            "           1.8016e-02, -5.1399e-05],\n",
            "         [ 4.1408e-03, -7.8738e-03, -1.1567e-02,  ..., -5.4166e-03,\n",
            "           5.1056e-04,  1.0778e-03],\n",
            "         ...,\n",
            "         [-2.7542e-03, -7.6211e-03, -1.6771e-02,  ...,  6.9316e-03,\n",
            "           6.3831e-03,  1.8298e-03],\n",
            "         [ 5.2490e-03, -6.7561e-03, -1.7544e-02,  ..., -2.3641e-03,\n",
            "           1.2687e-02,  9.0114e-03],\n",
            "         [ 6.6643e-03, -1.4715e-02, -1.9620e-02,  ..., -2.5640e-03,\n",
            "           8.1342e-03,  9.7705e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4524\n",
            "  Max: 0.4755\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/eurosat/DAPT/vit_b16_8shots/seed2/tensorboard)\n",
            "epoch [1/200] batch [1/2] time 1.756 (1.756) data 0.536 (0.536) loss 8.7928 (8.7928) acc 18.7500 (18.7500) lr 1.0000e-05 eta 0:11:40\n",
            "epoch [1/200] batch [2/2] time 0.297 (1.026) data 0.000 (0.268) loss 8.7127 (8.7528) acc 21.8750 (20.3125) lr 2.0000e+01 eta 0:06:48\n",
            "epoch [2/200] batch [1/2] time 6.045 (6.045) data 5.688 (5.688) loss 8.8200 (8.8200) acc 18.7500 (18.7500) lr 2.0000e+01 eta 0:40:00\n",
            "epoch [2/200] batch [2/2] time 0.300 (3.173) data 0.000 (2.844) loss 7.6043 (8.2121) acc 0.0000 (9.3750) lr 1.9999e+01 eta 0:20:56\n",
            "epoch [3/200] batch [1/2] time 0.715 (0.715) data 0.416 (0.416) loss 5.5389 (5.5389) acc 15.6250 (15.6250) lr 1.9999e+01 eta 0:04:42\n",
            "epoch [3/200] batch [2/2] time 0.303 (0.509) data 0.000 (0.208) loss 5.8518 (5.6954) acc 15.6250 (15.6250) lr 1.9995e+01 eta 0:03:20\n",
            "epoch [4/200] batch [1/2] time 0.749 (0.749) data 0.451 (0.451) loss 5.1701 (5.1701) acc 3.1250 (3.1250) lr 1.9995e+01 eta 0:04:54\n",
            "epoch [4/200] batch [2/2] time 0.303 (0.526) data 0.000 (0.226) loss 6.1811 (5.6756) acc 21.8750 (12.5000) lr 1.9989e+01 eta 0:03:26\n",
            "epoch [5/200] batch [1/2] time 1.295 (1.295) data 0.996 (0.996) loss 4.9208 (4.9208) acc 15.6250 (15.6250) lr 1.9989e+01 eta 0:08:26\n",
            "epoch [5/200] batch [2/2] time 0.303 (0.799) data 0.000 (0.498) loss 4.9225 (4.9216) acc 15.6250 (15.6250) lr 1.9980e+01 eta 0:05:11\n",
            "epoch [6/200] batch [1/2] time 0.749 (0.749) data 0.452 (0.452) loss 4.6278 (4.6278) acc 12.5000 (12.5000) lr 1.9980e+01 eta 0:04:51\n",
            "epoch [6/200] batch [2/2] time 0.302 (0.526) data 0.001 (0.227) loss 4.1362 (4.3820) acc 18.7500 (15.6250) lr 1.9969e+01 eta 0:03:24\n",
            "epoch [7/200] batch [1/2] time 0.729 (0.729) data 0.431 (0.431) loss 3.4815 (3.4815) acc 18.7500 (18.7500) lr 1.9969e+01 eta 0:04:42\n",
            "epoch [7/200] batch [2/2] time 0.303 (0.516) data 0.000 (0.216) loss 3.0033 (3.2424) acc 37.5000 (28.1250) lr 1.9956e+01 eta 0:03:19\n",
            "epoch [8/200] batch [1/2] time 0.725 (0.725) data 0.424 (0.424) loss 3.0991 (3.0991) acc 37.5000 (37.5000) lr 1.9956e+01 eta 0:04:39\n",
            "epoch [8/200] batch [2/2] time 0.304 (0.515) data 0.000 (0.212) loss 3.1085 (3.1038) acc 34.3750 (35.9375) lr 1.9940e+01 eta 0:03:17\n",
            "epoch [9/200] batch [1/2] time 0.741 (0.741) data 0.438 (0.438) loss 3.1125 (3.1125) acc 31.2500 (31.2500) lr 1.9940e+01 eta 0:04:43\n",
            "epoch [9/200] batch [2/2] time 0.303 (0.522) data 0.000 (0.219) loss 2.4555 (2.7840) acc 59.3750 (45.3125) lr 1.9921e+01 eta 0:03:19\n",
            "epoch [10/200] batch [1/2] time 0.707 (0.707) data 0.435 (0.435) loss 2.7630 (2.7630) acc 31.2500 (31.2500) lr 1.9921e+01 eta 0:04:29\n",
            "epoch [10/200] batch [2/2] time 0.302 (0.505) data 0.000 (0.218) loss 2.2144 (2.4887) acc 50.0000 (40.6250) lr 1.9900e+01 eta 0:03:11\n",
            "epoch [11/200] batch [1/2] time 0.729 (0.729) data 0.428 (0.428) loss 2.1000 (2.1000) acc 56.2500 (56.2500) lr 1.9900e+01 eta 0:04:36\n",
            "epoch [11/200] batch [2/2] time 0.301 (0.515) data 0.000 (0.214) loss 2.1471 (2.1235) acc 56.2500 (56.2500) lr 1.9877e+01 eta 0:03:14\n",
            "epoch [12/200] batch [1/2] time 0.746 (0.746) data 0.448 (0.448) loss 1.9001 (1.9001) acc 62.5000 (62.5000) lr 1.9877e+01 eta 0:04:41\n",
            "epoch [12/200] batch [2/2] time 0.304 (0.525) data 0.000 (0.224) loss 2.0692 (1.9847) acc 43.7500 (53.1250) lr 1.9851e+01 eta 0:03:17\n",
            "epoch [13/200] batch [1/2] time 0.733 (0.733) data 0.435 (0.435) loss 1.6830 (1.6830) acc 59.3750 (59.3750) lr 1.9851e+01 eta 0:04:34\n",
            "epoch [13/200] batch [2/2] time 0.305 (0.519) data 0.000 (0.218) loss 1.6594 (1.6712) acc 71.8750 (65.6250) lr 1.9823e+01 eta 0:03:14\n",
            "epoch [14/200] batch [1/2] time 0.713 (0.713) data 0.411 (0.411) loss 1.6897 (1.6897) acc 71.8750 (71.8750) lr 1.9823e+01 eta 0:04:26\n",
            "epoch [14/200] batch [2/2] time 0.305 (0.509) data 0.000 (0.206) loss 1.7299 (1.7098) acc 62.5000 (67.1875) lr 1.9792e+01 eta 0:03:09\n",
            "epoch [15/200] batch [1/2] time 0.757 (0.757) data 0.458 (0.458) loss 1.6374 (1.6374) acc 59.3750 (59.3750) lr 1.9792e+01 eta 0:04:40\n",
            "epoch [15/200] batch [2/2] time 0.303 (0.530) data 0.000 (0.229) loss 1.2625 (1.4499) acc 81.2500 (70.3125) lr 1.9759e+01 eta 0:03:16\n",
            "epoch [16/200] batch [1/2] time 0.762 (0.762) data 0.459 (0.459) loss 1.1648 (1.1648) acc 68.7500 (68.7500) lr 1.9759e+01 eta 0:04:41\n",
            "epoch [16/200] batch [2/2] time 0.306 (0.534) data 0.000 (0.230) loss 1.3499 (1.2573) acc 71.8750 (70.3125) lr 1.9724e+01 eta 0:03:16\n",
            "epoch [17/200] batch [1/2] time 0.778 (0.778) data 0.476 (0.476) loss 1.5953 (1.5953) acc 56.2500 (56.2500) lr 1.9724e+01 eta 0:04:45\n",
            "epoch [17/200] batch [2/2] time 0.306 (0.542) data 0.000 (0.238) loss 2.3466 (1.9709) acc 46.8750 (51.5625) lr 1.9686e+01 eta 0:03:18\n",
            "epoch [18/200] batch [1/2] time 0.736 (0.736) data 0.436 (0.436) loss 2.4036 (2.4036) acc 40.6250 (40.6250) lr 1.9686e+01 eta 0:04:28\n",
            "epoch [18/200] batch [2/2] time 0.307 (0.521) data 0.000 (0.218) loss 3.2856 (2.8446) acc 43.7500 (42.1875) lr 1.9646e+01 eta 0:03:09\n",
            "epoch [19/200] batch [1/2] time 0.721 (0.721) data 0.418 (0.418) loss 3.5963 (3.5963) acc 43.7500 (43.7500) lr 1.9646e+01 eta 0:04:21\n",
            "epoch [19/200] batch [2/2] time 0.309 (0.515) data 0.000 (0.209) loss 6.8477 (5.2220) acc 12.5000 (28.1250) lr 1.9603e+01 eta 0:03:06\n",
            "epoch [20/200] batch [1/2] time 0.743 (0.743) data 0.440 (0.440) loss 6.9443 (6.9443) acc 21.8750 (21.8750) lr 1.9603e+01 eta 0:04:28\n",
            "epoch [20/200] batch [2/2] time 0.308 (0.526) data 0.000 (0.220) loss 6.4841 (6.7142) acc 25.0000 (23.4375) lr 1.9558e+01 eta 0:03:09\n",
            "epoch [21/200] batch [1/2] time 0.719 (0.719) data 0.416 (0.416) loss 7.4719 (7.4719) acc 9.3750 (9.3750) lr 1.9558e+01 eta 0:04:18\n",
            "epoch [21/200] batch [2/2] time 0.309 (0.514) data 0.000 (0.208) loss 9.0269 (8.2494) acc 6.2500 (7.8125) lr 1.9511e+01 eta 0:03:03\n",
            "epoch [22/200] batch [1/2] time 0.761 (0.761) data 0.457 (0.457) loss 5.7985 (5.7985) acc 21.8750 (21.8750) lr 1.9511e+01 eta 0:04:31\n",
            "epoch [22/200] batch [2/2] time 0.308 (0.535) data 0.000 (0.229) loss 5.4025 (5.6005) acc 18.7500 (20.3125) lr 1.9461e+01 eta 0:03:10\n",
            "epoch [23/200] batch [1/2] time 0.742 (0.742) data 0.437 (0.437) loss 5.0770 (5.0770) acc 15.6250 (15.6250) lr 1.9461e+01 eta 0:04:23\n",
            "epoch [23/200] batch [2/2] time 0.308 (0.525) data 0.000 (0.219) loss 6.5597 (5.8183) acc 15.6250 (15.6250) lr 1.9409e+01 eta 0:03:05\n",
            "epoch [24/200] batch [1/2] time 0.731 (0.731) data 0.426 (0.426) loss 2.9524 (2.9524) acc 40.6250 (40.6250) lr 1.9409e+01 eta 0:04:18\n",
            "epoch [24/200] batch [2/2] time 0.311 (0.521) data 0.000 (0.213) loss 3.3316 (3.1420) acc 25.0000 (32.8125) lr 1.9354e+01 eta 0:03:03\n",
            "epoch [25/200] batch [1/2] time 0.738 (0.738) data 0.435 (0.435) loss 2.4785 (2.4785) acc 53.1250 (53.1250) lr 1.9354e+01 eta 0:04:18\n",
            "epoch [25/200] batch [2/2] time 0.310 (0.524) data 0.000 (0.218) loss 2.1625 (2.3205) acc 53.1250 (53.1250) lr 1.9298e+01 eta 0:03:03\n",
            "epoch [26/200] batch [1/2] time 0.740 (0.740) data 0.435 (0.435) loss 2.0865 (2.0865) acc 50.0000 (50.0000) lr 1.9298e+01 eta 0:04:18\n",
            "epoch [26/200] batch [2/2] time 0.310 (0.525) data 0.000 (0.218) loss 1.5769 (1.8317) acc 62.5000 (56.2500) lr 1.9239e+01 eta 0:03:02\n",
            "epoch [27/200] batch [1/2] time 0.738 (0.738) data 0.431 (0.431) loss 1.9227 (1.9227) acc 50.0000 (50.0000) lr 1.9239e+01 eta 0:04:16\n",
            "epoch [27/200] batch [2/2] time 0.311 (0.524) data 0.001 (0.216) loss 2.0018 (1.9623) acc 53.1250 (51.5625) lr 1.9178e+01 eta 0:03:01\n",
            "epoch [28/200] batch [1/2] time 0.766 (0.766) data 0.460 (0.460) loss 1.9346 (1.9346) acc 46.8750 (46.8750) lr 1.9178e+01 eta 0:04:24\n",
            "epoch [28/200] batch [2/2] time 0.313 (0.539) data 0.000 (0.230) loss 1.3444 (1.6395) acc 78.1250 (62.5000) lr 1.9114e+01 eta 0:03:05\n",
            "epoch [29/200] batch [1/2] time 0.724 (0.724) data 0.419 (0.419) loss 1.3010 (1.3010) acc 68.7500 (68.7500) lr 1.9114e+01 eta 0:04:08\n",
            "epoch [29/200] batch [2/2] time 0.309 (0.516) data 0.000 (0.210) loss 1.5609 (1.4310) acc 56.2500 (62.5000) lr 1.9048e+01 eta 0:02:56\n",
            "epoch [30/200] batch [1/2] time 0.739 (0.739) data 0.431 (0.431) loss 1.4286 (1.4286) acc 59.3750 (59.3750) lr 1.9048e+01 eta 0:04:11\n",
            "epoch [30/200] batch [2/2] time 0.312 (0.525) data 0.001 (0.216) loss 1.2895 (1.3590) acc 75.0000 (67.1875) lr 1.8980e+01 eta 0:02:58\n",
            "epoch [31/200] batch [1/2] time 0.746 (0.746) data 0.423 (0.423) loss 0.9899 (0.9899) acc 75.0000 (75.0000) lr 1.8980e+01 eta 0:04:12\n",
            "epoch [31/200] batch [2/2] time 0.306 (0.526) data 0.000 (0.212) loss 1.4149 (1.2024) acc 71.8750 (73.4375) lr 1.8910e+01 eta 0:02:57\n",
            "epoch [32/200] batch [1/2] time 0.759 (0.759) data 0.452 (0.452) loss 0.7474 (0.7474) acc 84.3750 (84.3750) lr 1.8910e+01 eta 0:04:15\n",
            "epoch [32/200] batch [2/2] time 0.312 (0.535) data 0.000 (0.226) loss 1.5427 (1.1451) acc 68.7500 (76.5625) lr 1.8838e+01 eta 0:02:59\n",
            "epoch [33/200] batch [1/2] time 0.723 (0.723) data 0.413 (0.413) loss 0.8941 (0.8941) acc 75.0000 (75.0000) lr 1.8838e+01 eta 0:04:02\n",
            "epoch [33/200] batch [2/2] time 0.312 (0.517) data 0.000 (0.207) loss 1.3170 (1.1056) acc 75.0000 (75.0000) lr 1.8763e+01 eta 0:02:52\n",
            "epoch [34/200] batch [1/2] time 0.730 (0.730) data 0.420 (0.420) loss 1.1690 (1.1690) acc 75.0000 (75.0000) lr 1.8763e+01 eta 0:04:03\n",
            "epoch [34/200] batch [2/2] time 0.314 (0.522) data 0.000 (0.210) loss 1.3521 (1.2606) acc 71.8750 (73.4375) lr 1.8686e+01 eta 0:02:53\n",
            "epoch [35/200] batch [1/2] time 0.740 (0.740) data 0.431 (0.431) loss 1.3601 (1.3601) acc 78.1250 (78.1250) lr 1.8686e+01 eta 0:04:05\n",
            "epoch [35/200] batch [2/2] time 0.316 (0.528) data 0.000 (0.216) loss 1.3957 (1.3779) acc 68.7500 (73.4375) lr 1.8607e+01 eta 0:02:54\n",
            "epoch [36/200] batch [1/2] time 0.735 (0.735) data 0.426 (0.426) loss 2.6305 (2.6305) acc 43.7500 (43.7500) lr 1.8607e+01 eta 0:04:01\n",
            "epoch [36/200] batch [2/2] time 0.316 (0.526) data 0.000 (0.213) loss 2.9638 (2.7971) acc 43.7500 (43.7500) lr 1.8526e+01 eta 0:02:52\n",
            "epoch [37/200] batch [1/2] time 0.735 (0.735) data 0.426 (0.426) loss 6.2032 (6.2032) acc 25.0000 (25.0000) lr 1.8526e+01 eta 0:04:00\n",
            "epoch [37/200] batch [2/2] time 0.315 (0.525) data 0.000 (0.213) loss 9.8488 (8.0260) acc 21.8750 (23.4375) lr 1.8443e+01 eta 0:02:51\n",
            "epoch [38/200] batch [1/2] time 0.773 (0.773) data 0.463 (0.463) loss 11.6338 (11.6338) acc 12.5000 (12.5000) lr 1.8443e+01 eta 0:04:11\n",
            "epoch [38/200] batch [2/2] time 0.315 (0.544) data 0.000 (0.232) loss 11.2907 (11.4622) acc 31.2500 (21.8750) lr 1.8358e+01 eta 0:02:56\n",
            "epoch [39/200] batch [1/2] time 0.781 (0.781) data 0.471 (0.471) loss 4.5970 (4.5970) acc 18.7500 (18.7500) lr 1.8358e+01 eta 0:04:12\n",
            "epoch [39/200] batch [2/2] time 0.316 (0.548) data 0.000 (0.236) loss 12.1886 (8.3928) acc 3.1250 (10.9375) lr 1.8271e+01 eta 0:02:56\n",
            "epoch [40/200] batch [1/2] time 0.753 (0.753) data 0.442 (0.442) loss 5.9642 (5.9642) acc 9.3750 (9.3750) lr 1.8271e+01 eta 0:04:01\n",
            "epoch [40/200] batch [2/2] time 0.314 (0.533) data 0.000 (0.221) loss 8.0343 (6.9993) acc 3.1250 (6.2500) lr 1.8181e+01 eta 0:02:50\n",
            "epoch [41/200] batch [1/2] time 0.736 (0.736) data 0.426 (0.426) loss 8.1918 (8.1918) acc 12.5000 (12.5000) lr 1.8181e+01 eta 0:03:54\n",
            "epoch [41/200] batch [2/2] time 0.316 (0.526) data 0.000 (0.213) loss 6.0551 (7.1235) acc 9.3750 (10.9375) lr 1.8090e+01 eta 0:02:47\n",
            "epoch [42/200] batch [1/2] time 0.721 (0.721) data 0.409 (0.409) loss 5.2539 (5.2539) acc 0.0000 (0.0000) lr 1.8090e+01 eta 0:03:48\n",
            "epoch [42/200] batch [2/2] time 0.314 (0.517) data 0.001 (0.205) loss 4.7540 (5.0040) acc 12.5000 (6.2500) lr 1.7997e+01 eta 0:02:43\n",
            "epoch [43/200] batch [1/2] time 0.723 (0.723) data 0.411 (0.411) loss 4.2262 (4.2262) acc 18.7500 (18.7500) lr 1.7997e+01 eta 0:03:47\n",
            "epoch [43/200] batch [2/2] time 0.315 (0.519) data 0.000 (0.206) loss 4.3038 (4.2650) acc 15.6250 (17.1875) lr 1.7902e+01 eta 0:02:42\n",
            "epoch [44/200] batch [1/2] time 0.741 (0.741) data 0.432 (0.432) loss 3.7824 (3.7824) acc 12.5000 (12.5000) lr 1.7902e+01 eta 0:03:52\n",
            "epoch [44/200] batch [2/2] time 0.314 (0.528) data 0.000 (0.216) loss 4.0358 (3.9091) acc 9.3750 (10.9375) lr 1.7804e+01 eta 0:02:44\n",
            "epoch [45/200] batch [1/2] time 0.715 (0.715) data 0.405 (0.405) loss 3.1574 (3.1574) acc 31.2500 (31.2500) lr 1.7804e+01 eta 0:03:42\n",
            "epoch [45/200] batch [2/2] time 0.316 (0.515) data 0.000 (0.203) loss 2.9502 (3.0538) acc 21.8750 (26.5625) lr 1.7705e+01 eta 0:02:39\n",
            "epoch [46/200] batch [1/2] time 0.720 (0.720) data 0.414 (0.414) loss 2.8356 (2.8356) acc 34.3750 (34.3750) lr 1.7705e+01 eta 0:03:42\n",
            "epoch [46/200] batch [2/2] time 0.316 (0.518) data 0.000 (0.207) loss 3.0395 (2.9375) acc 18.7500 (26.5625) lr 1.7604e+01 eta 0:02:39\n",
            "epoch [47/200] batch [1/2] time 0.737 (0.737) data 0.427 (0.427) loss 2.7378 (2.7378) acc 28.1250 (28.1250) lr 1.7604e+01 eta 0:03:46\n",
            "epoch [47/200] batch [2/2] time 0.316 (0.527) data 0.000 (0.214) loss 2.4843 (2.6110) acc 40.6250 (34.3750) lr 1.7501e+01 eta 0:02:41\n",
            "epoch [48/200] batch [1/2] time 0.786 (0.786) data 0.477 (0.477) loss 2.2171 (2.2171) acc 43.7500 (43.7500) lr 1.7501e+01 eta 0:03:59\n",
            "epoch [48/200] batch [2/2] time 0.314 (0.550) data 0.000 (0.239) loss 2.4194 (2.3183) acc 34.3750 (39.0625) lr 1.7396e+01 eta 0:02:47\n",
            "epoch [49/200] batch [1/2] time 0.776 (0.776) data 0.467 (0.467) loss 2.1477 (2.1477) acc 50.0000 (50.0000) lr 1.7396e+01 eta 0:03:55\n",
            "epoch [49/200] batch [2/2] time 0.313 (0.545) data 0.001 (0.234) loss 2.3723 (2.2600) acc 31.2500 (40.6250) lr 1.7290e+01 eta 0:02:44\n",
            "epoch [50/200] batch [1/2] time 0.746 (0.746) data 0.436 (0.436) loss 1.6469 (1.6469) acc 50.0000 (50.0000) lr 1.7290e+01 eta 0:03:44\n",
            "epoch [50/200] batch [2/2] time 0.313 (0.529) data 0.000 (0.218) loss 2.3915 (2.0192) acc 31.2500 (40.6250) lr 1.7181e+01 eta 0:02:38\n",
            "epoch [51/200] batch [1/2] time 0.744 (0.744) data 0.437 (0.437) loss 2.1742 (2.1742) acc 43.7500 (43.7500) lr 1.7181e+01 eta 0:03:42\n",
            "epoch [51/200] batch [2/2] time 0.312 (0.528) data 0.000 (0.218) loss 2.2227 (2.1985) acc 46.8750 (45.3125) lr 1.7071e+01 eta 0:02:37\n",
            "epoch [52/200] batch [1/2] time 0.738 (0.738) data 0.429 (0.429) loss 1.4611 (1.4611) acc 68.7500 (68.7500) lr 1.7071e+01 eta 0:03:39\n",
            "epoch [52/200] batch [2/2] time 0.314 (0.526) data 0.000 (0.215) loss 2.1246 (1.7928) acc 43.7500 (56.2500) lr 1.6959e+01 eta 0:02:35\n",
            "epoch [53/200] batch [1/2] time 0.746 (0.746) data 0.436 (0.436) loss 1.9175 (1.9175) acc 53.1250 (53.1250) lr 1.6959e+01 eta 0:03:39\n",
            "epoch [53/200] batch [2/2] time 0.314 (0.530) data 0.000 (0.218) loss 3.3174 (2.6175) acc 37.5000 (45.3125) lr 1.6845e+01 eta 0:02:35\n",
            "epoch [54/200] batch [1/2] time 0.715 (0.715) data 0.408 (0.408) loss 3.3652 (3.3652) acc 40.6250 (40.6250) lr 1.6845e+01 eta 0:03:29\n",
            "epoch [54/200] batch [2/2] time 0.312 (0.514) data 0.000 (0.204) loss 3.5638 (3.4645) acc 34.3750 (37.5000) lr 1.6730e+01 eta 0:02:30\n",
            "epoch [55/200] batch [1/2] time 0.701 (0.701) data 0.396 (0.396) loss 4.1589 (4.1589) acc 21.8750 (21.8750) lr 1.6730e+01 eta 0:03:24\n",
            "epoch [55/200] batch [2/2] time 0.311 (0.506) data 0.000 (0.198) loss 2.9980 (3.5784) acc 25.0000 (23.4375) lr 1.6613e+01 eta 0:02:26\n",
            "epoch [56/200] batch [1/2] time 0.740 (0.740) data 0.435 (0.435) loss 5.2886 (5.2886) acc 18.7500 (18.7500) lr 1.6613e+01 eta 0:03:33\n",
            "epoch [56/200] batch [2/2] time 0.311 (0.525) data 0.000 (0.218) loss 2.8648 (4.0767) acc 37.5000 (28.1250) lr 1.6494e+01 eta 0:02:31\n",
            "epoch [57/200] batch [1/2] time 0.713 (0.713) data 0.409 (0.409) loss 11.1960 (11.1960) acc 18.7500 (18.7500) lr 1.6494e+01 eta 0:03:24\n",
            "epoch [57/200] batch [2/2] time 0.311 (0.512) data 0.000 (0.205) loss 7.1594 (9.1777) acc 15.6250 (17.1875) lr 1.6374e+01 eta 0:02:26\n",
            "epoch [58/200] batch [1/2] time 0.732 (0.732) data 0.425 (0.425) loss 11.4142 (11.4142) acc 12.5000 (12.5000) lr 1.6374e+01 eta 0:03:28\n",
            "epoch [58/200] batch [2/2] time 0.312 (0.522) data 0.000 (0.213) loss 5.1485 (8.2813) acc 25.0000 (18.7500) lr 1.6252e+01 eta 0:02:28\n",
            "epoch [59/200] batch [1/2] time 0.749 (0.749) data 0.448 (0.448) loss 4.0475 (4.0475) acc 21.8750 (21.8750) lr 1.6252e+01 eta 0:03:32\n",
            "epoch [59/200] batch [2/2] time 0.309 (0.529) data 0.001 (0.224) loss 3.4010 (3.7242) acc 34.3750 (28.1250) lr 1.6129e+01 eta 0:02:29\n",
            "epoch [60/200] batch [1/2] time 0.759 (0.759) data 0.452 (0.452) loss 3.8608 (3.8608) acc 31.2500 (31.2500) lr 1.6129e+01 eta 0:03:33\n",
            "epoch [60/200] batch [2/2] time 0.311 (0.535) data 0.000 (0.226) loss 3.2330 (3.5469) acc 37.5000 (34.3750) lr 1.6004e+01 eta 0:02:29\n",
            "epoch [61/200] batch [1/2] time 0.752 (0.752) data 0.450 (0.450) loss 2.7911 (2.7911) acc 37.5000 (37.5000) lr 1.6004e+01 eta 0:03:29\n",
            "epoch [61/200] batch [2/2] time 0.310 (0.531) data 0.000 (0.225) loss 2.6247 (2.7079) acc 43.7500 (40.6250) lr 1.5878e+01 eta 0:02:27\n",
            "epoch [62/200] batch [1/2] time 0.745 (0.745) data 0.442 (0.442) loss 2.3410 (2.3410) acc 46.8750 (46.8750) lr 1.5878e+01 eta 0:03:26\n",
            "epoch [62/200] batch [2/2] time 0.309 (0.527) data 0.000 (0.221) loss 1.3535 (1.8472) acc 71.8750 (59.3750) lr 1.5750e+01 eta 0:02:25\n",
            "epoch [63/200] batch [1/2] time 0.741 (0.741) data 0.440 (0.440) loss 1.7158 (1.7158) acc 62.5000 (62.5000) lr 1.5750e+01 eta 0:03:23\n",
            "epoch [63/200] batch [2/2] time 0.307 (0.524) data 0.000 (0.220) loss 1.6098 (1.6628) acc 62.5000 (62.5000) lr 1.5621e+01 eta 0:02:23\n",
            "epoch [64/200] batch [1/2] time 0.748 (0.748) data 0.445 (0.445) loss 1.4265 (1.4265) acc 75.0000 (75.0000) lr 1.5621e+01 eta 0:03:24\n",
            "epoch [64/200] batch [2/2] time 0.311 (0.529) data 0.000 (0.223) loss 1.3913 (1.4089) acc 71.8750 (73.4375) lr 1.5490e+01 eta 0:02:23\n",
            "epoch [65/200] batch [1/2] time 0.750 (0.750) data 0.447 (0.447) loss 0.9427 (0.9427) acc 84.3750 (84.3750) lr 1.5490e+01 eta 0:03:23\n",
            "epoch [65/200] batch [2/2] time 0.307 (0.529) data 0.000 (0.224) loss 1.1613 (1.0520) acc 75.0000 (79.6875) lr 1.5358e+01 eta 0:02:22\n",
            "epoch [66/200] batch [1/2] time 0.725 (0.725) data 0.420 (0.420) loss 0.9365 (0.9365) acc 81.2500 (81.2500) lr 1.5358e+01 eta 0:03:14\n",
            "epoch [66/200] batch [2/2] time 0.308 (0.516) data 0.000 (0.210) loss 0.9605 (0.9485) acc 75.0000 (78.1250) lr 1.5225e+01 eta 0:02:18\n",
            "epoch [67/200] batch [1/2] time 0.764 (0.764) data 0.461 (0.461) loss 1.0168 (1.0168) acc 81.2500 (81.2500) lr 1.5225e+01 eta 0:03:23\n",
            "epoch [67/200] batch [2/2] time 0.305 (0.535) data 0.000 (0.231) loss 0.8457 (0.9312) acc 87.5000 (84.3750) lr 1.5090e+01 eta 0:02:22\n",
            "epoch [68/200] batch [1/2] time 0.741 (0.741) data 0.441 (0.441) loss 0.9160 (0.9160) acc 78.1250 (78.1250) lr 1.5090e+01 eta 0:03:16\n",
            "epoch [68/200] batch [2/2] time 0.304 (0.523) data 0.000 (0.220) loss 1.1010 (1.0085) acc 68.7500 (73.4375) lr 1.4955e+01 eta 0:02:17\n",
            "epoch [69/200] batch [1/2] time 0.748 (0.748) data 0.447 (0.447) loss 0.8018 (0.8018) acc 84.3750 (84.3750) lr 1.4955e+01 eta 0:03:16\n",
            "epoch [69/200] batch [2/2] time 0.306 (0.527) data 0.000 (0.224) loss 1.4780 (1.1399) acc 65.6250 (75.0000) lr 1.4818e+01 eta 0:02:18\n",
            "epoch [70/200] batch [1/2] time 0.756 (0.756) data 0.453 (0.453) loss 0.7814 (0.7814) acc 87.5000 (87.5000) lr 1.4818e+01 eta 0:03:17\n",
            "epoch [70/200] batch [2/2] time 0.307 (0.532) data 0.000 (0.227) loss 0.9166 (0.8490) acc 84.3750 (85.9375) lr 1.4679e+01 eta 0:02:18\n",
            "epoch [71/200] batch [1/2] time 0.767 (0.767) data 0.466 (0.466) loss 0.7814 (0.7814) acc 78.1250 (78.1250) lr 1.4679e+01 eta 0:03:18\n",
            "epoch [71/200] batch [2/2] time 0.306 (0.537) data 0.000 (0.233) loss 0.6780 (0.7297) acc 81.2500 (79.6875) lr 1.4540e+01 eta 0:02:18\n",
            "epoch [72/200] batch [1/2] time 0.735 (0.735) data 0.431 (0.431) loss 1.0028 (1.0028) acc 81.2500 (81.2500) lr 1.4540e+01 eta 0:03:08\n",
            "epoch [72/200] batch [2/2] time 0.306 (0.520) data 0.000 (0.216) loss 1.2495 (1.1262) acc 59.3750 (70.3125) lr 1.4399e+01 eta 0:02:13\n",
            "epoch [73/200] batch [1/2] time 0.730 (0.730) data 0.430 (0.430) loss 1.3078 (1.3078) acc 71.8750 (71.8750) lr 1.4399e+01 eta 0:03:06\n",
            "epoch [73/200] batch [2/2] time 0.307 (0.519) data 0.000 (0.215) loss 1.6396 (1.4737) acc 56.2500 (64.0625) lr 1.4258e+01 eta 0:02:11\n",
            "epoch [74/200] batch [1/2] time 0.732 (0.732) data 0.431 (0.431) loss 1.7314 (1.7314) acc 68.7500 (68.7500) lr 1.4258e+01 eta 0:03:05\n",
            "epoch [74/200] batch [2/2] time 0.305 (0.518) data 0.000 (0.216) loss 1.6020 (1.6667) acc 62.5000 (65.6250) lr 1.4115e+01 eta 0:02:10\n",
            "epoch [75/200] batch [1/2] time 0.700 (0.700) data 0.399 (0.399) loss 1.0276 (1.0276) acc 81.2500 (81.2500) lr 1.4115e+01 eta 0:02:55\n",
            "epoch [75/200] batch [2/2] time 0.304 (0.502) data 0.000 (0.200) loss 1.5153 (1.2715) acc 65.6250 (73.4375) lr 1.3971e+01 eta 0:02:05\n",
            "epoch [76/200] batch [1/2] time 0.767 (0.767) data 0.466 (0.466) loss 1.5074 (1.5074) acc 68.7500 (68.7500) lr 1.3971e+01 eta 0:03:10\n",
            "epoch [76/200] batch [2/2] time 0.306 (0.536) data 0.000 (0.233) loss 2.0254 (1.7664) acc 56.2500 (62.5000) lr 1.3827e+01 eta 0:02:13\n",
            "epoch [77/200] batch [1/2] time 0.737 (0.737) data 0.436 (0.436) loss 1.9001 (1.9001) acc 59.3750 (59.3750) lr 1.3827e+01 eta 0:03:02\n",
            "epoch [77/200] batch [2/2] time 0.305 (0.521) data 0.000 (0.218) loss 3.0896 (2.4948) acc 34.3750 (46.8750) lr 1.3681e+01 eta 0:02:08\n",
            "epoch [78/200] batch [1/2] time 0.751 (0.751) data 0.448 (0.448) loss 5.2715 (5.2715) acc 34.3750 (34.3750) lr 1.3681e+01 eta 0:03:04\n",
            "epoch [78/200] batch [2/2] time 0.308 (0.530) data 0.000 (0.224) loss 8.6221 (6.9468) acc 6.2500 (20.3125) lr 1.3535e+01 eta 0:02:09\n",
            "epoch [79/200] batch [1/2] time 0.722 (0.722) data 0.419 (0.419) loss 6.3851 (6.3851) acc 18.7500 (18.7500) lr 1.3535e+01 eta 0:02:55\n",
            "epoch [79/200] batch [2/2] time 0.306 (0.514) data 0.000 (0.210) loss 4.5379 (5.4615) acc 21.8750 (20.3125) lr 1.3387e+01 eta 0:02:04\n",
            "epoch [80/200] batch [1/2] time 0.770 (0.770) data 0.467 (0.467) loss 3.6150 (3.6150) acc 43.7500 (43.7500) lr 1.3387e+01 eta 0:03:05\n",
            "epoch [80/200] batch [2/2] time 0.303 (0.537) data 0.000 (0.234) loss 3.3064 (3.4607) acc 31.2500 (37.5000) lr 1.3239e+01 eta 0:02:08\n",
            "epoch [81/200] batch [1/2] time 0.718 (0.718) data 0.418 (0.418) loss 2.9738 (2.9738) acc 31.2500 (31.2500) lr 1.3239e+01 eta 0:02:51\n",
            "epoch [81/200] batch [2/2] time 0.306 (0.512) data 0.000 (0.209) loss 2.5846 (2.7792) acc 50.0000 (40.6250) lr 1.3090e+01 eta 0:02:01\n",
            "epoch [82/200] batch [1/2] time 0.752 (0.752) data 0.450 (0.450) loss 2.6007 (2.6007) acc 28.1250 (28.1250) lr 1.3090e+01 eta 0:02:58\n",
            "epoch [82/200] batch [2/2] time 0.305 (0.528) data 0.001 (0.225) loss 2.7797 (2.6902) acc 31.2500 (29.6875) lr 1.2940e+01 eta 0:02:04\n",
            "epoch [83/200] batch [1/2] time 0.765 (0.765) data 0.465 (0.465) loss 2.8138 (2.8138) acc 43.7500 (43.7500) lr 1.2940e+01 eta 0:02:59\n",
            "epoch [83/200] batch [2/2] time 0.305 (0.535) data 0.000 (0.233) loss 2.0343 (2.4241) acc 43.7500 (43.7500) lr 1.2790e+01 eta 0:02:05\n",
            "epoch [84/200] batch [1/2] time 0.770 (0.770) data 0.471 (0.471) loss 2.1707 (2.1707) acc 62.5000 (62.5000) lr 1.2790e+01 eta 0:02:59\n",
            "epoch [84/200] batch [2/2] time 0.306 (0.538) data 0.000 (0.236) loss 1.7635 (1.9671) acc 62.5000 (62.5000) lr 1.2639e+01 eta 0:02:04\n",
            "epoch [85/200] batch [1/2] time 0.752 (0.752) data 0.451 (0.451) loss 1.0691 (1.0691) acc 87.5000 (87.5000) lr 1.2639e+01 eta 0:02:53\n",
            "epoch [85/200] batch [2/2] time 0.306 (0.529) data 0.000 (0.225) loss 1.7597 (1.4144) acc 62.5000 (75.0000) lr 1.2487e+01 eta 0:02:01\n",
            "epoch [86/200] batch [1/2] time 0.721 (0.721) data 0.420 (0.420) loss 1.0463 (1.0463) acc 75.0000 (75.0000) lr 1.2487e+01 eta 0:02:45\n",
            "epoch [86/200] batch [2/2] time 0.304 (0.512) data 0.000 (0.210) loss 1.1729 (1.1096) acc 81.2500 (78.1250) lr 1.2334e+01 eta 0:01:56\n",
            "epoch [87/200] batch [1/2] time 0.737 (0.737) data 0.436 (0.436) loss 1.1038 (1.1038) acc 84.3750 (84.3750) lr 1.2334e+01 eta 0:02:47\n",
            "epoch [87/200] batch [2/2] time 0.306 (0.521) data 0.000 (0.218) loss 0.9455 (1.0247) acc 81.2500 (82.8125) lr 1.2181e+01 eta 0:01:57\n",
            "epoch [88/200] batch [1/2] time 0.720 (0.720) data 0.419 (0.419) loss 0.8047 (0.8047) acc 84.3750 (84.3750) lr 1.2181e+01 eta 0:02:41\n",
            "epoch [88/200] batch [2/2] time 0.304 (0.512) data 0.000 (0.210) loss 1.1409 (0.9728) acc 81.2500 (82.8125) lr 1.2028e+01 eta 0:01:54\n",
            "epoch [89/200] batch [1/2] time 0.723 (0.723) data 0.423 (0.423) loss 0.9157 (0.9157) acc 75.0000 (75.0000) lr 1.2028e+01 eta 0:02:41\n",
            "epoch [89/200] batch [2/2] time 0.307 (0.515) data 0.000 (0.212) loss 1.1240 (1.0198) acc 81.2500 (78.1250) lr 1.1874e+01 eta 0:01:54\n",
            "epoch [90/200] batch [1/2] time 0.744 (0.744) data 0.439 (0.439) loss 0.8886 (0.8886) acc 81.2500 (81.2500) lr 1.1874e+01 eta 0:02:44\n",
            "epoch [90/200] batch [2/2] time 0.308 (0.526) data 0.000 (0.220) loss 0.6509 (0.7697) acc 93.7500 (87.5000) lr 1.1719e+01 eta 0:01:55\n",
            "epoch [91/200] batch [1/2] time 0.763 (0.763) data 0.461 (0.461) loss 0.7489 (0.7489) acc 87.5000 (87.5000) lr 1.1719e+01 eta 0:02:46\n",
            "epoch [91/200] batch [2/2] time 0.309 (0.536) data 0.000 (0.231) loss 0.6654 (0.7071) acc 84.3750 (85.9375) lr 1.1564e+01 eta 0:01:56\n",
            "epoch [92/200] batch [1/2] time 0.720 (0.720) data 0.416 (0.416) loss 0.9594 (0.9594) acc 87.5000 (87.5000) lr 1.1564e+01 eta 0:02:36\n",
            "epoch [92/200] batch [2/2] time 0.306 (0.513) data 0.000 (0.208) loss 0.8068 (0.8831) acc 84.3750 (85.9375) lr 1.1409e+01 eta 0:01:50\n",
            "epoch [93/200] batch [1/2] time 0.759 (0.759) data 0.455 (0.455) loss 0.8175 (0.8175) acc 78.1250 (78.1250) lr 1.1409e+01 eta 0:02:43\n",
            "epoch [93/200] batch [2/2] time 0.306 (0.533) data 0.001 (0.228) loss 0.5840 (0.7007) acc 87.5000 (82.8125) lr 1.1253e+01 eta 0:01:54\n",
            "epoch [94/200] batch [1/2] time 0.756 (0.756) data 0.452 (0.452) loss 0.7677 (0.7677) acc 84.3750 (84.3750) lr 1.1253e+01 eta 0:02:40\n",
            "epoch [94/200] batch [2/2] time 0.308 (0.532) data 0.001 (0.226) loss 0.6383 (0.7030) acc 90.6250 (87.5000) lr 1.1097e+01 eta 0:01:52\n",
            "epoch [95/200] batch [1/2] time 0.745 (0.745) data 0.441 (0.441) loss 0.7041 (0.7041) acc 81.2500 (81.2500) lr 1.1097e+01 eta 0:02:37\n",
            "epoch [95/200] batch [2/2] time 0.305 (0.525) data 0.001 (0.221) loss 0.3717 (0.5379) acc 100.0000 (90.6250) lr 1.0941e+01 eta 0:01:50\n",
            "epoch [96/200] batch [1/2] time 0.729 (0.729) data 0.426 (0.426) loss 1.1690 (1.1690) acc 84.3750 (84.3750) lr 1.0941e+01 eta 0:02:32\n",
            "epoch [96/200] batch [2/2] time 0.308 (0.518) data 0.000 (0.213) loss 1.1786 (1.1738) acc 71.8750 (78.1250) lr 1.0785e+01 eta 0:01:47\n",
            "epoch [97/200] batch [1/2] time 0.732 (0.732) data 0.430 (0.430) loss 1.0997 (1.0997) acc 71.8750 (71.8750) lr 1.0785e+01 eta 0:02:31\n",
            "epoch [97/200] batch [2/2] time 0.309 (0.521) data 0.000 (0.215) loss 2.1633 (1.6315) acc 46.8750 (59.3750) lr 1.0628e+01 eta 0:01:47\n",
            "epoch [98/200] batch [1/2] time 0.749 (0.749) data 0.444 (0.444) loss 1.9847 (1.9847) acc 68.7500 (68.7500) lr 1.0628e+01 eta 0:02:33\n",
            "epoch [98/200] batch [2/2] time 0.308 (0.528) data 0.000 (0.222) loss 1.7604 (1.8725) acc 59.3750 (64.0625) lr 1.0471e+01 eta 0:01:47\n",
            "epoch [99/200] batch [1/2] time 0.731 (0.731) data 0.429 (0.429) loss 1.0827 (1.0827) acc 78.1250 (78.1250) lr 1.0471e+01 eta 0:02:28\n",
            "epoch [99/200] batch [2/2] time 0.308 (0.520) data 0.000 (0.215) loss 2.8392 (1.9609) acc 25.0000 (51.5625) lr 1.0314e+01 eta 0:01:44\n",
            "epoch [100/200] batch [1/2] time 0.720 (0.720) data 0.415 (0.415) loss 2.8964 (2.8964) acc 25.0000 (25.0000) lr 1.0314e+01 eta 0:02:24\n",
            "epoch [100/200] batch [2/2] time 0.310 (0.515) data 0.000 (0.208) loss 3.9491 (3.4227) acc 56.2500 (40.6250) lr 1.0157e+01 eta 0:01:42\n",
            "epoch [101/200] batch [1/2] time 0.738 (0.738) data 0.433 (0.433) loss 5.6236 (5.6236) acc 34.3750 (34.3750) lr 1.0157e+01 eta 0:02:26\n",
            "epoch [101/200] batch [2/2] time 0.310 (0.524) data 0.000 (0.217) loss 2.1653 (3.8945) acc 56.2500 (45.3125) lr 1.0000e+01 eta 0:01:43\n",
            "epoch [102/200] batch [1/2] time 0.697 (0.697) data 0.390 (0.390) loss 2.5984 (2.5984) acc 59.3750 (59.3750) lr 1.0000e+01 eta 0:02:17\n",
            "epoch [102/200] batch [2/2] time 0.310 (0.503) data 0.000 (0.195) loss 2.7104 (2.6544) acc 56.2500 (57.8125) lr 9.8429e+00 eta 0:01:38\n",
            "epoch [103/200] batch [1/2] time 0.734 (0.734) data 0.429 (0.429) loss 1.9635 (1.9635) acc 50.0000 (50.0000) lr 9.8429e+00 eta 0:02:23\n",
            "epoch [103/200] batch [2/2] time 0.311 (0.523) data 0.000 (0.215) loss 1.8694 (1.9164) acc 56.2500 (53.1250) lr 9.6859e+00 eta 0:01:41\n",
            "epoch [104/200] batch [1/2] time 0.722 (0.722) data 0.415 (0.415) loss 1.3717 (1.3717) acc 75.0000 (75.0000) lr 9.6859e+00 eta 0:02:19\n",
            "epoch [104/200] batch [2/2] time 0.311 (0.516) data 0.001 (0.208) loss 0.9562 (1.1639) acc 84.3750 (79.6875) lr 9.5289e+00 eta 0:01:39\n",
            "epoch [105/200] batch [1/2] time 0.750 (0.750) data 0.445 (0.445) loss 1.1754 (1.1754) acc 71.8750 (71.8750) lr 9.5289e+00 eta 0:02:23\n",
            "epoch [105/200] batch [2/2] time 0.308 (0.529) data 0.001 (0.223) loss 0.7702 (0.9728) acc 84.3750 (78.1250) lr 9.3721e+00 eta 0:01:40\n",
            "epoch [106/200] batch [1/2] time 0.750 (0.750) data 0.445 (0.445) loss 0.9645 (0.9645) acc 81.2500 (81.2500) lr 9.3721e+00 eta 0:02:21\n",
            "epoch [106/200] batch [2/2] time 0.309 (0.530) data 0.000 (0.223) loss 0.7420 (0.8532) acc 93.7500 (87.5000) lr 9.2154e+00 eta 0:01:39\n",
            "epoch [107/200] batch [1/2] time 0.721 (0.721) data 0.415 (0.415) loss 0.7009 (0.7009) acc 90.6250 (90.6250) lr 9.2154e+00 eta 0:02:14\n",
            "epoch [107/200] batch [2/2] time 0.311 (0.516) data 0.000 (0.208) loss 1.0591 (0.8800) acc 78.1250 (84.3750) lr 9.0589e+00 eta 0:01:36\n",
            "epoch [108/200] batch [1/2] time 0.741 (0.741) data 0.437 (0.437) loss 0.6846 (0.6846) acc 81.2500 (81.2500) lr 9.0589e+00 eta 0:02:17\n",
            "epoch [108/200] batch [2/2] time 0.309 (0.525) data 0.000 (0.219) loss 0.6822 (0.6834) acc 84.3750 (82.8125) lr 8.9027e+00 eta 0:01:36\n",
            "epoch [109/200] batch [1/2] time 0.738 (0.738) data 0.431 (0.431) loss 0.6784 (0.6784) acc 87.5000 (87.5000) lr 8.9027e+00 eta 0:02:15\n",
            "epoch [109/200] batch [2/2] time 0.310 (0.524) data 0.000 (0.216) loss 0.5562 (0.6173) acc 90.6250 (89.0625) lr 8.7467e+00 eta 0:01:35\n",
            "epoch [110/200] batch [1/2] time 0.731 (0.731) data 0.423 (0.423) loss 0.5647 (0.5647) acc 90.6250 (90.6250) lr 8.7467e+00 eta 0:02:12\n",
            "epoch [110/200] batch [2/2] time 0.311 (0.521) data 0.001 (0.212) loss 1.0482 (0.8065) acc 84.3750 (87.5000) lr 8.5910e+00 eta 0:01:33\n",
            "epoch [111/200] batch [1/2] time 0.751 (0.751) data 0.446 (0.446) loss 0.5834 (0.5834) acc 90.6250 (90.6250) lr 8.5910e+00 eta 0:02:14\n",
            "epoch [111/200] batch [2/2] time 0.311 (0.531) data 0.001 (0.223) loss 0.6449 (0.6142) acc 90.6250 (90.6250) lr 8.4357e+00 eta 0:01:34\n",
            "epoch [112/200] batch [1/2] time 0.766 (0.766) data 0.460 (0.460) loss 0.6720 (0.6720) acc 87.5000 (87.5000) lr 8.4357e+00 eta 0:02:15\n",
            "epoch [112/200] batch [2/2] time 0.311 (0.538) data 0.000 (0.230) loss 0.9546 (0.8133) acc 87.5000 (87.5000) lr 8.2807e+00 eta 0:01:34\n",
            "epoch [113/200] batch [1/2] time 0.714 (0.714) data 0.411 (0.411) loss 0.9047 (0.9047) acc 87.5000 (87.5000) lr 8.2807e+00 eta 0:02:05\n",
            "epoch [113/200] batch [2/2] time 0.312 (0.513) data 0.000 (0.206) loss 0.6764 (0.7905) acc 90.6250 (89.0625) lr 8.1262e+00 eta 0:01:29\n",
            "epoch [114/200] batch [1/2] time 0.745 (0.745) data 0.442 (0.442) loss 0.6275 (0.6275) acc 90.6250 (90.6250) lr 8.1262e+00 eta 0:02:08\n",
            "epoch [114/200] batch [2/2] time 0.309 (0.527) data 0.000 (0.221) loss 0.4919 (0.5597) acc 90.6250 (90.6250) lr 7.9721e+00 eta 0:01:30\n",
            "epoch [115/200] batch [1/2] time 0.760 (0.760) data 0.457 (0.457) loss 0.5672 (0.5672) acc 93.7500 (93.7500) lr 7.9721e+00 eta 0:02:09\n",
            "epoch [115/200] batch [2/2] time 0.309 (0.534) data 0.000 (0.228) loss 0.9757 (0.7715) acc 75.0000 (84.3750) lr 7.8186e+00 eta 0:01:30\n",
            "epoch [116/200] batch [1/2] time 0.779 (0.779) data 0.473 (0.473) loss 0.7754 (0.7754) acc 84.3750 (84.3750) lr 7.8186e+00 eta 0:02:11\n",
            "epoch [116/200] batch [2/2] time 0.308 (0.543) data 0.000 (0.237) loss 1.1914 (0.9834) acc 75.0000 (79.6875) lr 7.6655e+00 eta 0:01:31\n",
            "epoch [117/200] batch [1/2] time 0.765 (0.765) data 0.464 (0.464) loss 0.3643 (0.3643) acc 100.0000 (100.0000) lr 7.6655e+00 eta 0:02:07\n",
            "epoch [117/200] batch [2/2] time 0.308 (0.537) data 0.000 (0.232) loss 1.0231 (0.6937) acc 75.0000 (87.5000) lr 7.5131e+00 eta 0:01:29\n",
            "epoch [118/200] batch [1/2] time 0.739 (0.739) data 0.434 (0.434) loss 0.4743 (0.4743) acc 93.7500 (93.7500) lr 7.5131e+00 eta 0:02:01\n",
            "epoch [118/200] batch [2/2] time 0.307 (0.523) data 0.000 (0.217) loss 0.7089 (0.5916) acc 84.3750 (89.0625) lr 7.3613e+00 eta 0:01:25\n",
            "epoch [119/200] batch [1/2] time 0.744 (0.744) data 0.440 (0.440) loss 0.9095 (0.9095) acc 71.8750 (71.8750) lr 7.3613e+00 eta 0:02:01\n",
            "epoch [119/200] batch [2/2] time 0.309 (0.526) data 0.000 (0.220) loss 1.5429 (1.2262) acc 59.3750 (65.6250) lr 7.2101e+00 eta 0:01:25\n",
            "epoch [120/200] batch [1/2] time 0.763 (0.763) data 0.459 (0.459) loss 0.6654 (0.6654) acc 84.3750 (84.3750) lr 7.2101e+00 eta 0:02:02\n",
            "epoch [120/200] batch [2/2] time 0.308 (0.536) data 0.000 (0.230) loss 1.5587 (1.1120) acc 68.7500 (76.5625) lr 7.0596e+00 eta 0:01:25\n",
            "epoch [121/200] batch [1/2] time 0.741 (0.741) data 0.436 (0.436) loss 0.8729 (0.8729) acc 84.3750 (84.3750) lr 7.0596e+00 eta 0:01:57\n",
            "epoch [121/200] batch [2/2] time 0.311 (0.526) data 0.000 (0.218) loss 2.5518 (1.7124) acc 56.2500 (70.3125) lr 6.9098e+00 eta 0:01:23\n",
            "epoch [122/200] batch [1/2] time 0.744 (0.744) data 0.443 (0.443) loss 0.6828 (0.6828) acc 90.6250 (90.6250) lr 6.9098e+00 eta 0:01:56\n",
            "epoch [122/200] batch [2/2] time 0.309 (0.526) data 0.000 (0.222) loss 0.5285 (0.6056) acc 90.6250 (90.6250) lr 6.7608e+00 eta 0:01:22\n",
            "epoch [123/200] batch [1/2] time 0.760 (0.760) data 0.458 (0.458) loss 1.0175 (1.0175) acc 87.5000 (87.5000) lr 6.7608e+00 eta 0:01:57\n",
            "epoch [123/200] batch [2/2] time 0.310 (0.535) data 0.000 (0.229) loss 0.6706 (0.8440) acc 84.3750 (85.9375) lr 6.6126e+00 eta 0:01:22\n",
            "epoch [124/200] batch [1/2] time 0.734 (0.734) data 0.428 (0.428) loss 0.8509 (0.8509) acc 81.2500 (81.2500) lr 6.6126e+00 eta 0:01:52\n",
            "epoch [124/200] batch [2/2] time 0.309 (0.521) data 0.000 (0.214) loss 1.0294 (0.9402) acc 75.0000 (78.1250) lr 6.4653e+00 eta 0:01:19\n",
            "epoch [125/200] batch [1/2] time 0.747 (0.747) data 0.445 (0.445) loss 0.9731 (0.9731) acc 78.1250 (78.1250) lr 6.4653e+00 eta 0:01:52\n",
            "epoch [125/200] batch [2/2] time 0.308 (0.528) data 0.000 (0.223) loss 1.2307 (1.1019) acc 75.0000 (76.5625) lr 6.3188e+00 eta 0:01:19\n",
            "epoch [126/200] batch [1/2] time 0.713 (0.713) data 0.409 (0.409) loss 1.2960 (1.2960) acc 65.6250 (65.6250) lr 6.3188e+00 eta 0:01:46\n",
            "epoch [126/200] batch [2/2] time 0.311 (0.512) data 0.000 (0.205) loss 1.1666 (1.2313) acc 71.8750 (68.7500) lr 6.1732e+00 eta 0:01:15\n",
            "epoch [127/200] batch [1/2] time 0.758 (0.758) data 0.454 (0.454) loss 0.9453 (0.9453) acc 71.8750 (71.8750) lr 6.1732e+00 eta 0:01:51\n",
            "epoch [127/200] batch [2/2] time 0.309 (0.534) data 0.000 (0.227) loss 0.7321 (0.8387) acc 84.3750 (78.1250) lr 6.0285e+00 eta 0:01:17\n",
            "epoch [128/200] batch [1/2] time 0.721 (0.721) data 0.418 (0.418) loss 1.0446 (1.0446) acc 84.3750 (84.3750) lr 6.0285e+00 eta 0:01:44\n",
            "epoch [128/200] batch [2/2] time 0.310 (0.515) data 0.000 (0.209) loss 0.7136 (0.8791) acc 87.5000 (85.9375) lr 5.8849e+00 eta 0:01:14\n",
            "epoch [129/200] batch [1/2] time 0.738 (0.738) data 0.432 (0.432) loss 1.0284 (1.0284) acc 81.2500 (81.2500) lr 5.8849e+00 eta 0:01:45\n",
            "epoch [129/200] batch [2/2] time 0.309 (0.523) data 0.000 (0.216) loss 0.7995 (0.9140) acc 90.6250 (85.9375) lr 5.7422e+00 eta 0:01:14\n",
            "epoch [130/200] batch [1/2] time 0.708 (0.708) data 0.403 (0.403) loss 1.1039 (1.1039) acc 78.1250 (78.1250) lr 5.7422e+00 eta 0:01:39\n",
            "epoch [130/200] batch [2/2] time 0.306 (0.507) data 0.001 (0.202) loss 0.9154 (1.0096) acc 90.6250 (84.3750) lr 5.6006e+00 eta 0:01:10\n",
            "epoch [131/200] batch [1/2] time 0.749 (0.749) data 0.445 (0.445) loss 0.6836 (0.6836) acc 81.2500 (81.2500) lr 5.6006e+00 eta 0:01:44\n",
            "epoch [131/200] batch [2/2] time 0.310 (0.530) data 0.001 (0.223) loss 0.7615 (0.7225) acc 87.5000 (84.3750) lr 5.4601e+00 eta 0:01:13\n",
            "epoch [132/200] batch [1/2] time 0.725 (0.725) data 0.421 (0.421) loss 0.8994 (0.8994) acc 93.7500 (93.7500) lr 5.4601e+00 eta 0:01:39\n",
            "epoch [132/200] batch [2/2] time 0.309 (0.517) data 0.000 (0.211) loss 0.4524 (0.6759) acc 96.8750 (95.3125) lr 5.3207e+00 eta 0:01:10\n",
            "epoch [133/200] batch [1/2] time 0.738 (0.738) data 0.434 (0.434) loss 0.7230 (0.7230) acc 81.2500 (81.2500) lr 5.3207e+00 eta 0:01:39\n",
            "epoch [133/200] batch [2/2] time 0.305 (0.521) data 0.000 (0.217) loss 0.4875 (0.6053) acc 96.8750 (89.0625) lr 5.1825e+00 eta 0:01:09\n",
            "epoch [134/200] batch [1/2] time 0.747 (0.747) data 0.444 (0.444) loss 0.4503 (0.4503) acc 93.7500 (93.7500) lr 5.1825e+00 eta 0:01:39\n",
            "epoch [134/200] batch [2/2] time 0.307 (0.527) data 0.000 (0.222) loss 0.5069 (0.4786) acc 90.6250 (92.1875) lr 5.0454e+00 eta 0:01:09\n",
            "epoch [135/200] batch [1/2] time 0.751 (0.751) data 0.447 (0.447) loss 0.3712 (0.3712) acc 96.8750 (96.8750) lr 5.0454e+00 eta 0:01:38\n",
            "epoch [135/200] batch [2/2] time 0.307 (0.529) data 0.000 (0.224) loss 0.4634 (0.4173) acc 90.6250 (93.7500) lr 4.9096e+00 eta 0:01:08\n",
            "epoch [136/200] batch [1/2] time 0.737 (0.737) data 0.433 (0.433) loss 0.4174 (0.4174) acc 96.8750 (96.8750) lr 4.9096e+00 eta 0:01:35\n",
            "epoch [136/200] batch [2/2] time 0.309 (0.523) data 0.000 (0.217) loss 0.6168 (0.5171) acc 93.7500 (95.3125) lr 4.7750e+00 eta 0:01:06\n",
            "epoch [137/200] batch [1/2] time 0.733 (0.733) data 0.431 (0.431) loss 0.5005 (0.5005) acc 93.7500 (93.7500) lr 4.7750e+00 eta 0:01:33\n",
            "epoch [137/200] batch [2/2] time 0.308 (0.520) data 0.000 (0.216) loss 0.6865 (0.5935) acc 81.2500 (87.5000) lr 4.6417e+00 eta 0:01:05\n",
            "epoch [138/200] batch [1/2] time 0.751 (0.751) data 0.450 (0.450) loss 0.4013 (0.4013) acc 96.8750 (96.8750) lr 4.6417e+00 eta 0:01:33\n",
            "epoch [138/200] batch [2/2] time 0.306 (0.529) data 0.001 (0.225) loss 0.7151 (0.5582) acc 84.3750 (90.6250) lr 4.5098e+00 eta 0:01:05\n",
            "epoch [139/200] batch [1/2] time 0.773 (0.773) data 0.470 (0.470) loss 0.3175 (0.3175) acc 96.8750 (96.8750) lr 4.5098e+00 eta 0:01:35\n",
            "epoch [139/200] batch [2/2] time 0.303 (0.538) data 0.000 (0.235) loss 0.4384 (0.3779) acc 93.7500 (95.3125) lr 4.3792e+00 eta 0:01:05\n",
            "epoch [140/200] batch [1/2] time 0.733 (0.733) data 0.429 (0.429) loss 0.5354 (0.5354) acc 87.5000 (87.5000) lr 4.3792e+00 eta 0:01:28\n",
            "epoch [140/200] batch [2/2] time 0.308 (0.521) data 0.000 (0.215) loss 0.4080 (0.4717) acc 96.8750 (92.1875) lr 4.2499e+00 eta 0:01:02\n",
            "epoch [141/200] batch [1/2] time 0.751 (0.751) data 0.451 (0.451) loss 0.5282 (0.5282) acc 87.5000 (87.5000) lr 4.2499e+00 eta 0:01:29\n",
            "epoch [141/200] batch [2/2] time 0.304 (0.528) data 0.000 (0.226) loss 0.5805 (0.5544) acc 90.6250 (89.0625) lr 4.1221e+00 eta 0:01:02\n",
            "epoch [142/200] batch [1/2] time 0.741 (0.741) data 0.443 (0.443) loss 0.5561 (0.5561) acc 90.6250 (90.6250) lr 4.1221e+00 eta 0:01:26\n",
            "epoch [142/200] batch [2/2] time 0.306 (0.524) data 0.001 (0.222) loss 0.3913 (0.4737) acc 96.8750 (93.7500) lr 3.9958e+00 eta 0:01:00\n",
            "epoch [143/200] batch [1/2] time 0.747 (0.747) data 0.446 (0.446) loss 0.4997 (0.4997) acc 93.7500 (93.7500) lr 3.9958e+00 eta 0:01:25\n",
            "epoch [143/200] batch [2/2] time 0.310 (0.529) data 0.000 (0.223) loss 0.5651 (0.5324) acc 96.8750 (95.3125) lr 3.8709e+00 eta 0:01:00\n",
            "epoch [144/200] batch [1/2] time 0.741 (0.741) data 0.438 (0.438) loss 0.6969 (0.6969) acc 84.3750 (84.3750) lr 3.8709e+00 eta 0:01:23\n",
            "epoch [144/200] batch [2/2] time 0.307 (0.524) data 0.000 (0.219) loss 0.5456 (0.6212) acc 93.7500 (89.0625) lr 3.7476e+00 eta 0:00:58\n",
            "epoch [145/200] batch [1/2] time 0.751 (0.751) data 0.451 (0.451) loss 0.5368 (0.5368) acc 93.7500 (93.7500) lr 3.7476e+00 eta 0:01:23\n",
            "epoch [145/200] batch [2/2] time 0.305 (0.528) data 0.000 (0.226) loss 0.5077 (0.5222) acc 87.5000 (90.6250) lr 3.6258e+00 eta 0:00:58\n",
            "epoch [146/200] batch [1/2] time 0.728 (0.728) data 0.427 (0.427) loss 0.4635 (0.4635) acc 93.7500 (93.7500) lr 3.6258e+00 eta 0:01:19\n",
            "epoch [146/200] batch [2/2] time 0.307 (0.517) data 0.000 (0.214) loss 0.5318 (0.4976) acc 87.5000 (90.6250) lr 3.5055e+00 eta 0:00:55\n",
            "epoch [147/200] batch [1/2] time 0.756 (0.756) data 0.454 (0.454) loss 0.3263 (0.3263) acc 96.8750 (96.8750) lr 3.5055e+00 eta 0:01:20\n",
            "epoch [147/200] batch [2/2] time 0.306 (0.531) data 0.001 (0.227) loss 0.4739 (0.4001) acc 93.7500 (95.3125) lr 3.3869e+00 eta 0:00:56\n",
            "epoch [148/200] batch [1/2] time 0.737 (0.737) data 0.437 (0.437) loss 0.7082 (0.7082) acc 93.7500 (93.7500) lr 3.3869e+00 eta 0:01:17\n",
            "epoch [148/200] batch [2/2] time 0.305 (0.521) data 0.000 (0.219) loss 0.4337 (0.5709) acc 96.8750 (95.3125) lr 3.2699e+00 eta 0:00:54\n",
            "epoch [149/200] batch [1/2] time 0.773 (0.773) data 0.471 (0.471) loss 0.5600 (0.5600) acc 93.7500 (93.7500) lr 3.2699e+00 eta 0:01:19\n",
            "epoch [149/200] batch [2/2] time 0.306 (0.540) data 0.001 (0.236) loss 0.3925 (0.4762) acc 93.7500 (93.7500) lr 3.1545e+00 eta 0:00:55\n",
            "epoch [150/200] batch [1/2] time 0.748 (0.748) data 0.446 (0.446) loss 0.2829 (0.2829) acc 100.0000 (100.0000) lr 3.1545e+00 eta 0:01:15\n",
            "epoch [150/200] batch [2/2] time 0.303 (0.526) data 0.000 (0.223) loss 0.3623 (0.3226) acc 96.8750 (98.4375) lr 3.0409e+00 eta 0:00:52\n",
            "epoch [151/200] batch [1/2] time 0.774 (0.774) data 0.472 (0.472) loss 0.6193 (0.6193) acc 90.6250 (90.6250) lr 3.0409e+00 eta 0:01:16\n",
            "epoch [151/200] batch [2/2] time 0.305 (0.540) data 0.000 (0.236) loss 0.6024 (0.6108) acc 93.7500 (92.1875) lr 2.9289e+00 eta 0:00:52\n",
            "epoch [152/200] batch [1/2] time 0.740 (0.740) data 0.438 (0.438) loss 0.4661 (0.4661) acc 96.8750 (96.8750) lr 2.9289e+00 eta 0:01:11\n",
            "epoch [152/200] batch [2/2] time 0.302 (0.521) data 0.000 (0.219) loss 0.5277 (0.4969) acc 93.7500 (95.3125) lr 2.8187e+00 eta 0:00:50\n",
            "epoch [153/200] batch [1/2] time 0.746 (0.746) data 0.443 (0.443) loss 0.5683 (0.5683) acc 87.5000 (87.5000) lr 2.8187e+00 eta 0:01:10\n",
            "epoch [153/200] batch [2/2] time 0.305 (0.526) data 0.000 (0.222) loss 0.3495 (0.4589) acc 96.8750 (92.1875) lr 2.7103e+00 eta 0:00:49\n",
            "epoch [154/200] batch [1/2] time 0.732 (0.732) data 0.432 (0.432) loss 0.3087 (0.3087) acc 100.0000 (100.0000) lr 2.7103e+00 eta 0:01:08\n",
            "epoch [154/200] batch [2/2] time 0.306 (0.519) data 0.000 (0.216) loss 0.6863 (0.4975) acc 81.2500 (90.6250) lr 2.6037e+00 eta 0:00:47\n",
            "epoch [155/200] batch [1/2] time 0.742 (0.742) data 0.441 (0.441) loss 0.5872 (0.5872) acc 93.7500 (93.7500) lr 2.6037e+00 eta 0:01:07\n",
            "epoch [155/200] batch [2/2] time 0.306 (0.524) data 0.000 (0.221) loss 0.4463 (0.5168) acc 90.6250 (92.1875) lr 2.4989e+00 eta 0:00:47\n",
            "epoch [156/200] batch [1/2] time 0.743 (0.743) data 0.443 (0.443) loss 0.7280 (0.7280) acc 84.3750 (84.3750) lr 2.4989e+00 eta 0:01:06\n",
            "epoch [156/200] batch [2/2] time 0.306 (0.525) data 0.000 (0.221) loss 0.5239 (0.6259) acc 93.7500 (89.0625) lr 2.3959e+00 eta 0:00:46\n",
            "epoch [157/200] batch [1/2] time 0.745 (0.745) data 0.444 (0.444) loss 0.2852 (0.2852) acc 100.0000 (100.0000) lr 2.3959e+00 eta 0:01:04\n",
            "epoch [157/200] batch [2/2] time 0.304 (0.525) data 0.000 (0.222) loss 0.5951 (0.4401) acc 87.5000 (93.7500) lr 2.2949e+00 eta 0:00:45\n",
            "epoch [158/200] batch [1/2] time 0.710 (0.710) data 0.407 (0.407) loss 0.7048 (0.7048) acc 90.6250 (90.6250) lr 2.2949e+00 eta 0:01:00\n",
            "epoch [158/200] batch [2/2] time 0.306 (0.508) data 0.000 (0.203) loss 0.3781 (0.5415) acc 96.8750 (93.7500) lr 2.1957e+00 eta 0:00:42\n",
            "epoch [159/200] batch [1/2] time 0.731 (0.731) data 0.430 (0.430) loss 0.5394 (0.5394) acc 87.5000 (87.5000) lr 2.1957e+00 eta 0:01:00\n",
            "epoch [159/200] batch [2/2] time 0.307 (0.519) data 0.000 (0.215) loss 0.3063 (0.4229) acc 100.0000 (93.7500) lr 2.0984e+00 eta 0:00:42\n",
            "epoch [160/200] batch [1/2] time 0.754 (0.754) data 0.450 (0.450) loss 0.4065 (0.4065) acc 90.6250 (90.6250) lr 2.0984e+00 eta 0:01:01\n",
            "epoch [160/200] batch [2/2] time 0.308 (0.531) data 0.000 (0.225) loss 0.6381 (0.5223) acc 90.6250 (90.6250) lr 2.0032e+00 eta 0:00:42\n",
            "epoch [161/200] batch [1/2] time 0.738 (0.738) data 0.434 (0.434) loss 0.3179 (0.3179) acc 100.0000 (100.0000) lr 2.0032e+00 eta 0:00:58\n",
            "epoch [161/200] batch [2/2] time 0.305 (0.521) data 0.000 (0.217) loss 0.3245 (0.3212) acc 100.0000 (100.0000) lr 1.9098e+00 eta 0:00:40\n",
            "epoch [162/200] batch [1/2] time 0.765 (0.765) data 0.463 (0.463) loss 0.6714 (0.6714) acc 78.1250 (78.1250) lr 1.9098e+00 eta 0:00:58\n",
            "epoch [162/200] batch [2/2] time 0.303 (0.534) data 0.001 (0.232) loss 0.3654 (0.5184) acc 96.8750 (87.5000) lr 1.8185e+00 eta 0:00:40\n",
            "epoch [163/200] batch [1/2] time 0.743 (0.743) data 0.441 (0.441) loss 0.3281 (0.3281) acc 96.8750 (96.8750) lr 1.8185e+00 eta 0:00:55\n",
            "epoch [163/200] batch [2/2] time 0.306 (0.525) data 0.001 (0.221) loss 0.8573 (0.5927) acc 84.3750 (90.6250) lr 1.7292e+00 eta 0:00:38\n",
            "epoch [164/200] batch [1/2] time 0.748 (0.748) data 0.446 (0.446) loss 0.4654 (0.4654) acc 93.7500 (93.7500) lr 1.7292e+00 eta 0:00:54\n",
            "epoch [164/200] batch [2/2] time 0.304 (0.526) data 0.000 (0.223) loss 0.7124 (0.5889) acc 84.3750 (89.0625) lr 1.6419e+00 eta 0:00:37\n",
            "epoch [165/200] batch [1/2] time 0.737 (0.737) data 0.436 (0.436) loss 0.4163 (0.4163) acc 93.7500 (93.7500) lr 1.6419e+00 eta 0:00:52\n",
            "epoch [165/200] batch [2/2] time 0.306 (0.522) data 0.000 (0.218) loss 0.4139 (0.4151) acc 93.7500 (93.7500) lr 1.5567e+00 eta 0:00:36\n",
            "epoch [166/200] batch [1/2] time 0.713 (0.713) data 0.412 (0.412) loss 0.3705 (0.3705) acc 93.7500 (93.7500) lr 1.5567e+00 eta 0:00:49\n",
            "epoch [166/200] batch [2/2] time 0.305 (0.509) data 0.000 (0.206) loss 0.3147 (0.3426) acc 100.0000 (96.8750) lr 1.4736e+00 eta 0:00:34\n",
            "epoch [167/200] batch [1/2] time 0.735 (0.735) data 0.431 (0.431) loss 0.2751 (0.2751) acc 100.0000 (100.0000) lr 1.4736e+00 eta 0:00:49\n",
            "epoch [167/200] batch [2/2] time 0.307 (0.521) data 0.001 (0.216) loss 0.5121 (0.3936) acc 93.7500 (96.8750) lr 1.3926e+00 eta 0:00:34\n",
            "epoch [168/200] batch [1/2] time 0.732 (0.732) data 0.429 (0.429) loss 0.2983 (0.2983) acc 100.0000 (100.0000) lr 1.3926e+00 eta 0:00:47\n",
            "epoch [168/200] batch [2/2] time 0.303 (0.518) data 0.000 (0.215) loss 0.4297 (0.3640) acc 93.7500 (96.8750) lr 1.3137e+00 eta 0:00:33\n",
            "epoch [169/200] batch [1/2] time 0.738 (0.738) data 0.437 (0.437) loss 0.4484 (0.4484) acc 90.6250 (90.6250) lr 1.3137e+00 eta 0:00:46\n",
            "epoch [169/200] batch [2/2] time 0.307 (0.523) data 0.000 (0.219) loss 0.6815 (0.5650) acc 87.5000 (89.0625) lr 1.2369e+00 eta 0:00:32\n",
            "epoch [170/200] batch [1/2] time 0.736 (0.736) data 0.432 (0.432) loss 0.3631 (0.3631) acc 96.8750 (96.8750) lr 1.2369e+00 eta 0:00:44\n",
            "epoch [170/200] batch [2/2] time 0.309 (0.522) data 0.000 (0.216) loss 0.4191 (0.3911) acc 90.6250 (93.7500) lr 1.1623e+00 eta 0:00:31\n",
            "epoch [171/200] batch [1/2] time 0.731 (0.731) data 0.428 (0.428) loss 0.2991 (0.2991) acc 100.0000 (100.0000) lr 1.1623e+00 eta 0:00:43\n",
            "epoch [171/200] batch [2/2] time 0.305 (0.518) data 0.000 (0.214) loss 0.3485 (0.3238) acc 96.8750 (98.4375) lr 1.0899e+00 eta 0:00:30\n",
            "epoch [172/200] batch [1/2] time 0.770 (0.770) data 0.470 (0.470) loss 0.4595 (0.4595) acc 93.7500 (93.7500) lr 1.0899e+00 eta 0:00:43\n",
            "epoch [172/200] batch [2/2] time 0.304 (0.537) data 0.000 (0.235) loss 0.3364 (0.3980) acc 96.8750 (95.3125) lr 1.0197e+00 eta 0:00:30\n",
            "epoch [173/200] batch [1/2] time 0.781 (0.781) data 0.477 (0.477) loss 0.2876 (0.2876) acc 96.8750 (96.8750) lr 1.0197e+00 eta 0:00:42\n",
            "epoch [173/200] batch [2/2] time 0.307 (0.544) data 0.000 (0.239) loss 0.3298 (0.3087) acc 96.8750 (96.8750) lr 9.5173e-01 eta 0:00:29\n",
            "epoch [174/200] batch [1/2] time 0.755 (0.755) data 0.449 (0.449) loss 0.2610 (0.2610) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:40\n",
            "epoch [174/200] batch [2/2] time 0.308 (0.532) data 0.000 (0.225) loss 0.3199 (0.2904) acc 93.7500 (96.8750) lr 8.8597e-01 eta 0:00:27\n",
            "epoch [175/200] batch [1/2] time 0.719 (0.719) data 0.416 (0.416) loss 0.4093 (0.4093) acc 90.6250 (90.6250) lr 8.8597e-01 eta 0:00:36\n",
            "epoch [175/200] batch [2/2] time 0.307 (0.513) data 0.000 (0.208) loss 0.4493 (0.4293) acc 93.7500 (92.1875) lr 8.2245e-01 eta 0:00:25\n",
            "epoch [176/200] batch [1/2] time 0.756 (0.756) data 0.453 (0.453) loss 0.2762 (0.2762) acc 96.8750 (96.8750) lr 8.2245e-01 eta 0:00:37\n",
            "epoch [176/200] batch [2/2] time 0.306 (0.531) data 0.000 (0.227) loss 0.3019 (0.2891) acc 96.8750 (96.8750) lr 7.6120e-01 eta 0:00:25\n",
            "epoch [177/200] batch [1/2] time 0.742 (0.742) data 0.438 (0.438) loss 0.4237 (0.4237) acc 93.7500 (93.7500) lr 7.6120e-01 eta 0:00:34\n",
            "epoch [177/200] batch [2/2] time 0.308 (0.525) data 0.000 (0.219) loss 0.3661 (0.3949) acc 93.7500 (93.7500) lr 7.0224e-01 eta 0:00:24\n",
            "epoch [178/200] batch [1/2] time 0.748 (0.748) data 0.445 (0.445) loss 0.2800 (0.2800) acc 96.8750 (96.8750) lr 7.0224e-01 eta 0:00:33\n",
            "epoch [178/200] batch [2/2] time 0.306 (0.527) data 0.000 (0.223) loss 0.2704 (0.2752) acc 100.0000 (98.4375) lr 6.4556e-01 eta 0:00:23\n",
            "epoch [179/200] batch [1/2] time 0.738 (0.738) data 0.436 (0.436) loss 0.3376 (0.3376) acc 96.8750 (96.8750) lr 6.4556e-01 eta 0:00:31\n",
            "epoch [179/200] batch [2/2] time 0.306 (0.522) data 0.000 (0.218) loss 0.2598 (0.2987) acc 100.0000 (98.4375) lr 5.9119e-01 eta 0:00:21\n",
            "epoch [180/200] batch [1/2] time 0.730 (0.730) data 0.429 (0.429) loss 0.2429 (0.2429) acc 100.0000 (100.0000) lr 5.9119e-01 eta 0:00:29\n",
            "epoch [180/200] batch [2/2] time 0.308 (0.519) data 0.000 (0.215) loss 0.6641 (0.4535) acc 93.7500 (96.8750) lr 5.3915e-01 eta 0:00:20\n",
            "epoch [181/200] batch [1/2] time 0.726 (0.726) data 0.421 (0.421) loss 0.4990 (0.4990) acc 93.7500 (93.7500) lr 5.3915e-01 eta 0:00:28\n",
            "epoch [181/200] batch [2/2] time 0.309 (0.517) data 0.000 (0.211) loss 0.4642 (0.4816) acc 96.8750 (95.3125) lr 4.8943e-01 eta 0:00:19\n",
            "epoch [182/200] batch [1/2] time 0.739 (0.739) data 0.437 (0.437) loss 0.4689 (0.4689) acc 90.6250 (90.6250) lr 4.8943e-01 eta 0:00:27\n",
            "epoch [182/200] batch [2/2] time 0.306 (0.522) data 0.000 (0.219) loss 0.2419 (0.3554) acc 100.0000 (95.3125) lr 4.4207e-01 eta 0:00:18\n",
            "epoch [183/200] batch [1/2] time 0.751 (0.751) data 0.446 (0.446) loss 0.5825 (0.5825) acc 90.6250 (90.6250) lr 4.4207e-01 eta 0:00:26\n",
            "epoch [183/200] batch [2/2] time 0.308 (0.529) data 0.000 (0.223) loss 0.2780 (0.4302) acc 100.0000 (95.3125) lr 3.9706e-01 eta 0:00:17\n",
            "epoch [184/200] batch [1/2] time 0.765 (0.765) data 0.461 (0.461) loss 0.3165 (0.3165) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:25\n",
            "epoch [184/200] batch [2/2] time 0.309 (0.537) data 0.001 (0.231) loss 0.3880 (0.3522) acc 96.8750 (98.4375) lr 3.5443e-01 eta 0:00:17\n",
            "epoch [185/200] batch [1/2] time 0.716 (0.716) data 0.411 (0.411) loss 0.3266 (0.3266) acc 96.8750 (96.8750) lr 3.5443e-01 eta 0:00:22\n",
            "epoch [185/200] batch [2/2] time 0.309 (0.513) data 0.001 (0.206) loss 0.3886 (0.3576) acc 96.8750 (96.8750) lr 3.1417e-01 eta 0:00:15\n",
            "epoch [186/200] batch [1/2] time 0.731 (0.731) data 0.430 (0.430) loss 0.3595 (0.3595) acc 96.8750 (96.8750) lr 3.1417e-01 eta 0:00:21\n",
            "epoch [186/200] batch [2/2] time 0.307 (0.519) data 0.000 (0.215) loss 0.7008 (0.5301) acc 90.6250 (93.7500) lr 2.7630e-01 eta 0:00:14\n",
            "epoch [187/200] batch [1/2] time 0.728 (0.728) data 0.427 (0.427) loss 0.4720 (0.4720) acc 93.7500 (93.7500) lr 2.7630e-01 eta 0:00:19\n",
            "epoch [187/200] batch [2/2] time 0.309 (0.518) data 0.001 (0.214) loss 0.3588 (0.4154) acc 93.7500 (93.7500) lr 2.4083e-01 eta 0:00:13\n",
            "epoch [188/200] batch [1/2] time 0.745 (0.745) data 0.444 (0.444) loss 0.3207 (0.3207) acc 93.7500 (93.7500) lr 2.4083e-01 eta 0:00:18\n",
            "epoch [188/200] batch [2/2] time 0.306 (0.526) data 0.000 (0.222) loss 0.4959 (0.4083) acc 93.7500 (93.7500) lr 2.0777e-01 eta 0:00:12\n",
            "epoch [189/200] batch [1/2] time 0.729 (0.729) data 0.424 (0.424) loss 0.3109 (0.3109) acc 96.8750 (96.8750) lr 2.0777e-01 eta 0:00:16\n",
            "epoch [189/200] batch [2/2] time 0.308 (0.518) data 0.000 (0.212) loss 0.3701 (0.3405) acc 96.8750 (96.8750) lr 1.7713e-01 eta 0:00:11\n",
            "epoch [190/200] batch [1/2] time 0.729 (0.729) data 0.426 (0.426) loss 0.5288 (0.5288) acc 93.7500 (93.7500) lr 1.7713e-01 eta 0:00:15\n",
            "epoch [190/200] batch [2/2] time 0.308 (0.518) data 0.000 (0.213) loss 0.3395 (0.4341) acc 96.8750 (95.3125) lr 1.4891e-01 eta 0:00:10\n",
            "epoch [191/200] batch [1/2] time 0.734 (0.734) data 0.433 (0.433) loss 0.3071 (0.3071) acc 96.8750 (96.8750) lr 1.4891e-01 eta 0:00:13\n",
            "epoch [191/200] batch [2/2] time 0.305 (0.519) data 0.000 (0.217) loss 0.2811 (0.2941) acc 100.0000 (98.4375) lr 1.2312e-01 eta 0:00:09\n",
            "epoch [192/200] batch [1/2] time 0.779 (0.779) data 0.475 (0.475) loss 0.3961 (0.3961) acc 96.8750 (96.8750) lr 1.2312e-01 eta 0:00:13\n",
            "epoch [192/200] batch [2/2] time 0.308 (0.543) data 0.000 (0.238) loss 0.3048 (0.3504) acc 96.8750 (96.8750) lr 9.9763e-02 eta 0:00:08\n",
            "epoch [193/200] batch [1/2] time 0.746 (0.746) data 0.441 (0.441) loss 0.3059 (0.3059) acc 96.8750 (96.8750) lr 9.9763e-02 eta 0:00:11\n",
            "epoch [193/200] batch [2/2] time 0.309 (0.527) data 0.001 (0.221) loss 0.3283 (0.3171) acc 96.8750 (96.8750) lr 7.8853e-02 eta 0:00:07\n",
            "epoch [194/200] batch [1/2] time 0.787 (0.787) data 0.486 (0.486) loss 0.2944 (0.2944) acc 96.8750 (96.8750) lr 7.8853e-02 eta 0:00:10\n",
            "epoch [194/200] batch [2/2] time 0.307 (0.547) data 0.000 (0.243) loss 0.3403 (0.3173) acc 96.8750 (96.8750) lr 6.0390e-02 eta 0:00:06\n",
            "epoch [195/200] batch [1/2] time 0.766 (0.766) data 0.461 (0.461) loss 0.3676 (0.3676) acc 90.6250 (90.6250) lr 6.0390e-02 eta 0:00:08\n",
            "epoch [195/200] batch [2/2] time 0.307 (0.537) data 0.001 (0.231) loss 0.3771 (0.3723) acc 90.6250 (90.6250) lr 4.4380e-02 eta 0:00:05\n",
            "epoch [196/200] batch [1/2] time 0.750 (0.750) data 0.448 (0.448) loss 0.2705 (0.2705) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:06\n",
            "epoch [196/200] batch [2/2] time 0.304 (0.527) data 0.000 (0.224) loss 0.2520 (0.2613) acc 100.0000 (100.0000) lr 3.0827e-02 eta 0:00:04\n",
            "epoch [197/200] batch [1/2] time 0.724 (0.724) data 0.423 (0.423) loss 0.3787 (0.3787) acc 96.8750 (96.8750) lr 3.0827e-02 eta 0:00:05\n",
            "epoch [197/200] batch [2/2] time 0.306 (0.515) data 0.000 (0.212) loss 0.2987 (0.3387) acc 96.8750 (96.8750) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [1/2] time 0.733 (0.733) data 0.429 (0.429) loss 0.2467 (0.2467) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [2/2] time 0.305 (0.519) data 0.000 (0.215) loss 0.3350 (0.2908) acc 93.7500 (96.8750) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [1/2] time 0.742 (0.742) data 0.440 (0.440) loss 0.3209 (0.3209) acc 96.8750 (96.8750) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [2/2] time 0.306 (0.524) data 0.000 (0.220) loss 0.5342 (0.4275) acc 93.7500 (95.3125) lr 4.9344e-03 eta 0:00:01\n",
            "epoch [200/200] batch [1/2] time 0.745 (0.745) data 0.445 (0.445) loss 0.3584 (0.3584) acc 96.8750 (96.8750) lr 4.9344e-03 eta 0:00:00\n",
            "epoch [200/200] batch [2/2] time 0.306 (0.526) data 0.000 (0.223) loss 0.3919 (0.3752) acc 93.7500 (95.3125) lr 1.2337e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/eurosat/DAPT/vit_b16_8shots/seed2/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.49it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 6,396\n",
            "* accuracy: 79.0%\n",
            "* error: 21.0%\n",
            "* macro_f1: 78.6%\n",
            "Elapsed: 0:04:23\n"
          ]
        }
      ],
      "source": [
        "#eurosat-8shots-seed2\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 2 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/eurosat/DAPT/vit_b16_8shots/seed2 \\\n",
        "        DATASET.NUM_SHOTS 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecf9ff5-cfe5-4055-d8aa-35b9badacf27",
        "id": "Drx9BacNiUYe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 09:57:03.259290: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 09:57:03.279225: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 09:57:03.285188: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 09:57:03.299769: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 09:57:04.279031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8']\n",
            "output_dir: output/1207_new_diff_init/urosat/DAPT/vit_b16_8shots/seed3\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/urosat/DAPT/vit_b16_8shots/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_8-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  80\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0187, -0.5727,  0.1683,  ...,  0.4947,  0.4188,  0.0344],\n",
            "        [ 0.0173, -0.5677,  0.1739,  ...,  0.4990,  0.4190,  0.0388],\n",
            "        [ 0.0094, -0.5649,  0.1758,  ...,  0.4946,  0.4157,  0.0405],\n",
            "        ...,\n",
            "        [ 0.0227, -0.5676,  0.1747,  ...,  0.5071,  0.4204,  0.0413],\n",
            "        [ 0.0083, -0.5735,  0.1744,  ...,  0.5016,  0.4249,  0.0364],\n",
            "        [ 0.0210, -0.5687,  0.1704,  ...,  0.5030,  0.4152,  0.0323]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2340\n",
            "  Min: -1.5598\n",
            "  Max: 1.0156\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-1.2399e-04, -9.3844e-03, -2.1417e-02,  ..., -1.0112e-02,\n",
            "          -7.3685e-04,  7.9737e-04],\n",
            "         [ 8.8539e-03,  1.7411e-03, -1.0068e-02,  ..., -2.1264e-03,\n",
            "           5.1197e-03,  2.2050e-03],\n",
            "         [ 7.9726e-04,  2.6865e-04, -1.0991e-02,  ...,  1.9916e-03,\n",
            "           1.7770e-03,  6.4784e-03],\n",
            "         ...,\n",
            "         [ 5.0354e-03, -1.5631e-02, -2.0003e-02,  ..., -2.2618e-03,\n",
            "           3.1675e-03,  5.5781e-03],\n",
            "         [ 6.0940e-03, -7.5148e-03, -1.6824e-02,  ...,  2.1022e-03,\n",
            "           3.5852e-03,  3.3437e-03],\n",
            "         [ 1.2073e-03, -6.1362e-03, -1.8186e-02,  ..., -4.9664e-03,\n",
            "           4.9890e-03,  1.4233e-03]],\n",
            "\n",
            "        [[-1.3523e-03, -1.5127e-02, -2.2592e-02,  ..., -5.6130e-03,\n",
            "           3.0075e-04,  3.5373e-03],\n",
            "         [ 4.7073e-03, -1.5986e-02, -1.9756e-02,  ..., -4.8596e-03,\n",
            "           1.4182e-02, -3.2796e-04],\n",
            "         [ 1.4543e-03, -1.4349e-02, -2.0663e-02,  ..., -2.6314e-03,\n",
            "           4.7735e-03,  3.8005e-03],\n",
            "         ...,\n",
            "         [ 5.5084e-03, -3.1951e-03, -2.0837e-02,  ..., -5.2220e-03,\n",
            "           1.9029e-03, -2.8714e-03],\n",
            "         [-9.3079e-04, -5.4381e-03, -1.6982e-02,  ..., -1.7034e-03,\n",
            "           8.4127e-03, -3.7145e-03],\n",
            "         [ 3.6659e-03, -6.9316e-03, -1.9074e-02,  ..., -3.4453e-03,\n",
            "           7.3618e-03,  1.5713e-03]],\n",
            "\n",
            "        [[ 7.0343e-03, -3.3286e-03, -1.6511e-02,  ..., -3.5879e-03,\n",
            "           3.0702e-03,  7.3672e-03],\n",
            "         [-3.5019e-03, -9.3458e-03, -1.7247e-02,  ..., -9.8149e-03,\n",
            "           7.4180e-03,  5.5667e-03],\n",
            "         [-8.9646e-04, -9.7735e-03, -2.4316e-02,  ..., -9.1714e-04,\n",
            "           4.4855e-03, -3.6191e-03],\n",
            "         ...,\n",
            "         [-1.0166e-03, -2.6763e-03, -1.1029e-02,  ..., -5.1801e-03,\n",
            "           9.8337e-03,  1.7180e-03],\n",
            "         [ 2.5787e-03, -5.2588e-03, -2.2912e-02,  ..., -2.0396e-03,\n",
            "           6.2809e-03,  5.9863e-03],\n",
            "         [-1.2093e-03, -4.2937e-03, -1.6879e-02,  ..., -6.3016e-03,\n",
            "           1.0322e-02,  8.1798e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 7.8773e-03, -6.2831e-03, -1.5435e-02,  ..., -1.2378e-02,\n",
            "           8.3345e-03,  1.2885e-04],\n",
            "         [-2.5597e-03, -4.3204e-03, -2.0193e-02,  ..., -7.2286e-03,\n",
            "           1.5033e-02, -2.8485e-03],\n",
            "         [ 2.8663e-03, -7.2692e-03, -2.1650e-02,  ...,  2.5752e-03,\n",
            "           1.9525e-03,  5.0193e-03],\n",
            "         ...,\n",
            "         [ 6.3744e-03, -5.5869e-03, -1.8911e-02,  ...,  1.5147e-03,\n",
            "           6.1778e-03,  4.1877e-03],\n",
            "         [ 8.5792e-03, -1.0136e-02, -1.9858e-02,  ..., -1.6343e-03,\n",
            "           5.2963e-04,  6.1351e-03],\n",
            "         [ 8.3656e-03, -7.1452e-03, -2.0655e-02,  ..., -3.2823e-03,\n",
            "           2.0555e-03, -3.2758e-03]],\n",
            "\n",
            "        [[-2.7657e-03, -1.8561e-03, -1.9243e-02,  ..., -3.1335e-03,\n",
            "           6.7562e-03, -2.2286e-03],\n",
            "         [ 4.5147e-03, -1.7149e-02, -1.8531e-02,  ..., -2.5607e-03,\n",
            "           3.1713e-03, -8.0594e-03],\n",
            "         [ 1.0693e-02, -9.7430e-03, -2.0959e-02,  ..., -7.5604e-03,\n",
            "           8.6532e-04,  3.2340e-03],\n",
            "         ...,\n",
            "         [ 2.0814e-03, -8.0746e-03, -1.5357e-02,  ...,  2.1275e-05,\n",
            "           8.1009e-03,  6.4860e-04],\n",
            "         [ 9.9144e-03, -6.3556e-03, -2.0875e-02,  ...,  5.8482e-03,\n",
            "           1.1089e-02,  2.2231e-03],\n",
            "         [ 8.1825e-03, -7.4852e-03, -3.4047e-02,  ..., -6.5774e-04,\n",
            "           7.2036e-04,  3.8672e-03]],\n",
            "\n",
            "        [[ 6.8245e-03, -1.1767e-02, -1.8973e-02,  ..., -1.8107e-03,\n",
            "           9.5724e-03, -2.2744e-03],\n",
            "         [ 5.8460e-03, -5.9150e-03, -1.6915e-02,  ..., -8.5256e-03,\n",
            "           5.7009e-03, -6.1889e-05],\n",
            "         [ 5.6381e-03, -7.6225e-03, -1.5910e-02,  ..., -1.9767e-03,\n",
            "           1.3919e-02,  6.1008e-03],\n",
            "         ...,\n",
            "         [ 1.6021e-04, -9.1894e-03, -1.8812e-02,  ..., -6.8700e-03,\n",
            "           1.3755e-02,  1.7334e-03],\n",
            "         [-3.1090e-03, -6.0218e-03, -1.4627e-02,  ..., -2.9032e-03,\n",
            "           1.0263e-02,  4.4375e-03],\n",
            "         [ 8.0604e-03, -2.4817e-03, -1.5781e-02,  ...,  7.8700e-03,\n",
            "           4.9604e-03,  1.0472e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4549\n",
            "  Max: 0.4739\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/urosat/DAPT/vit_b16_8shots/seed3/tensorboard)\n",
            "epoch [1/200] batch [1/2] time 1.775 (1.775) data 0.528 (0.528) loss 8.5779 (8.5779) acc 21.8750 (21.8750) lr 1.0000e-05 eta 0:11:48\n",
            "epoch [1/200] batch [2/2] time 0.298 (1.036) data 0.001 (0.264) loss 8.5622 (8.5701) acc 28.1250 (25.0000) lr 2.0000e+01 eta 0:06:52\n",
            "epoch [2/200] batch [1/2] time 0.710 (0.710) data 0.417 (0.417) loss 8.2452 (8.2452) acc 37.5000 (37.5000) lr 2.0000e+01 eta 0:04:42\n",
            "epoch [2/200] batch [2/2] time 0.297 (0.504) data 0.001 (0.209) loss 7.1924 (7.7188) acc 28.1250 (32.8125) lr 1.9999e+01 eta 0:03:19\n",
            "epoch [3/200] batch [1/2] time 0.681 (0.681) data 0.385 (0.385) loss 5.6789 (5.6789) acc 25.0000 (25.0000) lr 1.9999e+01 eta 0:04:28\n",
            "epoch [3/200] batch [2/2] time 0.302 (0.491) data 0.000 (0.193) loss 5.2731 (5.4760) acc 3.1250 (14.0625) lr 1.9995e+01 eta 0:03:13\n",
            "epoch [4/200] batch [1/2] time 0.725 (0.725) data 0.426 (0.426) loss 5.5109 (5.5109) acc 21.8750 (21.8750) lr 1.9995e+01 eta 0:04:44\n",
            "epoch [4/200] batch [2/2] time 0.300 (0.512) data 0.000 (0.213) loss 6.6129 (6.0619) acc 12.5000 (17.1875) lr 1.9989e+01 eta 0:03:20\n",
            "epoch [5/200] batch [1/2] time 0.724 (0.724) data 0.428 (0.428) loss 6.6267 (6.6267) acc 9.3750 (9.3750) lr 1.9989e+01 eta 0:04:43\n",
            "epoch [5/200] batch [2/2] time 0.302 (0.513) data 0.000 (0.214) loss 6.0947 (6.3607) acc 3.1250 (6.2500) lr 1.9980e+01 eta 0:03:20\n",
            "epoch [6/200] batch [1/2] time 0.736 (0.736) data 0.442 (0.442) loss 4.5844 (4.5844) acc 18.7500 (18.7500) lr 1.9980e+01 eta 0:04:46\n",
            "epoch [6/200] batch [2/2] time 0.302 (0.519) data 0.000 (0.221) loss 4.4916 (4.5380) acc 15.6250 (17.1875) lr 1.9969e+01 eta 0:03:21\n",
            "epoch [7/200] batch [1/2] time 0.760 (0.760) data 0.459 (0.459) loss 3.4887 (3.4887) acc 28.1250 (28.1250) lr 1.9969e+01 eta 0:04:54\n",
            "epoch [7/200] batch [2/2] time 0.305 (0.533) data 0.001 (0.230) loss 3.3678 (3.4282) acc 28.1250 (28.1250) lr 1.9956e+01 eta 0:03:25\n",
            "epoch [8/200] batch [1/2] time 0.761 (0.761) data 0.461 (0.461) loss 3.1709 (3.1709) acc 31.2500 (31.2500) lr 1.9956e+01 eta 0:04:53\n",
            "epoch [8/200] batch [2/2] time 0.301 (0.531) data 0.000 (0.231) loss 2.8079 (2.9894) acc 34.3750 (32.8125) lr 1.9940e+01 eta 0:03:23\n",
            "epoch [9/200] batch [1/2] time 0.734 (0.734) data 0.435 (0.435) loss 2.6101 (2.6101) acc 46.8750 (46.8750) lr 1.9940e+01 eta 0:04:41\n",
            "epoch [9/200] batch [2/2] time 0.305 (0.519) data 0.000 (0.218) loss 2.8173 (2.7137) acc 37.5000 (42.1875) lr 1.9921e+01 eta 0:03:18\n",
            "epoch [10/200] batch [1/2] time 0.728 (0.728) data 0.425 (0.425) loss 2.5458 (2.5458) acc 37.5000 (37.5000) lr 1.9921e+01 eta 0:04:37\n",
            "epoch [10/200] batch [2/2] time 0.304 (0.516) data 0.000 (0.213) loss 2.7151 (2.6304) acc 28.1250 (32.8125) lr 1.9900e+01 eta 0:03:16\n",
            "epoch [11/200] batch [1/2] time 0.719 (0.719) data 0.420 (0.420) loss 2.0917 (2.0917) acc 59.3750 (59.3750) lr 1.9900e+01 eta 0:04:32\n",
            "epoch [11/200] batch [2/2] time 0.291 (0.505) data 0.000 (0.210) loss 2.3309 (2.2113) acc 50.0000 (54.6875) lr 1.9877e+01 eta 0:03:10\n",
            "epoch [12/200] batch [1/2] time 0.733 (0.733) data 0.431 (0.431) loss 1.8131 (1.8131) acc 68.7500 (68.7500) lr 1.9877e+01 eta 0:04:36\n",
            "epoch [12/200] batch [2/2] time 0.304 (0.518) data 0.000 (0.216) loss 2.0398 (1.9264) acc 50.0000 (59.3750) lr 1.9851e+01 eta 0:03:14\n",
            "epoch [13/200] batch [1/2] time 0.731 (0.731) data 0.432 (0.432) loss 1.8240 (1.8240) acc 56.2500 (56.2500) lr 1.9851e+01 eta 0:04:34\n",
            "epoch [13/200] batch [2/2] time 0.305 (0.518) data 0.000 (0.216) loss 1.9302 (1.8771) acc 43.7500 (50.0000) lr 1.9823e+01 eta 0:03:13\n",
            "epoch [14/200] batch [1/2] time 0.716 (0.716) data 0.414 (0.414) loss 1.3865 (1.3865) acc 78.1250 (78.1250) lr 1.9823e+01 eta 0:04:27\n",
            "epoch [14/200] batch [2/2] time 0.303 (0.510) data 0.001 (0.207) loss 1.7601 (1.5733) acc 62.5000 (70.3125) lr 1.9792e+01 eta 0:03:09\n",
            "epoch [15/200] batch [1/2] time 0.748 (0.748) data 0.447 (0.447) loss 1.1186 (1.1186) acc 87.5000 (87.5000) lr 1.9792e+01 eta 0:04:37\n",
            "epoch [15/200] batch [2/2] time 0.306 (0.527) data 0.001 (0.224) loss 1.9658 (1.5422) acc 50.0000 (68.7500) lr 1.9759e+01 eta 0:03:14\n",
            "epoch [16/200] batch [1/2] time 0.747 (0.747) data 0.447 (0.447) loss 1.1228 (1.1228) acc 81.2500 (81.2500) lr 1.9759e+01 eta 0:04:35\n",
            "epoch [16/200] batch [2/2] time 0.306 (0.526) data 0.001 (0.224) loss 1.5439 (1.3333) acc 68.7500 (75.0000) lr 1.9724e+01 eta 0:03:13\n",
            "epoch [17/200] batch [1/2] time 0.737 (0.737) data 0.436 (0.436) loss 0.7516 (0.7516) acc 93.7500 (93.7500) lr 1.9724e+01 eta 0:04:30\n",
            "epoch [17/200] batch [2/2] time 0.306 (0.521) data 0.001 (0.218) loss 2.3439 (1.5477) acc 43.7500 (68.7500) lr 1.9686e+01 eta 0:03:10\n",
            "epoch [18/200] batch [1/2] time 0.766 (0.766) data 0.462 (0.462) loss 1.7892 (1.7892) acc 59.3750 (59.3750) lr 1.9686e+01 eta 0:04:39\n",
            "epoch [18/200] batch [2/2] time 0.307 (0.537) data 0.001 (0.231) loss 3.4370 (2.6131) acc 40.6250 (50.0000) lr 1.9646e+01 eta 0:03:15\n",
            "epoch [19/200] batch [1/2] time 0.768 (0.768) data 0.464 (0.464) loss 6.9874 (6.9874) acc 31.2500 (31.2500) lr 1.9646e+01 eta 0:04:38\n",
            "epoch [19/200] batch [2/2] time 0.307 (0.537) data 0.000 (0.232) loss 9.2406 (8.1140) acc 21.8750 (26.5625) lr 1.9603e+01 eta 0:03:14\n",
            "epoch [20/200] batch [1/2] time 0.748 (0.748) data 0.446 (0.446) loss 5.7628 (5.7628) acc 25.0000 (25.0000) lr 1.9603e+01 eta 0:04:30\n",
            "epoch [20/200] batch [2/2] time 0.308 (0.528) data 0.000 (0.223) loss 5.1547 (5.4588) acc 25.0000 (25.0000) lr 1.9558e+01 eta 0:03:10\n",
            "epoch [21/200] batch [1/2] time 0.738 (0.738) data 0.433 (0.433) loss 13.5380 (13.5380) acc 18.7500 (18.7500) lr 1.9558e+01 eta 0:04:25\n",
            "epoch [21/200] batch [2/2] time 0.308 (0.523) data 0.001 (0.217) loss 7.1393 (10.3386) acc 43.7500 (31.2500) lr 1.9511e+01 eta 0:03:07\n",
            "epoch [22/200] batch [1/2] time 0.720 (0.720) data 0.417 (0.417) loss 4.2719 (4.2719) acc 25.0000 (25.0000) lr 1.9511e+01 eta 0:04:17\n",
            "epoch [22/200] batch [2/2] time 0.308 (0.514) data 0.000 (0.209) loss 5.8452 (5.0586) acc 18.7500 (21.8750) lr 1.9461e+01 eta 0:03:03\n",
            "epoch [23/200] batch [1/2] time 0.736 (0.736) data 0.435 (0.435) loss 8.6503 (8.6503) acc 9.3750 (9.3750) lr 1.9461e+01 eta 0:04:21\n",
            "epoch [23/200] batch [2/2] time 0.308 (0.522) data 0.000 (0.218) loss 7.4899 (8.0701) acc 25.0000 (17.1875) lr 1.9409e+01 eta 0:03:04\n",
            "epoch [24/200] batch [1/2] time 0.742 (0.742) data 0.440 (0.440) loss 5.3328 (5.3328) acc 40.6250 (40.6250) lr 1.9409e+01 eta 0:04:22\n",
            "epoch [24/200] batch [2/2] time 0.308 (0.525) data 0.000 (0.220) loss 3.4793 (4.4060) acc 34.3750 (37.5000) lr 1.9354e+01 eta 0:03:04\n",
            "epoch [25/200] batch [1/2] time 0.723 (0.723) data 0.418 (0.418) loss 2.7721 (2.7721) acc 56.2500 (56.2500) lr 1.9354e+01 eta 0:04:13\n",
            "epoch [25/200] batch [2/2] time 0.311 (0.517) data 0.000 (0.209) loss 2.0952 (2.4336) acc 68.7500 (62.5000) lr 1.9298e+01 eta 0:03:00\n",
            "epoch [26/200] batch [1/2] time 0.744 (0.744) data 0.438 (0.438) loss 1.9926 (1.9926) acc 56.2500 (56.2500) lr 1.9298e+01 eta 0:04:19\n",
            "epoch [26/200] batch [2/2] time 0.309 (0.526) data 0.000 (0.219) loss 1.5555 (1.7741) acc 56.2500 (56.2500) lr 1.9239e+01 eta 0:03:03\n",
            "epoch [27/200] batch [1/2] time 0.746 (0.746) data 0.443 (0.443) loss 1.4780 (1.4780) acc 65.6250 (65.6250) lr 1.9239e+01 eta 0:04:18\n",
            "epoch [27/200] batch [2/2] time 0.309 (0.527) data 0.000 (0.222) loss 1.1891 (1.3336) acc 75.0000 (70.3125) lr 1.9178e+01 eta 0:03:02\n",
            "epoch [28/200] batch [1/2] time 0.732 (0.732) data 0.425 (0.425) loss 1.1711 (1.1711) acc 81.2500 (81.2500) lr 1.9178e+01 eta 0:04:12\n",
            "epoch [28/200] batch [2/2] time 0.310 (0.521) data 0.000 (0.213) loss 0.8468 (1.0089) acc 84.3750 (82.8125) lr 1.9114e+01 eta 0:02:59\n",
            "epoch [29/200] batch [1/2] time 0.747 (0.747) data 0.441 (0.441) loss 0.7258 (0.7258) acc 90.6250 (90.6250) lr 1.9114e+01 eta 0:04:16\n",
            "epoch [29/200] batch [2/2] time 0.309 (0.528) data 0.001 (0.221) loss 1.2973 (1.0116) acc 71.8750 (81.2500) lr 1.9048e+01 eta 0:03:00\n",
            "epoch [30/200] batch [1/2] time 0.754 (0.754) data 0.448 (0.448) loss 1.0882 (1.0882) acc 65.6250 (65.6250) lr 1.9048e+01 eta 0:04:17\n",
            "epoch [30/200] batch [2/2] time 0.311 (0.532) data 0.001 (0.225) loss 1.2054 (1.1468) acc 78.1250 (71.8750) lr 1.8980e+01 eta 0:03:01\n",
            "epoch [31/200] batch [1/2] time 0.717 (0.717) data 0.443 (0.443) loss 1.1767 (1.1767) acc 71.8750 (71.8750) lr 1.8980e+01 eta 0:04:02\n",
            "epoch [31/200] batch [2/2] time 0.309 (0.513) data 0.000 (0.222) loss 1.0035 (1.0901) acc 84.3750 (78.1250) lr 1.8910e+01 eta 0:02:53\n",
            "epoch [32/200] batch [1/2] time 0.744 (0.744) data 0.437 (0.437) loss 0.6316 (0.6316) acc 90.6250 (90.6250) lr 1.8910e+01 eta 0:04:10\n",
            "epoch [32/200] batch [2/2] time 0.306 (0.525) data 0.000 (0.219) loss 1.1917 (0.9117) acc 71.8750 (81.2500) lr 1.8838e+01 eta 0:02:56\n",
            "epoch [33/200] batch [1/2] time 0.740 (0.740) data 0.435 (0.435) loss 0.8617 (0.8617) acc 84.3750 (84.3750) lr 1.8838e+01 eta 0:04:07\n",
            "epoch [33/200] batch [2/2] time 0.311 (0.526) data 0.000 (0.218) loss 0.8639 (0.8628) acc 81.2500 (82.8125) lr 1.8763e+01 eta 0:02:55\n",
            "epoch [34/200] batch [1/2] time 0.720 (0.720) data 0.414 (0.414) loss 0.9284 (0.9284) acc 78.1250 (78.1250) lr 1.8763e+01 eta 0:03:59\n",
            "epoch [34/200] batch [2/2] time 0.313 (0.517) data 0.000 (0.207) loss 0.7229 (0.8257) acc 87.5000 (82.8125) lr 1.8686e+01 eta 0:02:51\n",
            "epoch [35/200] batch [1/2] time 0.711 (0.711) data 0.404 (0.404) loss 0.8822 (0.8822) acc 78.1250 (78.1250) lr 1.8686e+01 eta 0:03:55\n",
            "epoch [35/200] batch [2/2] time 0.312 (0.511) data 0.000 (0.202) loss 1.4368 (1.1595) acc 78.1250 (78.1250) lr 1.8607e+01 eta 0:02:48\n",
            "epoch [36/200] batch [1/2] time 0.762 (0.762) data 0.453 (0.453) loss 2.9515 (2.9515) acc 50.0000 (50.0000) lr 1.8607e+01 eta 0:04:10\n",
            "epoch [36/200] batch [2/2] time 0.314 (0.538) data 0.001 (0.227) loss 3.4363 (3.1939) acc 46.8750 (48.4375) lr 1.8526e+01 eta 0:02:56\n",
            "epoch [37/200] batch [1/2] time 0.748 (0.748) data 0.440 (0.440) loss 5.3841 (5.3841) acc 40.6250 (40.6250) lr 1.8526e+01 eta 0:04:04\n",
            "epoch [37/200] batch [2/2] time 0.317 (0.532) data 0.000 (0.220) loss 4.7614 (5.0727) acc 28.1250 (34.3750) lr 1.8443e+01 eta 0:02:53\n",
            "epoch [38/200] batch [1/2] time 0.754 (0.754) data 0.442 (0.442) loss 8.2783 (8.2783) acc 3.1250 (3.1250) lr 1.8443e+01 eta 0:04:04\n",
            "epoch [38/200] batch [2/2] time 0.314 (0.534) data 0.000 (0.221) loss 8.0734 (8.1759) acc 37.5000 (20.3125) lr 1.8358e+01 eta 0:02:52\n",
            "epoch [39/200] batch [1/2] time 0.728 (0.728) data 0.414 (0.414) loss 4.0799 (4.0799) acc 37.5000 (37.5000) lr 1.8358e+01 eta 0:03:55\n",
            "epoch [39/200] batch [2/2] time 0.314 (0.521) data 0.000 (0.207) loss 5.0368 (4.5584) acc 18.7500 (28.1250) lr 1.8271e+01 eta 0:02:47\n",
            "epoch [40/200] batch [1/2] time 0.730 (0.730) data 0.418 (0.418) loss 5.3235 (5.3235) acc 31.2500 (31.2500) lr 1.8271e+01 eta 0:03:54\n",
            "epoch [40/200] batch [2/2] time 0.318 (0.524) data 0.000 (0.209) loss 5.5598 (5.4417) acc 18.7500 (25.0000) lr 1.8181e+01 eta 0:02:47\n",
            "epoch [41/200] batch [1/2] time 0.774 (0.774) data 0.466 (0.466) loss 4.0072 (4.0072) acc 28.1250 (28.1250) lr 1.8181e+01 eta 0:04:06\n",
            "epoch [41/200] batch [2/2] time 0.315 (0.544) data 0.000 (0.233) loss 3.7053 (3.8563) acc 28.1250 (28.1250) lr 1.8090e+01 eta 0:02:53\n",
            "epoch [42/200] batch [1/2] time 0.785 (0.785) data 0.475 (0.475) loss 4.2705 (4.2705) acc 6.2500 (6.2500) lr 1.8090e+01 eta 0:04:08\n",
            "epoch [42/200] batch [2/2] time 0.315 (0.550) data 0.000 (0.238) loss 3.7720 (4.0213) acc 34.3750 (20.3125) lr 1.7997e+01 eta 0:02:53\n",
            "epoch [43/200] batch [1/2] time 0.730 (0.730) data 0.421 (0.421) loss 2.6450 (2.6450) acc 43.7500 (43.7500) lr 1.7997e+01 eta 0:03:50\n",
            "epoch [43/200] batch [2/2] time 0.313 (0.522) data 0.000 (0.211) loss 3.2780 (2.9615) acc 25.0000 (34.3750) lr 1.7902e+01 eta 0:02:43\n",
            "epoch [44/200] batch [1/2] time 0.757 (0.757) data 0.445 (0.445) loss 2.4544 (2.4544) acc 37.5000 (37.5000) lr 1.7902e+01 eta 0:03:56\n",
            "epoch [44/200] batch [2/2] time 0.314 (0.535) data 0.000 (0.223) loss 2.4601 (2.4573) acc 46.8750 (42.1875) lr 1.7804e+01 eta 0:02:46\n",
            "epoch [45/200] batch [1/2] time 0.732 (0.732) data 0.423 (0.423) loss 1.9520 (1.9520) acc 62.5000 (62.5000) lr 1.7804e+01 eta 0:03:47\n",
            "epoch [45/200] batch [2/2] time 0.316 (0.524) data 0.000 (0.212) loss 1.7890 (1.8705) acc 71.8750 (67.1875) lr 1.7705e+01 eta 0:02:42\n",
            "epoch [46/200] batch [1/2] time 0.719 (0.719) data 0.408 (0.408) loss 1.6205 (1.6205) acc 65.6250 (65.6250) lr 1.7705e+01 eta 0:03:42\n",
            "epoch [46/200] batch [2/2] time 0.316 (0.517) data 0.000 (0.204) loss 1.4595 (1.5400) acc 62.5000 (64.0625) lr 1.7604e+01 eta 0:02:39\n",
            "epoch [47/200] batch [1/2] time 0.753 (0.753) data 0.444 (0.444) loss 1.1160 (1.1160) acc 81.2500 (81.2500) lr 1.7604e+01 eta 0:03:51\n",
            "epoch [47/200] batch [2/2] time 0.315 (0.534) data 0.000 (0.222) loss 1.2302 (1.1731) acc 71.8750 (76.5625) lr 1.7501e+01 eta 0:02:43\n",
            "epoch [48/200] batch [1/2] time 0.739 (0.739) data 0.431 (0.431) loss 1.3564 (1.3564) acc 68.7500 (68.7500) lr 1.7501e+01 eta 0:03:45\n",
            "epoch [48/200] batch [2/2] time 0.314 (0.527) data 0.000 (0.216) loss 0.9854 (1.1709) acc 81.2500 (75.0000) lr 1.7396e+01 eta 0:02:40\n",
            "epoch [49/200] batch [1/2] time 0.726 (0.726) data 0.419 (0.419) loss 0.8701 (0.8701) acc 87.5000 (87.5000) lr 1.7396e+01 eta 0:03:39\n",
            "epoch [49/200] batch [2/2] time 0.312 (0.519) data 0.000 (0.210) loss 1.3497 (1.1099) acc 65.6250 (76.5625) lr 1.7290e+01 eta 0:02:36\n",
            "epoch [50/200] batch [1/2] time 0.739 (0.739) data 0.431 (0.431) loss 1.0533 (1.0533) acc 71.8750 (71.8750) lr 1.7290e+01 eta 0:03:42\n",
            "epoch [50/200] batch [2/2] time 0.310 (0.524) data 0.000 (0.216) loss 1.3207 (1.1870) acc 68.7500 (70.3125) lr 1.7181e+01 eta 0:02:37\n",
            "epoch [51/200] batch [1/2] time 0.752 (0.752) data 0.443 (0.443) loss 0.9876 (0.9876) acc 78.1250 (78.1250) lr 1.7181e+01 eta 0:03:44\n",
            "epoch [51/200] batch [2/2] time 0.311 (0.532) data 0.000 (0.221) loss 0.9042 (0.9459) acc 78.1250 (78.1250) lr 1.7071e+01 eta 0:02:38\n",
            "epoch [52/200] batch [1/2] time 0.757 (0.757) data 0.449 (0.449) loss 0.7879 (0.7879) acc 90.6250 (90.6250) lr 1.7071e+01 eta 0:03:44\n",
            "epoch [52/200] batch [2/2] time 0.312 (0.535) data 0.000 (0.225) loss 0.6597 (0.7238) acc 93.7500 (92.1875) lr 1.6959e+01 eta 0:02:38\n",
            "epoch [53/200] batch [1/2] time 0.768 (0.768) data 0.462 (0.462) loss 1.0295 (1.0295) acc 71.8750 (71.8750) lr 1.6959e+01 eta 0:03:46\n",
            "epoch [53/200] batch [2/2] time 0.311 (0.540) data 0.000 (0.231) loss 1.4106 (1.2201) acc 65.6250 (68.7500) lr 1.6845e+01 eta 0:02:38\n",
            "epoch [54/200] batch [1/2] time 0.741 (0.741) data 0.435 (0.435) loss 1.1823 (1.1823) acc 78.1250 (78.1250) lr 1.6845e+01 eta 0:03:37\n",
            "epoch [54/200] batch [2/2] time 0.309 (0.525) data 0.000 (0.218) loss 2.1744 (1.6784) acc 59.3750 (68.7500) lr 1.6730e+01 eta 0:02:33\n",
            "epoch [55/200] batch [1/2] time 0.720 (0.720) data 0.411 (0.411) loss 2.4511 (2.4511) acc 31.2500 (31.2500) lr 1.6730e+01 eta 0:03:29\n",
            "epoch [55/200] batch [2/2] time 0.313 (0.516) data 0.000 (0.206) loss 4.2262 (3.3387) acc 40.6250 (35.9375) lr 1.6613e+01 eta 0:02:29\n",
            "epoch [56/200] batch [1/2] time 0.729 (0.729) data 0.424 (0.424) loss 3.0512 (3.0512) acc 53.1250 (53.1250) lr 1.6613e+01 eta 0:03:30\n",
            "epoch [56/200] batch [2/2] time 0.311 (0.520) data 0.000 (0.212) loss 3.0683 (3.0597) acc 40.6250 (46.8750) lr 1.6494e+01 eta 0:02:29\n",
            "epoch [57/200] batch [1/2] time 0.713 (0.713) data 0.411 (0.411) loss 2.7917 (2.7917) acc 43.7500 (43.7500) lr 1.6494e+01 eta 0:03:24\n",
            "epoch [57/200] batch [2/2] time 0.309 (0.511) data 0.000 (0.206) loss 1.7689 (2.2803) acc 68.7500 (56.2500) lr 1.6374e+01 eta 0:02:26\n",
            "epoch [58/200] batch [1/2] time 0.728 (0.728) data 0.421 (0.421) loss 1.9869 (1.9869) acc 50.0000 (50.0000) lr 1.6374e+01 eta 0:03:27\n",
            "epoch [58/200] batch [2/2] time 0.307 (0.518) data 0.000 (0.211) loss 1.5843 (1.7856) acc 75.0000 (62.5000) lr 1.6252e+01 eta 0:02:26\n",
            "epoch [59/200] batch [1/2] time 0.736 (0.736) data 0.432 (0.432) loss 1.6125 (1.6125) acc 71.8750 (71.8750) lr 1.6252e+01 eta 0:03:28\n",
            "epoch [59/200] batch [2/2] time 0.309 (0.523) data 0.001 (0.216) loss 1.3353 (1.4739) acc 75.0000 (73.4375) lr 1.6129e+01 eta 0:02:27\n",
            "epoch [60/200] batch [1/2] time 0.729 (0.729) data 0.427 (0.427) loss 2.0702 (2.0702) acc 65.6250 (65.6250) lr 1.6129e+01 eta 0:03:24\n",
            "epoch [60/200] batch [2/2] time 0.308 (0.518) data 0.000 (0.214) loss 1.5889 (1.8295) acc 71.8750 (68.7500) lr 1.6004e+01 eta 0:02:25\n",
            "epoch [61/200] batch [1/2] time 0.722 (0.722) data 0.418 (0.418) loss 1.8104 (1.8104) acc 65.6250 (65.6250) lr 1.6004e+01 eta 0:03:21\n",
            "epoch [61/200] batch [2/2] time 0.304 (0.513) data 0.000 (0.209) loss 0.9185 (1.3645) acc 87.5000 (76.5625) lr 1.5878e+01 eta 0:02:22\n",
            "epoch [62/200] batch [1/2] time 0.707 (0.707) data 0.402 (0.402) loss 1.2140 (1.2140) acc 75.0000 (75.0000) lr 1.5878e+01 eta 0:03:15\n",
            "epoch [62/200] batch [2/2] time 0.306 (0.507) data 0.001 (0.201) loss 1.2481 (1.2311) acc 75.0000 (75.0000) lr 1.5750e+01 eta 0:02:19\n",
            "epoch [63/200] batch [1/2] time 0.768 (0.768) data 0.464 (0.464) loss 1.2178 (1.2178) acc 81.2500 (81.2500) lr 1.5750e+01 eta 0:03:31\n",
            "epoch [63/200] batch [2/2] time 0.307 (0.537) data 0.000 (0.232) loss 0.8209 (1.0193) acc 78.1250 (79.6875) lr 1.5621e+01 eta 0:02:27\n",
            "epoch [64/200] batch [1/2] time 0.754 (0.754) data 0.449 (0.449) loss 1.3438 (1.3438) acc 71.8750 (71.8750) lr 1.5621e+01 eta 0:03:25\n",
            "epoch [64/200] batch [2/2] time 0.307 (0.530) data 0.000 (0.225) loss 0.8157 (1.0798) acc 75.0000 (73.4375) lr 1.5490e+01 eta 0:02:24\n",
            "epoch [65/200] batch [1/2] time 0.710 (0.710) data 0.410 (0.410) loss 0.4924 (0.4924) acc 96.8750 (96.8750) lr 1.5490e+01 eta 0:03:12\n",
            "epoch [65/200] batch [2/2] time 0.305 (0.508) data 0.000 (0.205) loss 0.7705 (0.6315) acc 84.3750 (90.6250) lr 1.5358e+01 eta 0:02:17\n",
            "epoch [66/200] batch [1/2] time 0.731 (0.731) data 0.429 (0.429) loss 0.5788 (0.5788) acc 84.3750 (84.3750) lr 1.5358e+01 eta 0:03:16\n",
            "epoch [66/200] batch [2/2] time 0.306 (0.518) data 0.000 (0.215) loss 0.6326 (0.6057) acc 96.8750 (90.6250) lr 1.5225e+01 eta 0:02:18\n",
            "epoch [67/200] batch [1/2] time 0.704 (0.704) data 0.405 (0.405) loss 0.6704 (0.6704) acc 87.5000 (87.5000) lr 1.5225e+01 eta 0:03:08\n",
            "epoch [67/200] batch [2/2] time 0.305 (0.505) data 0.000 (0.203) loss 0.9495 (0.8100) acc 81.2500 (84.3750) lr 1.5090e+01 eta 0:02:14\n",
            "epoch [68/200] batch [1/2] time 0.753 (0.753) data 0.455 (0.455) loss 0.7918 (0.7918) acc 84.3750 (84.3750) lr 1.5090e+01 eta 0:03:19\n",
            "epoch [68/200] batch [2/2] time 0.302 (0.528) data 0.000 (0.228) loss 1.0328 (0.9123) acc 75.0000 (79.6875) lr 1.4955e+01 eta 0:02:19\n",
            "epoch [69/200] batch [1/2] time 0.725 (0.725) data 0.426 (0.426) loss 0.6152 (0.6152) acc 90.6250 (90.6250) lr 1.4955e+01 eta 0:03:10\n",
            "epoch [69/200] batch [2/2] time 0.305 (0.515) data 0.001 (0.213) loss 0.8190 (0.7171) acc 75.0000 (82.8125) lr 1.4818e+01 eta 0:02:14\n",
            "epoch [70/200] batch [1/2] time 0.730 (0.730) data 0.431 (0.431) loss 0.4844 (0.4844) acc 93.7500 (93.7500) lr 1.4818e+01 eta 0:03:10\n",
            "epoch [70/200] batch [2/2] time 0.307 (0.518) data 0.000 (0.216) loss 0.8720 (0.6782) acc 81.2500 (87.5000) lr 1.4679e+01 eta 0:02:14\n",
            "epoch [71/200] batch [1/2] time 0.731 (0.731) data 0.431 (0.431) loss 1.0917 (1.0917) acc 81.2500 (81.2500) lr 1.4679e+01 eta 0:03:09\n",
            "epoch [71/200] batch [2/2] time 0.303 (0.517) data 0.000 (0.216) loss 0.6513 (0.8715) acc 84.3750 (82.8125) lr 1.4540e+01 eta 0:02:13\n",
            "epoch [72/200] batch [1/2] time 0.738 (0.738) data 0.437 (0.437) loss 0.6498 (0.6498) acc 81.2500 (81.2500) lr 1.4540e+01 eta 0:03:09\n",
            "epoch [72/200] batch [2/2] time 0.304 (0.521) data 0.000 (0.219) loss 0.5053 (0.5776) acc 93.7500 (87.5000) lr 1.4399e+01 eta 0:02:13\n",
            "epoch [73/200] batch [1/2] time 0.742 (0.742) data 0.442 (0.442) loss 1.5841 (1.5841) acc 65.6250 (65.6250) lr 1.4399e+01 eta 0:03:09\n",
            "epoch [73/200] batch [2/2] time 0.303 (0.523) data 0.001 (0.221) loss 1.3520 (1.4681) acc 59.3750 (62.5000) lr 1.4258e+01 eta 0:02:12\n",
            "epoch [74/200] batch [1/2] time 0.766 (0.766) data 0.463 (0.463) loss 1.5156 (1.5156) acc 65.6250 (65.6250) lr 1.4258e+01 eta 0:03:13\n",
            "epoch [74/200] batch [2/2] time 0.307 (0.537) data 0.001 (0.232) loss 1.6115 (1.5636) acc 65.6250 (65.6250) lr 1.4115e+01 eta 0:02:15\n",
            "epoch [75/200] batch [1/2] time 0.779 (0.779) data 0.479 (0.479) loss 1.2215 (1.2215) acc 71.8750 (71.8750) lr 1.4115e+01 eta 0:03:15\n",
            "epoch [75/200] batch [2/2] time 0.303 (0.541) data 0.001 (0.240) loss 0.9620 (1.0917) acc 78.1250 (75.0000) lr 1.3971e+01 eta 0:02:15\n",
            "epoch [76/200] batch [1/2] time 0.848 (0.848) data 0.543 (0.543) loss 2.2249 (2.2249) acc 46.8750 (46.8750) lr 1.3971e+01 eta 0:03:31\n",
            "epoch [76/200] batch [2/2] time 0.303 (0.575) data 0.000 (0.272) loss 1.1794 (1.7021) acc 90.6250 (68.7500) lr 1.3827e+01 eta 0:02:22\n",
            "epoch [77/200] batch [1/2] time 0.731 (0.731) data 0.434 (0.434) loss 2.8117 (2.8117) acc 53.1250 (53.1250) lr 1.3827e+01 eta 0:03:00\n",
            "epoch [77/200] batch [2/2] time 0.301 (0.516) data 0.000 (0.217) loss 2.5088 (2.6603) acc 56.2500 (54.6875) lr 1.3681e+01 eta 0:02:06\n",
            "epoch [78/200] batch [1/2] time 0.705 (0.705) data 0.406 (0.406) loss 2.7230 (2.7230) acc 56.2500 (56.2500) lr 1.3681e+01 eta 0:02:52\n",
            "epoch [78/200] batch [2/2] time 0.303 (0.504) data 0.000 (0.203) loss 1.4926 (2.1078) acc 68.7500 (62.5000) lr 1.3535e+01 eta 0:02:02\n",
            "epoch [79/200] batch [1/2] time 0.738 (0.738) data 0.438 (0.438) loss 1.7613 (1.7613) acc 65.6250 (65.6250) lr 1.3535e+01 eta 0:02:59\n",
            "epoch [79/200] batch [2/2] time 0.302 (0.520) data 0.000 (0.219) loss 4.9386 (3.3500) acc 21.8750 (43.7500) lr 1.3387e+01 eta 0:02:05\n",
            "epoch [80/200] batch [1/2] time 0.714 (0.714) data 0.416 (0.416) loss 2.2272 (2.2272) acc 62.5000 (62.5000) lr 1.3387e+01 eta 0:02:52\n",
            "epoch [80/200] batch [2/2] time 0.306 (0.510) data 0.000 (0.208) loss 2.5240 (2.3756) acc 56.2500 (59.3750) lr 1.3239e+01 eta 0:02:02\n",
            "epoch [81/200] batch [1/2] time 0.740 (0.740) data 0.441 (0.441) loss 0.9808 (0.9808) acc 90.6250 (90.6250) lr 1.3239e+01 eta 0:02:56\n",
            "epoch [81/200] batch [2/2] time 0.303 (0.521) data 0.000 (0.221) loss 0.7734 (0.8771) acc 87.5000 (89.0625) lr 1.3090e+01 eta 0:02:04\n",
            "epoch [82/200] batch [1/2] time 0.720 (0.720) data 0.420 (0.420) loss 1.7001 (1.7001) acc 78.1250 (78.1250) lr 1.3090e+01 eta 0:02:50\n",
            "epoch [82/200] batch [2/2] time 0.304 (0.512) data 0.000 (0.210) loss 1.1855 (1.4428) acc 81.2500 (79.6875) lr 1.2940e+01 eta 0:02:00\n",
            "epoch [83/200] batch [1/2] time 0.750 (0.750) data 0.450 (0.450) loss 0.6812 (0.6812) acc 93.7500 (93.7500) lr 1.2940e+01 eta 0:02:56\n",
            "epoch [83/200] batch [2/2] time 0.304 (0.527) data 0.001 (0.225) loss 0.7034 (0.6923) acc 90.6250 (92.1875) lr 1.2790e+01 eta 0:02:03\n",
            "epoch [84/200] batch [1/2] time 0.750 (0.750) data 0.451 (0.451) loss 0.7967 (0.7967) acc 90.6250 (90.6250) lr 1.2790e+01 eta 0:02:54\n",
            "epoch [84/200] batch [2/2] time 0.302 (0.526) data 0.001 (0.226) loss 0.8396 (0.8182) acc 90.6250 (90.6250) lr 1.2639e+01 eta 0:02:02\n",
            "epoch [85/200] batch [1/2] time 0.771 (0.771) data 0.469 (0.469) loss 0.7145 (0.7145) acc 84.3750 (84.3750) lr 1.2639e+01 eta 0:02:58\n",
            "epoch [85/200] batch [2/2] time 0.305 (0.538) data 0.000 (0.235) loss 0.4775 (0.5960) acc 93.7500 (89.0625) lr 1.2487e+01 eta 0:02:03\n",
            "epoch [86/200] batch [1/2] time 0.774 (0.774) data 0.473 (0.473) loss 0.8382 (0.8382) acc 81.2500 (81.2500) lr 1.2487e+01 eta 0:02:57\n",
            "epoch [86/200] batch [2/2] time 0.304 (0.539) data 0.000 (0.237) loss 1.2678 (1.0530) acc 81.2500 (81.2500) lr 1.2334e+01 eta 0:02:02\n",
            "epoch [87/200] batch [1/2] time 0.768 (0.768) data 0.468 (0.468) loss 0.8434 (0.8434) acc 84.3750 (84.3750) lr 1.2334e+01 eta 0:02:54\n",
            "epoch [87/200] batch [2/2] time 0.304 (0.536) data 0.000 (0.234) loss 0.6067 (0.7250) acc 87.5000 (85.9375) lr 1.2181e+01 eta 0:02:01\n",
            "epoch [88/200] batch [1/2] time 0.729 (0.729) data 0.430 (0.430) loss 0.8528 (0.8528) acc 87.5000 (87.5000) lr 1.2181e+01 eta 0:02:44\n",
            "epoch [88/200] batch [2/2] time 0.304 (0.516) data 0.000 (0.215) loss 0.5787 (0.7157) acc 90.6250 (89.0625) lr 1.2028e+01 eta 0:01:55\n",
            "epoch [89/200] batch [1/2] time 0.739 (0.739) data 0.437 (0.437) loss 0.7039 (0.7039) acc 96.8750 (96.8750) lr 1.2028e+01 eta 0:02:44\n",
            "epoch [89/200] batch [2/2] time 0.304 (0.522) data 0.000 (0.219) loss 0.5563 (0.6301) acc 93.7500 (95.3125) lr 1.1874e+01 eta 0:01:55\n",
            "epoch [90/200] batch [1/2] time 0.717 (0.717) data 0.417 (0.417) loss 0.8722 (0.8722) acc 78.1250 (78.1250) lr 1.1874e+01 eta 0:02:38\n",
            "epoch [90/200] batch [2/2] time 0.306 (0.512) data 0.001 (0.209) loss 0.6487 (0.7605) acc 84.3750 (81.2500) lr 1.1719e+01 eta 0:01:52\n",
            "epoch [91/200] batch [1/2] time 0.724 (0.724) data 0.424 (0.424) loss 0.7607 (0.7607) acc 90.6250 (90.6250) lr 1.1719e+01 eta 0:02:38\n",
            "epoch [91/200] batch [2/2] time 0.306 (0.515) data 0.000 (0.212) loss 0.7347 (0.7477) acc 87.5000 (89.0625) lr 1.1564e+01 eta 0:01:52\n",
            "epoch [92/200] batch [1/2] time 0.719 (0.719) data 0.417 (0.417) loss 0.9324 (0.9324) acc 81.2500 (81.2500) lr 1.1564e+01 eta 0:02:36\n",
            "epoch [92/200] batch [2/2] time 0.306 (0.513) data 0.001 (0.209) loss 0.8267 (0.8795) acc 87.5000 (84.3750) lr 1.1409e+01 eta 0:01:50\n",
            "epoch [93/200] batch [1/2] time 0.743 (0.743) data 0.441 (0.441) loss 0.4710 (0.4710) acc 93.7500 (93.7500) lr 1.1409e+01 eta 0:02:39\n",
            "epoch [93/200] batch [2/2] time 0.306 (0.524) data 0.000 (0.221) loss 1.0882 (0.7796) acc 78.1250 (85.9375) lr 1.1253e+01 eta 0:01:52\n",
            "epoch [94/200] batch [1/2] time 0.731 (0.731) data 0.431 (0.431) loss 0.7219 (0.7219) acc 84.3750 (84.3750) lr 1.1253e+01 eta 0:02:35\n",
            "epoch [94/200] batch [2/2] time 0.307 (0.519) data 0.001 (0.216) loss 1.5371 (1.1295) acc 71.8750 (78.1250) lr 1.1097e+01 eta 0:01:50\n",
            "epoch [95/200] batch [1/2] time 0.753 (0.753) data 0.451 (0.451) loss 0.8997 (0.8997) acc 84.3750 (84.3750) lr 1.1097e+01 eta 0:02:38\n",
            "epoch [95/200] batch [2/2] time 0.306 (0.530) data 0.000 (0.226) loss 0.9093 (0.9045) acc 78.1250 (81.2500) lr 1.0941e+01 eta 0:01:51\n",
            "epoch [96/200] batch [1/2] time 0.769 (0.769) data 0.466 (0.466) loss 1.6941 (1.6941) acc 75.0000 (75.0000) lr 1.0941e+01 eta 0:02:40\n",
            "epoch [96/200] batch [2/2] time 0.305 (0.537) data 0.000 (0.233) loss 1.8741 (1.7841) acc 75.0000 (75.0000) lr 1.0785e+01 eta 0:01:51\n",
            "epoch [97/200] batch [1/2] time 0.780 (0.780) data 0.475 (0.475) loss 1.6505 (1.6505) acc 68.7500 (68.7500) lr 1.0785e+01 eta 0:02:41\n",
            "epoch [97/200] batch [2/2] time 0.307 (0.543) data 0.000 (0.238) loss 4.0332 (2.8419) acc 53.1250 (60.9375) lr 1.0628e+01 eta 0:01:51\n",
            "epoch [98/200] batch [1/2] time 0.778 (0.778) data 0.476 (0.476) loss 2.4132 (2.4132) acc 50.0000 (50.0000) lr 1.0628e+01 eta 0:02:39\n",
            "epoch [98/200] batch [2/2] time 0.309 (0.544) data 0.000 (0.238) loss 3.3723 (2.8927) acc 34.3750 (42.1875) lr 1.0471e+01 eta 0:01:50\n",
            "epoch [99/200] batch [1/2] time 0.724 (0.724) data 0.423 (0.423) loss 1.9128 (1.9128) acc 56.2500 (56.2500) lr 1.0471e+01 eta 0:02:26\n",
            "epoch [99/200] batch [2/2] time 0.306 (0.515) data 0.000 (0.212) loss 1.0488 (1.4808) acc 90.6250 (73.4375) lr 1.0314e+01 eta 0:01:44\n",
            "epoch [100/200] batch [1/2] time 0.730 (0.730) data 0.424 (0.424) loss 0.6300 (0.6300) acc 96.8750 (96.8750) lr 1.0314e+01 eta 0:02:26\n",
            "epoch [100/200] batch [2/2] time 0.309 (0.520) data 0.000 (0.212) loss 1.1821 (0.9061) acc 81.2500 (89.0625) lr 1.0157e+01 eta 0:01:43\n",
            "epoch [101/200] batch [1/2] time 0.758 (0.758) data 0.454 (0.454) loss 1.7996 (1.7996) acc 75.0000 (75.0000) lr 1.0157e+01 eta 0:02:30\n",
            "epoch [101/200] batch [2/2] time 0.309 (0.533) data 0.000 (0.227) loss 0.9133 (1.3565) acc 87.5000 (81.2500) lr 1.0000e+01 eta 0:01:45\n",
            "epoch [102/200] batch [1/2] time 0.726 (0.726) data 0.422 (0.422) loss 1.1734 (1.1734) acc 71.8750 (71.8750) lr 1.0000e+01 eta 0:02:23\n",
            "epoch [102/200] batch [2/2] time 0.312 (0.519) data 0.000 (0.211) loss 1.8434 (1.5084) acc 62.5000 (67.1875) lr 9.8429e+00 eta 0:01:41\n",
            "epoch [103/200] batch [1/2] time 0.744 (0.744) data 0.436 (0.436) loss 1.0373 (1.0373) acc 75.0000 (75.0000) lr 9.8429e+00 eta 0:02:25\n",
            "epoch [103/200] batch [2/2] time 0.307 (0.526) data 0.001 (0.218) loss 1.8969 (1.4671) acc 68.7500 (71.8750) lr 9.6859e+00 eta 0:01:41\n",
            "epoch [104/200] batch [1/2] time 0.742 (0.742) data 0.439 (0.439) loss 1.0040 (1.0040) acc 84.3750 (84.3750) lr 9.6859e+00 eta 0:02:23\n",
            "epoch [104/200] batch [2/2] time 0.310 (0.526) data 0.000 (0.220) loss 0.6338 (0.8189) acc 87.5000 (85.9375) lr 9.5289e+00 eta 0:01:41\n",
            "epoch [105/200] batch [1/2] time 0.725 (0.725) data 0.419 (0.419) loss 0.9648 (0.9648) acc 75.0000 (75.0000) lr 9.5289e+00 eta 0:02:18\n",
            "epoch [105/200] batch [2/2] time 0.309 (0.517) data 0.001 (0.210) loss 0.9960 (0.9804) acc 87.5000 (81.2500) lr 9.3721e+00 eta 0:01:38\n",
            "epoch [106/200] batch [1/2] time 0.732 (0.732) data 0.427 (0.427) loss 0.7028 (0.7028) acc 93.7500 (93.7500) lr 9.3721e+00 eta 0:02:18\n",
            "epoch [106/200] batch [2/2] time 0.312 (0.522) data 0.000 (0.214) loss 0.6241 (0.6634) acc 90.6250 (92.1875) lr 9.2154e+00 eta 0:01:38\n",
            "epoch [107/200] batch [1/2] time 0.769 (0.769) data 0.463 (0.463) loss 0.4235 (0.4235) acc 96.8750 (96.8750) lr 9.2154e+00 eta 0:02:23\n",
            "epoch [107/200] batch [2/2] time 0.310 (0.540) data 0.000 (0.232) loss 0.9377 (0.6806) acc 81.2500 (89.0625) lr 9.0589e+00 eta 0:01:40\n",
            "epoch [108/200] batch [1/2] time 0.766 (0.766) data 0.459 (0.459) loss 0.6872 (0.6872) acc 87.5000 (87.5000) lr 9.0589e+00 eta 0:02:21\n",
            "epoch [108/200] batch [2/2] time 0.312 (0.539) data 0.001 (0.230) loss 0.9362 (0.8117) acc 78.1250 (82.8125) lr 8.9027e+00 eta 0:01:39\n",
            "epoch [109/200] batch [1/2] time 0.765 (0.765) data 0.459 (0.459) loss 0.4783 (0.4783) acc 96.8750 (96.8750) lr 8.9027e+00 eta 0:02:19\n",
            "epoch [109/200] batch [2/2] time 0.309 (0.537) data 0.000 (0.230) loss 1.0974 (0.7878) acc 84.3750 (90.6250) lr 8.7467e+00 eta 0:01:37\n",
            "epoch [110/200] batch [1/2] time 0.762 (0.762) data 0.453 (0.453) loss 0.7692 (0.7692) acc 90.6250 (90.6250) lr 8.7467e+00 eta 0:02:17\n",
            "epoch [110/200] batch [2/2] time 0.313 (0.537) data 0.001 (0.227) loss 0.6199 (0.6945) acc 93.7500 (92.1875) lr 8.5910e+00 eta 0:01:36\n",
            "epoch [111/200] batch [1/2] time 0.763 (0.763) data 0.457 (0.457) loss 0.9314 (0.9314) acc 81.2500 (81.2500) lr 8.5910e+00 eta 0:02:16\n",
            "epoch [111/200] batch [2/2] time 0.310 (0.537) data 0.001 (0.229) loss 0.5085 (0.7199) acc 93.7500 (87.5000) lr 8.4357e+00 eta 0:01:35\n",
            "epoch [112/200] batch [1/2] time 0.772 (0.772) data 0.467 (0.467) loss 0.6308 (0.6308) acc 90.6250 (90.6250) lr 8.4357e+00 eta 0:02:16\n",
            "epoch [112/200] batch [2/2] time 0.311 (0.541) data 0.000 (0.234) loss 0.6677 (0.6492) acc 93.7500 (92.1875) lr 8.2807e+00 eta 0:01:35\n",
            "epoch [113/200] batch [1/2] time 0.789 (0.789) data 0.482 (0.482) loss 0.4691 (0.4691) acc 96.8750 (96.8750) lr 8.2807e+00 eta 0:02:18\n",
            "epoch [113/200] batch [2/2] time 0.309 (0.549) data 0.001 (0.241) loss 0.8528 (0.6609) acc 84.3750 (90.6250) lr 8.1262e+00 eta 0:01:35\n",
            "epoch [114/200] batch [1/2] time 0.748 (0.748) data 0.443 (0.443) loss 0.5608 (0.5608) acc 87.5000 (87.5000) lr 8.1262e+00 eta 0:02:09\n",
            "epoch [114/200] batch [2/2] time 0.311 (0.529) data 0.001 (0.222) loss 0.4291 (0.4949) acc 96.8750 (92.1875) lr 7.9721e+00 eta 0:01:31\n",
            "epoch [115/200] batch [1/2] time 0.760 (0.760) data 0.455 (0.455) loss 0.5117 (0.5117) acc 90.6250 (90.6250) lr 7.9721e+00 eta 0:02:10\n",
            "epoch [115/200] batch [2/2] time 0.308 (0.534) data 0.001 (0.228) loss 0.3353 (0.4235) acc 100.0000 (95.3125) lr 7.8186e+00 eta 0:01:30\n",
            "epoch [116/200] batch [1/2] time 0.774 (0.774) data 0.469 (0.469) loss 0.5627 (0.5627) acc 93.7500 (93.7500) lr 7.8186e+00 eta 0:02:10\n",
            "epoch [116/200] batch [2/2] time 0.311 (0.543) data 0.001 (0.235) loss 0.4733 (0.5180) acc 93.7500 (93.7500) lr 7.6655e+00 eta 0:01:31\n",
            "epoch [117/200] batch [1/2] time 0.773 (0.773) data 0.469 (0.469) loss 0.6012 (0.6012) acc 90.6250 (90.6250) lr 7.6655e+00 eta 0:02:09\n",
            "epoch [117/200] batch [2/2] time 0.312 (0.542) data 0.001 (0.235) loss 0.3744 (0.4878) acc 96.8750 (93.7500) lr 7.5131e+00 eta 0:01:29\n",
            "epoch [118/200] batch [1/2] time 0.786 (0.786) data 0.481 (0.481) loss 0.8128 (0.8128) acc 87.5000 (87.5000) lr 7.5131e+00 eta 0:02:09\n",
            "epoch [118/200] batch [2/2] time 0.307 (0.546) data 0.000 (0.241) loss 0.3762 (0.5945) acc 93.7500 (90.6250) lr 7.3613e+00 eta 0:01:29\n",
            "epoch [119/200] batch [1/2] time 0.778 (0.778) data 0.474 (0.474) loss 0.5374 (0.5374) acc 90.6250 (90.6250) lr 7.3613e+00 eta 0:02:06\n",
            "epoch [119/200] batch [2/2] time 0.304 (0.541) data 0.001 (0.237) loss 0.4556 (0.4965) acc 90.6250 (90.6250) lr 7.2101e+00 eta 0:01:27\n",
            "epoch [120/200] batch [1/2] time 0.773 (0.773) data 0.469 (0.469) loss 0.5929 (0.5929) acc 90.6250 (90.6250) lr 7.2101e+00 eta 0:02:04\n",
            "epoch [120/200] batch [2/2] time 0.306 (0.540) data 0.001 (0.235) loss 0.6379 (0.6154) acc 87.5000 (89.0625) lr 7.0596e+00 eta 0:01:26\n",
            "epoch [121/200] batch [1/2] time 0.732 (0.732) data 0.431 (0.431) loss 0.7341 (0.7341) acc 87.5000 (87.5000) lr 7.0596e+00 eta 0:01:56\n",
            "epoch [121/200] batch [2/2] time 0.308 (0.520) data 0.001 (0.216) loss 0.6662 (0.7002) acc 90.6250 (89.0625) lr 6.9098e+00 eta 0:01:22\n",
            "epoch [122/200] batch [1/2] time 0.740 (0.740) data 0.439 (0.439) loss 0.4864 (0.4864) acc 93.7500 (93.7500) lr 6.9098e+00 eta 0:01:56\n",
            "epoch [122/200] batch [2/2] time 0.307 (0.523) data 0.001 (0.220) loss 0.7209 (0.6036) acc 81.2500 (87.5000) lr 6.7608e+00 eta 0:01:21\n",
            "epoch [123/200] batch [1/2] time 0.731 (0.731) data 0.427 (0.427) loss 0.5363 (0.5363) acc 90.6250 (90.6250) lr 6.7608e+00 eta 0:01:53\n",
            "epoch [123/200] batch [2/2] time 0.310 (0.520) data 0.001 (0.214) loss 0.9091 (0.7227) acc 75.0000 (82.8125) lr 6.6126e+00 eta 0:01:20\n",
            "epoch [124/200] batch [1/2] time 0.790 (0.790) data 0.488 (0.488) loss 1.4150 (1.4150) acc 68.7500 (68.7500) lr 6.6126e+00 eta 0:02:00\n",
            "epoch [124/200] batch [2/2] time 0.311 (0.550) data 0.001 (0.244) loss 0.8796 (1.1473) acc 84.3750 (76.5625) lr 6.4653e+00 eta 0:01:23\n",
            "epoch [125/200] batch [1/2] time 0.765 (0.765) data 0.463 (0.463) loss 0.4643 (0.4643) acc 96.8750 (96.8750) lr 6.4653e+00 eta 0:01:55\n",
            "epoch [125/200] batch [2/2] time 0.306 (0.536) data 0.000 (0.232) loss 0.8812 (0.6727) acc 84.3750 (90.6250) lr 6.3188e+00 eta 0:01:20\n",
            "epoch [126/200] batch [1/2] time 0.765 (0.765) data 0.459 (0.459) loss 0.7942 (0.7942) acc 75.0000 (75.0000) lr 6.3188e+00 eta 0:01:53\n",
            "epoch [126/200] batch [2/2] time 0.311 (0.538) data 0.001 (0.230) loss 1.4453 (1.1197) acc 68.7500 (71.8750) lr 6.1732e+00 eta 0:01:19\n",
            "epoch [127/200] batch [1/2] time 0.771 (0.771) data 0.466 (0.466) loss 0.8202 (0.8202) acc 87.5000 (87.5000) lr 6.1732e+00 eta 0:01:53\n",
            "epoch [127/200] batch [2/2] time 0.311 (0.541) data 0.001 (0.233) loss 0.7639 (0.7920) acc 87.5000 (87.5000) lr 6.0285e+00 eta 0:01:18\n",
            "epoch [128/200] batch [1/2] time 0.758 (0.758) data 0.458 (0.458) loss 0.5492 (0.5492) acc 87.5000 (87.5000) lr 6.0285e+00 eta 0:01:49\n",
            "epoch [128/200] batch [2/2] time 0.309 (0.533) data 0.000 (0.229) loss 1.3854 (0.9673) acc 71.8750 (79.6875) lr 5.8849e+00 eta 0:01:16\n",
            "epoch [129/200] batch [1/2] time 0.776 (0.776) data 0.470 (0.470) loss 1.1046 (1.1046) acc 75.0000 (75.0000) lr 5.8849e+00 eta 0:01:50\n",
            "epoch [129/200] batch [2/2] time 0.310 (0.543) data 0.001 (0.235) loss 2.0177 (1.5612) acc 68.7500 (71.8750) lr 5.7422e+00 eta 0:01:17\n",
            "epoch [130/200] batch [1/2] time 0.781 (0.781) data 0.477 (0.477) loss 1.1770 (1.1770) acc 78.1250 (78.1250) lr 5.7422e+00 eta 0:01:50\n",
            "epoch [130/200] batch [2/2] time 0.305 (0.543) data 0.001 (0.239) loss 0.5382 (0.8576) acc 87.5000 (82.8125) lr 5.6006e+00 eta 0:01:16\n",
            "epoch [131/200] batch [1/2] time 0.754 (0.754) data 0.452 (0.452) loss 0.6214 (0.6214) acc 90.6250 (90.6250) lr 5.6006e+00 eta 0:01:44\n",
            "epoch [131/200] batch [2/2] time 0.309 (0.531) data 0.000 (0.226) loss 0.9307 (0.7760) acc 71.8750 (81.2500) lr 5.4601e+00 eta 0:01:13\n",
            "epoch [132/200] batch [1/2] time 0.741 (0.741) data 0.436 (0.436) loss 0.9250 (0.9250) acc 78.1250 (78.1250) lr 5.4601e+00 eta 0:01:41\n",
            "epoch [132/200] batch [2/2] time 0.303 (0.522) data 0.000 (0.218) loss 0.9415 (0.9333) acc 84.3750 (81.2500) lr 5.3207e+00 eta 0:01:11\n",
            "epoch [133/200] batch [1/2] time 0.753 (0.753) data 0.452 (0.452) loss 0.9790 (0.9790) acc 81.2500 (81.2500) lr 5.3207e+00 eta 0:01:41\n",
            "epoch [133/200] batch [2/2] time 0.306 (0.529) data 0.000 (0.226) loss 0.9755 (0.9773) acc 90.6250 (85.9375) lr 5.1825e+00 eta 0:01:10\n",
            "epoch [134/200] batch [1/2] time 0.730 (0.730) data 0.427 (0.427) loss 0.3581 (0.3581) acc 100.0000 (100.0000) lr 5.1825e+00 eta 0:01:37\n",
            "epoch [134/200] batch [2/2] time 0.303 (0.516) data 0.000 (0.214) loss 0.4060 (0.3820) acc 96.8750 (98.4375) lr 5.0454e+00 eta 0:01:08\n",
            "epoch [135/200] batch [1/2] time 0.735 (0.735) data 0.432 (0.432) loss 0.5946 (0.5946) acc 87.5000 (87.5000) lr 5.0454e+00 eta 0:01:36\n",
            "epoch [135/200] batch [2/2] time 0.307 (0.521) data 0.000 (0.216) loss 0.6174 (0.6060) acc 93.7500 (90.6250) lr 4.9096e+00 eta 0:01:07\n",
            "epoch [136/200] batch [1/2] time 0.720 (0.720) data 0.418 (0.418) loss 0.8125 (0.8125) acc 84.3750 (84.3750) lr 4.9096e+00 eta 0:01:32\n",
            "epoch [136/200] batch [2/2] time 0.306 (0.513) data 0.000 (0.209) loss 0.4680 (0.6403) acc 93.7500 (89.0625) lr 4.7750e+00 eta 0:01:05\n",
            "epoch [137/200] batch [1/2] time 0.764 (0.764) data 0.463 (0.463) loss 0.5636 (0.5636) acc 93.7500 (93.7500) lr 4.7750e+00 eta 0:01:37\n",
            "epoch [137/200] batch [2/2] time 0.310 (0.537) data 0.000 (0.232) loss 0.5051 (0.5344) acc 93.7500 (93.7500) lr 4.6417e+00 eta 0:01:07\n",
            "epoch [138/200] batch [1/2] time 0.735 (0.735) data 0.432 (0.432) loss 0.6009 (0.6009) acc 90.6250 (90.6250) lr 4.6417e+00 eta 0:01:31\n",
            "epoch [138/200] batch [2/2] time 0.305 (0.520) data 0.000 (0.216) loss 0.4779 (0.5394) acc 96.8750 (93.7500) lr 4.5098e+00 eta 0:01:04\n",
            "epoch [139/200] batch [1/2] time 0.738 (0.738) data 0.437 (0.437) loss 0.4801 (0.4801) acc 90.6250 (90.6250) lr 4.5098e+00 eta 0:01:30\n",
            "epoch [139/200] batch [2/2] time 0.305 (0.522) data 0.000 (0.219) loss 0.5057 (0.4929) acc 96.8750 (93.7500) lr 4.3792e+00 eta 0:01:03\n",
            "epoch [140/200] batch [1/2] time 0.749 (0.749) data 0.446 (0.446) loss 0.3840 (0.3840) acc 96.8750 (96.8750) lr 4.3792e+00 eta 0:01:30\n",
            "epoch [140/200] batch [2/2] time 0.308 (0.528) data 0.000 (0.223) loss 0.4555 (0.4198) acc 93.7500 (95.3125) lr 4.2499e+00 eta 0:01:03\n",
            "epoch [141/200] batch [1/2] time 0.764 (0.764) data 0.464 (0.464) loss 0.7095 (0.7095) acc 93.7500 (93.7500) lr 4.2499e+00 eta 0:01:30\n",
            "epoch [141/200] batch [2/2] time 0.308 (0.536) data 0.001 (0.232) loss 0.3595 (0.5345) acc 96.8750 (95.3125) lr 4.1221e+00 eta 0:01:03\n",
            "epoch [142/200] batch [1/2] time 0.748 (0.748) data 0.448 (0.448) loss 0.5692 (0.5692) acc 87.5000 (87.5000) lr 4.1221e+00 eta 0:01:27\n",
            "epoch [142/200] batch [2/2] time 0.303 (0.526) data 0.000 (0.224) loss 0.4372 (0.5032) acc 96.8750 (92.1875) lr 3.9958e+00 eta 0:01:00\n",
            "epoch [143/200] batch [1/2] time 0.749 (0.749) data 0.448 (0.448) loss 0.4006 (0.4006) acc 93.7500 (93.7500) lr 3.9958e+00 eta 0:01:26\n",
            "epoch [143/200] batch [2/2] time 0.306 (0.527) data 0.000 (0.224) loss 0.3699 (0.3853) acc 96.8750 (95.3125) lr 3.8709e+00 eta 0:01:00\n",
            "epoch [144/200] batch [1/2] time 0.736 (0.736) data 0.433 (0.433) loss 0.4748 (0.4748) acc 96.8750 (96.8750) lr 3.8709e+00 eta 0:01:23\n",
            "epoch [144/200] batch [2/2] time 0.302 (0.519) data 0.000 (0.217) loss 0.4199 (0.4473) acc 93.7500 (95.3125) lr 3.7476e+00 eta 0:00:58\n",
            "epoch [145/200] batch [1/2] time 0.712 (0.712) data 0.409 (0.409) loss 0.3776 (0.3776) acc 96.8750 (96.8750) lr 3.7476e+00 eta 0:01:19\n",
            "epoch [145/200] batch [2/2] time 0.307 (0.509) data 0.000 (0.205) loss 0.5015 (0.4396) acc 93.7500 (95.3125) lr 3.6258e+00 eta 0:00:56\n",
            "epoch [146/200] batch [1/2] time 0.715 (0.715) data 0.415 (0.415) loss 0.5392 (0.5392) acc 90.6250 (90.6250) lr 3.6258e+00 eta 0:01:17\n",
            "epoch [146/200] batch [2/2] time 0.307 (0.511) data 0.000 (0.208) loss 0.2981 (0.4187) acc 100.0000 (95.3125) lr 3.5055e+00 eta 0:00:55\n",
            "epoch [147/200] batch [1/2] time 0.734 (0.734) data 0.431 (0.431) loss 0.3367 (0.3367) acc 96.8750 (96.8750) lr 3.5055e+00 eta 0:01:18\n",
            "epoch [147/200] batch [2/2] time 0.304 (0.519) data 0.000 (0.216) loss 0.3219 (0.3293) acc 96.8750 (96.8750) lr 3.3869e+00 eta 0:00:54\n",
            "epoch [148/200] batch [1/2] time 0.725 (0.725) data 0.422 (0.422) loss 0.4378 (0.4378) acc 90.6250 (90.6250) lr 3.3869e+00 eta 0:01:16\n",
            "epoch [148/200] batch [2/2] time 0.305 (0.515) data 0.001 (0.211) loss 0.3979 (0.4178) acc 96.8750 (93.7500) lr 3.2699e+00 eta 0:00:53\n",
            "epoch [149/200] batch [1/2] time 0.712 (0.712) data 0.409 (0.409) loss 0.2866 (0.2866) acc 100.0000 (100.0000) lr 3.2699e+00 eta 0:01:13\n",
            "epoch [149/200] batch [2/2] time 0.307 (0.509) data 0.000 (0.205) loss 0.3468 (0.3167) acc 96.8750 (98.4375) lr 3.1545e+00 eta 0:00:51\n",
            "epoch [150/200] batch [1/2] time 0.723 (0.723) data 0.421 (0.421) loss 0.7090 (0.7090) acc 93.7500 (93.7500) lr 3.1545e+00 eta 0:01:12\n",
            "epoch [150/200] batch [2/2] time 0.304 (0.513) data 0.001 (0.211) loss 0.2802 (0.4946) acc 100.0000 (96.8750) lr 3.0409e+00 eta 0:00:51\n",
            "epoch [151/200] batch [1/2] time 0.750 (0.750) data 0.448 (0.448) loss 0.2848 (0.2848) acc 100.0000 (100.0000) lr 3.0409e+00 eta 0:01:14\n",
            "epoch [151/200] batch [2/2] time 0.305 (0.528) data 0.001 (0.224) loss 0.3239 (0.3044) acc 96.8750 (98.4375) lr 2.9289e+00 eta 0:00:51\n",
            "epoch [152/200] batch [1/2] time 0.759 (0.759) data 0.459 (0.459) loss 0.3572 (0.3572) acc 93.7500 (93.7500) lr 2.9289e+00 eta 0:01:13\n",
            "epoch [152/200] batch [2/2] time 0.306 (0.533) data 0.001 (0.230) loss 0.5729 (0.4650) acc 90.6250 (92.1875) lr 2.8187e+00 eta 0:00:51\n",
            "epoch [153/200] batch [1/2] time 0.784 (0.784) data 0.482 (0.482) loss 0.2831 (0.2831) acc 100.0000 (100.0000) lr 2.8187e+00 eta 0:01:14\n",
            "epoch [153/200] batch [2/2] time 0.307 (0.545) data 0.001 (0.241) loss 0.4035 (0.3433) acc 96.8750 (98.4375) lr 2.7103e+00 eta 0:00:51\n",
            "epoch [154/200] batch [1/2] time 0.758 (0.758) data 0.458 (0.458) loss 0.3595 (0.3595) acc 96.8750 (96.8750) lr 2.7103e+00 eta 0:01:10\n",
            "epoch [154/200] batch [2/2] time 0.305 (0.532) data 0.001 (0.229) loss 0.4300 (0.3948) acc 93.7500 (95.3125) lr 2.6037e+00 eta 0:00:48\n",
            "epoch [155/200] batch [1/2] time 0.737 (0.737) data 0.434 (0.434) loss 0.4193 (0.4193) acc 90.6250 (90.6250) lr 2.6037e+00 eta 0:01:07\n",
            "epoch [155/200] batch [2/2] time 0.306 (0.522) data 0.001 (0.217) loss 0.3968 (0.4080) acc 93.7500 (92.1875) lr 2.4989e+00 eta 0:00:46\n",
            "epoch [156/200] batch [1/2] time 0.756 (0.756) data 0.452 (0.452) loss 0.2949 (0.2949) acc 100.0000 (100.0000) lr 2.4989e+00 eta 0:01:07\n",
            "epoch [156/200] batch [2/2] time 0.307 (0.531) data 0.000 (0.226) loss 0.2930 (0.2939) acc 96.8750 (98.4375) lr 2.3959e+00 eta 0:00:46\n",
            "epoch [157/200] batch [1/2] time 0.761 (0.761) data 0.459 (0.459) loss 0.4525 (0.4525) acc 96.8750 (96.8750) lr 2.3959e+00 eta 0:01:06\n",
            "epoch [157/200] batch [2/2] time 0.306 (0.534) data 0.001 (0.230) loss 0.3056 (0.3791) acc 96.8750 (96.8750) lr 2.2949e+00 eta 0:00:45\n",
            "epoch [158/200] batch [1/2] time 0.745 (0.745) data 0.445 (0.445) loss 0.2601 (0.2601) acc 100.0000 (100.0000) lr 2.2949e+00 eta 0:01:03\n",
            "epoch [158/200] batch [2/2] time 0.303 (0.524) data 0.001 (0.223) loss 0.2680 (0.2641) acc 100.0000 (100.0000) lr 2.1957e+00 eta 0:00:44\n",
            "epoch [159/200] batch [1/2] time 0.744 (0.744) data 0.442 (0.442) loss 0.2686 (0.2686) acc 100.0000 (100.0000) lr 2.1957e+00 eta 0:01:01\n",
            "epoch [159/200] batch [2/2] time 0.308 (0.526) data 0.001 (0.221) loss 0.7420 (0.5053) acc 87.5000 (93.7500) lr 2.0984e+00 eta 0:00:43\n",
            "epoch [160/200] batch [1/2] time 0.760 (0.760) data 0.456 (0.456) loss 0.4183 (0.4183) acc 93.7500 (93.7500) lr 2.0984e+00 eta 0:01:01\n",
            "epoch [160/200] batch [2/2] time 0.308 (0.534) data 0.001 (0.228) loss 0.2780 (0.3482) acc 100.0000 (96.8750) lr 2.0032e+00 eta 0:00:42\n",
            "epoch [161/200] batch [1/2] time 0.770 (0.770) data 0.468 (0.468) loss 0.3310 (0.3310) acc 96.8750 (96.8750) lr 2.0032e+00 eta 0:01:00\n",
            "epoch [161/200] batch [2/2] time 0.307 (0.538) data 0.001 (0.234) loss 0.5659 (0.4484) acc 90.6250 (93.7500) lr 1.9098e+00 eta 0:00:41\n",
            "epoch [162/200] batch [1/2] time 0.721 (0.721) data 0.418 (0.418) loss 0.4837 (0.4837) acc 93.7500 (93.7500) lr 1.9098e+00 eta 0:00:55\n",
            "epoch [162/200] batch [2/2] time 0.306 (0.513) data 0.000 (0.209) loss 0.5525 (0.5181) acc 87.5000 (90.6250) lr 1.8185e+00 eta 0:00:39\n",
            "epoch [163/200] batch [1/2] time 0.763 (0.763) data 0.457 (0.457) loss 0.4005 (0.4005) acc 96.8750 (96.8750) lr 1.8185e+00 eta 0:00:57\n",
            "epoch [163/200] batch [2/2] time 0.305 (0.534) data 0.000 (0.229) loss 0.3618 (0.3811) acc 93.7500 (95.3125) lr 1.7292e+00 eta 0:00:39\n",
            "epoch [164/200] batch [1/2] time 0.749 (0.749) data 0.445 (0.445) loss 0.8072 (0.8072) acc 84.3750 (84.3750) lr 1.7292e+00 eta 0:00:54\n",
            "epoch [164/200] batch [2/2] time 0.307 (0.528) data 0.001 (0.223) loss 0.4895 (0.6484) acc 93.7500 (89.0625) lr 1.6419e+00 eta 0:00:38\n",
            "epoch [165/200] batch [1/2] time 0.762 (0.762) data 0.461 (0.461) loss 0.9075 (0.9075) acc 81.2500 (81.2500) lr 1.6419e+00 eta 0:00:54\n",
            "epoch [165/200] batch [2/2] time 0.308 (0.535) data 0.000 (0.231) loss 0.2905 (0.5990) acc 100.0000 (90.6250) lr 1.5567e+00 eta 0:00:37\n",
            "epoch [166/200] batch [1/2] time 0.733 (0.733) data 0.432 (0.432) loss 0.3828 (0.3828) acc 96.8750 (96.8750) lr 1.5567e+00 eta 0:00:50\n",
            "epoch [166/200] batch [2/2] time 0.303 (0.518) data 0.000 (0.216) loss 0.5294 (0.4561) acc 93.7500 (95.3125) lr 1.4736e+00 eta 0:00:35\n",
            "epoch [167/200] batch [1/2] time 0.744 (0.744) data 0.443 (0.443) loss 0.4315 (0.4315) acc 96.8750 (96.8750) lr 1.4736e+00 eta 0:00:49\n",
            "epoch [167/200] batch [2/2] time 0.306 (0.525) data 0.000 (0.222) loss 0.2597 (0.3456) acc 100.0000 (98.4375) lr 1.3926e+00 eta 0:00:34\n",
            "epoch [168/200] batch [1/2] time 0.737 (0.737) data 0.433 (0.433) loss 0.4057 (0.4057) acc 93.7500 (93.7500) lr 1.3926e+00 eta 0:00:47\n",
            "epoch [168/200] batch [2/2] time 0.304 (0.521) data 0.000 (0.217) loss 0.2654 (0.3356) acc 100.0000 (96.8750) lr 1.3137e+00 eta 0:00:33\n",
            "epoch [169/200] batch [1/2] time 0.771 (0.771) data 0.470 (0.470) loss 0.4775 (0.4775) acc 96.8750 (96.8750) lr 1.3137e+00 eta 0:00:48\n",
            "epoch [169/200] batch [2/2] time 0.306 (0.538) data 0.000 (0.235) loss 0.3676 (0.4225) acc 96.8750 (96.8750) lr 1.2369e+00 eta 0:00:33\n",
            "epoch [170/200] batch [1/2] time 0.743 (0.743) data 0.439 (0.439) loss 0.4587 (0.4587) acc 90.6250 (90.6250) lr 1.2369e+00 eta 0:00:45\n",
            "epoch [170/200] batch [2/2] time 0.305 (0.524) data 0.000 (0.220) loss 0.3014 (0.3800) acc 96.8750 (93.7500) lr 1.1623e+00 eta 0:00:31\n",
            "epoch [171/200] batch [1/2] time 0.741 (0.741) data 0.441 (0.441) loss 0.3470 (0.3470) acc 96.8750 (96.8750) lr 1.1623e+00 eta 0:00:43\n",
            "epoch [171/200] batch [2/2] time 0.307 (0.524) data 0.001 (0.221) loss 0.3167 (0.3318) acc 96.8750 (96.8750) lr 1.0899e+00 eta 0:00:30\n",
            "epoch [172/200] batch [1/2] time 0.715 (0.715) data 0.416 (0.416) loss 0.2534 (0.2534) acc 100.0000 (100.0000) lr 1.0899e+00 eta 0:00:40\n",
            "epoch [172/200] batch [2/2] time 0.309 (0.512) data 0.000 (0.208) loss 0.3256 (0.2895) acc 100.0000 (100.0000) lr 1.0197e+00 eta 0:00:28\n",
            "epoch [173/200] batch [1/2] time 0.719 (0.719) data 0.416 (0.416) loss 0.4581 (0.4581) acc 93.7500 (93.7500) lr 1.0197e+00 eta 0:00:39\n",
            "epoch [173/200] batch [2/2] time 0.306 (0.513) data 0.001 (0.208) loss 0.2875 (0.3728) acc 96.8750 (95.3125) lr 9.5173e-01 eta 0:00:27\n",
            "epoch [174/200] batch [1/2] time 0.751 (0.751) data 0.449 (0.449) loss 0.2499 (0.2499) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:39\n",
            "epoch [174/200] batch [2/2] time 0.304 (0.527) data 0.000 (0.225) loss 0.2735 (0.2617) acc 100.0000 (100.0000) lr 8.8597e-01 eta 0:00:27\n",
            "epoch [175/200] batch [1/2] time 0.744 (0.744) data 0.440 (0.440) loss 0.3190 (0.3190) acc 96.8750 (96.8750) lr 8.8597e-01 eta 0:00:37\n",
            "epoch [175/200] batch [2/2] time 0.304 (0.524) data 0.001 (0.220) loss 0.2571 (0.2880) acc 100.0000 (98.4375) lr 8.2245e-01 eta 0:00:26\n",
            "epoch [176/200] batch [1/2] time 0.766 (0.766) data 0.465 (0.465) loss 0.4174 (0.4174) acc 90.6250 (90.6250) lr 8.2245e-01 eta 0:00:37\n",
            "epoch [176/200] batch [2/2] time 0.308 (0.537) data 0.000 (0.233) loss 0.5035 (0.4605) acc 93.7500 (92.1875) lr 7.6120e-01 eta 0:00:25\n",
            "epoch [177/200] batch [1/2] time 0.716 (0.716) data 0.414 (0.414) loss 0.5042 (0.5042) acc 93.7500 (93.7500) lr 7.6120e-01 eta 0:00:33\n",
            "epoch [177/200] batch [2/2] time 0.309 (0.513) data 0.000 (0.207) loss 0.4410 (0.4726) acc 90.6250 (92.1875) lr 7.0224e-01 eta 0:00:23\n",
            "epoch [178/200] batch [1/2] time 0.743 (0.743) data 0.439 (0.439) loss 0.3747 (0.3747) acc 96.8750 (96.8750) lr 7.0224e-01 eta 0:00:33\n",
            "epoch [178/200] batch [2/2] time 0.308 (0.526) data 0.000 (0.220) loss 0.3294 (0.3521) acc 93.7500 (95.3125) lr 6.4556e-01 eta 0:00:23\n",
            "epoch [179/200] batch [1/2] time 0.749 (0.749) data 0.448 (0.448) loss 0.2355 (0.2355) acc 100.0000 (100.0000) lr 6.4556e-01 eta 0:00:32\n",
            "epoch [179/200] batch [2/2] time 0.309 (0.529) data 0.000 (0.224) loss 0.2881 (0.2618) acc 100.0000 (100.0000) lr 5.9119e-01 eta 0:00:22\n",
            "epoch [180/200] batch [1/2] time 0.744 (0.744) data 0.441 (0.441) loss 0.3328 (0.3328) acc 93.7500 (93.7500) lr 5.9119e-01 eta 0:00:30\n",
            "epoch [180/200] batch [2/2] time 0.307 (0.526) data 0.000 (0.221) loss 0.2374 (0.2851) acc 100.0000 (96.8750) lr 5.3915e-01 eta 0:00:21\n",
            "epoch [181/200] batch [1/2] time 0.749 (0.749) data 0.445 (0.445) loss 0.3586 (0.3586) acc 96.8750 (96.8750) lr 5.3915e-01 eta 0:00:29\n",
            "epoch [181/200] batch [2/2] time 0.307 (0.528) data 0.000 (0.223) loss 0.2840 (0.3213) acc 100.0000 (98.4375) lr 4.8943e-01 eta 0:00:20\n",
            "epoch [182/200] batch [1/2] time 0.749 (0.749) data 0.450 (0.450) loss 0.3023 (0.3023) acc 96.8750 (96.8750) lr 4.8943e-01 eta 0:00:27\n",
            "epoch [182/200] batch [2/2] time 0.306 (0.528) data 0.000 (0.225) loss 0.2440 (0.2732) acc 100.0000 (98.4375) lr 4.4207e-01 eta 0:00:18\n",
            "epoch [183/200] batch [1/2] time 0.725 (0.725) data 0.425 (0.425) loss 0.2467 (0.2467) acc 100.0000 (100.0000) lr 4.4207e-01 eta 0:00:25\n",
            "epoch [183/200] batch [2/2] time 0.307 (0.516) data 0.000 (0.213) loss 0.8353 (0.5410) acc 84.3750 (92.1875) lr 3.9706e-01 eta 0:00:17\n",
            "epoch [184/200] batch [1/2] time 0.766 (0.766) data 0.464 (0.464) loss 0.2614 (0.2614) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:25\n",
            "epoch [184/200] batch [2/2] time 0.304 (0.535) data 0.000 (0.232) loss 0.4675 (0.3645) acc 96.8750 (98.4375) lr 3.5443e-01 eta 0:00:17\n",
            "epoch [185/200] batch [1/2] time 0.731 (0.731) data 0.427 (0.427) loss 0.2914 (0.2914) acc 96.8750 (96.8750) lr 3.5443e-01 eta 0:00:22\n",
            "epoch [185/200] batch [2/2] time 0.307 (0.519) data 0.001 (0.214) loss 0.2378 (0.2646) acc 100.0000 (98.4375) lr 3.1417e-01 eta 0:00:15\n",
            "epoch [186/200] batch [1/2] time 0.752 (0.752) data 0.449 (0.449) loss 0.3670 (0.3670) acc 90.6250 (90.6250) lr 3.1417e-01 eta 0:00:21\n",
            "epoch [186/200] batch [2/2] time 0.308 (0.530) data 0.001 (0.225) loss 0.3831 (0.3751) acc 96.8750 (93.7500) lr 2.7630e-01 eta 0:00:14\n",
            "epoch [187/200] batch [1/2] time 0.745 (0.745) data 0.440 (0.440) loss 0.2826 (0.2826) acc 96.8750 (96.8750) lr 2.7630e-01 eta 0:00:20\n",
            "epoch [187/200] batch [2/2] time 0.308 (0.526) data 0.000 (0.220) loss 0.3095 (0.2961) acc 96.8750 (96.8750) lr 2.4083e-01 eta 0:00:13\n",
            "epoch [188/200] batch [1/2] time 0.728 (0.728) data 0.424 (0.424) loss 0.2528 (0.2528) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:18\n",
            "epoch [188/200] batch [2/2] time 0.304 (0.516) data 0.000 (0.212) loss 0.4209 (0.3368) acc 93.7500 (96.8750) lr 2.0777e-01 eta 0:00:12\n",
            "epoch [189/200] batch [1/2] time 0.751 (0.751) data 0.448 (0.448) loss 0.2385 (0.2385) acc 100.0000 (100.0000) lr 2.0777e-01 eta 0:00:17\n",
            "epoch [189/200] batch [2/2] time 0.308 (0.530) data 0.000 (0.224) loss 0.2469 (0.2427) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:11\n",
            "epoch [190/200] batch [1/2] time 0.733 (0.733) data 0.428 (0.428) loss 0.3809 (0.3809) acc 93.7500 (93.7500) lr 1.7713e-01 eta 0:00:15\n",
            "epoch [190/200] batch [2/2] time 0.306 (0.520) data 0.000 (0.214) loss 0.3192 (0.3500) acc 93.7500 (93.7500) lr 1.4891e-01 eta 0:00:10\n",
            "epoch [191/200] batch [1/2] time 0.744 (0.744) data 0.440 (0.440) loss 0.3033 (0.3033) acc 96.8750 (96.8750) lr 1.4891e-01 eta 0:00:14\n",
            "epoch [191/200] batch [2/2] time 0.310 (0.527) data 0.000 (0.220) loss 0.3590 (0.3311) acc 96.8750 (96.8750) lr 1.2312e-01 eta 0:00:09\n",
            "epoch [192/200] batch [1/2] time 0.759 (0.759) data 0.458 (0.458) loss 0.2555 (0.2555) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:12\n",
            "epoch [192/200] batch [2/2] time 0.308 (0.534) data 0.000 (0.229) loss 0.2627 (0.2591) acc 100.0000 (100.0000) lr 9.9763e-02 eta 0:00:08\n",
            "epoch [193/200] batch [1/2] time 0.744 (0.744) data 0.444 (0.444) loss 0.4267 (0.4267) acc 93.7500 (93.7500) lr 9.9763e-02 eta 0:00:11\n",
            "epoch [193/200] batch [2/2] time 0.310 (0.527) data 0.000 (0.222) loss 0.2428 (0.3347) acc 100.0000 (96.8750) lr 7.8853e-02 eta 0:00:07\n",
            "epoch [194/200] batch [1/2] time 0.720 (0.720) data 0.417 (0.417) loss 0.3349 (0.3349) acc 96.8750 (96.8750) lr 7.8853e-02 eta 0:00:09\n",
            "epoch [194/200] batch [2/2] time 0.308 (0.514) data 0.000 (0.209) loss 0.2397 (0.2873) acc 100.0000 (98.4375) lr 6.0390e-02 eta 0:00:06\n",
            "epoch [195/200] batch [1/2] time 0.738 (0.738) data 0.438 (0.438) loss 0.2488 (0.2488) acc 100.0000 (100.0000) lr 6.0390e-02 eta 0:00:08\n",
            "epoch [195/200] batch [2/2] time 0.308 (0.523) data 0.000 (0.219) loss 0.3479 (0.2984) acc 96.8750 (98.4375) lr 4.4380e-02 eta 0:00:05\n",
            "epoch [196/200] batch [1/2] time 0.721 (0.721) data 0.419 (0.419) loss 0.2368 (0.2368) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:06\n",
            "epoch [196/200] batch [2/2] time 0.309 (0.515) data 0.000 (0.210) loss 0.3448 (0.2908) acc 96.8750 (98.4375) lr 3.0827e-02 eta 0:00:04\n",
            "epoch [197/200] batch [1/2] time 0.750 (0.750) data 0.447 (0.447) loss 0.2820 (0.2820) acc 96.8750 (96.8750) lr 3.0827e-02 eta 0:00:05\n",
            "epoch [197/200] batch [2/2] time 0.307 (0.529) data 0.000 (0.224) loss 0.2582 (0.2701) acc 100.0000 (98.4375) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [1/2] time 0.799 (0.799) data 0.496 (0.496) loss 0.2451 (0.2451) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:03\n",
            "epoch [198/200] batch [2/2] time 0.308 (0.553) data 0.000 (0.248) loss 0.2310 (0.2380) acc 100.0000 (100.0000) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [1/2] time 0.727 (0.727) data 0.425 (0.425) loss 0.3180 (0.3180) acc 96.8750 (96.8750) lr 1.1101e-02 eta 0:00:02\n",
            "epoch [199/200] batch [2/2] time 0.308 (0.517) data 0.000 (0.213) loss 0.2241 (0.2711) acc 100.0000 (98.4375) lr 4.9344e-03 eta 0:00:01\n",
            "epoch [200/200] batch [1/2] time 0.723 (0.723) data 0.422 (0.422) loss 0.2635 (0.2635) acc 96.8750 (96.8750) lr 4.9344e-03 eta 0:00:00\n",
            "epoch [200/200] batch [2/2] time 0.305 (0.514) data 0.000 (0.211) loss 0.2835 (0.2735) acc 96.8750 (96.8750) lr 1.2337e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/urosat/DAPT/vit_b16_8shots/seed3/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.50it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 6,759\n",
            "* accuracy: 83.4%\n",
            "* error: 16.6%\n",
            "* macro_f1: 83.3%\n",
            "Elapsed: 0:04:17\n"
          ]
        }
      ],
      "source": [
        "#eurosat-8shots-seed3\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 3 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/urosat/DAPT/vit_b16_8shots/seed3 \\\n",
        "        DATASET.NUM_SHOTS 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9affb9-a93d-48c8-b45a-0c3c730f2aa3",
        "id": "VphHEvR2iUYe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 10:01:39.440540: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 10:01:39.459775: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 10:01:39.465688: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 10:01:39.479623: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 10:01:40.488690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4']\n",
            "output_dir: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  40\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "        [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "        [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "        ...,\n",
            "        [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "        [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "        [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-5.2644e-04, -1.1168e-02, -2.1661e-02,  ..., -4.8596e-03,\n",
            "           5.8912e-03,  6.7493e-03],\n",
            "         [ 5.8841e-04, -1.8584e-02, -2.5060e-02,  ..., -3.4987e-03,\n",
            "           5.4105e-03, -1.7861e-03],\n",
            "         [ 1.7309e-03, -7.2606e-03, -2.3481e-02,  ..., -2.1107e-03,\n",
            "           4.9070e-03,  6.0702e-03],\n",
            "         ...,\n",
            "         [ 1.0223e-03, -1.2080e-02, -1.5868e-02,  ..., -1.2882e-02,\n",
            "          -2.0567e-03,  4.8362e-03],\n",
            "         [ 7.8468e-03, -1.1038e-02, -1.0770e-02,  ...,  7.4988e-04,\n",
            "           1.5215e-03, -1.6774e-03],\n",
            "         [ 2.3028e-03, -5.9798e-03, -1.7329e-02,  ...,  5.5724e-04,\n",
            "           1.5857e-02,  4.3269e-03]],\n",
            "\n",
            "        [[ 6.0997e-03,  1.7106e-03, -2.1188e-02,  ...,  2.5523e-03,\n",
            "           2.1776e-03,  7.7525e-03],\n",
            "         [ 6.9275e-03, -6.8095e-03, -1.3799e-02,  ..., -3.6580e-03,\n",
            "           5.6513e-03,  1.9980e-03],\n",
            "         [ 6.2942e-04, -7.5124e-03, -9.4807e-03,  ...,  9.2128e-03,\n",
            "           1.0227e-02,  3.9989e-03],\n",
            "         ...,\n",
            "         [-8.9265e-04, -1.5108e-02, -1.9400e-02,  ..., -3.9164e-03,\n",
            "           4.4435e-03, -3.3369e-04],\n",
            "         [ 9.0713e-03, -1.0043e-02, -2.8737e-02,  ..., -4.9359e-03,\n",
            "           9.1699e-03,  5.4980e-03],\n",
            "         [-8.9653e-05, -7.9620e-03, -2.2422e-02,  ..., -2.5861e-03,\n",
            "           2.1928e-03,  6.9095e-03]],\n",
            "\n",
            "        [[ 2.2506e-04, -1.2583e-02, -1.8306e-02,  ..., -1.1051e-02,\n",
            "           9.2462e-03,  8.1454e-03],\n",
            "         [-9.4987e-04, -1.2930e-02, -2.0111e-02,  ..., -9.7730e-03,\n",
            "           5.3495e-03,  1.8893e-03],\n",
            "         [-2.7428e-03, -8.8699e-03, -1.4280e-02,  ...,  5.0471e-03,\n",
            "           5.3924e-03,  1.0839e-02],\n",
            "         ...,\n",
            "         [ 7.2136e-03, -9.1107e-03, -1.5424e-02,  ...,  1.9115e-03,\n",
            "           6.7415e-03, -8.7823e-04],\n",
            "         [-4.8866e-03, -9.1675e-03, -1.5351e-02,  ..., -5.0427e-03,\n",
            "           8.2830e-03,  1.1029e-02],\n",
            "         [-4.4785e-03, -1.2034e-02, -1.6046e-02,  ..., -1.3892e-03,\n",
            "           1.1470e-02, -4.6686e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.1645e-03,  2.2294e-03, -2.3786e-02,  ..., -2.2859e-04,\n",
            "           6.9243e-03,  8.3819e-03],\n",
            "         [ 4.7722e-03, -5.7014e-03, -1.7818e-02,  ..., -5.6092e-03,\n",
            "           1.0013e-02,  1.3503e-03],\n",
            "         [-2.0599e-03, -4.0076e-03, -2.7204e-02,  ...,  2.8766e-03,\n",
            "           1.4182e-02, -4.1284e-04],\n",
            "         ...,\n",
            "         [-2.6131e-03, -1.2682e-02, -1.9781e-02,  ...,  2.6591e-03,\n",
            "           5.9818e-03,  1.0518e-02],\n",
            "         [ 5.3825e-03, -7.2549e-03, -1.3387e-02,  ..., -5.7954e-04,\n",
            "           5.6404e-03,  2.1621e-03],\n",
            "         [ 1.1158e-03,  5.8145e-04, -2.5354e-02,  ...,  1.2668e-03,\n",
            "           9.6658e-03,  5.6201e-03]],\n",
            "\n",
            "        [[ 5.7640e-03, -5.7109e-03, -1.9671e-02,  ..., -6.6449e-03,\n",
            "          -2.2398e-03,  3.5010e-04],\n",
            "         [ 4.0493e-03, -1.3674e-02, -2.2840e-02,  ..., -2.0630e-03,\n",
            "           1.2584e-02, -4.8818e-03],\n",
            "         [ 1.1436e-05, -3.8550e-03, -2.1955e-02,  ..., -3.6313e-03,\n",
            "           3.4708e-03,  2.7534e-03],\n",
            "         ...,\n",
            "         [ 7.8735e-03, -1.2430e-02, -2.0399e-02,  ..., -1.3462e-02,\n",
            "           1.2340e-02, -4.7330e-03],\n",
            "         [ 2.9120e-03, -1.2139e-02, -6.7417e-03,  ...,  4.5043e-04,\n",
            "           7.1866e-05,  9.1229e-04],\n",
            "         [-3.6163e-03, -4.2251e-03, -2.5861e-02,  ..., -6.7784e-03,\n",
            "           5.3829e-03, -2.0913e-03]],\n",
            "\n",
            "        [[ 1.0143e-02, -1.1706e-02, -2.0403e-02,  ..., -5.1286e-03,\n",
            "           9.2310e-03,  1.6630e-03],\n",
            "         [ 5.0354e-03, -6.9526e-03, -1.5557e-02,  ..., -3.4320e-03,\n",
            "           9.9329e-03,  5.0002e-03],\n",
            "         [ 3.1037e-03, -1.5677e-02, -2.5472e-02,  ..., -1.3558e-03,\n",
            "           2.2081e-03,  4.0732e-03],\n",
            "         ...,\n",
            "         [ 3.1457e-03, -4.8411e-03, -1.6359e-02,  ...,  3.0177e-03,\n",
            "           9.1890e-03, -4.2676e-03],\n",
            "         [ 5.7907e-03, -1.0981e-02, -2.1062e-02,  ..., -9.6700e-03,\n",
            "           4.3233e-03,  1.2432e-03],\n",
            "         [-5.3413e-05, -5.7033e-03, -2.5506e-02,  ..., -3.7009e-03,\n",
            "           7.3665e-03,  1.6910e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4741\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 22.536 (22.536) data 21.307 (21.307) loss 8.6282 (8.6282) acc 28.1250 (28.1250) lr 2.0000e+01 eta 0:37:11\n",
            "epoch [2/100] batch [1/1] time 4.433 (4.433) data 4.055 (4.055) loss 8.7395 (8.7395) acc 18.7500 (18.7500) lr 1.9995e+01 eta 0:07:14\n",
            "epoch [3/100] batch [1/1] time 1.478 (1.478) data 1.182 (1.182) loss 7.3295 (7.3295) acc 21.8750 (21.8750) lr 1.9980e+01 eta 0:02:23\n",
            "epoch [4/100] batch [1/1] time 0.699 (0.699) data 0.401 (0.401) loss 6.1930 (6.1930) acc 15.6250 (15.6250) lr 1.9956e+01 eta 0:01:07\n",
            "epoch [5/100] batch [1/1] time 0.686 (0.686) data 0.390 (0.390) loss 7.0878 (7.0878) acc 12.5000 (12.5000) lr 1.9921e+01 eta 0:01:05\n",
            "epoch [6/100] batch [1/1] time 0.694 (0.694) data 0.398 (0.398) loss 6.8839 (6.8839) acc 15.6250 (15.6250) lr 1.9877e+01 eta 0:01:05\n",
            "epoch [7/100] batch [1/1] time 0.706 (0.706) data 0.410 (0.410) loss 7.1456 (7.1456) acc 9.3750 (9.3750) lr 1.9823e+01 eta 0:01:05\n",
            "epoch [8/100] batch [1/1] time 0.707 (0.707) data 0.410 (0.410) loss 7.8557 (7.8557) acc 15.6250 (15.6250) lr 1.9759e+01 eta 0:01:05\n",
            "epoch [9/100] batch [1/1] time 0.709 (0.709) data 0.413 (0.413) loss 6.1597 (6.1597) acc 9.3750 (9.3750) lr 1.9686e+01 eta 0:01:04\n",
            "epoch [10/100] batch [1/1] time 0.712 (0.712) data 0.413 (0.413) loss 6.0759 (6.0759) acc 12.5000 (12.5000) lr 1.9603e+01 eta 0:01:04\n",
            "epoch [11/100] batch [1/1] time 0.718 (0.718) data 0.418 (0.418) loss 4.5287 (4.5287) acc 12.5000 (12.5000) lr 1.9511e+01 eta 0:01:03\n",
            "epoch [12/100] batch [1/1] time 0.720 (0.720) data 0.420 (0.420) loss 3.8478 (3.8478) acc 15.6250 (15.6250) lr 1.9409e+01 eta 0:01:03\n",
            "epoch [13/100] batch [1/1] time 0.733 (0.733) data 0.434 (0.434) loss 3.5617 (3.5617) acc 25.0000 (25.0000) lr 1.9298e+01 eta 0:01:03\n",
            "epoch [14/100] batch [1/1] time 0.742 (0.742) data 0.441 (0.441) loss 3.6352 (3.6352) acc 18.7500 (18.7500) lr 1.9178e+01 eta 0:01:03\n",
            "epoch [15/100] batch [1/1] time 0.712 (0.712) data 0.412 (0.412) loss 3.3024 (3.3024) acc 18.7500 (18.7500) lr 1.9048e+01 eta 0:01:00\n",
            "epoch [16/100] batch [1/1] time 0.720 (0.720) data 0.420 (0.420) loss 3.2599 (3.2599) acc 28.1250 (28.1250) lr 1.8910e+01 eta 0:01:00\n",
            "epoch [17/100] batch [1/1] time 0.685 (0.685) data 0.385 (0.385) loss 3.1065 (3.1065) acc 18.7500 (18.7500) lr 1.8763e+01 eta 0:00:56\n",
            "epoch [18/100] batch [1/1] time 0.678 (0.678) data 0.407 (0.407) loss 2.7388 (2.7388) acc 28.1250 (28.1250) lr 1.8607e+01 eta 0:00:55\n",
            "epoch [19/100] batch [1/1] time 0.717 (0.717) data 0.418 (0.418) loss 2.7502 (2.7502) acc 21.8750 (21.8750) lr 1.8443e+01 eta 0:00:58\n",
            "epoch [20/100] batch [1/1] time 0.731 (0.731) data 0.428 (0.428) loss 2.5135 (2.5135) acc 34.3750 (34.3750) lr 1.8271e+01 eta 0:00:58\n",
            "epoch [21/100] batch [1/1] time 0.731 (0.731) data 0.429 (0.429) loss 2.4540 (2.4540) acc 43.7500 (43.7500) lr 1.8090e+01 eta 0:00:57\n",
            "epoch [22/100] batch [1/1] time 0.693 (0.693) data 0.393 (0.393) loss 2.2210 (2.2210) acc 46.8750 (46.8750) lr 1.7902e+01 eta 0:00:54\n",
            "epoch [23/100] batch [1/1] time 0.712 (0.712) data 0.410 (0.410) loss 2.3753 (2.3753) acc 50.0000 (50.0000) lr 1.7705e+01 eta 0:00:54\n",
            "epoch [24/100] batch [1/1] time 0.727 (0.727) data 0.426 (0.426) loss 1.9582 (1.9582) acc 59.3750 (59.3750) lr 1.7501e+01 eta 0:00:55\n",
            "epoch [25/100] batch [1/1] time 0.705 (0.705) data 0.404 (0.404) loss 2.1881 (2.1881) acc 40.6250 (40.6250) lr 1.7290e+01 eta 0:00:52\n",
            "epoch [26/100] batch [1/1] time 0.741 (0.741) data 0.439 (0.439) loss 1.9118 (1.9118) acc 56.2500 (56.2500) lr 1.7071e+01 eta 0:00:54\n",
            "epoch [27/100] batch [1/1] time 0.714 (0.714) data 0.414 (0.414) loss 1.9098 (1.9098) acc 46.8750 (46.8750) lr 1.6845e+01 eta 0:00:52\n",
            "epoch [28/100] batch [1/1] time 0.760 (0.760) data 0.458 (0.458) loss 1.8065 (1.8065) acc 50.0000 (50.0000) lr 1.6613e+01 eta 0:00:54\n",
            "epoch [29/100] batch [1/1] time 0.768 (0.768) data 0.464 (0.464) loss 1.7520 (1.7520) acc 59.3750 (59.3750) lr 1.6374e+01 eta 0:00:54\n",
            "epoch [30/100] batch [1/1] time 0.737 (0.737) data 0.432 (0.432) loss 1.4897 (1.4897) acc 62.5000 (62.5000) lr 1.6129e+01 eta 0:00:51\n",
            "epoch [31/100] batch [1/1] time 0.743 (0.743) data 0.440 (0.440) loss 1.8196 (1.8196) acc 46.8750 (46.8750) lr 1.5878e+01 eta 0:00:51\n",
            "epoch [32/100] batch [1/1] time 0.701 (0.701) data 0.398 (0.398) loss 1.6194 (1.6194) acc 62.5000 (62.5000) lr 1.5621e+01 eta 0:00:47\n",
            "epoch [33/100] batch [1/1] time 0.702 (0.702) data 0.400 (0.400) loss 1.9489 (1.9489) acc 59.3750 (59.3750) lr 1.5358e+01 eta 0:00:47\n",
            "epoch [34/100] batch [1/1] time 0.708 (0.708) data 0.406 (0.406) loss 1.9194 (1.9194) acc 50.0000 (50.0000) lr 1.5090e+01 eta 0:00:46\n",
            "epoch [35/100] batch [1/1] time 0.682 (0.682) data 0.379 (0.379) loss 1.8380 (1.8380) acc 59.3750 (59.3750) lr 1.4818e+01 eta 0:00:44\n",
            "epoch [36/100] batch [1/1] time 0.716 (0.716) data 0.414 (0.414) loss 1.5561 (1.5561) acc 59.3750 (59.3750) lr 1.4540e+01 eta 0:00:45\n",
            "epoch [37/100] batch [1/1] time 0.729 (0.729) data 0.425 (0.425) loss 2.1929 (2.1929) acc 53.1250 (53.1250) lr 1.4258e+01 eta 0:00:45\n",
            "epoch [38/100] batch [1/1] time 0.710 (0.710) data 0.408 (0.408) loss 2.9193 (2.9193) acc 56.2500 (56.2500) lr 1.3971e+01 eta 0:00:44\n",
            "epoch [39/100] batch [1/1] time 0.722 (0.722) data 0.420 (0.420) loss 2.5472 (2.5472) acc 56.2500 (56.2500) lr 1.3681e+01 eta 0:00:44\n",
            "epoch [40/100] batch [1/1] time 0.711 (0.711) data 0.406 (0.406) loss 5.1668 (5.1668) acc 34.3750 (34.3750) lr 1.3387e+01 eta 0:00:42\n",
            "epoch [41/100] batch [1/1] time 0.725 (0.725) data 0.418 (0.418) loss 5.0916 (5.0916) acc 31.2500 (31.2500) lr 1.3090e+01 eta 0:00:42\n",
            "epoch [42/100] batch [1/1] time 0.704 (0.704) data 0.399 (0.399) loss 6.3006 (6.3006) acc 31.2500 (31.2500) lr 1.2790e+01 eta 0:00:40\n",
            "epoch [43/100] batch [1/1] time 0.707 (0.707) data 0.403 (0.403) loss 7.5653 (7.5653) acc 18.7500 (18.7500) lr 1.2487e+01 eta 0:00:40\n",
            "epoch [44/100] batch [1/1] time 0.743 (0.743) data 0.438 (0.438) loss 5.9879 (5.9879) acc 15.6250 (15.6250) lr 1.2181e+01 eta 0:00:41\n",
            "epoch [45/100] batch [1/1] time 0.738 (0.738) data 0.432 (0.432) loss 6.2689 (6.2689) acc 21.8750 (21.8750) lr 1.1874e+01 eta 0:00:40\n",
            "epoch [46/100] batch [1/1] time 0.753 (0.753) data 0.448 (0.448) loss 9.7390 (9.7390) acc 9.3750 (9.3750) lr 1.1564e+01 eta 0:00:40\n",
            "epoch [47/100] batch [1/1] time 0.725 (0.725) data 0.419 (0.419) loss 10.2733 (10.2733) acc 15.6250 (15.6250) lr 1.1253e+01 eta 0:00:38\n",
            "epoch [48/100] batch [1/1] time 0.692 (0.692) data 0.385 (0.385) loss 7.4902 (7.4902) acc 9.3750 (9.3750) lr 1.0941e+01 eta 0:00:36\n",
            "epoch [49/100] batch [1/1] time 0.734 (0.734) data 0.427 (0.427) loss 6.8295 (6.8295) acc 18.7500 (18.7500) lr 1.0628e+01 eta 0:00:37\n",
            "epoch [50/100] batch [1/1] time 0.713 (0.713) data 0.404 (0.404) loss 10.0345 (10.0345) acc 12.5000 (12.5000) lr 1.0314e+01 eta 0:00:35\n",
            "epoch [51/100] batch [1/1] time 0.737 (0.737) data 0.430 (0.430) loss 6.2731 (6.2731) acc 15.6250 (15.6250) lr 1.0000e+01 eta 0:00:36\n",
            "epoch [52/100] batch [1/1] time 0.714 (0.714) data 0.405 (0.405) loss 6.3142 (6.3142) acc 15.6250 (15.6250) lr 9.6859e+00 eta 0:00:34\n",
            "epoch [53/100] batch [1/1] time 0.729 (0.729) data 0.421 (0.421) loss 6.7645 (6.7645) acc 18.7500 (18.7500) lr 9.3721e+00 eta 0:00:34\n",
            "epoch [54/100] batch [1/1] time 0.686 (0.686) data 0.390 (0.390) loss 9.9367 (9.9367) acc 15.6250 (15.6250) lr 9.0589e+00 eta 0:00:31\n",
            "epoch [55/100] batch [1/1] time 0.718 (0.718) data 0.409 (0.409) loss 9.0968 (9.0968) acc 15.6250 (15.6250) lr 8.7467e+00 eta 0:00:32\n",
            "epoch [56/100] batch [1/1] time 0.726 (0.726) data 0.418 (0.418) loss 5.0562 (5.0562) acc 34.3750 (34.3750) lr 8.4357e+00 eta 0:00:31\n",
            "epoch [57/100] batch [1/1] time 0.714 (0.714) data 0.404 (0.404) loss 5.1997 (5.1997) acc 31.2500 (31.2500) lr 8.1262e+00 eta 0:00:30\n",
            "epoch [58/100] batch [1/1] time 0.733 (0.733) data 0.423 (0.423) loss 4.9116 (4.9116) acc 31.2500 (31.2500) lr 7.8186e+00 eta 0:00:30\n",
            "epoch [59/100] batch [1/1] time 0.711 (0.711) data 0.400 (0.400) loss 4.4164 (4.4164) acc 37.5000 (37.5000) lr 7.5131e+00 eta 0:00:29\n",
            "epoch [60/100] batch [1/1] time 0.742 (0.742) data 0.433 (0.433) loss 3.2282 (3.2282) acc 34.3750 (34.3750) lr 7.2101e+00 eta 0:00:29\n",
            "epoch [61/100] batch [1/1] time 0.747 (0.747) data 0.439 (0.439) loss 2.5690 (2.5690) acc 46.8750 (46.8750) lr 6.9098e+00 eta 0:00:29\n",
            "epoch [62/100] batch [1/1] time 0.730 (0.730) data 0.423 (0.423) loss 2.4582 (2.4582) acc 53.1250 (53.1250) lr 6.6126e+00 eta 0:00:27\n",
            "epoch [63/100] batch [1/1] time 0.729 (0.729) data 0.419 (0.419) loss 2.0261 (2.0261) acc 53.1250 (53.1250) lr 6.3188e+00 eta 0:00:26\n",
            "epoch [64/100] batch [1/1] time 0.729 (0.729) data 0.418 (0.418) loss 1.6147 (1.6147) acc 59.3750 (59.3750) lr 6.0285e+00 eta 0:00:26\n",
            "epoch [65/100] batch [1/1] time 0.697 (0.697) data 0.388 (0.388) loss 1.8751 (1.8751) acc 59.3750 (59.3750) lr 5.7422e+00 eta 0:00:24\n",
            "epoch [66/100] batch [1/1] time 0.734 (0.734) data 0.423 (0.423) loss 1.6327 (1.6327) acc 65.6250 (65.6250) lr 5.4601e+00 eta 0:00:24\n",
            "epoch [67/100] batch [1/1] time 0.708 (0.708) data 0.400 (0.400) loss 1.5132 (1.5132) acc 68.7500 (68.7500) lr 5.1825e+00 eta 0:00:23\n",
            "epoch [68/100] batch [1/1] time 0.714 (0.714) data 0.404 (0.404) loss 1.1933 (1.1933) acc 78.1250 (78.1250) lr 4.9096e+00 eta 0:00:22\n",
            "epoch [69/100] batch [1/1] time 0.735 (0.735) data 0.429 (0.429) loss 1.3431 (1.3431) acc 71.8750 (71.8750) lr 4.6417e+00 eta 0:00:22\n",
            "epoch [70/100] batch [1/1] time 0.705 (0.705) data 0.394 (0.394) loss 1.3395 (1.3395) acc 65.6250 (65.6250) lr 4.3792e+00 eta 0:00:21\n",
            "epoch [71/100] batch [1/1] time 0.732 (0.732) data 0.422 (0.422) loss 1.1985 (1.1985) acc 81.2500 (81.2500) lr 4.1221e+00 eta 0:00:21\n",
            "epoch [72/100] batch [1/1] time 0.728 (0.728) data 0.417 (0.417) loss 1.3632 (1.3632) acc 68.7500 (68.7500) lr 3.8709e+00 eta 0:00:20\n",
            "epoch [73/100] batch [1/1] time 0.727 (0.727) data 0.418 (0.418) loss 1.0280 (1.0280) acc 81.2500 (81.2500) lr 3.6258e+00 eta 0:00:19\n",
            "epoch [74/100] batch [1/1] time 0.716 (0.716) data 0.407 (0.407) loss 1.2422 (1.2422) acc 68.7500 (68.7500) lr 3.3869e+00 eta 0:00:18\n",
            "epoch [75/100] batch [1/1] time 0.737 (0.737) data 0.432 (0.432) loss 1.2264 (1.2264) acc 71.8750 (71.8750) lr 3.1545e+00 eta 0:00:18\n",
            "epoch [76/100] batch [1/1] time 0.753 (0.753) data 0.444 (0.444) loss 1.1165 (1.1165) acc 78.1250 (78.1250) lr 2.9289e+00 eta 0:00:18\n",
            "epoch [77/100] batch [1/1] time 0.719 (0.719) data 0.412 (0.412) loss 1.1087 (1.1087) acc 78.1250 (78.1250) lr 2.7103e+00 eta 0:00:16\n",
            "epoch [78/100] batch [1/1] time 0.754 (0.754) data 0.448 (0.448) loss 1.0075 (1.0075) acc 81.2500 (81.2500) lr 2.4989e+00 eta 0:00:16\n",
            "epoch [79/100] batch [1/1] time 0.707 (0.707) data 0.398 (0.398) loss 0.9669 (0.9669) acc 81.2500 (81.2500) lr 2.2949e+00 eta 0:00:14\n",
            "epoch [80/100] batch [1/1] time 0.719 (0.719) data 0.412 (0.412) loss 1.0752 (1.0752) acc 81.2500 (81.2500) lr 2.0984e+00 eta 0:00:14\n",
            "epoch [81/100] batch [1/1] time 0.728 (0.728) data 0.424 (0.424) loss 1.0714 (1.0714) acc 81.2500 (81.2500) lr 1.9098e+00 eta 0:00:13\n",
            "epoch [82/100] batch [1/1] time 0.723 (0.723) data 0.418 (0.418) loss 1.0446 (1.0446) acc 75.0000 (75.0000) lr 1.7292e+00 eta 0:00:13\n",
            "epoch [83/100] batch [1/1] time 0.728 (0.728) data 0.422 (0.422) loss 1.0807 (1.0807) acc 75.0000 (75.0000) lr 1.5567e+00 eta 0:00:12\n",
            "epoch [84/100] batch [1/1] time 0.686 (0.686) data 0.380 (0.380) loss 1.0043 (1.0043) acc 84.3750 (84.3750) lr 1.3926e+00 eta 0:00:10\n",
            "epoch [85/100] batch [1/1] time 0.733 (0.733) data 0.428 (0.428) loss 0.8663 (0.8663) acc 90.6250 (90.6250) lr 1.2369e+00 eta 0:00:10\n",
            "epoch [86/100] batch [1/1] time 0.707 (0.707) data 0.402 (0.402) loss 1.0141 (1.0141) acc 84.3750 (84.3750) lr 1.0899e+00 eta 0:00:09\n",
            "epoch [87/100] batch [1/1] time 0.686 (0.686) data 0.382 (0.382) loss 0.8426 (0.8426) acc 90.6250 (90.6250) lr 9.5173e-01 eta 0:00:08\n",
            "epoch [88/100] batch [1/1] time 0.704 (0.704) data 0.398 (0.398) loss 0.8329 (0.8329) acc 90.6250 (90.6250) lr 8.2245e-01 eta 0:00:08\n",
            "epoch [89/100] batch [1/1] time 0.703 (0.703) data 0.400 (0.400) loss 0.9961 (0.9961) acc 78.1250 (78.1250) lr 7.0224e-01 eta 0:00:07\n",
            "epoch [90/100] batch [1/1] time 0.717 (0.717) data 0.417 (0.417) loss 0.7528 (0.7528) acc 93.7500 (93.7500) lr 5.9119e-01 eta 0:00:07\n",
            "epoch [91/100] batch [1/1] time 0.680 (0.680) data 0.380 (0.380) loss 0.6958 (0.6958) acc 93.7500 (93.7500) lr 4.8943e-01 eta 0:00:06\n",
            "epoch [92/100] batch [1/1] time 0.712 (0.712) data 0.409 (0.409) loss 0.7456 (0.7456) acc 90.6250 (90.6250) lr 3.9706e-01 eta 0:00:05\n",
            "epoch [93/100] batch [1/1] time 0.761 (0.761) data 0.458 (0.458) loss 0.8675 (0.8675) acc 87.5000 (87.5000) lr 3.1417e-01 eta 0:00:05\n",
            "epoch [94/100] batch [1/1] time 0.739 (0.739) data 0.437 (0.437) loss 0.7715 (0.7715) acc 93.7500 (93.7500) lr 2.4083e-01 eta 0:00:04\n",
            "epoch [95/100] batch [1/1] time 0.744 (0.744) data 0.442 (0.442) loss 0.7928 (0.7928) acc 93.7500 (93.7500) lr 1.7713e-01 eta 0:00:03\n",
            "epoch [96/100] batch [1/1] time 0.689 (0.689) data 0.388 (0.388) loss 0.7883 (0.7883) acc 90.6250 (90.6250) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.701 (0.701) data 0.397 (0.397) loss 0.8942 (0.8942) acc 81.2500 (81.2500) lr 7.8853e-02 eta 0:00:02\n",
            "epoch [98/100] batch [1/1] time 0.704 (0.704) data 0.400 (0.400) loss 0.7793 (0.7793) acc 90.6250 (90.6250) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.683 (0.683) data 0.381 (0.381) loss 0.8633 (0.8633) acc 84.3750 (84.3750) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.720 (0.720) data 0.419 (0.419) loss 0.8842 (0.8842) acc 87.5000 (87.5000) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.53it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 4,910\n",
            "* accuracy: 60.6%\n",
            "* error: 39.4%\n",
            "* macro_f1: 60.5%\n",
            "Elapsed: 0:02:17\n"
          ]
        }
      ],
      "source": [
        "#eurosat-4shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6085068a-a92b-4c94-82b5-e7624c04e70c",
        "id": "cPJ-VxS1iUYe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 10:04:16.868393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 10:04:16.887887: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 10:04:16.893705: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 10:04:16.907778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 10:04:17.955227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4']\n",
            "output_dir: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed2\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  40\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0173, -0.5723,  0.1752,  ...,  0.4941,  0.4187,  0.0375],\n",
            "        [ 0.0214, -0.5706,  0.1699,  ...,  0.5021,  0.4206,  0.0425],\n",
            "        [ 0.0149, -0.5713,  0.1720,  ...,  0.4970,  0.4210,  0.0462],\n",
            "        ...,\n",
            "        [ 0.0324, -0.5680,  0.1748,  ...,  0.5016,  0.4188,  0.0404],\n",
            "        [ 0.0201, -0.5696,  0.1726,  ...,  0.4989,  0.4190,  0.0354],\n",
            "        [ 0.0228, -0.5696,  0.1684,  ...,  0.5053,  0.4212,  0.0412]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5609\n",
            "  Max: 1.0111\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 1.1860e-02, -2.6458e-03, -2.5006e-02,  ..., -3.9643e-04,\n",
            "           1.1337e-02,  1.2487e-02],\n",
            "         [ 5.5103e-03, -1.7622e-02, -2.4362e-02,  ..., -1.3186e-03,\n",
            "           5.9513e-03,  1.6259e-03],\n",
            "         [ 9.3536e-03,  1.0087e-03, -1.5165e-02,  ..., -3.1478e-03,\n",
            "           6.7614e-03,  4.8820e-03],\n",
            "         ...,\n",
            "         [ 2.1362e-04, -7.9000e-03, -2.3896e-02,  ..., -1.4779e-03,\n",
            "           1.1997e-02, -3.9166e-03],\n",
            "         [ 1.1013e-02, -7.4499e-03, -2.3729e-02,  ...,  3.6269e-04,\n",
            "           5.6833e-03, -2.5281e-03],\n",
            "         [-2.5940e-03, -1.1364e-02, -2.0847e-02,  ..., -2.7323e-03,\n",
            "           4.0220e-03, -7.1897e-03]],\n",
            "\n",
            "        [[ 1.6603e-03, -6.0676e-03, -1.4711e-02,  ..., -2.9127e-03,\n",
            "           1.9296e-03,  9.3089e-03],\n",
            "         [-2.8915e-03, -6.1133e-03, -1.3448e-02,  ..., -6.4027e-03,\n",
            "           4.6552e-03,  9.3776e-03],\n",
            "         [-1.5755e-03, -1.1160e-02, -1.4180e-02,  ..., -1.6028e-03,\n",
            "           3.9724e-03,  3.2044e-03],\n",
            "         ...,\n",
            "         [ 4.7645e-03, -1.1244e-02, -1.5756e-02,  ..., -3.3547e-03,\n",
            "           7.3913e-03,  2.4758e-03],\n",
            "         [ 7.0801e-03, -7.1700e-03, -1.9013e-02,  ..., -1.0163e-03,\n",
            "           2.7002e-03,  5.4027e-03],\n",
            "         [ 5.8899e-03, -6.5368e-03, -2.4068e-02,  ...,  5.2455e-03,\n",
            "           1.2527e-02, -4.9466e-03]],\n",
            "\n",
            "        [[ 1.4686e-04, -8.2129e-03, -1.6530e-02,  ..., -4.4667e-03,\n",
            "           6.4542e-03,  6.4059e-03],\n",
            "         [-4.8981e-03, -1.1071e-02, -2.5045e-02,  ..., -3.8926e-03,\n",
            "           6.6517e-03,  1.2408e-04],\n",
            "         [ 3.3245e-03, -1.0737e-02, -1.9468e-02,  ..., -5.5749e-03,\n",
            "           1.1035e-02,  7.8831e-04],\n",
            "         ...,\n",
            "         [-3.4180e-03, -4.5054e-03, -1.2708e-02,  ..., -2.1145e-03,\n",
            "           3.3296e-03,  1.9134e-03],\n",
            "         [ 4.5376e-03, -8.9677e-03, -2.3088e-02,  ..., -4.5306e-03,\n",
            "           1.3023e-02,  5.9138e-03],\n",
            "         [ 9.3574e-03, -8.1519e-03, -2.2479e-02,  ..., -1.7821e-03,\n",
            "           4.5026e-03,  2.8945e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.0692e-03, -1.3144e-02, -1.6256e-02,  ..., -9.4716e-03,\n",
            "           7.2750e-03,  1.0649e-03],\n",
            "         [ 6.2599e-03, -8.7433e-03, -1.7177e-02,  ..., -3.8430e-03,\n",
            "           1.0294e-03, -2.1371e-03],\n",
            "         [-2.1286e-03, -9.9261e-03, -2.9645e-02,  ..., -7.6253e-03,\n",
            "           8.4852e-03,  8.8320e-04],\n",
            "         ...,\n",
            "         [-4.9095e-03, -3.9542e-03, -2.2611e-02,  ...,  5.4057e-03,\n",
            "           7.7175e-03,  5.5609e-04],\n",
            "         [ 2.0873e-03,  2.1607e-03, -2.0847e-02,  ...,  5.8585e-04,\n",
            "           7.9912e-03,  2.8716e-03],\n",
            "         [-1.7548e-03, -1.4300e-02, -2.3313e-02,  ...,  1.7131e-03,\n",
            "           3.8103e-03,  3.7662e-03]],\n",
            "\n",
            "        [[ 7.1334e-04,  2.7558e-03, -2.0714e-02,  ...,  7.9310e-03,\n",
            "           1.0803e-02,  4.1495e-03],\n",
            "         [ 6.0177e-03, -5.6785e-03, -1.8350e-02,  ..., -5.0325e-04,\n",
            "           6.6672e-03,  8.7787e-03],\n",
            "         [ 8.4114e-03, -8.2713e-03, -2.0122e-02,  ..., -3.4172e-03,\n",
            "           4.1345e-03, -5.8164e-03],\n",
            "         ...,\n",
            "         [ 9.1743e-04, -5.1101e-03, -1.8438e-02,  ...,  2.2155e-04,\n",
            "           3.6560e-04,  6.9476e-03],\n",
            "         [ 7.8316e-03, -6.9430e-03, -2.2275e-02,  ...,  2.8422e-03,\n",
            "           3.2896e-03,  3.4438e-03],\n",
            "         [ 3.4103e-03, -1.1050e-02, -2.0027e-02,  ...,  5.7185e-03,\n",
            "           7.1662e-03, -8.0766e-04]],\n",
            "\n",
            "        [[ 2.7560e-04, -1.0767e-02, -2.1695e-02,  ..., -1.2066e-02,\n",
            "           1.0478e-02, -4.3096e-03],\n",
            "         [ 9.3173e-04,  9.4767e-04, -1.9784e-02,  ..., -4.0280e-03,\n",
            "           1.8016e-02, -5.1399e-05],\n",
            "         [ 4.1408e-03, -7.8738e-03, -1.1567e-02,  ..., -5.4166e-03,\n",
            "           5.1056e-04,  1.0778e-03],\n",
            "         ...,\n",
            "         [-2.7542e-03, -7.6211e-03, -1.6771e-02,  ...,  6.9316e-03,\n",
            "           6.3831e-03,  1.8298e-03],\n",
            "         [ 5.2490e-03, -6.7561e-03, -1.7544e-02,  ..., -2.3641e-03,\n",
            "           1.2687e-02,  9.0114e-03],\n",
            "         [ 6.6643e-03, -1.4715e-02, -1.9620e-02,  ..., -2.5640e-03,\n",
            "           8.1342e-03,  9.7705e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4524\n",
            "  Max: 0.4755\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed2/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 20.526 (20.526) data 19.299 (19.299) loss 8.8514 (8.8514) acc 18.7500 (18.7500) lr 2.0000e+01 eta 0:33:52\n",
            "epoch [2/100] batch [1/1] time 5.621 (5.621) data 5.256 (5.256) loss 8.7459 (8.7459) acc 34.3750 (34.3750) lr 1.9995e+01 eta 0:09:10\n",
            "epoch [3/100] batch [1/1] time 1.447 (1.447) data 1.150 (1.150) loss 7.2771 (7.2771) acc 18.7500 (18.7500) lr 1.9980e+01 eta 0:02:20\n",
            "epoch [4/100] batch [1/1] time 0.703 (0.703) data 0.404 (0.404) loss 5.4016 (5.4016) acc 15.6250 (15.6250) lr 1.9956e+01 eta 0:01:07\n",
            "epoch [5/100] batch [1/1] time 0.697 (0.697) data 0.401 (0.401) loss 4.9402 (4.9402) acc 12.5000 (12.5000) lr 1.9921e+01 eta 0:01:06\n",
            "epoch [6/100] batch [1/1] time 0.688 (0.688) data 0.389 (0.389) loss 5.2671 (5.2671) acc 12.5000 (12.5000) lr 1.9877e+01 eta 0:01:04\n",
            "epoch [7/100] batch [1/1] time 0.708 (0.708) data 0.409 (0.409) loss 4.9069 (4.9069) acc 15.6250 (15.6250) lr 1.9823e+01 eta 0:01:05\n",
            "epoch [8/100] batch [1/1] time 0.695 (0.695) data 0.394 (0.394) loss 4.5053 (4.5053) acc 28.1250 (28.1250) lr 1.9759e+01 eta 0:01:03\n",
            "epoch [9/100] batch [1/1] time 0.727 (0.727) data 0.426 (0.426) loss 4.2706 (4.2706) acc 31.2500 (31.2500) lr 1.9686e+01 eta 0:01:06\n",
            "epoch [10/100] batch [1/1] time 0.690 (0.690) data 0.391 (0.391) loss 3.7819 (3.7819) acc 46.8750 (46.8750) lr 1.9603e+01 eta 0:01:02\n",
            "epoch [11/100] batch [1/1] time 0.693 (0.693) data 0.396 (0.396) loss 3.4652 (3.4652) acc 46.8750 (46.8750) lr 1.9511e+01 eta 0:01:01\n",
            "epoch [12/100] batch [1/1] time 0.701 (0.701) data 0.421 (0.421) loss 2.5690 (2.5690) acc 62.5000 (62.5000) lr 1.9409e+01 eta 0:01:01\n",
            "epoch [13/100] batch [1/1] time 0.708 (0.708) data 0.407 (0.407) loss 2.0433 (2.0433) acc 56.2500 (56.2500) lr 1.9298e+01 eta 0:01:01\n",
            "epoch [14/100] batch [1/1] time 0.735 (0.735) data 0.437 (0.437) loss 1.9768 (1.9768) acc 65.6250 (65.6250) lr 1.9178e+01 eta 0:01:03\n",
            "epoch [15/100] batch [1/1] time 0.701 (0.701) data 0.399 (0.399) loss 1.8616 (1.8616) acc 65.6250 (65.6250) lr 1.9048e+01 eta 0:00:59\n",
            "epoch [16/100] batch [1/1] time 0.683 (0.683) data 0.383 (0.383) loss 1.8568 (1.8568) acc 65.6250 (65.6250) lr 1.8910e+01 eta 0:00:57\n",
            "epoch [17/100] batch [1/1] time 0.719 (0.719) data 0.419 (0.419) loss 1.4747 (1.4747) acc 81.2500 (81.2500) lr 1.8763e+01 eta 0:00:59\n",
            "epoch [18/100] batch [1/1] time 0.675 (0.675) data 0.375 (0.375) loss 1.6943 (1.6943) acc 68.7500 (68.7500) lr 1.8607e+01 eta 0:00:55\n",
            "epoch [19/100] batch [1/1] time 0.691 (0.691) data 0.392 (0.392) loss 1.2591 (1.2591) acc 75.0000 (75.0000) lr 1.8443e+01 eta 0:00:55\n",
            "epoch [20/100] batch [1/1] time 0.678 (0.678) data 0.377 (0.377) loss 1.7567 (1.7567) acc 62.5000 (62.5000) lr 1.8271e+01 eta 0:00:54\n",
            "epoch [21/100] batch [1/1] time 0.715 (0.715) data 0.412 (0.412) loss 1.5342 (1.5342) acc 62.5000 (62.5000) lr 1.8090e+01 eta 0:00:56\n",
            "epoch [22/100] batch [1/1] time 0.681 (0.681) data 0.377 (0.377) loss 1.4259 (1.4259) acc 68.7500 (68.7500) lr 1.7902e+01 eta 0:00:53\n",
            "epoch [23/100] batch [1/1] time 0.684 (0.684) data 0.383 (0.383) loss 1.3072 (1.3072) acc 81.2500 (81.2500) lr 1.7705e+01 eta 0:00:52\n",
            "epoch [24/100] batch [1/1] time 0.694 (0.694) data 0.392 (0.392) loss 1.4721 (1.4721) acc 75.0000 (75.0000) lr 1.7501e+01 eta 0:00:52\n",
            "epoch [25/100] batch [1/1] time 0.719 (0.719) data 0.418 (0.418) loss 1.3834 (1.3834) acc 71.8750 (71.8750) lr 1.7290e+01 eta 0:00:53\n",
            "epoch [26/100] batch [1/1] time 0.728 (0.728) data 0.425 (0.425) loss 1.0835 (1.0835) acc 78.1250 (78.1250) lr 1.7071e+01 eta 0:00:53\n",
            "epoch [27/100] batch [1/1] time 0.707 (0.707) data 0.406 (0.406) loss 1.1831 (1.1831) acc 81.2500 (81.2500) lr 1.6845e+01 eta 0:00:51\n",
            "epoch [28/100] batch [1/1] time 0.729 (0.729) data 0.427 (0.427) loss 1.0759 (1.0759) acc 78.1250 (78.1250) lr 1.6613e+01 eta 0:00:52\n",
            "epoch [29/100] batch [1/1] time 0.758 (0.758) data 0.454 (0.454) loss 0.9703 (0.9703) acc 81.2500 (81.2500) lr 1.6374e+01 eta 0:00:53\n",
            "epoch [30/100] batch [1/1] time 0.738 (0.738) data 0.433 (0.433) loss 1.1794 (1.1794) acc 65.6250 (65.6250) lr 1.6129e+01 eta 0:00:51\n",
            "epoch [31/100] batch [1/1] time 0.743 (0.743) data 0.442 (0.442) loss 0.9978 (0.9978) acc 84.3750 (84.3750) lr 1.5878e+01 eta 0:00:51\n",
            "epoch [32/100] batch [1/1] time 0.717 (0.717) data 0.414 (0.414) loss 1.0594 (1.0594) acc 81.2500 (81.2500) lr 1.5621e+01 eta 0:00:48\n",
            "epoch [33/100] batch [1/1] time 0.737 (0.737) data 0.432 (0.432) loss 1.4906 (1.4906) acc 62.5000 (62.5000) lr 1.5358e+01 eta 0:00:49\n",
            "epoch [34/100] batch [1/1] time 0.735 (0.735) data 0.428 (0.428) loss 1.3786 (1.3786) acc 65.6250 (65.6250) lr 1.5090e+01 eta 0:00:48\n",
            "epoch [35/100] batch [1/1] time 0.702 (0.702) data 0.398 (0.398) loss 1.2701 (1.2701) acc 68.7500 (68.7500) lr 1.4818e+01 eta 0:00:45\n",
            "epoch [36/100] batch [1/1] time 0.740 (0.740) data 0.438 (0.438) loss 1.0547 (1.0547) acc 81.2500 (81.2500) lr 1.4540e+01 eta 0:00:47\n",
            "epoch [37/100] batch [1/1] time 0.720 (0.720) data 0.416 (0.416) loss 2.4271 (2.4271) acc 53.1250 (53.1250) lr 1.4258e+01 eta 0:00:45\n",
            "epoch [38/100] batch [1/1] time 0.727 (0.727) data 0.421 (0.421) loss 4.6613 (4.6613) acc 34.3750 (34.3750) lr 1.3971e+01 eta 0:00:45\n",
            "epoch [39/100] batch [1/1] time 0.737 (0.737) data 0.431 (0.431) loss 4.0625 (4.0625) acc 31.2500 (31.2500) lr 1.3681e+01 eta 0:00:44\n",
            "epoch [40/100] batch [1/1] time 0.684 (0.684) data 0.378 (0.378) loss 5.9954 (5.9954) acc 37.5000 (37.5000) lr 1.3387e+01 eta 0:00:41\n",
            "epoch [41/100] batch [1/1] time 0.712 (0.712) data 0.408 (0.408) loss 4.4519 (4.4519) acc 28.1250 (28.1250) lr 1.3090e+01 eta 0:00:42\n",
            "epoch [42/100] batch [1/1] time 0.705 (0.705) data 0.399 (0.399) loss 6.3481 (6.3481) acc 21.8750 (21.8750) lr 1.2790e+01 eta 0:00:40\n",
            "epoch [43/100] batch [1/1] time 0.751 (0.751) data 0.444 (0.444) loss 9.0627 (9.0627) acc 12.5000 (12.5000) lr 1.2487e+01 eta 0:00:42\n",
            "epoch [44/100] batch [1/1] time 0.754 (0.754) data 0.447 (0.447) loss 7.7297 (7.7297) acc 12.5000 (12.5000) lr 1.2181e+01 eta 0:00:42\n",
            "epoch [45/100] batch [1/1] time 0.722 (0.722) data 0.415 (0.415) loss 5.4411 (5.4411) acc 34.3750 (34.3750) lr 1.1874e+01 eta 0:00:39\n",
            "epoch [46/100] batch [1/1] time 0.761 (0.761) data 0.454 (0.454) loss 6.1871 (6.1871) acc 31.2500 (31.2500) lr 1.1564e+01 eta 0:00:41\n",
            "epoch [47/100] batch [1/1] time 0.689 (0.689) data 0.384 (0.384) loss 6.5104 (6.5104) acc 31.2500 (31.2500) lr 1.1253e+01 eta 0:00:36\n",
            "epoch [48/100] batch [1/1] time 0.738 (0.738) data 0.432 (0.432) loss 4.0130 (4.0130) acc 53.1250 (53.1250) lr 1.0941e+01 eta 0:00:38\n",
            "epoch [49/100] batch [1/1] time 0.701 (0.701) data 0.392 (0.392) loss 2.6478 (2.6478) acc 46.8750 (46.8750) lr 1.0628e+01 eta 0:00:35\n",
            "epoch [50/100] batch [1/1] time 0.727 (0.727) data 0.411 (0.411) loss 1.6794 (1.6794) acc 50.0000 (50.0000) lr 1.0314e+01 eta 0:00:36\n",
            "epoch [51/100] batch [1/1] time 0.709 (0.709) data 0.399 (0.399) loss 1.7621 (1.7621) acc 59.3750 (59.3750) lr 1.0000e+01 eta 0:00:34\n",
            "epoch [52/100] batch [1/1] time 0.695 (0.695) data 0.385 (0.385) loss 1.6791 (1.6791) acc 56.2500 (56.2500) lr 9.6859e+00 eta 0:00:33\n",
            "epoch [53/100] batch [1/1] time 0.726 (0.726) data 0.418 (0.418) loss 1.6597 (1.6597) acc 50.0000 (50.0000) lr 9.3721e+00 eta 0:00:34\n",
            "epoch [54/100] batch [1/1] time 0.716 (0.716) data 0.407 (0.407) loss 1.3147 (1.3147) acc 62.5000 (62.5000) lr 9.0589e+00 eta 0:00:32\n",
            "epoch [55/100] batch [1/1] time 0.694 (0.694) data 0.384 (0.384) loss 1.2091 (1.2091) acc 62.5000 (62.5000) lr 8.7467e+00 eta 0:00:31\n",
            "epoch [56/100] batch [1/1] time 0.705 (0.705) data 0.396 (0.396) loss 1.2009 (1.2009) acc 75.0000 (75.0000) lr 8.4357e+00 eta 0:00:31\n",
            "epoch [57/100] batch [1/1] time 0.720 (0.720) data 0.413 (0.413) loss 1.0993 (1.0993) acc 81.2500 (81.2500) lr 8.1262e+00 eta 0:00:30\n",
            "epoch [58/100] batch [1/1] time 0.737 (0.737) data 0.427 (0.427) loss 1.0076 (1.0076) acc 75.0000 (75.0000) lr 7.8186e+00 eta 0:00:30\n",
            "epoch [59/100] batch [1/1] time 0.752 (0.752) data 0.442 (0.442) loss 1.0149 (1.0149) acc 78.1250 (78.1250) lr 7.5131e+00 eta 0:00:30\n",
            "epoch [60/100] batch [1/1] time 0.731 (0.731) data 0.422 (0.422) loss 0.8528 (0.8528) acc 87.5000 (87.5000) lr 7.2101e+00 eta 0:00:29\n",
            "epoch [61/100] batch [1/1] time 0.739 (0.739) data 0.429 (0.429) loss 0.8408 (0.8408) acc 78.1250 (78.1250) lr 6.9098e+00 eta 0:00:28\n",
            "epoch [62/100] batch [1/1] time 0.727 (0.727) data 0.416 (0.416) loss 0.8997 (0.8997) acc 84.3750 (84.3750) lr 6.6126e+00 eta 0:00:27\n",
            "epoch [63/100] batch [1/1] time 0.750 (0.750) data 0.442 (0.442) loss 0.7139 (0.7139) acc 84.3750 (84.3750) lr 6.3188e+00 eta 0:00:27\n",
            "epoch [64/100] batch [1/1] time 0.696 (0.696) data 0.388 (0.388) loss 0.7020 (0.7020) acc 87.5000 (87.5000) lr 6.0285e+00 eta 0:00:25\n",
            "epoch [65/100] batch [1/1] time 0.707 (0.707) data 0.397 (0.397) loss 0.5219 (0.5219) acc 96.8750 (96.8750) lr 5.7422e+00 eta 0:00:24\n",
            "epoch [66/100] batch [1/1] time 0.694 (0.694) data 0.386 (0.386) loss 0.5217 (0.5217) acc 87.5000 (87.5000) lr 5.4601e+00 eta 0:00:23\n",
            "epoch [67/100] batch [1/1] time 0.713 (0.713) data 0.405 (0.405) loss 0.6694 (0.6694) acc 87.5000 (87.5000) lr 5.1825e+00 eta 0:00:23\n",
            "epoch [68/100] batch [1/1] time 0.683 (0.683) data 0.376 (0.376) loss 0.6224 (0.6224) acc 87.5000 (87.5000) lr 4.9096e+00 eta 0:00:21\n",
            "epoch [69/100] batch [1/1] time 0.708 (0.708) data 0.402 (0.402) loss 0.6327 (0.6327) acc 87.5000 (87.5000) lr 4.6417e+00 eta 0:00:21\n",
            "epoch [70/100] batch [1/1] time 0.704 (0.704) data 0.395 (0.395) loss 0.7730 (0.7730) acc 87.5000 (87.5000) lr 4.3792e+00 eta 0:00:21\n",
            "epoch [71/100] batch [1/1] time 0.700 (0.700) data 0.394 (0.394) loss 0.4485 (0.4485) acc 96.8750 (96.8750) lr 4.1221e+00 eta 0:00:20\n",
            "epoch [72/100] batch [1/1] time 0.716 (0.716) data 0.409 (0.409) loss 0.4926 (0.4926) acc 96.8750 (96.8750) lr 3.8709e+00 eta 0:00:20\n",
            "epoch [73/100] batch [1/1] time 0.695 (0.695) data 0.385 (0.385) loss 0.4973 (0.4973) acc 90.6250 (90.6250) lr 3.6258e+00 eta 0:00:18\n",
            "epoch [74/100] batch [1/1] time 0.712 (0.712) data 0.407 (0.407) loss 0.5763 (0.5763) acc 90.6250 (90.6250) lr 3.3869e+00 eta 0:00:18\n",
            "epoch [75/100] batch [1/1] time 0.746 (0.746) data 0.439 (0.439) loss 0.4416 (0.4416) acc 96.8750 (96.8750) lr 3.1545e+00 eta 0:00:18\n",
            "epoch [76/100] batch [1/1] time 0.721 (0.721) data 0.415 (0.415) loss 0.8672 (0.8672) acc 87.5000 (87.5000) lr 2.9289e+00 eta 0:00:17\n",
            "epoch [77/100] batch [1/1] time 0.738 (0.738) data 0.431 (0.431) loss 0.9029 (0.9029) acc 75.0000 (75.0000) lr 2.7103e+00 eta 0:00:16\n",
            "epoch [78/100] batch [1/1] time 0.730 (0.730) data 0.426 (0.426) loss 0.5498 (0.5498) acc 90.6250 (90.6250) lr 2.4989e+00 eta 0:00:16\n",
            "epoch [79/100] batch [1/1] time 0.706 (0.706) data 0.401 (0.401) loss 0.4906 (0.4906) acc 93.7500 (93.7500) lr 2.2949e+00 eta 0:00:14\n",
            "epoch [80/100] batch [1/1] time 0.715 (0.715) data 0.414 (0.414) loss 0.4442 (0.4442) acc 96.8750 (96.8750) lr 2.0984e+00 eta 0:00:14\n",
            "epoch [81/100] batch [1/1] time 0.715 (0.715) data 0.411 (0.411) loss 0.3734 (0.3734) acc 100.0000 (100.0000) lr 1.9098e+00 eta 0:00:13\n",
            "epoch [82/100] batch [1/1] time 0.712 (0.712) data 0.409 (0.409) loss 0.5834 (0.5834) acc 90.6250 (90.6250) lr 1.7292e+00 eta 0:00:12\n",
            "epoch [83/100] batch [1/1] time 0.685 (0.685) data 0.382 (0.382) loss 0.6224 (0.6224) acc 90.6250 (90.6250) lr 1.5567e+00 eta 0:00:11\n",
            "epoch [84/100] batch [1/1] time 0.704 (0.704) data 0.400 (0.400) loss 0.5206 (0.5206) acc 96.8750 (96.8750) lr 1.3926e+00 eta 0:00:11\n",
            "epoch [85/100] batch [1/1] time 0.697 (0.697) data 0.393 (0.393) loss 0.5102 (0.5102) acc 87.5000 (87.5000) lr 1.2369e+00 eta 0:00:10\n",
            "epoch [86/100] batch [1/1] time 0.688 (0.688) data 0.384 (0.384) loss 0.5350 (0.5350) acc 96.8750 (96.8750) lr 1.0899e+00 eta 0:00:09\n",
            "epoch [87/100] batch [1/1] time 0.704 (0.704) data 0.403 (0.403) loss 0.4526 (0.4526) acc 93.7500 (93.7500) lr 9.5173e-01 eta 0:00:09\n",
            "epoch [88/100] batch [1/1] time 0.690 (0.690) data 0.385 (0.385) loss 0.5961 (0.5961) acc 87.5000 (87.5000) lr 8.2245e-01 eta 0:00:08\n",
            "epoch [89/100] batch [1/1] time 0.727 (0.727) data 0.424 (0.424) loss 0.4967 (0.4967) acc 90.6250 (90.6250) lr 7.0224e-01 eta 0:00:07\n",
            "epoch [90/100] batch [1/1] time 0.708 (0.708) data 0.406 (0.406) loss 0.4391 (0.4391) acc 96.8750 (96.8750) lr 5.9119e-01 eta 0:00:07\n",
            "epoch [91/100] batch [1/1] time 0.718 (0.718) data 0.413 (0.413) loss 0.5837 (0.5837) acc 90.6250 (90.6250) lr 4.8943e-01 eta 0:00:06\n",
            "epoch [92/100] batch [1/1] time 0.729 (0.729) data 0.428 (0.428) loss 0.3810 (0.3810) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:05\n",
            "epoch [93/100] batch [1/1] time 0.719 (0.719) data 0.417 (0.417) loss 0.4285 (0.4285) acc 93.7500 (93.7500) lr 3.1417e-01 eta 0:00:05\n",
            "epoch [94/100] batch [1/1] time 0.739 (0.739) data 0.438 (0.438) loss 0.3675 (0.3675) acc 96.8750 (96.8750) lr 2.4083e-01 eta 0:00:04\n",
            "epoch [95/100] batch [1/1] time 0.678 (0.678) data 0.375 (0.375) loss 0.3672 (0.3672) acc 93.7500 (93.7500) lr 1.7713e-01 eta 0:00:03\n",
            "epoch [96/100] batch [1/1] time 0.705 (0.705) data 0.405 (0.405) loss 0.4124 (0.4124) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.708 (0.708) data 0.407 (0.407) loss 0.4044 (0.4044) acc 93.7500 (93.7500) lr 7.8853e-02 eta 0:00:02\n",
            "epoch [98/100] batch [1/1] time 0.676 (0.676) data 0.376 (0.376) loss 0.3898 (0.3898) acc 96.8750 (96.8750) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.714 (0.714) data 0.413 (0.413) loss 0.4035 (0.4035) acc 96.8750 (96.8750) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.693 (0.693) data 0.390 (0.390) loss 0.4298 (0.4298) acc 93.7500 (93.7500) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed2/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.51it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 5,436\n",
            "* accuracy: 67.1%\n",
            "* error: 32.9%\n",
            "* macro_f1: 66.6%\n",
            "Elapsed: 0:02:16\n"
          ]
        }
      ],
      "source": [
        "#eurosat-4shots-seed2\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 2 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed2 \\\n",
        "        DATASET.NUM_SHOTS 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a0d588-ab16-499d-f6c1-a2e0f15b553f",
        "id": "41h9znYmiUYf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 10:06:53.513616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 10:06:53.533590: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 10:06:53.539538: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 10:06:53.559286: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 10:06:54.550555: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4']\n",
            "output_dir: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed3\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_4-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  40\n",
            "# val      40\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0187, -0.5727,  0.1683,  ...,  0.4947,  0.4188,  0.0344],\n",
            "        [ 0.0173, -0.5677,  0.1739,  ...,  0.4990,  0.4190,  0.0388],\n",
            "        [ 0.0094, -0.5649,  0.1758,  ...,  0.4946,  0.4157,  0.0405],\n",
            "        ...,\n",
            "        [ 0.0227, -0.5676,  0.1747,  ...,  0.5071,  0.4204,  0.0413],\n",
            "        [ 0.0083, -0.5735,  0.1744,  ...,  0.5016,  0.4249,  0.0364],\n",
            "        [ 0.0210, -0.5687,  0.1704,  ...,  0.5030,  0.4152,  0.0323]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2340\n",
            "  Min: -1.5598\n",
            "  Max: 1.0156\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-1.2399e-04, -9.3844e-03, -2.1417e-02,  ..., -1.0112e-02,\n",
            "          -7.3685e-04,  7.9737e-04],\n",
            "         [ 8.8539e-03,  1.7411e-03, -1.0068e-02,  ..., -2.1264e-03,\n",
            "           5.1197e-03,  2.2050e-03],\n",
            "         [ 7.9726e-04,  2.6865e-04, -1.0991e-02,  ...,  1.9916e-03,\n",
            "           1.7770e-03,  6.4784e-03],\n",
            "         ...,\n",
            "         [ 5.0354e-03, -1.5631e-02, -2.0003e-02,  ..., -2.2618e-03,\n",
            "           3.1675e-03,  5.5781e-03],\n",
            "         [ 6.0940e-03, -7.5148e-03, -1.6824e-02,  ...,  2.1022e-03,\n",
            "           3.5852e-03,  3.3437e-03],\n",
            "         [ 1.2073e-03, -6.1362e-03, -1.8186e-02,  ..., -4.9664e-03,\n",
            "           4.9890e-03,  1.4233e-03]],\n",
            "\n",
            "        [[-1.3523e-03, -1.5127e-02, -2.2592e-02,  ..., -5.6130e-03,\n",
            "           3.0075e-04,  3.5373e-03],\n",
            "         [ 4.7073e-03, -1.5986e-02, -1.9756e-02,  ..., -4.8596e-03,\n",
            "           1.4182e-02, -3.2796e-04],\n",
            "         [ 1.4543e-03, -1.4349e-02, -2.0663e-02,  ..., -2.6314e-03,\n",
            "           4.7735e-03,  3.8005e-03],\n",
            "         ...,\n",
            "         [ 5.5084e-03, -3.1951e-03, -2.0837e-02,  ..., -5.2220e-03,\n",
            "           1.9029e-03, -2.8714e-03],\n",
            "         [-9.3079e-04, -5.4381e-03, -1.6982e-02,  ..., -1.7034e-03,\n",
            "           8.4127e-03, -3.7145e-03],\n",
            "         [ 3.6659e-03, -6.9316e-03, -1.9074e-02,  ..., -3.4453e-03,\n",
            "           7.3618e-03,  1.5713e-03]],\n",
            "\n",
            "        [[ 7.0343e-03, -3.3286e-03, -1.6511e-02,  ..., -3.5879e-03,\n",
            "           3.0702e-03,  7.3672e-03],\n",
            "         [-3.5019e-03, -9.3458e-03, -1.7247e-02,  ..., -9.8149e-03,\n",
            "           7.4180e-03,  5.5667e-03],\n",
            "         [-8.9646e-04, -9.7735e-03, -2.4316e-02,  ..., -9.1714e-04,\n",
            "           4.4855e-03, -3.6191e-03],\n",
            "         ...,\n",
            "         [-1.0166e-03, -2.6763e-03, -1.1029e-02,  ..., -5.1801e-03,\n",
            "           9.8337e-03,  1.7180e-03],\n",
            "         [ 2.5787e-03, -5.2588e-03, -2.2912e-02,  ..., -2.0396e-03,\n",
            "           6.2809e-03,  5.9863e-03],\n",
            "         [-1.2093e-03, -4.2937e-03, -1.6879e-02,  ..., -6.3016e-03,\n",
            "           1.0322e-02,  8.1798e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 7.8773e-03, -6.2831e-03, -1.5435e-02,  ..., -1.2378e-02,\n",
            "           8.3345e-03,  1.2885e-04],\n",
            "         [-2.5597e-03, -4.3204e-03, -2.0193e-02,  ..., -7.2286e-03,\n",
            "           1.5033e-02, -2.8485e-03],\n",
            "         [ 2.8663e-03, -7.2692e-03, -2.1650e-02,  ...,  2.5752e-03,\n",
            "           1.9525e-03,  5.0193e-03],\n",
            "         ...,\n",
            "         [ 6.3744e-03, -5.5869e-03, -1.8911e-02,  ...,  1.5147e-03,\n",
            "           6.1778e-03,  4.1877e-03],\n",
            "         [ 8.5792e-03, -1.0136e-02, -1.9858e-02,  ..., -1.6343e-03,\n",
            "           5.2963e-04,  6.1351e-03],\n",
            "         [ 8.3656e-03, -7.1452e-03, -2.0655e-02,  ..., -3.2823e-03,\n",
            "           2.0555e-03, -3.2758e-03]],\n",
            "\n",
            "        [[-2.7657e-03, -1.8561e-03, -1.9243e-02,  ..., -3.1335e-03,\n",
            "           6.7562e-03, -2.2286e-03],\n",
            "         [ 4.5147e-03, -1.7149e-02, -1.8531e-02,  ..., -2.5607e-03,\n",
            "           3.1713e-03, -8.0594e-03],\n",
            "         [ 1.0693e-02, -9.7430e-03, -2.0959e-02,  ..., -7.5604e-03,\n",
            "           8.6532e-04,  3.2340e-03],\n",
            "         ...,\n",
            "         [ 2.0814e-03, -8.0746e-03, -1.5357e-02,  ...,  2.1275e-05,\n",
            "           8.1009e-03,  6.4860e-04],\n",
            "         [ 9.9144e-03, -6.3556e-03, -2.0875e-02,  ...,  5.8482e-03,\n",
            "           1.1089e-02,  2.2231e-03],\n",
            "         [ 8.1825e-03, -7.4852e-03, -3.4047e-02,  ..., -6.5774e-04,\n",
            "           7.2036e-04,  3.8672e-03]],\n",
            "\n",
            "        [[ 6.8245e-03, -1.1767e-02, -1.8973e-02,  ..., -1.8107e-03,\n",
            "           9.5724e-03, -2.2744e-03],\n",
            "         [ 5.8460e-03, -5.9150e-03, -1.6915e-02,  ..., -8.5256e-03,\n",
            "           5.7009e-03, -6.1889e-05],\n",
            "         [ 5.6381e-03, -7.6225e-03, -1.5910e-02,  ..., -1.9767e-03,\n",
            "           1.3919e-02,  6.1008e-03],\n",
            "         ...,\n",
            "         [ 1.6021e-04, -9.1894e-03, -1.8812e-02,  ..., -6.8700e-03,\n",
            "           1.3755e-02,  1.7334e-03],\n",
            "         [-3.1090e-03, -6.0218e-03, -1.4627e-02,  ..., -2.9032e-03,\n",
            "           1.0263e-02,  4.4375e-03],\n",
            "         [ 8.0604e-03, -2.4817e-03, -1.5781e-02,  ...,  7.8700e-03,\n",
            "           4.9604e-03,  1.0472e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4549\n",
            "  Max: 0.4739\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed3/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 1.750 (1.750) data 0.526 (0.526) loss 8.8885 (8.8885) acc 18.7500 (18.7500) lr 2.0000e+01 eta 0:02:53\n",
            "epoch [2/100] batch [1/1] time 0.728 (0.728) data 0.429 (0.429) loss 8.8766 (8.8766) acc 18.7500 (18.7500) lr 1.9995e+01 eta 0:01:11\n",
            "epoch [3/100] batch [1/1] time 0.693 (0.693) data 0.398 (0.398) loss 7.9712 (7.9712) acc 15.6250 (15.6250) lr 1.9980e+01 eta 0:01:07\n",
            "epoch [4/100] batch [1/1] time 0.721 (0.721) data 0.424 (0.424) loss 5.3387 (5.3387) acc 25.0000 (25.0000) lr 1.9956e+01 eta 0:01:09\n",
            "epoch [5/100] batch [1/1] time 0.691 (0.691) data 0.394 (0.394) loss 4.9999 (4.9999) acc 25.0000 (25.0000) lr 1.9921e+01 eta 0:01:05\n",
            "epoch [6/100] batch [1/1] time 0.721 (0.721) data 0.424 (0.424) loss 4.6299 (4.6299) acc 28.1250 (28.1250) lr 1.9877e+01 eta 0:01:07\n",
            "epoch [7/100] batch [1/1] time 0.690 (0.690) data 0.393 (0.393) loss 4.9050 (4.9050) acc 9.3750 (9.3750) lr 1.9823e+01 eta 0:01:04\n",
            "epoch [8/100] batch [1/1] time 0.698 (0.698) data 0.402 (0.402) loss 4.4623 (4.4623) acc 25.0000 (25.0000) lr 1.9759e+01 eta 0:01:04\n",
            "epoch [9/100] batch [1/1] time 0.746 (0.746) data 0.448 (0.448) loss 4.1732 (4.1732) acc 12.5000 (12.5000) lr 1.9686e+01 eta 0:01:07\n",
            "epoch [10/100] batch [1/1] time 0.692 (0.692) data 0.396 (0.396) loss 3.9023 (3.9023) acc 31.2500 (31.2500) lr 1.9603e+01 eta 0:01:02\n",
            "epoch [11/100] batch [1/1] time 0.724 (0.724) data 0.425 (0.425) loss 3.8328 (3.8328) acc 12.5000 (12.5000) lr 1.9511e+01 eta 0:01:04\n",
            "epoch [12/100] batch [1/1] time 0.720 (0.720) data 0.420 (0.420) loss 3.2917 (3.2917) acc 21.8750 (21.8750) lr 1.9409e+01 eta 0:01:03\n",
            "epoch [13/100] batch [1/1] time 0.691 (0.691) data 0.394 (0.394) loss 3.0596 (3.0596) acc 31.2500 (31.2500) lr 1.9298e+01 eta 0:01:00\n",
            "epoch [14/100] batch [1/1] time 0.699 (0.699) data 0.399 (0.399) loss 2.9333 (2.9333) acc 25.0000 (25.0000) lr 1.9178e+01 eta 0:01:00\n",
            "epoch [15/100] batch [1/1] time 0.676 (0.676) data 0.378 (0.378) loss 2.8650 (2.8650) acc 34.3750 (34.3750) lr 1.9048e+01 eta 0:00:57\n",
            "epoch [16/100] batch [1/1] time 0.698 (0.698) data 0.400 (0.400) loss 2.5115 (2.5115) acc 46.8750 (46.8750) lr 1.8910e+01 eta 0:00:58\n",
            "epoch [17/100] batch [1/1] time 0.689 (0.689) data 0.389 (0.389) loss 2.5333 (2.5333) acc 43.7500 (43.7500) lr 1.8763e+01 eta 0:00:57\n",
            "epoch [18/100] batch [1/1] time 0.731 (0.731) data 0.432 (0.432) loss 2.4961 (2.4961) acc 37.5000 (37.5000) lr 1.8607e+01 eta 0:00:59\n",
            "epoch [19/100] batch [1/1] time 0.712 (0.712) data 0.409 (0.409) loss 2.2012 (2.2012) acc 34.3750 (34.3750) lr 1.8443e+01 eta 0:00:57\n",
            "epoch [20/100] batch [1/1] time 0.718 (0.718) data 0.417 (0.417) loss 2.0690 (2.0690) acc 53.1250 (53.1250) lr 1.8271e+01 eta 0:00:57\n",
            "epoch [21/100] batch [1/1] time 0.702 (0.702) data 0.402 (0.402) loss 1.9627 (1.9627) acc 56.2500 (56.2500) lr 1.8090e+01 eta 0:00:55\n",
            "epoch [22/100] batch [1/1] time 0.687 (0.687) data 0.384 (0.384) loss 1.9597 (1.9597) acc 59.3750 (59.3750) lr 1.7902e+01 eta 0:00:53\n",
            "epoch [23/100] batch [1/1] time 0.719 (0.719) data 0.417 (0.417) loss 1.5834 (1.5834) acc 65.6250 (65.6250) lr 1.7705e+01 eta 0:00:55\n",
            "epoch [24/100] batch [1/1] time 0.711 (0.711) data 0.410 (0.410) loss 1.8953 (1.8953) acc 53.1250 (53.1250) lr 1.7501e+01 eta 0:00:54\n",
            "epoch [25/100] batch [1/1] time 0.694 (0.694) data 0.391 (0.391) loss 1.5522 (1.5522) acc 68.7500 (68.7500) lr 1.7290e+01 eta 0:00:52\n",
            "epoch [26/100] batch [1/1] time 0.723 (0.723) data 0.421 (0.421) loss 1.5132 (1.5132) acc 65.6250 (65.6250) lr 1.7071e+01 eta 0:00:53\n",
            "epoch [27/100] batch [1/1] time 0.701 (0.701) data 0.401 (0.401) loss 1.9367 (1.9367) acc 56.2500 (56.2500) lr 1.6845e+01 eta 0:00:51\n",
            "epoch [28/100] batch [1/1] time 0.719 (0.719) data 0.416 (0.416) loss 1.9517 (1.9517) acc 59.3750 (59.3750) lr 1.6613e+01 eta 0:00:51\n",
            "epoch [29/100] batch [1/1] time 0.706 (0.706) data 0.402 (0.402) loss 2.1863 (2.1863) acc 59.3750 (59.3750) lr 1.6374e+01 eta 0:00:50\n",
            "epoch [30/100] batch [1/1] time 0.682 (0.682) data 0.381 (0.381) loss 2.1788 (2.1788) acc 59.3750 (59.3750) lr 1.6129e+01 eta 0:00:47\n",
            "epoch [31/100] batch [1/1] time 0.733 (0.733) data 0.429 (0.429) loss 2.3691 (2.3691) acc 46.8750 (46.8750) lr 1.5878e+01 eta 0:00:50\n",
            "epoch [32/100] batch [1/1] time 0.699 (0.699) data 0.396 (0.396) loss 1.9206 (1.9206) acc 50.0000 (50.0000) lr 1.5621e+01 eta 0:00:47\n",
            "epoch [33/100] batch [1/1] time 0.728 (0.728) data 0.425 (0.425) loss 3.7292 (3.7292) acc 43.7500 (43.7500) lr 1.5358e+01 eta 0:00:48\n",
            "epoch [34/100] batch [1/1] time 0.689 (0.689) data 0.384 (0.384) loss 2.8130 (2.8130) acc 28.1250 (28.1250) lr 1.5090e+01 eta 0:00:45\n",
            "epoch [35/100] batch [1/1] time 0.697 (0.697) data 0.392 (0.392) loss 3.6780 (3.6780) acc 37.5000 (37.5000) lr 1.4818e+01 eta 0:00:45\n",
            "epoch [36/100] batch [1/1] time 0.708 (0.708) data 0.403 (0.403) loss 4.7859 (4.7859) acc 40.6250 (40.6250) lr 1.4540e+01 eta 0:00:45\n",
            "epoch [37/100] batch [1/1] time 0.680 (0.680) data 0.377 (0.377) loss 3.0663 (3.0663) acc 50.0000 (50.0000) lr 1.4258e+01 eta 0:00:42\n",
            "epoch [38/100] batch [1/1] time 0.693 (0.693) data 0.388 (0.388) loss 3.8940 (3.8940) acc 43.7500 (43.7500) lr 1.3971e+01 eta 0:00:42\n",
            "epoch [39/100] batch [1/1] time 0.700 (0.700) data 0.396 (0.396) loss 5.9163 (5.9163) acc 46.8750 (46.8750) lr 1.3681e+01 eta 0:00:42\n",
            "epoch [40/100] batch [1/1] time 0.731 (0.731) data 0.425 (0.425) loss 6.7180 (6.7180) acc 31.2500 (31.2500) lr 1.3387e+01 eta 0:00:43\n",
            "epoch [41/100] batch [1/1] time 0.746 (0.746) data 0.438 (0.438) loss 5.6570 (5.6570) acc 28.1250 (28.1250) lr 1.3090e+01 eta 0:00:44\n",
            "epoch [42/100] batch [1/1] time 0.727 (0.727) data 0.422 (0.422) loss 3.6924 (3.6924) acc 37.5000 (37.5000) lr 1.2790e+01 eta 0:00:42\n",
            "epoch [43/100] batch [1/1] time 0.740 (0.740) data 0.434 (0.434) loss 2.6192 (2.6192) acc 31.2500 (31.2500) lr 1.2487e+01 eta 0:00:42\n",
            "epoch [44/100] batch [1/1] time 0.723 (0.723) data 0.418 (0.418) loss 1.8917 (1.8917) acc 59.3750 (59.3750) lr 1.2181e+01 eta 0:00:40\n",
            "epoch [45/100] batch [1/1] time 0.714 (0.714) data 0.407 (0.407) loss 2.7002 (2.7002) acc 56.2500 (56.2500) lr 1.1874e+01 eta 0:00:39\n",
            "epoch [46/100] batch [1/1] time 0.694 (0.694) data 0.388 (0.388) loss 1.8519 (1.8519) acc 62.5000 (62.5000) lr 1.1564e+01 eta 0:00:37\n",
            "epoch [47/100] batch [1/1] time 0.691 (0.691) data 0.384 (0.384) loss 1.5508 (1.5508) acc 65.6250 (65.6250) lr 1.1253e+01 eta 0:00:36\n",
            "epoch [48/100] batch [1/1] time 0.692 (0.692) data 0.411 (0.411) loss 1.1047 (1.1047) acc 71.8750 (71.8750) lr 1.0941e+01 eta 0:00:35\n",
            "epoch [49/100] batch [1/1] time 0.716 (0.716) data 0.408 (0.408) loss 0.9573 (0.9573) acc 84.3750 (84.3750) lr 1.0628e+01 eta 0:00:36\n",
            "epoch [50/100] batch [1/1] time 0.705 (0.705) data 0.398 (0.398) loss 0.8901 (0.8901) acc 81.2500 (81.2500) lr 1.0314e+01 eta 0:00:35\n",
            "epoch [51/100] batch [1/1] time 0.693 (0.693) data 0.386 (0.386) loss 1.1184 (1.1184) acc 78.1250 (78.1250) lr 1.0000e+01 eta 0:00:33\n",
            "epoch [52/100] batch [1/1] time 0.708 (0.708) data 0.400 (0.400) loss 1.0104 (1.0104) acc 84.3750 (84.3750) lr 9.6859e+00 eta 0:00:33\n",
            "epoch [53/100] batch [1/1] time 0.724 (0.724) data 0.416 (0.416) loss 1.1544 (1.1544) acc 75.0000 (75.0000) lr 9.3721e+00 eta 0:00:34\n",
            "epoch [54/100] batch [1/1] time 0.705 (0.705) data 0.395 (0.395) loss 0.7163 (0.7163) acc 90.6250 (90.6250) lr 9.0589e+00 eta 0:00:32\n",
            "epoch [55/100] batch [1/1] time 0.716 (0.716) data 0.408 (0.408) loss 1.0599 (1.0599) acc 75.0000 (75.0000) lr 8.7467e+00 eta 0:00:32\n",
            "epoch [56/100] batch [1/1] time 0.693 (0.693) data 0.385 (0.385) loss 0.7190 (0.7190) acc 84.3750 (84.3750) lr 8.4357e+00 eta 0:00:30\n",
            "epoch [57/100] batch [1/1] time 0.716 (0.716) data 0.407 (0.407) loss 0.9424 (0.9424) acc 78.1250 (78.1250) lr 8.1262e+00 eta 0:00:30\n",
            "epoch [58/100] batch [1/1] time 0.731 (0.731) data 0.421 (0.421) loss 0.7649 (0.7649) acc 90.6250 (90.6250) lr 7.8186e+00 eta 0:00:30\n",
            "epoch [59/100] batch [1/1] time 0.716 (0.716) data 0.409 (0.409) loss 0.5663 (0.5663) acc 93.7500 (93.7500) lr 7.5131e+00 eta 0:00:29\n",
            "epoch [60/100] batch [1/1] time 0.744 (0.744) data 0.437 (0.437) loss 0.7796 (0.7796) acc 84.3750 (84.3750) lr 7.2101e+00 eta 0:00:29\n",
            "epoch [61/100] batch [1/1] time 0.708 (0.708) data 0.400 (0.400) loss 0.5554 (0.5554) acc 87.5000 (87.5000) lr 6.9098e+00 eta 0:00:27\n",
            "epoch [62/100] batch [1/1] time 0.716 (0.716) data 0.406 (0.406) loss 0.8861 (0.8861) acc 75.0000 (75.0000) lr 6.6126e+00 eta 0:00:27\n",
            "epoch [63/100] batch [1/1] time 0.724 (0.724) data 0.412 (0.412) loss 0.6863 (0.6863) acc 84.3750 (84.3750) lr 6.3188e+00 eta 0:00:26\n",
            "epoch [64/100] batch [1/1] time 0.675 (0.675) data 0.369 (0.369) loss 0.7031 (0.7031) acc 87.5000 (87.5000) lr 6.0285e+00 eta 0:00:24\n",
            "epoch [65/100] batch [1/1] time 0.735 (0.735) data 0.426 (0.426) loss 0.6575 (0.6575) acc 90.6250 (90.6250) lr 5.7422e+00 eta 0:00:25\n",
            "epoch [66/100] batch [1/1] time 0.701 (0.701) data 0.393 (0.393) loss 0.5838 (0.5838) acc 90.6250 (90.6250) lr 5.4601e+00 eta 0:00:23\n",
            "epoch [67/100] batch [1/1] time 0.721 (0.721) data 0.416 (0.416) loss 0.5289 (0.5289) acc 93.7500 (93.7500) lr 5.1825e+00 eta 0:00:23\n",
            "epoch [68/100] batch [1/1] time 0.678 (0.678) data 0.374 (0.374) loss 0.4168 (0.4168) acc 96.8750 (96.8750) lr 4.9096e+00 eta 0:00:21\n",
            "epoch [69/100] batch [1/1] time 0.694 (0.694) data 0.386 (0.386) loss 0.4801 (0.4801) acc 93.7500 (93.7500) lr 4.6417e+00 eta 0:00:21\n",
            "epoch [70/100] batch [1/1] time 0.705 (0.705) data 0.399 (0.399) loss 0.4373 (0.4373) acc 96.8750 (96.8750) lr 4.3792e+00 eta 0:00:21\n",
            "epoch [71/100] batch [1/1] time 0.684 (0.684) data 0.378 (0.378) loss 0.7515 (0.7515) acc 90.6250 (90.6250) lr 4.1221e+00 eta 0:00:19\n",
            "epoch [72/100] batch [1/1] time 0.705 (0.705) data 0.398 (0.398) loss 0.5666 (0.5666) acc 93.7500 (93.7500) lr 3.8709e+00 eta 0:00:19\n",
            "epoch [73/100] batch [1/1] time 0.715 (0.715) data 0.410 (0.410) loss 0.6136 (0.6136) acc 93.7500 (93.7500) lr 3.6258e+00 eta 0:00:19\n",
            "epoch [74/100] batch [1/1] time 0.743 (0.743) data 0.439 (0.439) loss 0.5703 (0.5703) acc 93.7500 (93.7500) lr 3.3869e+00 eta 0:00:19\n",
            "epoch [75/100] batch [1/1] time 0.743 (0.743) data 0.433 (0.433) loss 0.5019 (0.5019) acc 93.7500 (93.7500) lr 3.1545e+00 eta 0:00:18\n",
            "epoch [76/100] batch [1/1] time 0.727 (0.727) data 0.420 (0.420) loss 0.5150 (0.5150) acc 93.7500 (93.7500) lr 2.9289e+00 eta 0:00:17\n",
            "epoch [77/100] batch [1/1] time 0.705 (0.705) data 0.399 (0.399) loss 0.4248 (0.4248) acc 96.8750 (96.8750) lr 2.7103e+00 eta 0:00:16\n",
            "epoch [78/100] batch [1/1] time 0.716 (0.716) data 0.412 (0.412) loss 0.3425 (0.3425) acc 100.0000 (100.0000) lr 2.4989e+00 eta 0:00:15\n",
            "epoch [79/100] batch [1/1] time 0.709 (0.709) data 0.404 (0.404) loss 0.7574 (0.7574) acc 87.5000 (87.5000) lr 2.2949e+00 eta 0:00:14\n",
            "epoch [80/100] batch [1/1] time 0.703 (0.703) data 0.399 (0.399) loss 0.3938 (0.3938) acc 96.8750 (96.8750) lr 2.0984e+00 eta 0:00:14\n",
            "epoch [81/100] batch [1/1] time 0.706 (0.706) data 0.399 (0.399) loss 0.4382 (0.4382) acc 96.8750 (96.8750) lr 1.9098e+00 eta 0:00:13\n",
            "epoch [82/100] batch [1/1] time 0.722 (0.722) data 0.419 (0.419) loss 0.4785 (0.4785) acc 93.7500 (93.7500) lr 1.7292e+00 eta 0:00:13\n",
            "epoch [83/100] batch [1/1] time 0.694 (0.694) data 0.391 (0.391) loss 0.6819 (0.6819) acc 87.5000 (87.5000) lr 1.5567e+00 eta 0:00:11\n",
            "epoch [84/100] batch [1/1] time 0.696 (0.696) data 0.393 (0.393) loss 0.3654 (0.3654) acc 96.8750 (96.8750) lr 1.3926e+00 eta 0:00:11\n",
            "epoch [85/100] batch [1/1] time 0.682 (0.682) data 0.376 (0.376) loss 0.3867 (0.3867) acc 96.8750 (96.8750) lr 1.2369e+00 eta 0:00:10\n",
            "epoch [86/100] batch [1/1] time 0.674 (0.674) data 0.370 (0.370) loss 0.3523 (0.3523) acc 100.0000 (100.0000) lr 1.0899e+00 eta 0:00:09\n",
            "epoch [87/100] batch [1/1] time 0.693 (0.693) data 0.389 (0.389) loss 0.3501 (0.3501) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:09\n",
            "epoch [88/100] batch [1/1] time 0.700 (0.700) data 0.396 (0.396) loss 0.4047 (0.4047) acc 96.8750 (96.8750) lr 8.2245e-01 eta 0:00:08\n",
            "epoch [89/100] batch [1/1] time 0.730 (0.730) data 0.428 (0.428) loss 0.3983 (0.3983) acc 96.8750 (96.8750) lr 7.0224e-01 eta 0:00:08\n",
            "epoch [90/100] batch [1/1] time 0.707 (0.707) data 0.405 (0.405) loss 0.3725 (0.3725) acc 96.8750 (96.8750) lr 5.9119e-01 eta 0:00:07\n",
            "epoch [91/100] batch [1/1] time 0.727 (0.727) data 0.423 (0.423) loss 0.3351 (0.3351) acc 100.0000 (100.0000) lr 4.8943e-01 eta 0:00:06\n",
            "epoch [92/100] batch [1/1] time 0.723 (0.723) data 0.421 (0.421) loss 0.4934 (0.4934) acc 90.6250 (90.6250) lr 3.9706e-01 eta 0:00:05\n",
            "epoch [93/100] batch [1/1] time 0.692 (0.692) data 0.387 (0.387) loss 0.3600 (0.3600) acc 100.0000 (100.0000) lr 3.1417e-01 eta 0:00:04\n",
            "epoch [94/100] batch [1/1] time 0.690 (0.690) data 0.386 (0.386) loss 0.4773 (0.4773) acc 90.6250 (90.6250) lr 2.4083e-01 eta 0:00:04\n",
            "epoch [95/100] batch [1/1] time 0.689 (0.689) data 0.386 (0.386) loss 0.4321 (0.4321) acc 96.8750 (96.8750) lr 1.7713e-01 eta 0:00:03\n",
            "epoch [96/100] batch [1/1] time 0.691 (0.691) data 0.388 (0.388) loss 0.4064 (0.4064) acc 96.8750 (96.8750) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.714 (0.714) data 0.413 (0.413) loss 0.3959 (0.3959) acc 96.8750 (96.8750) lr 7.8853e-02 eta 0:00:02\n",
            "epoch [98/100] batch [1/1] time 0.686 (0.686) data 0.384 (0.384) loss 0.2975 (0.2975) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.715 (0.715) data 0.418 (0.418) loss 0.4296 (0.4296) acc 96.8750 (96.8750) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.676 (0.676) data 0.376 (0.376) loss 0.4782 (0.4782) acc 96.8750 (96.8750) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed3/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.52it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 6,297\n",
            "* accuracy: 77.7%\n",
            "* error: 22.3%\n",
            "* macro_f1: 77.3%\n",
            "Elapsed: 0:01:51\n"
          ]
        }
      ],
      "source": [
        "#eurosat-4shots-seed3\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 3 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_4shots/seed3 \\\n",
        "        DATASET.NUM_SHOTS 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1585731-0007-47dc-f341-b4a989f39aae",
        "id": "rYanlZ5RiUYf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 10:09:03.300919: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 10:09:03.320641: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 10:09:03.326510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 10:09:03.340585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 10:09:04.323311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2']\n",
            "output_dir: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  20\n",
            "# val      20\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "        [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "        [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "        ...,\n",
            "        [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "        [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "        [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-5.2644e-04, -1.1168e-02, -2.1661e-02,  ..., -4.8596e-03,\n",
            "           5.8912e-03,  6.7493e-03],\n",
            "         [ 5.8841e-04, -1.8584e-02, -2.5060e-02,  ..., -3.4987e-03,\n",
            "           5.4105e-03, -1.7861e-03],\n",
            "         [ 1.7309e-03, -7.2606e-03, -2.3481e-02,  ..., -2.1107e-03,\n",
            "           4.9070e-03,  6.0702e-03],\n",
            "         ...,\n",
            "         [ 1.0223e-03, -1.2080e-02, -1.5868e-02,  ..., -1.2882e-02,\n",
            "          -2.0567e-03,  4.8362e-03],\n",
            "         [ 7.8468e-03, -1.1038e-02, -1.0770e-02,  ...,  7.4988e-04,\n",
            "           1.5215e-03, -1.6774e-03],\n",
            "         [ 2.3028e-03, -5.9798e-03, -1.7329e-02,  ...,  5.5724e-04,\n",
            "           1.5857e-02,  4.3269e-03]],\n",
            "\n",
            "        [[ 6.0997e-03,  1.7106e-03, -2.1188e-02,  ...,  2.5523e-03,\n",
            "           2.1776e-03,  7.7525e-03],\n",
            "         [ 6.9275e-03, -6.8095e-03, -1.3799e-02,  ..., -3.6580e-03,\n",
            "           5.6513e-03,  1.9980e-03],\n",
            "         [ 6.2942e-04, -7.5124e-03, -9.4807e-03,  ...,  9.2128e-03,\n",
            "           1.0227e-02,  3.9989e-03],\n",
            "         ...,\n",
            "         [-8.9265e-04, -1.5108e-02, -1.9400e-02,  ..., -3.9164e-03,\n",
            "           4.4435e-03, -3.3369e-04],\n",
            "         [ 9.0713e-03, -1.0043e-02, -2.8737e-02,  ..., -4.9359e-03,\n",
            "           9.1699e-03,  5.4980e-03],\n",
            "         [-8.9653e-05, -7.9620e-03, -2.2422e-02,  ..., -2.5861e-03,\n",
            "           2.1928e-03,  6.9095e-03]],\n",
            "\n",
            "        [[ 2.2506e-04, -1.2583e-02, -1.8306e-02,  ..., -1.1051e-02,\n",
            "           9.2462e-03,  8.1454e-03],\n",
            "         [-9.4987e-04, -1.2930e-02, -2.0111e-02,  ..., -9.7730e-03,\n",
            "           5.3495e-03,  1.8893e-03],\n",
            "         [-2.7428e-03, -8.8699e-03, -1.4280e-02,  ...,  5.0471e-03,\n",
            "           5.3924e-03,  1.0839e-02],\n",
            "         ...,\n",
            "         [ 7.2136e-03, -9.1107e-03, -1.5424e-02,  ...,  1.9115e-03,\n",
            "           6.7415e-03, -8.7823e-04],\n",
            "         [-4.8866e-03, -9.1675e-03, -1.5351e-02,  ..., -5.0427e-03,\n",
            "           8.2830e-03,  1.1029e-02],\n",
            "         [-4.4785e-03, -1.2034e-02, -1.6046e-02,  ..., -1.3892e-03,\n",
            "           1.1470e-02, -4.6686e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.1645e-03,  2.2294e-03, -2.3786e-02,  ..., -2.2859e-04,\n",
            "           6.9243e-03,  8.3819e-03],\n",
            "         [ 4.7722e-03, -5.7014e-03, -1.7818e-02,  ..., -5.6092e-03,\n",
            "           1.0013e-02,  1.3503e-03],\n",
            "         [-2.0599e-03, -4.0076e-03, -2.7204e-02,  ...,  2.8766e-03,\n",
            "           1.4182e-02, -4.1284e-04],\n",
            "         ...,\n",
            "         [-2.6131e-03, -1.2682e-02, -1.9781e-02,  ...,  2.6591e-03,\n",
            "           5.9818e-03,  1.0518e-02],\n",
            "         [ 5.3825e-03, -7.2549e-03, -1.3387e-02,  ..., -5.7954e-04,\n",
            "           5.6404e-03,  2.1621e-03],\n",
            "         [ 1.1158e-03,  5.8145e-04, -2.5354e-02,  ...,  1.2668e-03,\n",
            "           9.6658e-03,  5.6201e-03]],\n",
            "\n",
            "        [[ 5.7640e-03, -5.7109e-03, -1.9671e-02,  ..., -6.6449e-03,\n",
            "          -2.2398e-03,  3.5010e-04],\n",
            "         [ 4.0493e-03, -1.3674e-02, -2.2840e-02,  ..., -2.0630e-03,\n",
            "           1.2584e-02, -4.8818e-03],\n",
            "         [ 1.1436e-05, -3.8550e-03, -2.1955e-02,  ..., -3.6313e-03,\n",
            "           3.4708e-03,  2.7534e-03],\n",
            "         ...,\n",
            "         [ 7.8735e-03, -1.2430e-02, -2.0399e-02,  ..., -1.3462e-02,\n",
            "           1.2340e-02, -4.7330e-03],\n",
            "         [ 2.9120e-03, -1.2139e-02, -6.7417e-03,  ...,  4.5043e-04,\n",
            "           7.1866e-05,  9.1229e-04],\n",
            "         [-3.6163e-03, -4.2251e-03, -2.5861e-02,  ..., -6.7784e-03,\n",
            "           5.3829e-03, -2.0913e-03]],\n",
            "\n",
            "        [[ 1.0143e-02, -1.1706e-02, -2.0403e-02,  ..., -5.1286e-03,\n",
            "           9.2310e-03,  1.6630e-03],\n",
            "         [ 5.0354e-03, -6.9526e-03, -1.5557e-02,  ..., -3.4320e-03,\n",
            "           9.9329e-03,  5.0002e-03],\n",
            "         [ 3.1037e-03, -1.5677e-02, -2.5472e-02,  ..., -1.3558e-03,\n",
            "           2.2081e-03,  4.0732e-03],\n",
            "         ...,\n",
            "         [ 3.1457e-03, -4.8411e-03, -1.6359e-02,  ...,  3.0177e-03,\n",
            "           9.1890e-03, -4.2676e-03],\n",
            "         [ 5.7907e-03, -1.0981e-02, -2.1062e-02,  ..., -9.6700e-03,\n",
            "           4.3233e-03,  1.2432e-03],\n",
            "         [-5.3413e-05, -5.7033e-03, -2.5506e-02,  ..., -3.7009e-03,\n",
            "           7.3665e-03,  1.6910e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4741\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 15.301 (15.301) data 14.274 (14.274) loss 8.6731 (8.6731) acc 25.0000 (25.0000) lr 2.0000e+01 eta 0:25:14\n",
            "epoch [2/100] batch [1/1] time 0.543 (0.543) data 0.341 (0.341) loss 8.6867 (8.6867) acc 25.0000 (25.0000) lr 1.9995e+01 eta 0:00:53\n",
            "epoch [3/100] batch [1/1] time 0.538 (0.538) data 0.335 (0.335) loss 7.6638 (7.6638) acc 20.0000 (20.0000) lr 1.9980e+01 eta 0:00:52\n",
            "epoch [4/100] batch [1/1] time 0.537 (0.537) data 0.333 (0.333) loss 7.4821 (7.4821) acc 15.0000 (15.0000) lr 1.9956e+01 eta 0:00:51\n",
            "epoch [5/100] batch [1/1] time 0.525 (0.525) data 0.322 (0.322) loss 10.8212 (10.8212) acc 10.0000 (10.0000) lr 1.9921e+01 eta 0:00:49\n",
            "epoch [6/100] batch [1/1] time 0.583 (0.583) data 0.379 (0.379) loss 7.2732 (7.2732) acc 15.0000 (15.0000) lr 1.9877e+01 eta 0:00:54\n",
            "epoch [7/100] batch [1/1] time 0.560 (0.560) data 0.352 (0.352) loss 5.9584 (5.9584) acc 5.0000 (5.0000) lr 1.9823e+01 eta 0:00:52\n",
            "epoch [8/100] batch [1/1] time 0.544 (0.544) data 0.340 (0.340) loss 4.4864 (4.4864) acc 15.0000 (15.0000) lr 1.9759e+01 eta 0:00:50\n",
            "epoch [9/100] batch [1/1] time 0.556 (0.556) data 0.349 (0.349) loss 3.8882 (3.8882) acc 25.0000 (25.0000) lr 1.9686e+01 eta 0:00:50\n",
            "epoch [10/100] batch [1/1] time 0.579 (0.579) data 0.375 (0.375) loss 3.5767 (3.5767) acc 20.0000 (20.0000) lr 1.9603e+01 eta 0:00:52\n",
            "epoch [11/100] batch [1/1] time 0.541 (0.541) data 0.338 (0.338) loss 3.5863 (3.5863) acc 30.0000 (30.0000) lr 1.9511e+01 eta 0:00:48\n",
            "epoch [12/100] batch [1/1] time 0.531 (0.531) data 0.326 (0.326) loss 3.3079 (3.3079) acc 35.0000 (35.0000) lr 1.9409e+01 eta 0:00:46\n",
            "epoch [13/100] batch [1/1] time 0.544 (0.544) data 0.339 (0.339) loss 2.6978 (2.6978) acc 45.0000 (45.0000) lr 1.9298e+01 eta 0:00:47\n",
            "epoch [14/100] batch [1/1] time 0.544 (0.544) data 0.341 (0.341) loss 2.6250 (2.6250) acc 40.0000 (40.0000) lr 1.9178e+01 eta 0:00:46\n",
            "epoch [15/100] batch [1/1] time 0.524 (0.524) data 0.319 (0.319) loss 2.5822 (2.5822) acc 45.0000 (45.0000) lr 1.9048e+01 eta 0:00:44\n",
            "epoch [16/100] batch [1/1] time 0.558 (0.558) data 0.352 (0.352) loss 2.6071 (2.6071) acc 50.0000 (50.0000) lr 1.8910e+01 eta 0:00:46\n",
            "epoch [17/100] batch [1/1] time 0.541 (0.541) data 0.335 (0.335) loss 2.6711 (2.6711) acc 35.0000 (35.0000) lr 1.8763e+01 eta 0:00:44\n",
            "epoch [18/100] batch [1/1] time 0.543 (0.543) data 0.337 (0.337) loss 2.2595 (2.2595) acc 40.0000 (40.0000) lr 1.8607e+01 eta 0:00:44\n",
            "epoch [19/100] batch [1/1] time 0.534 (0.534) data 0.326 (0.326) loss 2.0174 (2.0174) acc 60.0000 (60.0000) lr 1.8443e+01 eta 0:00:43\n",
            "epoch [20/100] batch [1/1] time 0.532 (0.532) data 0.327 (0.327) loss 2.0178 (2.0178) acc 60.0000 (60.0000) lr 1.8271e+01 eta 0:00:42\n",
            "epoch [21/100] batch [1/1] time 0.546 (0.546) data 0.338 (0.338) loss 1.7674 (1.7674) acc 65.0000 (65.0000) lr 1.8090e+01 eta 0:00:43\n",
            "epoch [22/100] batch [1/1] time 0.536 (0.536) data 0.328 (0.328) loss 1.6323 (1.6323) acc 70.0000 (70.0000) lr 1.7902e+01 eta 0:00:41\n",
            "epoch [23/100] batch [1/1] time 0.550 (0.550) data 0.341 (0.341) loss 1.2868 (1.2868) acc 85.0000 (85.0000) lr 1.7705e+01 eta 0:00:42\n",
            "epoch [24/100] batch [1/1] time 0.531 (0.531) data 0.325 (0.325) loss 1.2727 (1.2727) acc 80.0000 (80.0000) lr 1.7501e+01 eta 0:00:40\n",
            "epoch [25/100] batch [1/1] time 0.527 (0.527) data 0.318 (0.318) loss 1.4015 (1.4015) acc 70.0000 (70.0000) lr 1.7290e+01 eta 0:00:39\n",
            "epoch [26/100] batch [1/1] time 0.541 (0.541) data 0.333 (0.333) loss 1.2810 (1.2810) acc 85.0000 (85.0000) lr 1.7071e+01 eta 0:00:40\n",
            "epoch [27/100] batch [1/1] time 0.554 (0.554) data 0.345 (0.345) loss 1.2994 (1.2994) acc 75.0000 (75.0000) lr 1.6845e+01 eta 0:00:40\n",
            "epoch [28/100] batch [1/1] time 0.590 (0.590) data 0.383 (0.383) loss 1.4486 (1.4486) acc 75.0000 (75.0000) lr 1.6613e+01 eta 0:00:42\n",
            "epoch [29/100] batch [1/1] time 0.569 (0.569) data 0.362 (0.362) loss 1.0086 (1.0086) acc 80.0000 (80.0000) lr 1.6374e+01 eta 0:00:40\n",
            "epoch [30/100] batch [1/1] time 0.578 (0.578) data 0.373 (0.373) loss 1.0759 (1.0759) acc 75.0000 (75.0000) lr 1.6129e+01 eta 0:00:40\n",
            "epoch [31/100] batch [1/1] time 0.534 (0.534) data 0.326 (0.326) loss 0.8426 (0.8426) acc 90.0000 (90.0000) lr 1.5878e+01 eta 0:00:36\n",
            "epoch [32/100] batch [1/1] time 0.537 (0.537) data 0.329 (0.329) loss 0.7834 (0.7834) acc 90.0000 (90.0000) lr 1.5621e+01 eta 0:00:36\n",
            "epoch [33/100] batch [1/1] time 0.549 (0.549) data 0.343 (0.343) loss 0.8661 (0.8661) acc 90.0000 (90.0000) lr 1.5358e+01 eta 0:00:36\n",
            "epoch [34/100] batch [1/1] time 0.557 (0.557) data 0.350 (0.350) loss 0.5611 (0.5611) acc 95.0000 (95.0000) lr 1.5090e+01 eta 0:00:36\n",
            "epoch [35/100] batch [1/1] time 0.543 (0.543) data 0.335 (0.335) loss 1.1283 (1.1283) acc 85.0000 (85.0000) lr 1.4818e+01 eta 0:00:35\n",
            "epoch [36/100] batch [1/1] time 0.539 (0.539) data 0.331 (0.331) loss 1.3861 (1.3861) acc 75.0000 (75.0000) lr 1.4540e+01 eta 0:00:34\n",
            "epoch [37/100] batch [1/1] time 0.533 (0.533) data 0.326 (0.326) loss 2.1706 (2.1706) acc 65.0000 (65.0000) lr 1.4258e+01 eta 0:00:33\n",
            "epoch [38/100] batch [1/1] time 0.551 (0.551) data 0.344 (0.344) loss 1.1967 (1.1967) acc 75.0000 (75.0000) lr 1.3971e+01 eta 0:00:34\n",
            "epoch [39/100] batch [1/1] time 0.537 (0.537) data 0.331 (0.331) loss 7.1445 (7.1445) acc 25.0000 (25.0000) lr 1.3681e+01 eta 0:00:32\n",
            "epoch [40/100] batch [1/1] time 0.549 (0.549) data 0.338 (0.338) loss 3.0719 (3.0719) acc 45.0000 (45.0000) lr 1.3387e+01 eta 0:00:32\n",
            "epoch [41/100] batch [1/1] time 0.558 (0.558) data 0.350 (0.350) loss 3.1596 (3.1596) acc 45.0000 (45.0000) lr 1.3090e+01 eta 0:00:32\n",
            "epoch [42/100] batch [1/1] time 0.545 (0.545) data 0.337 (0.337) loss 4.0691 (4.0691) acc 40.0000 (40.0000) lr 1.2790e+01 eta 0:00:31\n",
            "epoch [43/100] batch [1/1] time 0.557 (0.557) data 0.349 (0.349) loss 4.3801 (4.3801) acc 45.0000 (45.0000) lr 1.2487e+01 eta 0:00:31\n",
            "epoch [44/100] batch [1/1] time 0.541 (0.541) data 0.334 (0.334) loss 3.6386 (3.6386) acc 45.0000 (45.0000) lr 1.2181e+01 eta 0:00:30\n",
            "epoch [45/100] batch [1/1] time 0.545 (0.545) data 0.341 (0.341) loss 1.9815 (1.9815) acc 65.0000 (65.0000) lr 1.1874e+01 eta 0:00:29\n",
            "epoch [46/100] batch [1/1] time 0.548 (0.548) data 0.338 (0.338) loss 1.6968 (1.6968) acc 60.0000 (60.0000) lr 1.1564e+01 eta 0:00:29\n",
            "epoch [47/100] batch [1/1] time 0.567 (0.567) data 0.357 (0.357) loss 2.0009 (2.0009) acc 65.0000 (65.0000) lr 1.1253e+01 eta 0:00:30\n",
            "epoch [48/100] batch [1/1] time 0.566 (0.566) data 0.356 (0.356) loss 1.0045 (1.0045) acc 75.0000 (75.0000) lr 1.0941e+01 eta 0:00:29\n",
            "epoch [49/100] batch [1/1] time 0.569 (0.569) data 0.357 (0.357) loss 0.8580 (0.8580) acc 85.0000 (85.0000) lr 1.0628e+01 eta 0:00:29\n",
            "epoch [50/100] batch [1/1] time 0.580 (0.580) data 0.372 (0.372) loss 0.6734 (0.6734) acc 90.0000 (90.0000) lr 1.0314e+01 eta 0:00:29\n",
            "epoch [51/100] batch [1/1] time 0.530 (0.530) data 0.323 (0.323) loss 1.0679 (1.0679) acc 70.0000 (70.0000) lr 1.0000e+01 eta 0:00:25\n",
            "epoch [52/100] batch [1/1] time 0.559 (0.559) data 0.348 (0.348) loss 1.1043 (1.1043) acc 70.0000 (70.0000) lr 9.6859e+00 eta 0:00:26\n",
            "epoch [53/100] batch [1/1] time 0.529 (0.529) data 0.320 (0.320) loss 0.7561 (0.7561) acc 85.0000 (85.0000) lr 9.3721e+00 eta 0:00:24\n",
            "epoch [54/100] batch [1/1] time 0.536 (0.536) data 0.327 (0.327) loss 0.6237 (0.6237) acc 90.0000 (90.0000) lr 9.0589e+00 eta 0:00:24\n",
            "epoch [55/100] batch [1/1] time 0.555 (0.555) data 0.344 (0.344) loss 0.6448 (0.6448) acc 90.0000 (90.0000) lr 8.7467e+00 eta 0:00:24\n",
            "epoch [56/100] batch [1/1] time 0.534 (0.534) data 0.325 (0.325) loss 0.6510 (0.6510) acc 90.0000 (90.0000) lr 8.4357e+00 eta 0:00:23\n",
            "epoch [57/100] batch [1/1] time 0.553 (0.553) data 0.345 (0.345) loss 0.5773 (0.5773) acc 90.0000 (90.0000) lr 8.1262e+00 eta 0:00:23\n",
            "epoch [58/100] batch [1/1] time 0.524 (0.524) data 0.316 (0.316) loss 0.4935 (0.4935) acc 90.0000 (90.0000) lr 7.8186e+00 eta 0:00:22\n",
            "epoch [59/100] batch [1/1] time 0.560 (0.560) data 0.352 (0.352) loss 0.6316 (0.6316) acc 95.0000 (95.0000) lr 7.5131e+00 eta 0:00:22\n",
            "epoch [60/100] batch [1/1] time 0.530 (0.530) data 0.320 (0.320) loss 0.4613 (0.4613) acc 95.0000 (95.0000) lr 7.2101e+00 eta 0:00:21\n",
            "epoch [61/100] batch [1/1] time 0.560 (0.560) data 0.348 (0.348) loss 0.3678 (0.3678) acc 100.0000 (100.0000) lr 6.9098e+00 eta 0:00:21\n",
            "epoch [62/100] batch [1/1] time 0.539 (0.539) data 0.328 (0.328) loss 0.4877 (0.4877) acc 95.0000 (95.0000) lr 6.6126e+00 eta 0:00:20\n",
            "epoch [63/100] batch [1/1] time 0.556 (0.556) data 0.348 (0.348) loss 0.4790 (0.4790) acc 100.0000 (100.0000) lr 6.3188e+00 eta 0:00:20\n",
            "epoch [64/100] batch [1/1] time 0.520 (0.520) data 0.314 (0.314) loss 0.3281 (0.3281) acc 100.0000 (100.0000) lr 6.0285e+00 eta 0:00:18\n",
            "epoch [65/100] batch [1/1] time 0.555 (0.555) data 0.346 (0.346) loss 0.4932 (0.4932) acc 95.0000 (95.0000) lr 5.7422e+00 eta 0:00:19\n",
            "epoch [66/100] batch [1/1] time 0.534 (0.534) data 0.324 (0.324) loss 0.6848 (0.6848) acc 95.0000 (95.0000) lr 5.4601e+00 eta 0:00:18\n",
            "epoch [67/100] batch [1/1] time 0.596 (0.596) data 0.384 (0.384) loss 0.3702 (0.3702) acc 95.0000 (95.0000) lr 5.1825e+00 eta 0:00:19\n",
            "epoch [68/100] batch [1/1] time 0.574 (0.574) data 0.364 (0.364) loss 0.3745 (0.3745) acc 95.0000 (95.0000) lr 4.9096e+00 eta 0:00:18\n",
            "epoch [69/100] batch [1/1] time 0.599 (0.599) data 0.393 (0.393) loss 0.3856 (0.3856) acc 95.0000 (95.0000) lr 4.6417e+00 eta 0:00:18\n",
            "epoch [70/100] batch [1/1] time 0.568 (0.568) data 0.357 (0.357) loss 0.7318 (0.7318) acc 85.0000 (85.0000) lr 4.3792e+00 eta 0:00:17\n",
            "epoch [71/100] batch [1/1] time 0.546 (0.546) data 0.336 (0.336) loss 0.6299 (0.6299) acc 90.0000 (90.0000) lr 4.1221e+00 eta 0:00:15\n",
            "epoch [72/100] batch [1/1] time 0.532 (0.532) data 0.320 (0.320) loss 0.3938 (0.3938) acc 95.0000 (95.0000) lr 3.8709e+00 eta 0:00:14\n",
            "epoch [73/100] batch [1/1] time 0.559 (0.559) data 0.347 (0.347) loss 0.3174 (0.3174) acc 100.0000 (100.0000) lr 3.6258e+00 eta 0:00:15\n",
            "epoch [74/100] batch [1/1] time 0.550 (0.550) data 0.339 (0.339) loss 0.4800 (0.4800) acc 95.0000 (95.0000) lr 3.3869e+00 eta 0:00:14\n",
            "epoch [75/100] batch [1/1] time 0.570 (0.570) data 0.359 (0.359) loss 0.4696 (0.4696) acc 90.0000 (90.0000) lr 3.1545e+00 eta 0:00:14\n",
            "epoch [76/100] batch [1/1] time 0.539 (0.539) data 0.328 (0.328) loss 0.4956 (0.4956) acc 95.0000 (95.0000) lr 2.9289e+00 eta 0:00:12\n",
            "epoch [77/100] batch [1/1] time 0.550 (0.550) data 0.337 (0.337) loss 0.3060 (0.3060) acc 100.0000 (100.0000) lr 2.7103e+00 eta 0:00:12\n",
            "epoch [78/100] batch [1/1] time 0.535 (0.535) data 0.324 (0.324) loss 0.3455 (0.3455) acc 95.0000 (95.0000) lr 2.4989e+00 eta 0:00:11\n",
            "epoch [79/100] batch [1/1] time 0.578 (0.578) data 0.365 (0.365) loss 0.3041 (0.3041) acc 100.0000 (100.0000) lr 2.2949e+00 eta 0:00:12\n",
            "epoch [80/100] batch [1/1] time 0.538 (0.538) data 0.329 (0.329) loss 0.3166 (0.3166) acc 100.0000 (100.0000) lr 2.0984e+00 eta 0:00:10\n",
            "epoch [81/100] batch [1/1] time 0.547 (0.547) data 0.334 (0.334) loss 0.4661 (0.4661) acc 95.0000 (95.0000) lr 1.9098e+00 eta 0:00:10\n",
            "epoch [82/100] batch [1/1] time 0.537 (0.537) data 0.326 (0.326) loss 0.3147 (0.3147) acc 100.0000 (100.0000) lr 1.7292e+00 eta 0:00:09\n",
            "epoch [83/100] batch [1/1] time 0.559 (0.559) data 0.345 (0.345) loss 0.4576 (0.4576) acc 95.0000 (95.0000) lr 1.5567e+00 eta 0:00:09\n",
            "epoch [84/100] batch [1/1] time 0.533 (0.533) data 0.324 (0.324) loss 0.4836 (0.4836) acc 95.0000 (95.0000) lr 1.3926e+00 eta 0:00:08\n",
            "epoch [85/100] batch [1/1] time 0.548 (0.548) data 0.337 (0.337) loss 0.3773 (0.3773) acc 95.0000 (95.0000) lr 1.2369e+00 eta 0:00:08\n",
            "epoch [86/100] batch [1/1] time 0.532 (0.532) data 0.322 (0.322) loss 0.3107 (0.3107) acc 100.0000 (100.0000) lr 1.0899e+00 eta 0:00:07\n",
            "epoch [87/100] batch [1/1] time 0.583 (0.583) data 0.367 (0.367) loss 0.2790 (0.2790) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:07\n",
            "epoch [88/100] batch [1/1] time 0.569 (0.569) data 0.359 (0.359) loss 0.3854 (0.3854) acc 90.0000 (90.0000) lr 8.2245e-01 eta 0:00:06\n",
            "epoch [89/100] batch [1/1] time 0.588 (0.588) data 0.377 (0.377) loss 0.2808 (0.2808) acc 100.0000 (100.0000) lr 7.0224e-01 eta 0:00:06\n",
            "epoch [90/100] batch [1/1] time 0.572 (0.572) data 0.363 (0.363) loss 0.2744 (0.2744) acc 100.0000 (100.0000) lr 5.9119e-01 eta 0:00:05\n",
            "epoch [91/100] batch [1/1] time 0.548 (0.548) data 0.338 (0.338) loss 0.4222 (0.4222) acc 95.0000 (95.0000) lr 4.8943e-01 eta 0:00:04\n",
            "epoch [92/100] batch [1/1] time 0.541 (0.541) data 0.331 (0.331) loss 0.2845 (0.2845) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:04\n",
            "epoch [93/100] batch [1/1] time 0.551 (0.551) data 0.343 (0.343) loss 0.2821 (0.2821) acc 100.0000 (100.0000) lr 3.1417e-01 eta 0:00:03\n",
            "epoch [94/100] batch [1/1] time 0.545 (0.545) data 0.332 (0.332) loss 0.3065 (0.3065) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:03\n",
            "epoch [95/100] batch [1/1] time 0.559 (0.559) data 0.350 (0.350) loss 0.4329 (0.4329) acc 95.0000 (95.0000) lr 1.7713e-01 eta 0:00:02\n",
            "epoch [96/100] batch [1/1] time 0.534 (0.534) data 0.324 (0.324) loss 0.2874 (0.2874) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.560 (0.560) data 0.350 (0.350) loss 0.2790 (0.2790) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:01\n",
            "epoch [98/100] batch [1/1] time 0.541 (0.541) data 0.331 (0.331) loss 0.2818 (0.2818) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.551 (0.551) data 0.344 (0.344) loss 0.2880 (0.2880) acc 100.0000 (100.0000) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.532 (0.532) data 0.320 (0.320) loss 0.2860 (0.2860) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.50it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 4,497\n",
            "* accuracy: 55.5%\n",
            "* error: 44.5%\n",
            "* macro_f1: 53.3%\n",
            "Elapsed: 0:01:49\n"
          ]
        }
      ],
      "source": [
        "#eurosat-2shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5715f1fe-9cc4-4d89-a622-023f6f057322",
        "id": "2b1njZPYiUYf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 10:11:12.816427: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 10:11:12.836498: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 10:11:12.845826: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 10:11:12.860481: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 10:11:13.844274: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2']\n",
            "output_dir: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed2\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  20\n",
            "# val      20\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0173, -0.5723,  0.1752,  ...,  0.4941,  0.4187,  0.0375],\n",
            "        [ 0.0214, -0.5706,  0.1699,  ...,  0.5021,  0.4206,  0.0425],\n",
            "        [ 0.0149, -0.5713,  0.1720,  ...,  0.4970,  0.4210,  0.0462],\n",
            "        ...,\n",
            "        [ 0.0324, -0.5680,  0.1748,  ...,  0.5016,  0.4188,  0.0404],\n",
            "        [ 0.0201, -0.5696,  0.1726,  ...,  0.4989,  0.4190,  0.0354],\n",
            "        [ 0.0228, -0.5696,  0.1684,  ...,  0.5053,  0.4212,  0.0412]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5609\n",
            "  Max: 1.0111\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 1.1860e-02, -2.6458e-03, -2.5006e-02,  ..., -3.9643e-04,\n",
            "           1.1337e-02,  1.2487e-02],\n",
            "         [ 5.5103e-03, -1.7622e-02, -2.4362e-02,  ..., -1.3186e-03,\n",
            "           5.9513e-03,  1.6259e-03],\n",
            "         [ 9.3536e-03,  1.0087e-03, -1.5165e-02,  ..., -3.1478e-03,\n",
            "           6.7614e-03,  4.8820e-03],\n",
            "         ...,\n",
            "         [ 2.1362e-04, -7.9000e-03, -2.3896e-02,  ..., -1.4779e-03,\n",
            "           1.1997e-02, -3.9166e-03],\n",
            "         [ 1.1013e-02, -7.4499e-03, -2.3729e-02,  ...,  3.6269e-04,\n",
            "           5.6833e-03, -2.5281e-03],\n",
            "         [-2.5940e-03, -1.1364e-02, -2.0847e-02,  ..., -2.7323e-03,\n",
            "           4.0220e-03, -7.1897e-03]],\n",
            "\n",
            "        [[ 1.6603e-03, -6.0676e-03, -1.4711e-02,  ..., -2.9127e-03,\n",
            "           1.9296e-03,  9.3089e-03],\n",
            "         [-2.8915e-03, -6.1133e-03, -1.3448e-02,  ..., -6.4027e-03,\n",
            "           4.6552e-03,  9.3776e-03],\n",
            "         [-1.5755e-03, -1.1160e-02, -1.4180e-02,  ..., -1.6028e-03,\n",
            "           3.9724e-03,  3.2044e-03],\n",
            "         ...,\n",
            "         [ 4.7645e-03, -1.1244e-02, -1.5756e-02,  ..., -3.3547e-03,\n",
            "           7.3913e-03,  2.4758e-03],\n",
            "         [ 7.0801e-03, -7.1700e-03, -1.9013e-02,  ..., -1.0163e-03,\n",
            "           2.7002e-03,  5.4027e-03],\n",
            "         [ 5.8899e-03, -6.5368e-03, -2.4068e-02,  ...,  5.2455e-03,\n",
            "           1.2527e-02, -4.9466e-03]],\n",
            "\n",
            "        [[ 1.4686e-04, -8.2129e-03, -1.6530e-02,  ..., -4.4667e-03,\n",
            "           6.4542e-03,  6.4059e-03],\n",
            "         [-4.8981e-03, -1.1071e-02, -2.5045e-02,  ..., -3.8926e-03,\n",
            "           6.6517e-03,  1.2408e-04],\n",
            "         [ 3.3245e-03, -1.0737e-02, -1.9468e-02,  ..., -5.5749e-03,\n",
            "           1.1035e-02,  7.8831e-04],\n",
            "         ...,\n",
            "         [-3.4180e-03, -4.5054e-03, -1.2708e-02,  ..., -2.1145e-03,\n",
            "           3.3296e-03,  1.9134e-03],\n",
            "         [ 4.5376e-03, -8.9677e-03, -2.3088e-02,  ..., -4.5306e-03,\n",
            "           1.3023e-02,  5.9138e-03],\n",
            "         [ 9.3574e-03, -8.1519e-03, -2.2479e-02,  ..., -1.7821e-03,\n",
            "           4.5026e-03,  2.8945e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.0692e-03, -1.3144e-02, -1.6256e-02,  ..., -9.4716e-03,\n",
            "           7.2750e-03,  1.0649e-03],\n",
            "         [ 6.2599e-03, -8.7433e-03, -1.7177e-02,  ..., -3.8430e-03,\n",
            "           1.0294e-03, -2.1371e-03],\n",
            "         [-2.1286e-03, -9.9261e-03, -2.9645e-02,  ..., -7.6253e-03,\n",
            "           8.4852e-03,  8.8320e-04],\n",
            "         ...,\n",
            "         [-4.9095e-03, -3.9542e-03, -2.2611e-02,  ...,  5.4057e-03,\n",
            "           7.7175e-03,  5.5609e-04],\n",
            "         [ 2.0873e-03,  2.1607e-03, -2.0847e-02,  ...,  5.8585e-04,\n",
            "           7.9912e-03,  2.8716e-03],\n",
            "         [-1.7548e-03, -1.4300e-02, -2.3313e-02,  ...,  1.7131e-03,\n",
            "           3.8103e-03,  3.7662e-03]],\n",
            "\n",
            "        [[ 7.1334e-04,  2.7558e-03, -2.0714e-02,  ...,  7.9310e-03,\n",
            "           1.0803e-02,  4.1495e-03],\n",
            "         [ 6.0177e-03, -5.6785e-03, -1.8350e-02,  ..., -5.0325e-04,\n",
            "           6.6672e-03,  8.7787e-03],\n",
            "         [ 8.4114e-03, -8.2713e-03, -2.0122e-02,  ..., -3.4172e-03,\n",
            "           4.1345e-03, -5.8164e-03],\n",
            "         ...,\n",
            "         [ 9.1743e-04, -5.1101e-03, -1.8438e-02,  ...,  2.2155e-04,\n",
            "           3.6560e-04,  6.9476e-03],\n",
            "         [ 7.8316e-03, -6.9430e-03, -2.2275e-02,  ...,  2.8422e-03,\n",
            "           3.2896e-03,  3.4438e-03],\n",
            "         [ 3.4103e-03, -1.1050e-02, -2.0027e-02,  ...,  5.7185e-03,\n",
            "           7.1662e-03, -8.0766e-04]],\n",
            "\n",
            "        [[ 2.7560e-04, -1.0767e-02, -2.1695e-02,  ..., -1.2066e-02,\n",
            "           1.0478e-02, -4.3096e-03],\n",
            "         [ 9.3173e-04,  9.4767e-04, -1.9784e-02,  ..., -4.0280e-03,\n",
            "           1.8016e-02, -5.1399e-05],\n",
            "         [ 4.1408e-03, -7.8738e-03, -1.1567e-02,  ..., -5.4166e-03,\n",
            "           5.1056e-04,  1.0778e-03],\n",
            "         ...,\n",
            "         [-2.7542e-03, -7.6211e-03, -1.6771e-02,  ...,  6.9316e-03,\n",
            "           6.3831e-03,  1.8298e-03],\n",
            "         [ 5.2490e-03, -6.7561e-03, -1.7544e-02,  ..., -2.3641e-03,\n",
            "           1.2687e-02,  9.0114e-03],\n",
            "         [ 6.6643e-03, -1.4715e-02, -1.9620e-02,  ..., -2.5640e-03,\n",
            "           8.1342e-03,  9.7705e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4524\n",
            "  Max: 0.4755\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed2/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 12.966 (12.966) data 11.905 (11.905) loss 8.6385 (8.6385) acc 35.0000 (35.0000) lr 2.0000e+01 eta 0:21:23\n",
            "epoch [2/100] batch [1/1] time 0.551 (0.551) data 0.346 (0.346) loss 8.7107 (8.7107) acc 35.0000 (35.0000) lr 1.9995e+01 eta 0:00:54\n",
            "epoch [3/100] batch [1/1] time 0.547 (0.547) data 0.342 (0.342) loss 7.1540 (7.1540) acc 10.0000 (10.0000) lr 1.9980e+01 eta 0:00:53\n",
            "epoch [4/100] batch [1/1] time 0.579 (0.579) data 0.375 (0.375) loss 5.4461 (5.4461) acc 20.0000 (20.0000) lr 1.9956e+01 eta 0:00:55\n",
            "epoch [5/100] batch [1/1] time 0.557 (0.557) data 0.354 (0.354) loss 8.5564 (8.5564) acc 10.0000 (10.0000) lr 1.9921e+01 eta 0:00:52\n",
            "epoch [6/100] batch [1/1] time 0.542 (0.542) data 0.342 (0.342) loss 9.4039 (9.4039) acc 10.0000 (10.0000) lr 1.9877e+01 eta 0:00:50\n",
            "epoch [7/100] batch [1/1] time 0.541 (0.541) data 0.340 (0.340) loss 9.7482 (9.7482) acc 10.0000 (10.0000) lr 1.9823e+01 eta 0:00:50\n",
            "epoch [8/100] batch [1/1] time 0.535 (0.535) data 0.329 (0.329) loss 8.4945 (8.4945) acc 5.0000 (5.0000) lr 1.9759e+01 eta 0:00:49\n",
            "epoch [9/100] batch [1/1] time 0.562 (0.562) data 0.357 (0.357) loss 7.5792 (7.5792) acc 10.0000 (10.0000) lr 1.9686e+01 eta 0:00:51\n",
            "epoch [10/100] batch [1/1] time 0.562 (0.562) data 0.356 (0.356) loss 7.1353 (7.1353) acc 10.0000 (10.0000) lr 1.9603e+01 eta 0:00:50\n",
            "epoch [11/100] batch [1/1] time 0.560 (0.560) data 0.357 (0.357) loss 5.9741 (5.9741) acc 10.0000 (10.0000) lr 1.9511e+01 eta 0:00:49\n",
            "epoch [12/100] batch [1/1] time 0.571 (0.571) data 0.365 (0.365) loss 4.9723 (4.9723) acc 10.0000 (10.0000) lr 1.9409e+01 eta 0:00:50\n",
            "epoch [13/100] batch [1/1] time 0.560 (0.560) data 0.355 (0.355) loss 4.7460 (4.7460) acc 10.0000 (10.0000) lr 1.9298e+01 eta 0:00:48\n",
            "epoch [14/100] batch [1/1] time 0.590 (0.590) data 0.385 (0.385) loss 4.1268 (4.1268) acc 10.0000 (10.0000) lr 1.9178e+01 eta 0:00:50\n",
            "epoch [15/100] batch [1/1] time 0.539 (0.539) data 0.335 (0.335) loss 3.8729 (3.8729) acc 10.0000 (10.0000) lr 1.9048e+01 eta 0:00:45\n",
            "epoch [16/100] batch [1/1] time 0.556 (0.556) data 0.352 (0.352) loss 4.0382 (4.0382) acc 10.0000 (10.0000) lr 1.8910e+01 eta 0:00:46\n",
            "epoch [17/100] batch [1/1] time 0.559 (0.559) data 0.352 (0.352) loss 3.9658 (3.9658) acc 10.0000 (10.0000) lr 1.8763e+01 eta 0:00:46\n",
            "epoch [18/100] batch [1/1] time 0.534 (0.534) data 0.329 (0.329) loss 3.6354 (3.6354) acc 10.0000 (10.0000) lr 1.8607e+01 eta 0:00:43\n",
            "epoch [19/100] batch [1/1] time 0.531 (0.531) data 0.326 (0.326) loss 3.5481 (3.5481) acc 15.0000 (15.0000) lr 1.8443e+01 eta 0:00:43\n",
            "epoch [20/100] batch [1/1] time 0.551 (0.551) data 0.344 (0.344) loss 3.4787 (3.4787) acc 20.0000 (20.0000) lr 1.8271e+01 eta 0:00:44\n",
            "epoch [21/100] batch [1/1] time 0.542 (0.542) data 0.336 (0.336) loss 3.3935 (3.3935) acc 15.0000 (15.0000) lr 1.8090e+01 eta 0:00:42\n",
            "epoch [22/100] batch [1/1] time 0.552 (0.552) data 0.345 (0.345) loss 3.3334 (3.3334) acc 25.0000 (25.0000) lr 1.7902e+01 eta 0:00:43\n",
            "epoch [23/100] batch [1/1] time 0.538 (0.538) data 0.331 (0.331) loss 3.0653 (3.0653) acc 25.0000 (25.0000) lr 1.7705e+01 eta 0:00:41\n",
            "epoch [24/100] batch [1/1] time 0.536 (0.536) data 0.332 (0.332) loss 2.8696 (2.8696) acc 50.0000 (50.0000) lr 1.7501e+01 eta 0:00:40\n",
            "epoch [25/100] batch [1/1] time 0.553 (0.553) data 0.348 (0.348) loss 2.7874 (2.7874) acc 40.0000 (40.0000) lr 1.7290e+01 eta 0:00:41\n",
            "epoch [26/100] batch [1/1] time 0.524 (0.524) data 0.319 (0.319) loss 2.5422 (2.5422) acc 30.0000 (30.0000) lr 1.7071e+01 eta 0:00:38\n",
            "epoch [27/100] batch [1/1] time 0.554 (0.554) data 0.350 (0.350) loss 2.5421 (2.5421) acc 15.0000 (15.0000) lr 1.6845e+01 eta 0:00:40\n",
            "epoch [28/100] batch [1/1] time 0.583 (0.583) data 0.376 (0.376) loss 2.0802 (2.0802) acc 65.0000 (65.0000) lr 1.6613e+01 eta 0:00:41\n",
            "epoch [29/100] batch [1/1] time 0.545 (0.545) data 0.340 (0.340) loss 2.2312 (2.2312) acc 60.0000 (60.0000) lr 1.6374e+01 eta 0:00:38\n",
            "epoch [30/100] batch [1/1] time 0.564 (0.564) data 0.357 (0.357) loss 2.2413 (2.2413) acc 50.0000 (50.0000) lr 1.6129e+01 eta 0:00:39\n",
            "epoch [31/100] batch [1/1] time 0.591 (0.591) data 0.369 (0.369) loss 1.6560 (1.6560) acc 55.0000 (55.0000) lr 1.5878e+01 eta 0:00:40\n",
            "epoch [32/100] batch [1/1] time 0.562 (0.562) data 0.356 (0.356) loss 1.5330 (1.5330) acc 75.0000 (75.0000) lr 1.5621e+01 eta 0:00:38\n",
            "epoch [33/100] batch [1/1] time 0.571 (0.571) data 0.361 (0.361) loss 1.4348 (1.4348) acc 60.0000 (60.0000) lr 1.5358e+01 eta 0:00:38\n",
            "epoch [34/100] batch [1/1] time 0.570 (0.570) data 0.362 (0.362) loss 2.6066 (2.6066) acc 40.0000 (40.0000) lr 1.5090e+01 eta 0:00:37\n",
            "epoch [35/100] batch [1/1] time 0.548 (0.548) data 0.342 (0.342) loss 2.9548 (2.9548) acc 45.0000 (45.0000) lr 1.4818e+01 eta 0:00:35\n",
            "epoch [36/100] batch [1/1] time 0.541 (0.541) data 0.333 (0.333) loss 2.6878 (2.6878) acc 40.0000 (40.0000) lr 1.4540e+01 eta 0:00:34\n",
            "epoch [37/100] batch [1/1] time 0.558 (0.558) data 0.348 (0.348) loss 2.2180 (2.2180) acc 50.0000 (50.0000) lr 1.4258e+01 eta 0:00:35\n",
            "epoch [38/100] batch [1/1] time 0.558 (0.558) data 0.350 (0.350) loss 2.9851 (2.9851) acc 35.0000 (35.0000) lr 1.3971e+01 eta 0:00:34\n",
            "epoch [39/100] batch [1/1] time 0.543 (0.543) data 0.336 (0.336) loss 2.8005 (2.8005) acc 45.0000 (45.0000) lr 1.3681e+01 eta 0:00:33\n",
            "epoch [40/100] batch [1/1] time 0.549 (0.549) data 0.343 (0.343) loss 4.8743 (4.8743) acc 25.0000 (25.0000) lr 1.3387e+01 eta 0:00:32\n",
            "epoch [41/100] batch [1/1] time 0.537 (0.537) data 0.329 (0.329) loss 2.5266 (2.5266) acc 55.0000 (55.0000) lr 1.3090e+01 eta 0:00:31\n",
            "epoch [42/100] batch [1/1] time 0.558 (0.558) data 0.350 (0.350) loss 2.9455 (2.9455) acc 55.0000 (55.0000) lr 1.2790e+01 eta 0:00:32\n",
            "epoch [43/100] batch [1/1] time 0.541 (0.541) data 0.336 (0.336) loss 6.6415 (6.6415) acc 10.0000 (10.0000) lr 1.2487e+01 eta 0:00:30\n",
            "epoch [44/100] batch [1/1] time 0.554 (0.554) data 0.345 (0.345) loss 5.4303 (5.4303) acc 30.0000 (30.0000) lr 1.2181e+01 eta 0:00:31\n",
            "epoch [45/100] batch [1/1] time 0.542 (0.542) data 0.334 (0.334) loss 4.7975 (4.7975) acc 10.0000 (10.0000) lr 1.1874e+01 eta 0:00:29\n",
            "epoch [46/100] batch [1/1] time 0.553 (0.553) data 0.345 (0.345) loss 4.6127 (4.6127) acc 20.0000 (20.0000) lr 1.1564e+01 eta 0:00:29\n",
            "epoch [47/100] batch [1/1] time 0.559 (0.559) data 0.351 (0.351) loss 3.5700 (3.5700) acc 50.0000 (50.0000) lr 1.1253e+01 eta 0:00:29\n",
            "epoch [48/100] batch [1/1] time 0.553 (0.553) data 0.344 (0.344) loss 2.5694 (2.5694) acc 60.0000 (60.0000) lr 1.0941e+01 eta 0:00:28\n",
            "epoch [49/100] batch [1/1] time 0.568 (0.568) data 0.362 (0.362) loss 1.2086 (1.2086) acc 75.0000 (75.0000) lr 1.0628e+01 eta 0:00:28\n",
            "epoch [50/100] batch [1/1] time 0.573 (0.573) data 0.363 (0.363) loss 1.3362 (1.3362) acc 65.0000 (65.0000) lr 1.0314e+01 eta 0:00:28\n",
            "epoch [51/100] batch [1/1] time 0.592 (0.592) data 0.383 (0.383) loss 0.9517 (0.9517) acc 80.0000 (80.0000) lr 1.0000e+01 eta 0:00:29\n",
            "epoch [52/100] batch [1/1] time 0.566 (0.566) data 0.356 (0.356) loss 1.0399 (1.0399) acc 80.0000 (80.0000) lr 9.6859e+00 eta 0:00:27\n",
            "epoch [53/100] batch [1/1] time 0.595 (0.595) data 0.386 (0.386) loss 0.6690 (0.6690) acc 95.0000 (95.0000) lr 9.3721e+00 eta 0:00:27\n",
            "epoch [54/100] batch [1/1] time 0.576 (0.576) data 0.366 (0.366) loss 0.8010 (0.8010) acc 90.0000 (90.0000) lr 9.0589e+00 eta 0:00:26\n",
            "epoch [55/100] batch [1/1] time 0.560 (0.560) data 0.350 (0.350) loss 0.8160 (0.8160) acc 85.0000 (85.0000) lr 8.7467e+00 eta 0:00:25\n",
            "epoch [56/100] batch [1/1] time 0.543 (0.543) data 0.335 (0.335) loss 0.6059 (0.6059) acc 95.0000 (95.0000) lr 8.4357e+00 eta 0:00:23\n",
            "epoch [57/100] batch [1/1] time 0.557 (0.557) data 0.348 (0.348) loss 0.9381 (0.9381) acc 90.0000 (90.0000) lr 8.1262e+00 eta 0:00:23\n",
            "epoch [58/100] batch [1/1] time 0.538 (0.538) data 0.328 (0.328) loss 0.5892 (0.5892) acc 95.0000 (95.0000) lr 7.8186e+00 eta 0:00:22\n",
            "epoch [59/100] batch [1/1] time 0.559 (0.559) data 0.349 (0.349) loss 1.3703 (1.3703) acc 75.0000 (75.0000) lr 7.5131e+00 eta 0:00:22\n",
            "epoch [60/100] batch [1/1] time 0.542 (0.542) data 0.334 (0.334) loss 0.4695 (0.4695) acc 100.0000 (100.0000) lr 7.2101e+00 eta 0:00:21\n",
            "epoch [61/100] batch [1/1] time 0.560 (0.560) data 0.350 (0.350) loss 0.5639 (0.5639) acc 90.0000 (90.0000) lr 6.9098e+00 eta 0:00:21\n",
            "epoch [62/100] batch [1/1] time 0.558 (0.558) data 0.350 (0.350) loss 0.4958 (0.4958) acc 100.0000 (100.0000) lr 6.6126e+00 eta 0:00:21\n",
            "epoch [63/100] batch [1/1] time 0.553 (0.553) data 0.342 (0.342) loss 1.0631 (1.0631) acc 75.0000 (75.0000) lr 6.3188e+00 eta 0:00:20\n",
            "epoch [64/100] batch [1/1] time 0.536 (0.536) data 0.326 (0.326) loss 0.6575 (0.6575) acc 85.0000 (85.0000) lr 6.0285e+00 eta 0:00:19\n",
            "epoch [65/100] batch [1/1] time 0.563 (0.563) data 0.352 (0.352) loss 0.7237 (0.7237) acc 85.0000 (85.0000) lr 5.7422e+00 eta 0:00:19\n",
            "epoch [66/100] batch [1/1] time 0.562 (0.562) data 0.351 (0.351) loss 0.6498 (0.6498) acc 90.0000 (90.0000) lr 5.4601e+00 eta 0:00:19\n",
            "epoch [67/100] batch [1/1] time 0.580 (0.580) data 0.372 (0.372) loss 0.5270 (0.5270) acc 90.0000 (90.0000) lr 5.1825e+00 eta 0:00:19\n",
            "epoch [68/100] batch [1/1] time 0.556 (0.556) data 0.348 (0.348) loss 1.0033 (1.0033) acc 80.0000 (80.0000) lr 4.9096e+00 eta 0:00:17\n",
            "epoch [69/100] batch [1/1] time 0.555 (0.555) data 0.346 (0.346) loss 0.3811 (0.3811) acc 100.0000 (100.0000) lr 4.6417e+00 eta 0:00:17\n",
            "epoch [70/100] batch [1/1] time 0.555 (0.555) data 0.345 (0.345) loss 0.6151 (0.6151) acc 90.0000 (90.0000) lr 4.3792e+00 eta 0:00:16\n",
            "epoch [71/100] batch [1/1] time 0.578 (0.578) data 0.363 (0.363) loss 0.9151 (0.9151) acc 90.0000 (90.0000) lr 4.1221e+00 eta 0:00:16\n",
            "epoch [72/100] batch [1/1] time 0.560 (0.560) data 0.349 (0.349) loss 0.4607 (0.4607) acc 95.0000 (95.0000) lr 3.8709e+00 eta 0:00:15\n",
            "epoch [73/100] batch [1/1] time 0.581 (0.581) data 0.371 (0.371) loss 0.4520 (0.4520) acc 95.0000 (95.0000) lr 3.6258e+00 eta 0:00:15\n",
            "epoch [74/100] batch [1/1] time 0.575 (0.575) data 0.366 (0.366) loss 0.4748 (0.4748) acc 95.0000 (95.0000) lr 3.3869e+00 eta 0:00:14\n",
            "epoch [75/100] batch [1/1] time 0.564 (0.564) data 0.354 (0.354) loss 0.3586 (0.3586) acc 100.0000 (100.0000) lr 3.1545e+00 eta 0:00:14\n",
            "epoch [76/100] batch [1/1] time 0.534 (0.534) data 0.325 (0.325) loss 0.3472 (0.3472) acc 100.0000 (100.0000) lr 2.9289e+00 eta 0:00:12\n",
            "epoch [77/100] batch [1/1] time 0.540 (0.540) data 0.332 (0.332) loss 0.3675 (0.3675) acc 100.0000 (100.0000) lr 2.7103e+00 eta 0:00:12\n",
            "epoch [78/100] batch [1/1] time 0.535 (0.535) data 0.330 (0.330) loss 0.6889 (0.6889) acc 95.0000 (95.0000) lr 2.4989e+00 eta 0:00:11\n",
            "epoch [79/100] batch [1/1] time 0.534 (0.534) data 0.329 (0.329) loss 0.5921 (0.5921) acc 90.0000 (90.0000) lr 2.2949e+00 eta 0:00:11\n",
            "epoch [80/100] batch [1/1] time 0.541 (0.541) data 0.331 (0.331) loss 0.4032 (0.4032) acc 100.0000 (100.0000) lr 2.0984e+00 eta 0:00:10\n",
            "epoch [81/100] batch [1/1] time 0.537 (0.537) data 0.326 (0.326) loss 0.4813 (0.4813) acc 95.0000 (95.0000) lr 1.9098e+00 eta 0:00:10\n",
            "epoch [82/100] batch [1/1] time 0.549 (0.549) data 0.337 (0.337) loss 0.4748 (0.4748) acc 100.0000 (100.0000) lr 1.7292e+00 eta 0:00:09\n",
            "epoch [83/100] batch [1/1] time 0.547 (0.547) data 0.337 (0.337) loss 0.3819 (0.3819) acc 100.0000 (100.0000) lr 1.5567e+00 eta 0:00:09\n",
            "epoch [84/100] batch [1/1] time 0.546 (0.546) data 0.333 (0.333) loss 0.4507 (0.4507) acc 95.0000 (95.0000) lr 1.3926e+00 eta 0:00:08\n",
            "epoch [85/100] batch [1/1] time 0.534 (0.534) data 0.325 (0.325) loss 0.4200 (0.4200) acc 95.0000 (95.0000) lr 1.2369e+00 eta 0:00:08\n",
            "epoch [86/100] batch [1/1] time 0.577 (0.577) data 0.369 (0.369) loss 0.3307 (0.3307) acc 100.0000 (100.0000) lr 1.0899e+00 eta 0:00:08\n",
            "epoch [87/100] batch [1/1] time 0.572 (0.572) data 0.361 (0.361) loss 0.5734 (0.5734) acc 90.0000 (90.0000) lr 9.5173e-01 eta 0:00:07\n",
            "epoch [88/100] batch [1/1] time 0.553 (0.553) data 0.342 (0.342) loss 0.3932 (0.3932) acc 95.0000 (95.0000) lr 8.2245e-01 eta 0:00:06\n",
            "epoch [89/100] batch [1/1] time 0.548 (0.548) data 0.333 (0.333) loss 0.3836 (0.3836) acc 95.0000 (95.0000) lr 7.0224e-01 eta 0:00:06\n",
            "epoch [90/100] batch [1/1] time 0.551 (0.551) data 0.342 (0.342) loss 0.4161 (0.4161) acc 95.0000 (95.0000) lr 5.9119e-01 eta 0:00:05\n",
            "epoch [91/100] batch [1/1] time 0.569 (0.569) data 0.356 (0.356) loss 0.4404 (0.4404) acc 95.0000 (95.0000) lr 4.8943e-01 eta 0:00:05\n",
            "epoch [92/100] batch [1/1] time 0.544 (0.544) data 0.329 (0.329) loss 0.3294 (0.3294) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:04\n",
            "epoch [93/100] batch [1/1] time 0.576 (0.576) data 0.366 (0.366) loss 0.5898 (0.5898) acc 90.0000 (90.0000) lr 3.1417e-01 eta 0:00:04\n",
            "epoch [94/100] batch [1/1] time 0.561 (0.561) data 0.350 (0.350) loss 0.3291 (0.3291) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:03\n",
            "epoch [95/100] batch [1/1] time 0.543 (0.543) data 0.333 (0.333) loss 0.4365 (0.4365) acc 95.0000 (95.0000) lr 1.7713e-01 eta 0:00:02\n",
            "epoch [96/100] batch [1/1] time 0.560 (0.560) data 0.350 (0.350) loss 0.3269 (0.3269) acc 100.0000 (100.0000) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.542 (0.542) data 0.332 (0.332) loss 0.3590 (0.3590) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:01\n",
            "epoch [98/100] batch [1/1] time 0.543 (0.543) data 0.332 (0.332) loss 0.3437 (0.3437) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.543 (0.543) data 0.330 (0.330) loss 0.4810 (0.4810) acc 90.0000 (90.0000) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.580 (0.580) data 0.367 (0.367) loss 0.3289 (0.3289) acc 100.0000 (100.0000) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed2/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.48it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 4,164\n",
            "* accuracy: 51.4%\n",
            "* error: 48.6%\n",
            "* macro_f1: 49.4%\n",
            "Elapsed: 0:01:48\n"
          ]
        }
      ],
      "source": [
        "#eurosat-2shots-seed2\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 2 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed2 \\\n",
        "        DATASET.NUM_SHOTS 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f369d0-f77f-4288-b4af-dbe794646972",
        "id": "jjF6qN9OiUYf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 10:13:21.002471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 10:13:21.022238: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 10:13:21.028145: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 10:13:21.042518: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 10:13:22.046228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2']\n",
            "output_dir: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed3\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_2-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  20\n",
            "# val      20\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0187, -0.5727,  0.1683,  ...,  0.4947,  0.4188,  0.0344],\n",
            "        [ 0.0173, -0.5677,  0.1739,  ...,  0.4990,  0.4190,  0.0388],\n",
            "        [ 0.0094, -0.5649,  0.1758,  ...,  0.4946,  0.4157,  0.0405],\n",
            "        ...,\n",
            "        [ 0.0227, -0.5676,  0.1747,  ...,  0.5071,  0.4204,  0.0413],\n",
            "        [ 0.0083, -0.5735,  0.1744,  ...,  0.5016,  0.4249,  0.0364],\n",
            "        [ 0.0210, -0.5687,  0.1704,  ...,  0.5030,  0.4152,  0.0323]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2340\n",
            "  Min: -1.5598\n",
            "  Max: 1.0156\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-1.2399e-04, -9.3844e-03, -2.1417e-02,  ..., -1.0112e-02,\n",
            "          -7.3685e-04,  7.9737e-04],\n",
            "         [ 8.8539e-03,  1.7411e-03, -1.0068e-02,  ..., -2.1264e-03,\n",
            "           5.1197e-03,  2.2050e-03],\n",
            "         [ 7.9726e-04,  2.6865e-04, -1.0991e-02,  ...,  1.9916e-03,\n",
            "           1.7770e-03,  6.4784e-03],\n",
            "         ...,\n",
            "         [ 5.0354e-03, -1.5631e-02, -2.0003e-02,  ..., -2.2618e-03,\n",
            "           3.1675e-03,  5.5781e-03],\n",
            "         [ 6.0940e-03, -7.5148e-03, -1.6824e-02,  ...,  2.1022e-03,\n",
            "           3.5852e-03,  3.3437e-03],\n",
            "         [ 1.2073e-03, -6.1362e-03, -1.8186e-02,  ..., -4.9664e-03,\n",
            "           4.9890e-03,  1.4233e-03]],\n",
            "\n",
            "        [[-1.3523e-03, -1.5127e-02, -2.2592e-02,  ..., -5.6130e-03,\n",
            "           3.0075e-04,  3.5373e-03],\n",
            "         [ 4.7073e-03, -1.5986e-02, -1.9756e-02,  ..., -4.8596e-03,\n",
            "           1.4182e-02, -3.2796e-04],\n",
            "         [ 1.4543e-03, -1.4349e-02, -2.0663e-02,  ..., -2.6314e-03,\n",
            "           4.7735e-03,  3.8005e-03],\n",
            "         ...,\n",
            "         [ 5.5084e-03, -3.1951e-03, -2.0837e-02,  ..., -5.2220e-03,\n",
            "           1.9029e-03, -2.8714e-03],\n",
            "         [-9.3079e-04, -5.4381e-03, -1.6982e-02,  ..., -1.7034e-03,\n",
            "           8.4127e-03, -3.7145e-03],\n",
            "         [ 3.6659e-03, -6.9316e-03, -1.9074e-02,  ..., -3.4453e-03,\n",
            "           7.3618e-03,  1.5713e-03]],\n",
            "\n",
            "        [[ 7.0343e-03, -3.3286e-03, -1.6511e-02,  ..., -3.5879e-03,\n",
            "           3.0702e-03,  7.3672e-03],\n",
            "         [-3.5019e-03, -9.3458e-03, -1.7247e-02,  ..., -9.8149e-03,\n",
            "           7.4180e-03,  5.5667e-03],\n",
            "         [-8.9646e-04, -9.7735e-03, -2.4316e-02,  ..., -9.1714e-04,\n",
            "           4.4855e-03, -3.6191e-03],\n",
            "         ...,\n",
            "         [-1.0166e-03, -2.6763e-03, -1.1029e-02,  ..., -5.1801e-03,\n",
            "           9.8337e-03,  1.7180e-03],\n",
            "         [ 2.5787e-03, -5.2588e-03, -2.2912e-02,  ..., -2.0396e-03,\n",
            "           6.2809e-03,  5.9863e-03],\n",
            "         [-1.2093e-03, -4.2937e-03, -1.6879e-02,  ..., -6.3016e-03,\n",
            "           1.0322e-02,  8.1798e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 7.8773e-03, -6.2831e-03, -1.5435e-02,  ..., -1.2378e-02,\n",
            "           8.3345e-03,  1.2885e-04],\n",
            "         [-2.5597e-03, -4.3204e-03, -2.0193e-02,  ..., -7.2286e-03,\n",
            "           1.5033e-02, -2.8485e-03],\n",
            "         [ 2.8663e-03, -7.2692e-03, -2.1650e-02,  ...,  2.5752e-03,\n",
            "           1.9525e-03,  5.0193e-03],\n",
            "         ...,\n",
            "         [ 6.3744e-03, -5.5869e-03, -1.8911e-02,  ...,  1.5147e-03,\n",
            "           6.1778e-03,  4.1877e-03],\n",
            "         [ 8.5792e-03, -1.0136e-02, -1.9858e-02,  ..., -1.6343e-03,\n",
            "           5.2963e-04,  6.1351e-03],\n",
            "         [ 8.3656e-03, -7.1452e-03, -2.0655e-02,  ..., -3.2823e-03,\n",
            "           2.0555e-03, -3.2758e-03]],\n",
            "\n",
            "        [[-2.7657e-03, -1.8561e-03, -1.9243e-02,  ..., -3.1335e-03,\n",
            "           6.7562e-03, -2.2286e-03],\n",
            "         [ 4.5147e-03, -1.7149e-02, -1.8531e-02,  ..., -2.5607e-03,\n",
            "           3.1713e-03, -8.0594e-03],\n",
            "         [ 1.0693e-02, -9.7430e-03, -2.0959e-02,  ..., -7.5604e-03,\n",
            "           8.6532e-04,  3.2340e-03],\n",
            "         ...,\n",
            "         [ 2.0814e-03, -8.0746e-03, -1.5357e-02,  ...,  2.1275e-05,\n",
            "           8.1009e-03,  6.4860e-04],\n",
            "         [ 9.9144e-03, -6.3556e-03, -2.0875e-02,  ...,  5.8482e-03,\n",
            "           1.1089e-02,  2.2231e-03],\n",
            "         [ 8.1825e-03, -7.4852e-03, -3.4047e-02,  ..., -6.5774e-04,\n",
            "           7.2036e-04,  3.8672e-03]],\n",
            "\n",
            "        [[ 6.8245e-03, -1.1767e-02, -1.8973e-02,  ..., -1.8107e-03,\n",
            "           9.5724e-03, -2.2744e-03],\n",
            "         [ 5.8460e-03, -5.9150e-03, -1.6915e-02,  ..., -8.5256e-03,\n",
            "           5.7009e-03, -6.1889e-05],\n",
            "         [ 5.6381e-03, -7.6225e-03, -1.5910e-02,  ..., -1.9767e-03,\n",
            "           1.3919e-02,  6.1008e-03],\n",
            "         ...,\n",
            "         [ 1.6021e-04, -9.1894e-03, -1.8812e-02,  ..., -6.8700e-03,\n",
            "           1.3755e-02,  1.7334e-03],\n",
            "         [-3.1090e-03, -6.0218e-03, -1.4627e-02,  ..., -2.9032e-03,\n",
            "           1.0263e-02,  4.4375e-03],\n",
            "         [ 8.0604e-03, -2.4817e-03, -1.5781e-02,  ...,  7.8700e-03,\n",
            "           4.9604e-03,  1.0472e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4549\n",
            "  Max: 0.4739\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed3/tensorboard)\n",
            "epoch [1/100] batch [1/1] time 13.590 (13.590) data 12.538 (12.538) loss 8.5330 (8.5330) acc 30.0000 (30.0000) lr 2.0000e+01 eta 0:22:25\n",
            "epoch [2/100] batch [1/1] time 0.554 (0.554) data 0.351 (0.351) loss 8.4607 (8.4607) acc 35.0000 (35.0000) lr 1.9995e+01 eta 0:00:54\n",
            "epoch [3/100] batch [1/1] time 0.542 (0.542) data 0.339 (0.339) loss 6.8377 (6.8377) acc 20.0000 (20.0000) lr 1.9980e+01 eta 0:00:52\n",
            "epoch [4/100] batch [1/1] time 0.564 (0.564) data 0.360 (0.360) loss 5.6478 (5.6478) acc 10.0000 (10.0000) lr 1.9956e+01 eta 0:00:54\n",
            "epoch [5/100] batch [1/1] time 0.530 (0.530) data 0.326 (0.326) loss 5.5219 (5.5219) acc 15.0000 (15.0000) lr 1.9921e+01 eta 0:00:50\n",
            "epoch [6/100] batch [1/1] time 0.546 (0.546) data 0.340 (0.340) loss 8.0507 (8.0507) acc 10.0000 (10.0000) lr 1.9877e+01 eta 0:00:51\n",
            "epoch [7/100] batch [1/1] time 0.535 (0.535) data 0.330 (0.330) loss 8.3297 (8.3297) acc 10.0000 (10.0000) lr 1.9823e+01 eta 0:00:49\n",
            "epoch [8/100] batch [1/1] time 0.523 (0.523) data 0.322 (0.322) loss 5.4349 (5.4349) acc 10.0000 (10.0000) lr 1.9759e+01 eta 0:00:48\n",
            "epoch [9/100] batch [1/1] time 0.568 (0.568) data 0.365 (0.365) loss 5.6060 (5.6060) acc 15.0000 (15.0000) lr 1.9686e+01 eta 0:00:51\n",
            "epoch [10/100] batch [1/1] time 0.541 (0.541) data 0.339 (0.339) loss 5.2367 (5.2367) acc 35.0000 (35.0000) lr 1.9603e+01 eta 0:00:48\n",
            "epoch [11/100] batch [1/1] time 0.556 (0.556) data 0.355 (0.355) loss 4.8666 (4.8666) acc 25.0000 (25.0000) lr 1.9511e+01 eta 0:00:49\n",
            "epoch [12/100] batch [1/1] time 0.546 (0.546) data 0.340 (0.340) loss 4.4571 (4.4571) acc 25.0000 (25.0000) lr 1.9409e+01 eta 0:00:48\n",
            "epoch [13/100] batch [1/1] time 0.558 (0.558) data 0.352 (0.352) loss 4.1067 (4.1067) acc 15.0000 (15.0000) lr 1.9298e+01 eta 0:00:48\n",
            "epoch [14/100] batch [1/1] time 0.596 (0.596) data 0.393 (0.393) loss 3.3208 (3.3208) acc 30.0000 (30.0000) lr 1.9178e+01 eta 0:00:51\n",
            "epoch [15/100] batch [1/1] time 0.552 (0.552) data 0.346 (0.346) loss 3.1923 (3.1923) acc 40.0000 (40.0000) lr 1.9048e+01 eta 0:00:46\n",
            "epoch [16/100] batch [1/1] time 0.557 (0.557) data 0.353 (0.353) loss 3.1255 (3.1255) acc 35.0000 (35.0000) lr 1.8910e+01 eta 0:00:46\n",
            "epoch [17/100] batch [1/1] time 0.530 (0.530) data 0.328 (0.328) loss 2.4492 (2.4492) acc 55.0000 (55.0000) lr 1.8763e+01 eta 0:00:43\n",
            "epoch [18/100] batch [1/1] time 0.523 (0.523) data 0.320 (0.320) loss 2.3745 (2.3745) acc 50.0000 (50.0000) lr 1.8607e+01 eta 0:00:42\n",
            "epoch [19/100] batch [1/1] time 0.555 (0.555) data 0.350 (0.350) loss 2.1633 (2.1633) acc 55.0000 (55.0000) lr 1.8443e+01 eta 0:00:44\n",
            "epoch [20/100] batch [1/1] time 0.544 (0.544) data 0.339 (0.339) loss 2.1741 (2.1741) acc 45.0000 (45.0000) lr 1.8271e+01 eta 0:00:43\n",
            "epoch [21/100] batch [1/1] time 0.551 (0.551) data 0.346 (0.346) loss 1.8549 (1.8549) acc 45.0000 (45.0000) lr 1.8090e+01 eta 0:00:43\n",
            "epoch [22/100] batch [1/1] time 0.551 (0.551) data 0.348 (0.348) loss 1.6664 (1.6664) acc 65.0000 (65.0000) lr 1.7902e+01 eta 0:00:42\n",
            "epoch [23/100] batch [1/1] time 0.539 (0.539) data 0.334 (0.334) loss 1.6392 (1.6392) acc 65.0000 (65.0000) lr 1.7705e+01 eta 0:00:41\n",
            "epoch [24/100] batch [1/1] time 0.532 (0.532) data 0.326 (0.326) loss 1.6146 (1.6146) acc 60.0000 (60.0000) lr 1.7501e+01 eta 0:00:40\n",
            "epoch [25/100] batch [1/1] time 0.545 (0.545) data 0.344 (0.344) loss 1.3525 (1.3525) acc 75.0000 (75.0000) lr 1.7290e+01 eta 0:00:40\n",
            "epoch [26/100] batch [1/1] time 0.545 (0.545) data 0.341 (0.341) loss 1.2471 (1.2471) acc 75.0000 (75.0000) lr 1.7071e+01 eta 0:00:40\n",
            "epoch [27/100] batch [1/1] time 0.528 (0.528) data 0.321 (0.321) loss 1.5376 (1.5376) acc 70.0000 (70.0000) lr 1.6845e+01 eta 0:00:38\n",
            "epoch [28/100] batch [1/1] time 0.544 (0.544) data 0.338 (0.338) loss 1.5403 (1.5403) acc 60.0000 (60.0000) lr 1.6613e+01 eta 0:00:39\n",
            "epoch [29/100] batch [1/1] time 0.547 (0.547) data 0.340 (0.340) loss 1.1336 (1.1336) acc 70.0000 (70.0000) lr 1.6374e+01 eta 0:00:38\n",
            "epoch [30/100] batch [1/1] time 0.533 (0.533) data 0.329 (0.329) loss 0.9316 (0.9316) acc 80.0000 (80.0000) lr 1.6129e+01 eta 0:00:37\n",
            "epoch [31/100] batch [1/1] time 0.542 (0.542) data 0.339 (0.339) loss 0.6073 (0.6073) acc 100.0000 (100.0000) lr 1.5878e+01 eta 0:00:37\n",
            "epoch [32/100] batch [1/1] time 0.537 (0.537) data 0.333 (0.333) loss 0.7310 (0.7310) acc 85.0000 (85.0000) lr 1.5621e+01 eta 0:00:36\n",
            "epoch [33/100] batch [1/1] time 0.596 (0.596) data 0.390 (0.390) loss 0.8996 (0.8996) acc 80.0000 (80.0000) lr 1.5358e+01 eta 0:00:39\n",
            "epoch [34/100] batch [1/1] time 0.554 (0.554) data 0.350 (0.350) loss 1.1483 (1.1483) acc 80.0000 (80.0000) lr 1.5090e+01 eta 0:00:36\n",
            "epoch [35/100] batch [1/1] time 0.537 (0.537) data 0.330 (0.330) loss 0.7763 (0.7763) acc 80.0000 (80.0000) lr 1.4818e+01 eta 0:00:34\n",
            "epoch [36/100] batch [1/1] time 0.563 (0.563) data 0.341 (0.341) loss 0.9889 (0.9889) acc 80.0000 (80.0000) lr 1.4540e+01 eta 0:00:36\n",
            "epoch [37/100] batch [1/1] time 0.540 (0.540) data 0.337 (0.337) loss 1.4805 (1.4805) acc 60.0000 (60.0000) lr 1.4258e+01 eta 0:00:34\n",
            "epoch [38/100] batch [1/1] time 0.555 (0.555) data 0.347 (0.347) loss 1.1317 (1.1317) acc 85.0000 (85.0000) lr 1.3971e+01 eta 0:00:34\n",
            "epoch [39/100] batch [1/1] time 0.532 (0.532) data 0.328 (0.328) loss 0.6387 (0.6387) acc 95.0000 (95.0000) lr 1.3681e+01 eta 0:00:32\n",
            "epoch [40/100] batch [1/1] time 0.543 (0.543) data 0.337 (0.337) loss 1.0506 (1.0506) acc 70.0000 (70.0000) lr 1.3387e+01 eta 0:00:32\n",
            "epoch [41/100] batch [1/1] time 0.543 (0.543) data 0.336 (0.336) loss 1.9093 (1.9093) acc 80.0000 (80.0000) lr 1.3090e+01 eta 0:00:32\n",
            "epoch [42/100] batch [1/1] time 0.555 (0.555) data 0.345 (0.345) loss 1.7791 (1.7791) acc 55.0000 (55.0000) lr 1.2790e+01 eta 0:00:32\n",
            "epoch [43/100] batch [1/1] time 0.557 (0.557) data 0.348 (0.348) loss 3.9931 (3.9931) acc 55.0000 (55.0000) lr 1.2487e+01 eta 0:00:31\n",
            "epoch [44/100] batch [1/1] time 0.545 (0.545) data 0.337 (0.337) loss 3.0995 (3.0995) acc 50.0000 (50.0000) lr 1.2181e+01 eta 0:00:30\n",
            "epoch [45/100] batch [1/1] time 0.555 (0.555) data 0.346 (0.346) loss 4.4625 (4.4625) acc 65.0000 (65.0000) lr 1.1874e+01 eta 0:00:30\n",
            "epoch [46/100] batch [1/1] time 0.527 (0.527) data 0.321 (0.321) loss 11.1546 (11.1546) acc 10.0000 (10.0000) lr 1.1564e+01 eta 0:00:28\n",
            "epoch [47/100] batch [1/1] time 0.557 (0.557) data 0.348 (0.348) loss 3.1349 (3.1349) acc 60.0000 (60.0000) lr 1.1253e+01 eta 0:00:29\n",
            "epoch [48/100] batch [1/1] time 0.535 (0.535) data 0.327 (0.327) loss 2.5826 (2.5826) acc 55.0000 (55.0000) lr 1.0941e+01 eta 0:00:27\n",
            "epoch [49/100] batch [1/1] time 0.567 (0.567) data 0.360 (0.360) loss 2.9607 (2.9607) acc 35.0000 (35.0000) lr 1.0628e+01 eta 0:00:28\n",
            "epoch [50/100] batch [1/1] time 0.531 (0.531) data 0.325 (0.325) loss 3.1710 (3.1710) acc 50.0000 (50.0000) lr 1.0314e+01 eta 0:00:26\n",
            "epoch [51/100] batch [1/1] time 0.555 (0.555) data 0.347 (0.347) loss 4.2519 (4.2519) acc 65.0000 (65.0000) lr 1.0000e+01 eta 0:00:27\n",
            "epoch [52/100] batch [1/1] time 0.557 (0.557) data 0.348 (0.348) loss 4.9374 (4.9374) acc 50.0000 (50.0000) lr 9.6859e+00 eta 0:00:26\n",
            "epoch [53/100] batch [1/1] time 0.579 (0.579) data 0.372 (0.372) loss 3.4504 (3.4504) acc 60.0000 (60.0000) lr 9.3721e+00 eta 0:00:27\n",
            "epoch [54/100] batch [1/1] time 0.568 (0.568) data 0.360 (0.360) loss 1.3405 (1.3405) acc 80.0000 (80.0000) lr 9.0589e+00 eta 0:00:26\n",
            "epoch [55/100] batch [1/1] time 0.576 (0.576) data 0.365 (0.365) loss 1.9961 (1.9961) acc 65.0000 (65.0000) lr 8.7467e+00 eta 0:00:25\n",
            "epoch [56/100] batch [1/1] time 0.550 (0.550) data 0.345 (0.345) loss 1.3482 (1.3482) acc 65.0000 (65.0000) lr 8.4357e+00 eta 0:00:24\n",
            "epoch [57/100] batch [1/1] time 0.557 (0.557) data 0.349 (0.349) loss 1.6171 (1.6171) acc 75.0000 (75.0000) lr 8.1262e+00 eta 0:00:23\n",
            "epoch [58/100] batch [1/1] time 0.542 (0.542) data 0.333 (0.333) loss 0.9949 (0.9949) acc 80.0000 (80.0000) lr 7.8186e+00 eta 0:00:22\n",
            "epoch [59/100] batch [1/1] time 0.553 (0.553) data 0.344 (0.344) loss 0.4756 (0.4756) acc 100.0000 (100.0000) lr 7.5131e+00 eta 0:00:22\n",
            "epoch [60/100] batch [1/1] time 0.563 (0.563) data 0.356 (0.356) loss 0.7336 (0.7336) acc 85.0000 (85.0000) lr 7.2101e+00 eta 0:00:22\n",
            "epoch [61/100] batch [1/1] time 0.535 (0.535) data 0.327 (0.327) loss 1.1053 (1.1053) acc 75.0000 (75.0000) lr 6.9098e+00 eta 0:00:20\n",
            "epoch [62/100] batch [1/1] time 0.555 (0.555) data 0.348 (0.348) loss 0.8367 (0.8367) acc 80.0000 (80.0000) lr 6.6126e+00 eta 0:00:21\n",
            "epoch [63/100] batch [1/1] time 0.549 (0.549) data 0.340 (0.340) loss 0.5493 (0.5493) acc 95.0000 (95.0000) lr 6.3188e+00 eta 0:00:20\n",
            "epoch [64/100] batch [1/1] time 0.557 (0.557) data 0.349 (0.349) loss 0.6508 (0.6508) acc 95.0000 (95.0000) lr 6.0285e+00 eta 0:00:20\n",
            "epoch [65/100] batch [1/1] time 0.557 (0.557) data 0.347 (0.347) loss 1.0222 (1.0222) acc 80.0000 (80.0000) lr 5.7422e+00 eta 0:00:19\n",
            "epoch [66/100] batch [1/1] time 0.532 (0.532) data 0.324 (0.324) loss 0.5667 (0.5667) acc 90.0000 (90.0000) lr 5.4601e+00 eta 0:00:18\n",
            "epoch [67/100] batch [1/1] time 0.538 (0.538) data 0.329 (0.329) loss 1.4558 (1.4558) acc 75.0000 (75.0000) lr 5.1825e+00 eta 0:00:17\n",
            "epoch [68/100] batch [1/1] time 0.542 (0.542) data 0.332 (0.332) loss 0.6340 (0.6340) acc 90.0000 (90.0000) lr 4.9096e+00 eta 0:00:17\n",
            "epoch [69/100] batch [1/1] time 0.556 (0.556) data 0.344 (0.344) loss 0.6664 (0.6664) acc 90.0000 (90.0000) lr 4.6417e+00 eta 0:00:17\n",
            "epoch [70/100] batch [1/1] time 0.544 (0.544) data 0.335 (0.335) loss 0.6015 (0.6015) acc 85.0000 (85.0000) lr 4.3792e+00 eta 0:00:16\n",
            "epoch [71/100] batch [1/1] time 0.537 (0.537) data 0.326 (0.326) loss 0.7088 (0.7088) acc 95.0000 (95.0000) lr 4.1221e+00 eta 0:00:15\n",
            "epoch [72/100] batch [1/1] time 0.551 (0.551) data 0.341 (0.341) loss 0.3733 (0.3733) acc 100.0000 (100.0000) lr 3.8709e+00 eta 0:00:15\n",
            "epoch [73/100] batch [1/1] time 0.561 (0.561) data 0.352 (0.352) loss 0.4457 (0.4457) acc 95.0000 (95.0000) lr 3.6258e+00 eta 0:00:15\n",
            "epoch [74/100] batch [1/1] time 0.581 (0.581) data 0.373 (0.373) loss 0.4948 (0.4948) acc 95.0000 (95.0000) lr 3.3869e+00 eta 0:00:15\n",
            "epoch [75/100] batch [1/1] time 0.567 (0.567) data 0.355 (0.355) loss 0.4808 (0.4808) acc 95.0000 (95.0000) lr 3.1545e+00 eta 0:00:14\n",
            "epoch [76/100] batch [1/1] time 0.551 (0.551) data 0.341 (0.341) loss 0.5398 (0.5398) acc 90.0000 (90.0000) lr 2.9289e+00 eta 0:00:13\n",
            "epoch [77/100] batch [1/1] time 0.568 (0.568) data 0.358 (0.358) loss 0.4461 (0.4461) acc 95.0000 (95.0000) lr 2.7103e+00 eta 0:00:13\n",
            "epoch [78/100] batch [1/1] time 0.550 (0.550) data 0.339 (0.339) loss 0.4649 (0.4649) acc 95.0000 (95.0000) lr 2.4989e+00 eta 0:00:12\n",
            "epoch [79/100] batch [1/1] time 0.545 (0.545) data 0.334 (0.334) loss 0.4584 (0.4584) acc 95.0000 (95.0000) lr 2.2949e+00 eta 0:00:11\n",
            "epoch [80/100] batch [1/1] time 0.538 (0.538) data 0.328 (0.328) loss 0.4431 (0.4431) acc 100.0000 (100.0000) lr 2.0984e+00 eta 0:00:10\n",
            "epoch [81/100] batch [1/1] time 0.557 (0.557) data 0.345 (0.345) loss 0.5046 (0.5046) acc 90.0000 (90.0000) lr 1.9098e+00 eta 0:00:10\n",
            "epoch [82/100] batch [1/1] time 0.579 (0.579) data 0.376 (0.376) loss 0.4385 (0.4385) acc 95.0000 (95.0000) lr 1.7292e+00 eta 0:00:10\n",
            "epoch [83/100] batch [1/1] time 0.552 (0.552) data 0.343 (0.343) loss 0.4505 (0.4505) acc 95.0000 (95.0000) lr 1.5567e+00 eta 0:00:09\n",
            "epoch [84/100] batch [1/1] time 0.551 (0.551) data 0.341 (0.341) loss 0.4744 (0.4744) acc 95.0000 (95.0000) lr 1.3926e+00 eta 0:00:08\n",
            "epoch [85/100] batch [1/1] time 0.543 (0.543) data 0.332 (0.332) loss 0.6269 (0.6269) acc 95.0000 (95.0000) lr 1.2369e+00 eta 0:00:08\n",
            "epoch [86/100] batch [1/1] time 0.545 (0.545) data 0.333 (0.333) loss 0.6012 (0.6012) acc 95.0000 (95.0000) lr 1.0899e+00 eta 0:00:07\n",
            "epoch [87/100] batch [1/1] time 0.546 (0.546) data 0.336 (0.336) loss 0.3600 (0.3600) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:07\n",
            "epoch [88/100] batch [1/1] time 0.554 (0.554) data 0.339 (0.339) loss 0.5584 (0.5584) acc 80.0000 (80.0000) lr 8.2245e-01 eta 0:00:06\n",
            "epoch [89/100] batch [1/1] time 0.556 (0.556) data 0.344 (0.344) loss 0.4880 (0.4880) acc 90.0000 (90.0000) lr 7.0224e-01 eta 0:00:06\n",
            "epoch [90/100] batch [1/1] time 0.534 (0.534) data 0.323 (0.323) loss 0.4366 (0.4366) acc 95.0000 (95.0000) lr 5.9119e-01 eta 0:00:05\n",
            "epoch [91/100] batch [1/1] time 0.534 (0.534) data 0.322 (0.322) loss 0.4036 (0.4036) acc 100.0000 (100.0000) lr 4.8943e-01 eta 0:00:04\n",
            "epoch [92/100] batch [1/1] time 0.541 (0.541) data 0.329 (0.329) loss 0.3842 (0.3842) acc 100.0000 (100.0000) lr 3.9706e-01 eta 0:00:04\n",
            "epoch [93/100] batch [1/1] time 0.580 (0.580) data 0.368 (0.368) loss 0.5129 (0.5129) acc 90.0000 (90.0000) lr 3.1417e-01 eta 0:00:04\n",
            "epoch [94/100] batch [1/1] time 0.571 (0.571) data 0.361 (0.361) loss 0.3558 (0.3558) acc 100.0000 (100.0000) lr 2.4083e-01 eta 0:00:03\n",
            "epoch [95/100] batch [1/1] time 0.555 (0.555) data 0.343 (0.343) loss 0.6216 (0.6216) acc 90.0000 (90.0000) lr 1.7713e-01 eta 0:00:02\n",
            "epoch [96/100] batch [1/1] time 0.565 (0.565) data 0.354 (0.354) loss 0.4702 (0.4702) acc 95.0000 (95.0000) lr 1.2312e-01 eta 0:00:02\n",
            "epoch [97/100] batch [1/1] time 0.543 (0.543) data 0.330 (0.330) loss 0.4110 (0.4110) acc 95.0000 (95.0000) lr 7.8853e-02 eta 0:00:01\n",
            "epoch [98/100] batch [1/1] time 0.539 (0.539) data 0.328 (0.328) loss 0.3568 (0.3568) acc 100.0000 (100.0000) lr 4.4380e-02 eta 0:00:01\n",
            "epoch [99/100] batch [1/1] time 0.560 (0.560) data 0.348 (0.348) loss 0.4389 (0.4389) acc 95.0000 (95.0000) lr 1.9733e-02 eta 0:00:00\n",
            "epoch [100/100] batch [1/1] time 0.546 (0.546) data 0.334 (0.334) loss 0.8674 (0.8674) acc 90.0000 (90.0000) lr 4.9344e-03 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed3/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.46it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 5,452\n",
            "* accuracy: 67.3%\n",
            "* error: 32.7%\n",
            "* macro_f1: 66.4%\n",
            "Elapsed: 0:01:48\n"
          ]
        }
      ],
      "source": [
        "#eurosat-2shots-seed3\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 3 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep100_2shots/seed3 \\\n",
        "        DATASET.NUM_SHOTS 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#eurosat-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_2shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0c2d3c-3f69-4f4f-f72b-d4f0619e3b58",
        "id": "qfxM2C3EiUYf"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 10:15:28.733158: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 10:15:28.753402: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 10:15:28.759813: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 10:15:28.776023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 10:15:29.780421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_2shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_2shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  10\n",
            "# val      10\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "        [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "        [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "        ...,\n",
            "        [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "        [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "        [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-5.2644e-04, -1.1168e-02, -2.1661e-02,  ..., -4.8596e-03,\n",
            "           5.8912e-03,  6.7493e-03],\n",
            "         [ 5.8841e-04, -1.8584e-02, -2.5060e-02,  ..., -3.4987e-03,\n",
            "           5.4105e-03, -1.7861e-03],\n",
            "         [ 1.7309e-03, -7.2606e-03, -2.3481e-02,  ..., -2.1107e-03,\n",
            "           4.9070e-03,  6.0702e-03],\n",
            "         ...,\n",
            "         [ 1.0223e-03, -1.2080e-02, -1.5868e-02,  ..., -1.2882e-02,\n",
            "          -2.0567e-03,  4.8362e-03],\n",
            "         [ 7.8468e-03, -1.1038e-02, -1.0770e-02,  ...,  7.4988e-04,\n",
            "           1.5215e-03, -1.6774e-03],\n",
            "         [ 2.3028e-03, -5.9798e-03, -1.7329e-02,  ...,  5.5724e-04,\n",
            "           1.5857e-02,  4.3269e-03]],\n",
            "\n",
            "        [[ 6.0997e-03,  1.7106e-03, -2.1188e-02,  ...,  2.5523e-03,\n",
            "           2.1776e-03,  7.7525e-03],\n",
            "         [ 6.9275e-03, -6.8095e-03, -1.3799e-02,  ..., -3.6580e-03,\n",
            "           5.6513e-03,  1.9980e-03],\n",
            "         [ 6.2942e-04, -7.5124e-03, -9.4807e-03,  ...,  9.2128e-03,\n",
            "           1.0227e-02,  3.9989e-03],\n",
            "         ...,\n",
            "         [-8.9265e-04, -1.5108e-02, -1.9400e-02,  ..., -3.9164e-03,\n",
            "           4.4435e-03, -3.3369e-04],\n",
            "         [ 9.0713e-03, -1.0043e-02, -2.8737e-02,  ..., -4.9359e-03,\n",
            "           9.1699e-03,  5.4980e-03],\n",
            "         [-8.9653e-05, -7.9620e-03, -2.2422e-02,  ..., -2.5861e-03,\n",
            "           2.1928e-03,  6.9095e-03]],\n",
            "\n",
            "        [[ 2.2506e-04, -1.2583e-02, -1.8306e-02,  ..., -1.1051e-02,\n",
            "           9.2462e-03,  8.1454e-03],\n",
            "         [-9.4987e-04, -1.2930e-02, -2.0111e-02,  ..., -9.7730e-03,\n",
            "           5.3495e-03,  1.8893e-03],\n",
            "         [-2.7428e-03, -8.8699e-03, -1.4280e-02,  ...,  5.0471e-03,\n",
            "           5.3924e-03,  1.0839e-02],\n",
            "         ...,\n",
            "         [ 7.2136e-03, -9.1107e-03, -1.5424e-02,  ...,  1.9115e-03,\n",
            "           6.7415e-03, -8.7823e-04],\n",
            "         [-4.8866e-03, -9.1675e-03, -1.5351e-02,  ..., -5.0427e-03,\n",
            "           8.2830e-03,  1.1029e-02],\n",
            "         [-4.4785e-03, -1.2034e-02, -1.6046e-02,  ..., -1.3892e-03,\n",
            "           1.1470e-02, -4.6686e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.1645e-03,  2.2294e-03, -2.3786e-02,  ..., -2.2859e-04,\n",
            "           6.9243e-03,  8.3819e-03],\n",
            "         [ 4.7722e-03, -5.7014e-03, -1.7818e-02,  ..., -5.6092e-03,\n",
            "           1.0013e-02,  1.3503e-03],\n",
            "         [-2.0599e-03, -4.0076e-03, -2.7204e-02,  ...,  2.8766e-03,\n",
            "           1.4182e-02, -4.1284e-04],\n",
            "         ...,\n",
            "         [-2.6131e-03, -1.2682e-02, -1.9781e-02,  ...,  2.6591e-03,\n",
            "           5.9818e-03,  1.0518e-02],\n",
            "         [ 5.3825e-03, -7.2549e-03, -1.3387e-02,  ..., -5.7954e-04,\n",
            "           5.6404e-03,  2.1621e-03],\n",
            "         [ 1.1158e-03,  5.8145e-04, -2.5354e-02,  ...,  1.2668e-03,\n",
            "           9.6658e-03,  5.6201e-03]],\n",
            "\n",
            "        [[ 5.7640e-03, -5.7109e-03, -1.9671e-02,  ..., -6.6449e-03,\n",
            "          -2.2398e-03,  3.5010e-04],\n",
            "         [ 4.0493e-03, -1.3674e-02, -2.2840e-02,  ..., -2.0630e-03,\n",
            "           1.2584e-02, -4.8818e-03],\n",
            "         [ 1.1436e-05, -3.8550e-03, -2.1955e-02,  ..., -3.6313e-03,\n",
            "           3.4708e-03,  2.7534e-03],\n",
            "         ...,\n",
            "         [ 7.8735e-03, -1.2430e-02, -2.0399e-02,  ..., -1.3462e-02,\n",
            "           1.2340e-02, -4.7330e-03],\n",
            "         [ 2.9120e-03, -1.2139e-02, -6.7417e-03,  ...,  4.5043e-04,\n",
            "           7.1866e-05,  9.1229e-04],\n",
            "         [-3.6163e-03, -4.2251e-03, -2.5861e-02,  ..., -6.7784e-03,\n",
            "           5.3829e-03, -2.0913e-03]],\n",
            "\n",
            "        [[ 1.0143e-02, -1.1706e-02, -2.0403e-02,  ..., -5.1286e-03,\n",
            "           9.2310e-03,  1.6630e-03],\n",
            "         [ 5.0354e-03, -6.9526e-03, -1.5557e-02,  ..., -3.4320e-03,\n",
            "           9.9329e-03,  5.0002e-03],\n",
            "         [ 3.1037e-03, -1.5677e-02, -2.5472e-02,  ..., -1.3558e-03,\n",
            "           2.2081e-03,  4.0732e-03],\n",
            "         ...,\n",
            "         [ 3.1457e-03, -4.8411e-03, -1.6359e-02,  ...,  3.0177e-03,\n",
            "           9.1890e-03, -4.2676e-03],\n",
            "         [ 5.7907e-03, -1.0981e-02, -2.1062e-02,  ..., -9.6700e-03,\n",
            "           4.3233e-03,  1.2432e-03],\n",
            "         [-5.3413e-05, -5.7033e-03, -2.5506e-02,  ..., -3.7009e-03,\n",
            "           7.3665e-03,  1.6910e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4741\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_2shots/seed1/tensorboard)\n",
            "epoch [1/50] batch [1/1] time 1.252 (1.252) data 0.383 (0.383) loss 9.1536 (9.1536) acc 20.0000 (20.0000) lr 2.0000e+01 eta 0:01:01\n",
            "epoch [2/50] batch [1/1] time 0.435 (0.435) data 0.296 (0.296) loss 8.8782 (8.8782) acc 20.0000 (20.0000) lr 1.9980e+01 eta 0:00:20\n",
            "epoch [3/50] batch [1/1] time 0.404 (0.404) data 0.278 (0.278) loss 7.1908 (7.1908) acc 0.0000 (0.0000) lr 1.9921e+01 eta 0:00:18\n",
            "epoch [4/50] batch [1/1] time 0.413 (0.413) data 0.285 (0.285) loss 4.2273 (4.2273) acc 40.0000 (40.0000) lr 1.9823e+01 eta 0:00:19\n",
            "epoch [5/50] batch [1/1] time 0.425 (0.425) data 0.299 (0.299) loss 3.3823 (3.3823) acc 50.0000 (50.0000) lr 1.9686e+01 eta 0:00:19\n",
            "epoch [6/50] batch [1/1] time 0.426 (0.426) data 0.300 (0.300) loss 2.9183 (2.9183) acc 40.0000 (40.0000) lr 1.9511e+01 eta 0:00:18\n",
            "epoch [7/50] batch [1/1] time 0.419 (0.419) data 0.293 (0.293) loss 3.0693 (3.0693) acc 20.0000 (20.0000) lr 1.9298e+01 eta 0:00:18\n",
            "epoch [8/50] batch [1/1] time 0.414 (0.414) data 0.286 (0.286) loss 3.9482 (3.9482) acc 20.0000 (20.0000) lr 1.9048e+01 eta 0:00:17\n",
            "epoch [9/50] batch [1/1] time 0.415 (0.415) data 0.286 (0.286) loss 4.3134 (4.3134) acc 20.0000 (20.0000) lr 1.8763e+01 eta 0:00:17\n",
            "epoch [10/50] batch [1/1] time 0.419 (0.419) data 0.292 (0.292) loss 3.9958 (3.9958) acc 50.0000 (50.0000) lr 1.8443e+01 eta 0:00:16\n",
            "epoch [11/50] batch [1/1] time 0.406 (0.406) data 0.276 (0.276) loss 2.6909 (2.6909) acc 70.0000 (70.0000) lr 1.8090e+01 eta 0:00:15\n",
            "epoch [12/50] batch [1/1] time 0.413 (0.413) data 0.284 (0.284) loss 2.9277 (2.9277) acc 50.0000 (50.0000) lr 1.7705e+01 eta 0:00:15\n",
            "epoch [13/50] batch [1/1] time 0.408 (0.408) data 0.280 (0.280) loss 1.7916 (1.7916) acc 70.0000 (70.0000) lr 1.7290e+01 eta 0:00:15\n",
            "epoch [14/50] batch [1/1] time 0.404 (0.404) data 0.278 (0.278) loss 1.8181 (1.8181) acc 70.0000 (70.0000) lr 1.6845e+01 eta 0:00:14\n",
            "epoch [15/50] batch [1/1] time 0.424 (0.424) data 0.292 (0.292) loss 1.7564 (1.7564) acc 70.0000 (70.0000) lr 1.6374e+01 eta 0:00:14\n",
            "epoch [16/50] batch [1/1] time 0.414 (0.414) data 0.284 (0.284) loss 2.0799 (2.0799) acc 70.0000 (70.0000) lr 1.5878e+01 eta 0:00:14\n",
            "epoch [17/50] batch [1/1] time 0.437 (0.437) data 0.310 (0.310) loss 1.5060 (1.5060) acc 90.0000 (90.0000) lr 1.5358e+01 eta 0:00:14\n",
            "epoch [18/50] batch [1/1] time 0.448 (0.448) data 0.322 (0.322) loss 1.1070 (1.1070) acc 90.0000 (90.0000) lr 1.4818e+01 eta 0:00:14\n",
            "epoch [19/50] batch [1/1] time 0.424 (0.424) data 0.298 (0.298) loss 1.4563 (1.4563) acc 70.0000 (70.0000) lr 1.4258e+01 eta 0:00:13\n",
            "epoch [20/50] batch [1/1] time 0.464 (0.464) data 0.334 (0.334) loss 1.0358 (1.0358) acc 90.0000 (90.0000) lr 1.3681e+01 eta 0:00:13\n",
            "epoch [21/50] batch [1/1] time 0.434 (0.434) data 0.305 (0.305) loss 1.1599 (1.1599) acc 80.0000 (80.0000) lr 1.3090e+01 eta 0:00:12\n",
            "epoch [22/50] batch [1/1] time 0.420 (0.420) data 0.293 (0.293) loss 1.0372 (1.0372) acc 90.0000 (90.0000) lr 1.2487e+01 eta 0:00:11\n",
            "epoch [23/50] batch [1/1] time 0.405 (0.405) data 0.276 (0.276) loss 0.8919 (0.8919) acc 90.0000 (90.0000) lr 1.1874e+01 eta 0:00:10\n",
            "epoch [24/50] batch [1/1] time 0.410 (0.410) data 0.284 (0.284) loss 0.9762 (0.9762) acc 80.0000 (80.0000) lr 1.1253e+01 eta 0:00:10\n",
            "epoch [25/50] batch [1/1] time 0.410 (0.410) data 0.280 (0.280) loss 1.3855 (1.3855) acc 80.0000 (80.0000) lr 1.0628e+01 eta 0:00:10\n",
            "epoch [26/50] batch [1/1] time 0.411 (0.411) data 0.284 (0.284) loss 1.0323 (1.0323) acc 80.0000 (80.0000) lr 1.0000e+01 eta 0:00:09\n",
            "epoch [27/50] batch [1/1] time 0.415 (0.415) data 0.288 (0.288) loss 0.6157 (0.6157) acc 100.0000 (100.0000) lr 9.3721e+00 eta 0:00:09\n",
            "epoch [28/50] batch [1/1] time 0.411 (0.411) data 0.283 (0.283) loss 0.7402 (0.7402) acc 90.0000 (90.0000) lr 8.7467e+00 eta 0:00:09\n",
            "epoch [29/50] batch [1/1] time 0.436 (0.436) data 0.307 (0.307) loss 0.5485 (0.5485) acc 100.0000 (100.0000) lr 8.1262e+00 eta 0:00:09\n",
            "epoch [30/50] batch [1/1] time 0.409 (0.409) data 0.279 (0.279) loss 0.9026 (0.9026) acc 90.0000 (90.0000) lr 7.5131e+00 eta 0:00:08\n",
            "epoch [31/50] batch [1/1] time 0.421 (0.421) data 0.289 (0.289) loss 0.5458 (0.5458) acc 100.0000 (100.0000) lr 6.9098e+00 eta 0:00:07\n",
            "epoch [32/50] batch [1/1] time 0.409 (0.409) data 0.282 (0.282) loss 0.5759 (0.5759) acc 100.0000 (100.0000) lr 6.3188e+00 eta 0:00:07\n",
            "epoch [33/50] batch [1/1] time 0.414 (0.414) data 0.283 (0.283) loss 0.5576 (0.5576) acc 100.0000 (100.0000) lr 5.7422e+00 eta 0:00:07\n",
            "epoch [34/50] batch [1/1] time 0.426 (0.426) data 0.296 (0.296) loss 0.6201 (0.6201) acc 90.0000 (90.0000) lr 5.1825e+00 eta 0:00:06\n",
            "epoch [35/50] batch [1/1] time 0.419 (0.419) data 0.290 (0.290) loss 0.7934 (0.7934) acc 90.0000 (90.0000) lr 4.6417e+00 eta 0:00:06\n",
            "epoch [36/50] batch [1/1] time 0.402 (0.402) data 0.273 (0.273) loss 0.5069 (0.5069) acc 100.0000 (100.0000) lr 4.1221e+00 eta 0:00:05\n",
            "epoch [37/50] batch [1/1] time 0.407 (0.407) data 0.280 (0.280) loss 0.5405 (0.5405) acc 100.0000 (100.0000) lr 3.6258e+00 eta 0:00:05\n",
            "epoch [38/50] batch [1/1] time 0.407 (0.407) data 0.277 (0.277) loss 0.4533 (0.4533) acc 100.0000 (100.0000) lr 3.1545e+00 eta 0:00:04\n",
            "epoch [39/50] batch [1/1] time 0.421 (0.421) data 0.292 (0.292) loss 0.7915 (0.7915) acc 90.0000 (90.0000) lr 2.7103e+00 eta 0:00:04\n",
            "epoch [40/50] batch [1/1] time 0.400 (0.400) data 0.272 (0.272) loss 0.4325 (0.4325) acc 100.0000 (100.0000) lr 2.2949e+00 eta 0:00:04\n",
            "epoch [41/50] batch [1/1] time 0.412 (0.412) data 0.281 (0.281) loss 0.5752 (0.5752) acc 100.0000 (100.0000) lr 1.9098e+00 eta 0:00:03\n",
            "epoch [42/50] batch [1/1] time 0.411 (0.411) data 0.283 (0.283) loss 0.4173 (0.4173) acc 100.0000 (100.0000) lr 1.5567e+00 eta 0:00:03\n",
            "epoch [43/50] batch [1/1] time 0.430 (0.430) data 0.300 (0.300) loss 0.4261 (0.4261) acc 100.0000 (100.0000) lr 1.2369e+00 eta 0:00:03\n",
            "epoch [44/50] batch [1/1] time 0.452 (0.452) data 0.321 (0.321) loss 0.4859 (0.4859) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:02\n",
            "epoch [45/50] batch [1/1] time 0.425 (0.425) data 0.295 (0.295) loss 0.4322 (0.4322) acc 100.0000 (100.0000) lr 7.0224e-01 eta 0:00:02\n",
            "epoch [46/50] batch [1/1] time 0.447 (0.447) data 0.318 (0.318) loss 0.9639 (0.9639) acc 90.0000 (90.0000) lr 4.8943e-01 eta 0:00:01\n",
            "epoch [47/50] batch [1/1] time 0.425 (0.425) data 0.295 (0.295) loss 0.5023 (0.5023) acc 100.0000 (100.0000) lr 3.1417e-01 eta 0:00:01\n",
            "epoch [48/50] batch [1/1] time 0.418 (0.418) data 0.288 (0.288) loss 0.3848 (0.3848) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:00\n",
            "epoch [49/50] batch [1/1] time 0.397 (0.397) data 0.279 (0.279) loss 0.3807 (0.3807) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:00\n",
            "epoch [50/50] batch [1/1] time 0.408 (0.408) data 0.278 (0.278) loss 0.8928 (0.8928) acc 90.0000 (90.0000) lr 1.9733e-02 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_2shots/seed1/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:32<00:00,  2.46it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 3,719\n",
            "* accuracy: 45.9%\n",
            "* error: 54.1%\n",
            "* macro_f1: 42.5%\n",
            "Elapsed: 0:00:58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-J0_SOKousA"
      },
      "source": [
        "#Oxfordpets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUQduJ4MxAah"
      },
      "source": [
        "##prototype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "c187tYLmozdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b02b52a-320f-4f9d-c44c-0e67bb00960c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 16:04:47.525444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 16:04:47.545691: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 16:04:47.551630: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 16:04:47.565991: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 16:04:48.566678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_16-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  592\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "OxfordPets (SHOTS: 16)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets prototype-16shots-seed1\n",
        "\n",
        "!python train.py \\\n",
        "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "            --seed 1 \\\n",
        "            --trainer DAPT \\\n",
        "            --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "            --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "            DATASET.NUM_SHOTS 16 \\\n",
        "            TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "WU7zM8d1xRJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426e540e-4460-4911-a4e1-04d8ece033bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 16:12:14.085238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 16:12:14.104782: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 16:12:14.110706: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 16:12:14.124805: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 16:12:15.130202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_8-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  296\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "OxfordPets (SHOTS: 8)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets prototype-8shots-seed1\n",
        "\n",
        "!python train.py \\\n",
        "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "            --seed 1 \\\n",
        "            --trainer DAPT \\\n",
        "            --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "            --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "            DATASET.NUM_SHOTS 8 \\\n",
        "            TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Q8m2C6mtxVaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1832b0-930a-4345-9362-f8ce329f1164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 16:15:13.309144: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 16:15:13.328960: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 16:15:13.334750: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 16:15:13.348917: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 16:15:14.356061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_4-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  148\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "OxfordPets (SHOTS: 4)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets prototype-4shots-seed1\n",
        "\n",
        "!python train.py \\\n",
        "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "            --seed 1 \\\n",
        "            --trainer DAPT \\\n",
        "            --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "            --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "            DATASET.NUM_SHOTS 4 \\\n",
        "            TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7sJnrR6YxYeU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8403944c-67a4-4ccd-a85b-020494e96aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 16:16:48.929946: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 16:16:48.950133: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 16:16:48.956347: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 16:16:48.971104: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 16:16:49.969653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_2-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  74\n",
            "# val      74\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "OxfordPets (SHOTS: 2)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets prototype-2shots-seed1\n",
        "\n",
        "!python train.py \\\n",
        "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "            --seed 1 \\\n",
        "            --trainer DAPT \\\n",
        "            --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "            --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "            DATASET.NUM_SHOTS 2 \\\n",
        "            TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XD4SC5mVxcb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74e4bc7-4e0a-45da-9f18-fec4fb2af49d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 16:17:37.387318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 16:17:37.407042: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 16:17:37.412983: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 16:17:37.426988: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 16:17:38.445940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1', 'TRAINER.DAPT.PROTOTYPE_GEN', 'True']\n",
            "output_dir: \n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: ./output\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: True\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  37\n",
            "# val      37\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "=================================\n",
            "Prototype generator\n",
            "OxfordPets (SHOTS: 1)\n",
            "=================================\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Making prototype finished!!\n",
            "Loading evaluator: Classification\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets prototype-1shots-seed1\n",
        "\n",
        "!python train.py \\\n",
        "            --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "            --seed 1 \\\n",
        "            --trainer DAPT \\\n",
        "            --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "            --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "            DATASET.NUM_SHOTS 1 \\\n",
        "            TRAINER.DAPT.PROTOTYPE_GEN True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OabBykT5x3jo"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "rMzYnHP0x5EA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07ad159-faea-4f9c-a2c6-fbfbe5e154f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-07 16:39:03.979006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 16:39:03.999143: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 16:39:04.005221: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 16:39:04.020437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 16:39:05.019528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16']\n",
            "output_dir: output/1207_new_init/oxford_pets/DAPT/vit_b16_16shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/oxford_pets/DAPT/vit_b16_16shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_16-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  592\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/oxford_pets/DAPT/vit_b16_16shots/seed1/tensorboard)\n",
            "epoch [1/200] batch [5/18] time 0.398 (0.844) data 0.000 (0.201) loss 0.8605 (1.0948) acc 68.7500 (66.2500) lr 1.0000e-05 eta 0:50:32\n",
            "epoch [1/200] batch [10/18] time 0.404 (0.623) data 0.000 (0.101) loss 1.2071 (1.1991) acc 62.5000 (64.6875) lr 1.0000e-05 eta 0:37:17\n",
            "epoch [1/200] batch [15/18] time 0.405 (0.550) data 0.000 (0.067) loss 1.3497 (1.1963) acc 62.5000 (65.6250) lr 1.0000e-05 eta 0:32:51\n",
            "epoch [2/200] batch [5/18] time 0.405 (0.578) data 0.000 (0.176) loss 1.2038 (0.9923) acc 78.1250 (77.5000) lr 2.0000e-02 eta 0:34:27\n",
            "epoch [2/200] batch [10/18] time 0.408 (0.491) data 0.000 (0.088) loss 0.7728 (0.9274) acc 81.2500 (75.9375) lr 2.0000e-02 eta 0:29:14\n",
            "epoch [2/200] batch [15/18] time 0.406 (0.463) data 0.000 (0.059) loss 1.0603 (0.9989) acc 75.0000 (73.5417) lr 2.0000e-02 eta 0:27:31\n",
            "epoch [3/200] batch [5/18] time 0.409 (0.588) data 0.000 (0.180) loss 0.4986 (0.7526) acc 84.3750 (77.5000) lr 1.9999e-02 eta 0:34:52\n",
            "epoch [3/200] batch [10/18] time 0.410 (0.499) data 0.000 (0.090) loss 0.8420 (0.7741) acc 78.1250 (76.8750) lr 1.9999e-02 eta 0:29:34\n",
            "epoch [3/200] batch [15/18] time 0.408 (0.470) data 0.000 (0.060) loss 0.6072 (0.8208) acc 84.3750 (76.2500) lr 1.9999e-02 eta 0:27:46\n",
            "epoch [4/200] batch [5/18] time 0.411 (0.590) data 0.000 (0.178) loss 0.7601 (0.7569) acc 81.2500 (79.3750) lr 1.9995e-02 eta 0:34:48\n",
            "epoch [4/200] batch [10/18] time 0.413 (0.502) data 0.000 (0.089) loss 0.6022 (0.7892) acc 81.2500 (78.1250) lr 1.9995e-02 eta 0:29:34\n",
            "epoch [4/200] batch [15/18] time 0.417 (0.473) data 0.000 (0.060) loss 0.5189 (0.7122) acc 81.2500 (79.5833) lr 1.9995e-02 eta 0:27:51\n",
            "epoch [5/200] batch [5/18] time 0.420 (0.593) data 0.000 (0.174) loss 1.0250 (0.8357) acc 71.8750 (75.0000) lr 1.9989e-02 eta 0:34:47\n",
            "epoch [5/200] batch [10/18] time 0.417 (0.504) data 0.000 (0.087) loss 0.7797 (0.7587) acc 87.5000 (77.8125) lr 1.9989e-02 eta 0:29:33\n",
            "epoch [5/200] batch [15/18] time 0.415 (0.475) data 0.000 (0.058) loss 0.4728 (0.7303) acc 93.7500 (78.5417) lr 1.9989e-02 eta 0:27:49\n",
            "epoch [6/200] batch [5/18] time 0.421 (0.608) data 0.000 (0.188) loss 0.5196 (0.7654) acc 90.6250 (77.5000) lr 1.9980e-02 eta 0:35:32\n",
            "epoch [6/200] batch [10/18] time 0.423 (0.515) data 0.000 (0.094) loss 0.5290 (0.7366) acc 84.3750 (78.1250) lr 1.9980e-02 eta 0:30:01\n",
            "epoch [6/200] batch [15/18] time 0.427 (0.484) data 0.000 (0.063) loss 0.4334 (0.6979) acc 90.6250 (80.0000) lr 1.9980e-02 eta 0:28:12\n",
            "epoch [7/200] batch [5/18] time 0.426 (0.600) data 0.000 (0.176) loss 0.4661 (0.6218) acc 90.6250 (84.3750) lr 1.9969e-02 eta 0:34:52\n",
            "epoch [7/200] batch [10/18] time 0.424 (0.512) data 0.001 (0.088) loss 0.6532 (0.5912) acc 75.0000 (84.0625) lr 1.9969e-02 eta 0:29:43\n",
            "epoch [7/200] batch [15/18] time 0.426 (0.483) data 0.000 (0.059) loss 0.7649 (0.6213) acc 75.0000 (82.0833) lr 1.9969e-02 eta 0:28:00\n",
            "epoch [8/200] batch [5/18] time 0.434 (0.598) data 0.000 (0.171) loss 0.6831 (0.6352) acc 75.0000 (83.1250) lr 1.9956e-02 eta 0:34:32\n",
            "epoch [8/200] batch [10/18] time 0.433 (0.513) data 0.000 (0.085) loss 0.9247 (0.6167) acc 68.7500 (81.8750) lr 1.9956e-02 eta 0:29:36\n",
            "epoch [8/200] batch [15/18] time 0.429 (0.485) data 0.000 (0.057) loss 0.4467 (0.6246) acc 81.2500 (82.2917) lr 1.9956e-02 eta 0:27:58\n",
            "epoch [9/200] batch [5/18] time 0.446 (0.621) data 0.000 (0.187) loss 0.3399 (0.5164) acc 93.7500 (88.1250) lr 1.9940e-02 eta 0:35:43\n",
            "epoch [9/200] batch [10/18] time 0.437 (0.528) data 0.000 (0.094) loss 0.5713 (0.5236) acc 87.5000 (86.8750) lr 1.9940e-02 eta 0:30:19\n",
            "epoch [9/200] batch [15/18] time 0.441 (0.498) data 0.000 (0.062) loss 0.5955 (0.5774) acc 68.7500 (83.1250) lr 1.9940e-02 eta 0:28:33\n",
            "epoch [10/200] batch [5/18] time 0.449 (0.618) data 0.000 (0.178) loss 0.4561 (0.5467) acc 90.6250 (85.0000) lr 1.9921e-02 eta 0:35:20\n",
            "epoch [10/200] batch [10/18] time 0.442 (0.529) data 0.000 (0.089) loss 0.4146 (0.5601) acc 84.3750 (84.3750) lr 1.9921e-02 eta 0:30:12\n",
            "epoch [10/200] batch [15/18] time 0.446 (0.500) data 0.000 (0.059) loss 0.3737 (0.5638) acc 90.6250 (83.7500) lr 1.9921e-02 eta 0:28:31\n",
            "epoch [11/200] batch [5/18] time 0.454 (0.630) data 0.000 (0.184) loss 0.6026 (0.5777) acc 84.3750 (82.5000) lr 1.9900e-02 eta 0:35:51\n",
            "epoch [11/200] batch [10/18] time 0.449 (0.539) data 0.000 (0.092) loss 0.1785 (0.4902) acc 100.0000 (86.2500) lr 1.9900e-02 eta 0:30:38\n",
            "epoch [11/200] batch [15/18] time 0.450 (0.510) data 0.000 (0.061) loss 0.4394 (0.5218) acc 87.5000 (84.5833) lr 1.9900e-02 eta 0:28:55\n",
            "epoch [12/200] batch [5/18] time 0.460 (0.638) data 0.000 (0.184) loss 0.4567 (0.4945) acc 87.5000 (85.0000) lr 1.9877e-02 eta 0:36:05\n",
            "epoch [12/200] batch [10/18] time 0.456 (0.547) data 0.000 (0.092) loss 0.5178 (0.5519) acc 87.5000 (84.6875) lr 1.9877e-02 eta 0:30:54\n",
            "epoch [12/200] batch [15/18] time 0.454 (0.517) data 0.000 (0.061) loss 0.7003 (0.5589) acc 78.1250 (83.3333) lr 1.9877e-02 eta 0:29:09\n",
            "epoch [13/200] batch [5/18] time 0.459 (0.626) data 0.000 (0.171) loss 0.5517 (0.5848) acc 87.5000 (81.8750) lr 1.9851e-02 eta 0:35:15\n",
            "epoch [13/200] batch [10/18] time 0.453 (0.540) data 0.000 (0.086) loss 0.4409 (0.5984) acc 90.6250 (81.8750) lr 1.9851e-02 eta 0:30:22\n",
            "epoch [13/200] batch [15/18] time 0.451 (0.511) data 0.000 (0.057) loss 0.4221 (0.5777) acc 93.7500 (82.7083) lr 1.9851e-02 eta 0:28:41\n",
            "epoch [14/200] batch [5/18] time 0.451 (0.643) data 0.000 (0.197) loss 0.2322 (0.4090) acc 90.6250 (87.5000) lr 1.9823e-02 eta 0:36:01\n",
            "epoch [14/200] batch [10/18] time 0.449 (0.545) data 0.000 (0.099) loss 0.5609 (0.4968) acc 84.3750 (86.2500) lr 1.9823e-02 eta 0:30:29\n",
            "epoch [14/200] batch [15/18] time 0.445 (0.512) data 0.000 (0.066) loss 0.7220 (0.5322) acc 75.0000 (85.6250) lr 1.9823e-02 eta 0:28:34\n",
            "epoch [15/200] batch [5/18] time 0.451 (0.629) data 0.000 (0.186) loss 0.4495 (0.5189) acc 84.3750 (84.3750) lr 1.9792e-02 eta 0:35:02\n",
            "epoch [15/200] batch [10/18] time 0.444 (0.536) data 0.000 (0.093) loss 0.6693 (0.5000) acc 81.2500 (84.3750) lr 1.9792e-02 eta 0:29:49\n",
            "epoch [15/200] batch [15/18] time 0.441 (0.505) data 0.000 (0.062) loss 0.4341 (0.4971) acc 84.3750 (85.0000) lr 1.9792e-02 eta 0:28:01\n",
            "epoch [16/200] batch [5/18] time 0.447 (0.613) data 0.000 (0.174) loss 0.3620 (0.5223) acc 87.5000 (82.5000) lr 1.9759e-02 eta 0:33:59\n",
            "epoch [16/200] batch [10/18] time 0.441 (0.527) data 0.000 (0.087) loss 0.3631 (0.4973) acc 87.5000 (84.6875) lr 1.9759e-02 eta 0:29:08\n",
            "epoch [16/200] batch [15/18] time 0.439 (0.498) data 0.000 (0.058) loss 0.1982 (0.4465) acc 93.7500 (85.8333) lr 1.9759e-02 eta 0:27:30\n",
            "epoch [17/200] batch [5/18] time 0.442 (0.630) data 0.000 (0.192) loss 0.5576 (0.4324) acc 84.3750 (88.1250) lr 1.9724e-02 eta 0:34:43\n",
            "epoch [17/200] batch [10/18] time 0.438 (0.535) data 0.000 (0.096) loss 0.4066 (0.5139) acc 87.5000 (86.2500) lr 1.9724e-02 eta 0:29:26\n",
            "epoch [17/200] batch [15/18] time 0.440 (0.503) data 0.000 (0.064) loss 0.1176 (0.4770) acc 100.0000 (87.2917) lr 1.9724e-02 eta 0:27:37\n",
            "epoch [18/200] batch [5/18] time 0.447 (0.620) data 0.000 (0.179) loss 0.4217 (0.4696) acc 90.6250 (86.2500) lr 1.9686e-02 eta 0:33:58\n",
            "epoch [18/200] batch [10/18] time 0.441 (0.529) data 0.000 (0.090) loss 0.5302 (0.5085) acc 90.6250 (85.6250) lr 1.9686e-02 eta 0:28:58\n",
            "epoch [18/200] batch [15/18] time 0.443 (0.500) data 0.000 (0.060) loss 0.2328 (0.5177) acc 96.8750 (85.8333) lr 1.9686e-02 eta 0:27:18\n",
            "epoch [19/200] batch [5/18] time 0.449 (0.621) data 0.000 (0.180) loss 0.3846 (0.4323) acc 90.6250 (86.2500) lr 1.9646e-02 eta 0:33:51\n",
            "epoch [19/200] batch [10/18] time 0.437 (0.531) data 0.000 (0.090) loss 0.4455 (0.4353) acc 87.5000 (86.8750) lr 1.9646e-02 eta 0:28:53\n",
            "epoch [19/200] batch [15/18] time 0.441 (0.501) data 0.000 (0.060) loss 0.5674 (0.4443) acc 78.1250 (87.5000) lr 1.9646e-02 eta 0:27:14\n",
            "epoch [20/200] batch [5/18] time 0.451 (0.622) data 0.000 (0.179) loss 0.3243 (0.3586) acc 90.6250 (90.6250) lr 1.9603e-02 eta 0:33:44\n",
            "epoch [20/200] batch [10/18] time 0.443 (0.533) data 0.000 (0.090) loss 0.6542 (0.4359) acc 81.2500 (88.1250) lr 1.9603e-02 eta 0:28:50\n",
            "epoch [20/200] batch [15/18] time 0.443 (0.503) data 0.000 (0.060) loss 0.5232 (0.4058) acc 84.3750 (89.3750) lr 1.9603e-02 eta 0:27:11\n",
            "epoch [21/200] batch [5/18] time 0.451 (0.628) data 0.000 (0.185) loss 0.6048 (0.4050) acc 78.1250 (88.1250) lr 1.9558e-02 eta 0:33:52\n",
            "epoch [21/200] batch [10/18] time 0.448 (0.536) data 0.000 (0.093) loss 0.6438 (0.4490) acc 84.3750 (86.5625) lr 1.9558e-02 eta 0:28:52\n",
            "epoch [21/200] batch [15/18] time 0.445 (0.506) data 0.000 (0.062) loss 0.4787 (0.4227) acc 87.5000 (88.1250) lr 1.9558e-02 eta 0:27:11\n",
            "epoch [22/200] batch [5/18] time 0.453 (0.634) data 0.000 (0.188) loss 0.4188 (0.5194) acc 87.5000 (86.8750) lr 1.9511e-02 eta 0:33:59\n",
            "epoch [22/200] batch [10/18] time 0.447 (0.540) data 0.000 (0.094) loss 0.2139 (0.4833) acc 93.7500 (86.8750) lr 1.9511e-02 eta 0:28:55\n",
            "epoch [22/200] batch [15/18] time 0.447 (0.509) data 0.000 (0.063) loss 0.2789 (0.4632) acc 90.6250 (87.0833) lr 1.9511e-02 eta 0:27:11\n",
            "epoch [23/200] batch [5/18] time 0.455 (0.624) data 0.000 (0.178) loss 0.3475 (0.3905) acc 87.5000 (88.7500) lr 1.9461e-02 eta 0:33:15\n",
            "epoch [23/200] batch [10/18] time 0.445 (0.535) data 0.000 (0.089) loss 0.4054 (0.4164) acc 90.6250 (88.7500) lr 1.9461e-02 eta 0:28:27\n",
            "epoch [23/200] batch [15/18] time 0.451 (0.505) data 0.000 (0.059) loss 0.4021 (0.4193) acc 84.3750 (89.1667) lr 1.9461e-02 eta 0:26:48\n",
            "epoch [24/200] batch [5/18] time 0.445 (0.604) data 0.000 (0.162) loss 0.3665 (0.5173) acc 93.7500 (88.1250) lr 1.9409e-02 eta 0:32:01\n",
            "epoch [24/200] batch [10/18] time 0.446 (0.524) data 0.000 (0.081) loss 0.5277 (0.4339) acc 87.5000 (90.3125) lr 1.9409e-02 eta 0:27:44\n",
            "epoch [24/200] batch [15/18] time 0.446 (0.497) data 0.000 (0.054) loss 0.3893 (0.4781) acc 90.6250 (88.3333) lr 1.9409e-02 eta 0:26:17\n",
            "epoch [25/200] batch [5/18] time 0.449 (0.628) data 0.000 (0.183) loss 0.6652 (0.4822) acc 84.3750 (85.6250) lr 1.9354e-02 eta 0:33:05\n",
            "epoch [25/200] batch [10/18] time 0.446 (0.535) data 0.000 (0.092) loss 0.3147 (0.4691) acc 90.6250 (87.1875) lr 1.9354e-02 eta 0:28:11\n",
            "epoch [25/200] batch [15/18] time 0.444 (0.505) data 0.000 (0.061) loss 0.2565 (0.4250) acc 93.7500 (87.7083) lr 1.9354e-02 eta 0:26:32\n",
            "epoch [26/200] batch [5/18] time 0.450 (0.632) data 0.000 (0.191) loss 0.3441 (0.3686) acc 87.5000 (88.1250) lr 1.9298e-02 eta 0:33:08\n",
            "epoch [26/200] batch [10/18] time 0.447 (0.538) data 0.000 (0.096) loss 0.6401 (0.4210) acc 78.1250 (86.2500) lr 1.9298e-02 eta 0:28:09\n",
            "epoch [26/200] batch [15/18] time 0.445 (0.507) data 0.000 (0.064) loss 0.2155 (0.4227) acc 90.6250 (87.2917) lr 1.9298e-02 eta 0:26:29\n",
            "epoch [27/200] batch [5/18] time 0.451 (0.629) data 0.001 (0.187) loss 0.3883 (0.3120) acc 90.6250 (92.5000) lr 1.9239e-02 eta 0:32:46\n",
            "epoch [27/200] batch [10/18] time 0.444 (0.536) data 0.000 (0.094) loss 0.3701 (0.3630) acc 90.6250 (90.3125) lr 1.9239e-02 eta 0:27:53\n",
            "epoch [27/200] batch [15/18] time 0.446 (0.505) data 0.000 (0.062) loss 0.6966 (0.3729) acc 84.3750 (90.0000) lr 1.9239e-02 eta 0:26:15\n",
            "epoch [28/200] batch [5/18] time 0.447 (0.633) data 0.000 (0.192) loss 0.2755 (0.4180) acc 90.6250 (90.0000) lr 1.9178e-02 eta 0:32:48\n",
            "epoch [28/200] batch [10/18] time 0.444 (0.539) data 0.000 (0.096) loss 0.6137 (0.4363) acc 87.5000 (89.0625) lr 1.9178e-02 eta 0:27:53\n",
            "epoch [28/200] batch [15/18] time 0.445 (0.508) data 0.000 (0.064) loss 0.2425 (0.4301) acc 87.5000 (88.3333) lr 1.9178e-02 eta 0:26:13\n",
            "epoch [29/200] batch [5/18] time 0.450 (0.623) data 0.000 (0.179) loss 0.6572 (0.4350) acc 81.2500 (88.1250) lr 1.9114e-02 eta 0:32:04\n",
            "epoch [29/200] batch [10/18] time 0.448 (0.533) data 0.000 (0.090) loss 0.4905 (0.4201) acc 90.6250 (88.4375) lr 1.9114e-02 eta 0:27:26\n",
            "epoch [29/200] batch [15/18] time 0.449 (0.504) data 0.000 (0.060) loss 0.5352 (0.3931) acc 81.2500 (89.3750) lr 1.9114e-02 eta 0:25:52\n",
            "epoch [30/200] batch [5/18] time 0.447 (0.609) data 0.000 (0.168) loss 0.2498 (0.4055) acc 96.8750 (91.2500) lr 1.9048e-02 eta 0:31:11\n",
            "epoch [30/200] batch [10/18] time 0.445 (0.527) data 0.000 (0.084) loss 0.2808 (0.3868) acc 93.7500 (89.6875) lr 1.9048e-02 eta 0:26:56\n",
            "epoch [30/200] batch [15/18] time 0.446 (0.500) data 0.000 (0.056) loss 0.2895 (0.3638) acc 93.7500 (90.6250) lr 1.9048e-02 eta 0:25:30\n",
            "epoch [31/200] batch [5/18] time 0.447 (0.615) data 0.000 (0.174) loss 0.5834 (0.3421) acc 81.2500 (90.6250) lr 1.8980e-02 eta 0:31:20\n",
            "epoch [31/200] batch [10/18] time 0.445 (0.530) data 0.000 (0.087) loss 0.4048 (0.4129) acc 84.3750 (88.4375) lr 1.8980e-02 eta 0:26:55\n",
            "epoch [31/200] batch [15/18] time 0.444 (0.501) data 0.000 (0.058) loss 0.4466 (0.3862) acc 87.5000 (88.3333) lr 1.8980e-02 eta 0:25:25\n",
            "epoch [32/200] batch [5/18] time 0.453 (0.639) data 0.000 (0.195) loss 0.2825 (0.3327) acc 93.7500 (90.6250) lr 1.8910e-02 eta 0:32:19\n",
            "epoch [32/200] batch [10/18] time 0.444 (0.541) data 0.000 (0.098) loss 0.1947 (0.2829) acc 93.7500 (93.1250) lr 1.8910e-02 eta 0:27:21\n",
            "epoch [32/200] batch [15/18] time 0.443 (0.509) data 0.000 (0.065) loss 0.1842 (0.2750) acc 93.7500 (93.1250) lr 1.8910e-02 eta 0:25:41\n",
            "epoch [33/200] batch [5/18] time 0.452 (0.634) data 0.000 (0.190) loss 0.4370 (0.4056) acc 84.3750 (89.3750) lr 1.8838e-02 eta 0:31:53\n",
            "epoch [33/200] batch [10/18] time 0.445 (0.539) data 0.000 (0.095) loss 0.4698 (0.4453) acc 87.5000 (88.1250) lr 1.8838e-02 eta 0:27:03\n",
            "epoch [33/200] batch [15/18] time 0.446 (0.508) data 0.000 (0.063) loss 0.2739 (0.3809) acc 93.7500 (90.2083) lr 1.8838e-02 eta 0:25:27\n",
            "epoch [34/200] batch [5/18] time 0.451 (0.609) data 0.000 (0.165) loss 0.3435 (0.3626) acc 90.6250 (91.2500) lr 1.8763e-02 eta 0:30:27\n",
            "epoch [34/200] batch [10/18] time 0.445 (0.527) data 0.000 (0.083) loss 0.4753 (0.3195) acc 87.5000 (92.8125) lr 1.8763e-02 eta 0:26:18\n",
            "epoch [34/200] batch [15/18] time 0.445 (0.500) data 0.000 (0.055) loss 0.5431 (0.3924) acc 84.3750 (90.4167) lr 1.8763e-02 eta 0:24:54\n",
            "epoch [35/200] batch [5/18] time 0.454 (0.623) data 0.000 (0.177) loss 0.2568 (0.3500) acc 93.7500 (90.6250) lr 1.8686e-02 eta 0:30:57\n",
            "epoch [35/200] batch [10/18] time 0.447 (0.533) data 0.000 (0.089) loss 0.3543 (0.3708) acc 87.5000 (89.0625) lr 1.8686e-02 eta 0:26:28\n",
            "epoch [35/200] batch [15/18] time 0.445 (0.504) data 0.000 (0.059) loss 0.4887 (0.3648) acc 87.5000 (90.2083) lr 1.8686e-02 eta 0:24:58\n",
            "epoch [36/200] batch [5/18] time 0.453 (0.628) data 0.000 (0.185) loss 0.5734 (0.4420) acc 84.3750 (88.7500) lr 1.8607e-02 eta 0:31:02\n",
            "epoch [36/200] batch [10/18] time 0.444 (0.536) data 0.000 (0.093) loss 0.4438 (0.4035) acc 90.6250 (90.9375) lr 1.8607e-02 eta 0:26:25\n",
            "epoch [36/200] batch [15/18] time 0.446 (0.506) data 0.000 (0.062) loss 0.1776 (0.3960) acc 90.6250 (90.2083) lr 1.8607e-02 eta 0:24:54\n",
            "epoch [37/200] batch [5/18] time 0.457 (0.615) data 0.000 (0.170) loss 0.2704 (0.2167) acc 90.6250 (94.3750) lr 1.8526e-02 eta 0:30:13\n",
            "epoch [37/200] batch [10/18] time 0.445 (0.529) data 0.001 (0.085) loss 0.4709 (0.3040) acc 84.3750 (91.8750) lr 1.8526e-02 eta 0:25:57\n",
            "epoch [37/200] batch [15/18] time 0.448 (0.502) data 0.000 (0.057) loss 0.1324 (0.3287) acc 100.0000 (90.8333) lr 1.8526e-02 eta 0:24:32\n",
            "epoch [38/200] batch [5/18] time 0.452 (0.627) data 0.000 (0.184) loss 0.5031 (0.3670) acc 84.3750 (90.0000) lr 1.8443e-02 eta 0:30:35\n",
            "epoch [38/200] batch [10/18] time 0.447 (0.535) data 0.000 (0.092) loss 0.2533 (0.3536) acc 90.6250 (89.6875) lr 1.8443e-02 eta 0:26:03\n",
            "epoch [38/200] batch [15/18] time 0.444 (0.505) data 0.000 (0.061) loss 0.2446 (0.3316) acc 90.6250 (91.2500) lr 1.8443e-02 eta 0:24:32\n",
            "epoch [39/200] batch [5/18] time 0.449 (0.616) data 0.000 (0.172) loss 0.4213 (0.2639) acc 84.3750 (92.5000) lr 1.8358e-02 eta 0:29:52\n",
            "epoch [39/200] batch [10/18] time 0.443 (0.530) data 0.000 (0.086) loss 0.3053 (0.2540) acc 90.6250 (92.5000) lr 1.8358e-02 eta 0:25:41\n",
            "epoch [39/200] batch [15/18] time 0.442 (0.501) data 0.000 (0.058) loss 0.1567 (0.2591) acc 100.0000 (92.5000) lr 1.8358e-02 eta 0:24:14\n",
            "epoch [40/200] batch [5/18] time 0.452 (0.612) data 0.000 (0.169) loss 0.5474 (0.2917) acc 75.0000 (91.8750) lr 1.8271e-02 eta 0:29:29\n",
            "epoch [40/200] batch [10/18] time 0.441 (0.528) data 0.000 (0.085) loss 0.4581 (0.3050) acc 84.3750 (91.8750) lr 1.8271e-02 eta 0:25:23\n",
            "epoch [40/200] batch [15/18] time 0.442 (0.500) data 0.000 (0.057) loss 0.3892 (0.3397) acc 93.7500 (90.8333) lr 1.8271e-02 eta 0:24:01\n",
            "epoch [41/200] batch [5/18] time 0.453 (0.637) data 0.000 (0.195) loss 0.2376 (0.3219) acc 93.7500 (91.8750) lr 1.8181e-02 eta 0:30:32\n",
            "epoch [41/200] batch [10/18] time 0.445 (0.541) data 0.000 (0.098) loss 0.4482 (0.3918) acc 87.5000 (89.6875) lr 1.8181e-02 eta 0:25:53\n",
            "epoch [41/200] batch [15/18] time 0.443 (0.509) data 0.000 (0.065) loss 0.2360 (0.3339) acc 96.8750 (91.8750) lr 1.8181e-02 eta 0:24:19\n",
            "epoch [42/200] batch [5/18] time 0.454 (0.615) data 0.000 (0.172) loss 0.3270 (0.3263) acc 93.7500 (90.6250) lr 1.8090e-02 eta 0:29:17\n",
            "epoch [42/200] batch [10/18] time 0.446 (0.529) data 0.000 (0.086) loss 0.4671 (0.3769) acc 87.5000 (89.6875) lr 1.8090e-02 eta 0:25:09\n",
            "epoch [42/200] batch [15/18] time 0.446 (0.501) data 0.000 (0.057) loss 0.1894 (0.3657) acc 93.7500 (89.7917) lr 1.8090e-02 eta 0:23:47\n",
            "epoch [43/200] batch [5/18] time 0.449 (0.632) data 0.000 (0.190) loss 0.1042 (0.2663) acc 100.0000 (93.7500) lr 1.7997e-02 eta 0:29:55\n",
            "epoch [43/200] batch [10/18] time 0.441 (0.538) data 0.000 (0.095) loss 0.2420 (0.3293) acc 96.8750 (90.0000) lr 1.7997e-02 eta 0:25:24\n",
            "epoch [43/200] batch [15/18] time 0.445 (0.507) data 0.000 (0.063) loss 0.2642 (0.3006) acc 90.6250 (90.8333) lr 1.7997e-02 eta 0:23:54\n",
            "epoch [44/200] batch [5/18] time 0.454 (0.614) data 0.000 (0.171) loss 0.5619 (0.3068) acc 81.2500 (91.8750) lr 1.7902e-02 eta 0:28:52\n",
            "epoch [44/200] batch [10/18] time 0.445 (0.529) data 0.000 (0.086) loss 0.4414 (0.3445) acc 84.3750 (90.6250) lr 1.7902e-02 eta 0:24:49\n",
            "epoch [44/200] batch [15/18] time 0.446 (0.501) data 0.000 (0.057) loss 0.5085 (0.3570) acc 78.1250 (89.5833) lr 1.7902e-02 eta 0:23:27\n",
            "epoch [45/200] batch [5/18] time 0.453 (0.612) data 0.000 (0.168) loss 0.3732 (0.2293) acc 90.6250 (95.6250) lr 1.7804e-02 eta 0:28:35\n",
            "epoch [45/200] batch [10/18] time 0.444 (0.528) data 0.000 (0.084) loss 0.1541 (0.2219) acc 100.0000 (95.3125) lr 1.7804e-02 eta 0:24:37\n",
            "epoch [45/200] batch [15/18] time 0.444 (0.500) data 0.000 (0.056) loss 0.2849 (0.2627) acc 93.7500 (94.1667) lr 1.7804e-02 eta 0:23:17\n",
            "epoch [46/200] batch [5/18] time 0.451 (0.631) data 0.000 (0.188) loss 0.1553 (0.2513) acc 96.8750 (92.5000) lr 1.7705e-02 eta 0:29:16\n",
            "epoch [46/200] batch [10/18] time 0.445 (0.538) data 0.000 (0.094) loss 0.4108 (0.2809) acc 90.6250 (90.9375) lr 1.7705e-02 eta 0:24:54\n",
            "epoch [46/200] batch [15/18] time 0.444 (0.506) data 0.000 (0.063) loss 0.3451 (0.2785) acc 87.5000 (91.0417) lr 1.7705e-02 eta 0:23:24\n",
            "epoch [47/200] batch [5/18] time 0.456 (0.632) data 0.000 (0.189) loss 0.3399 (0.3411) acc 90.6250 (90.0000) lr 1.7604e-02 eta 0:29:09\n",
            "epoch [47/200] batch [10/18] time 0.444 (0.537) data 0.000 (0.094) loss 0.4979 (0.3532) acc 84.3750 (89.3750) lr 1.7604e-02 eta 0:24:43\n",
            "epoch [47/200] batch [15/18] time 0.441 (0.506) data 0.000 (0.063) loss 0.3429 (0.3018) acc 93.7500 (91.6667) lr 1.7604e-02 eta 0:23:15\n",
            "epoch [48/200] batch [5/18] time 0.451 (0.632) data 0.000 (0.189) loss 0.2300 (0.2293) acc 90.6250 (93.7500) lr 1.7501e-02 eta 0:28:57\n",
            "epoch [48/200] batch [10/18] time 0.447 (0.538) data 0.000 (0.095) loss 0.2901 (0.2612) acc 90.6250 (91.8750) lr 1.7501e-02 eta 0:24:37\n",
            "epoch [48/200] batch [15/18] time 0.442 (0.507) data 0.000 (0.063) loss 0.4575 (0.2996) acc 84.3750 (91.0417) lr 1.7501e-02 eta 0:23:08\n",
            "epoch [49/200] batch [5/18] time 0.450 (0.614) data 0.000 (0.172) loss 0.4583 (0.3479) acc 87.5000 (89.3750) lr 1.7396e-02 eta 0:27:57\n",
            "epoch [49/200] batch [10/18] time 0.448 (0.529) data 0.000 (0.086) loss 0.3032 (0.3661) acc 90.6250 (89.3750) lr 1.7396e-02 eta 0:24:02\n",
            "epoch [49/200] batch [15/18] time 0.444 (0.501) data 0.000 (0.057) loss 0.4780 (0.3501) acc 78.1250 (89.7917) lr 1.7396e-02 eta 0:22:42\n",
            "epoch [50/200] batch [5/18] time 0.445 (0.619) data 0.000 (0.177) loss 0.0971 (0.2904) acc 96.8750 (91.8750) lr 1.7290e-02 eta 0:27:58\n",
            "epoch [50/200] batch [10/18] time 0.442 (0.531) data 0.000 (0.089) loss 0.1358 (0.3115) acc 96.8750 (90.3125) lr 1.7290e-02 eta 0:23:57\n",
            "epoch [50/200] batch [15/18] time 0.444 (0.502) data 0.001 (0.059) loss 0.4041 (0.3304) acc 87.5000 (90.4167) lr 1.7290e-02 eta 0:22:36\n",
            "epoch [51/200] batch [5/18] time 0.453 (0.622) data 0.000 (0.177) loss 0.1163 (0.3253) acc 96.8750 (89.3750) lr 1.7181e-02 eta 0:27:56\n",
            "epoch [51/200] batch [10/18] time 0.447 (0.534) data 0.000 (0.089) loss 0.4369 (0.3354) acc 90.6250 (89.0625) lr 1.7181e-02 eta 0:23:55\n",
            "epoch [51/200] batch [15/18] time 0.444 (0.503) data 0.000 (0.059) loss 0.1369 (0.3445) acc 100.0000 (90.0000) lr 1.7181e-02 eta 0:22:31\n",
            "epoch [52/200] batch [5/18] time 0.452 (0.625) data 0.000 (0.180) loss 0.2498 (0.2214) acc 93.7500 (95.0000) lr 1.7071e-02 eta 0:27:53\n",
            "epoch [52/200] batch [10/18] time 0.444 (0.534) data 0.000 (0.090) loss 0.2264 (0.2117) acc 93.7500 (95.0000) lr 1.7071e-02 eta 0:23:46\n",
            "epoch [52/200] batch [15/18] time 0.445 (0.504) data 0.000 (0.060) loss 0.0864 (0.2503) acc 100.0000 (93.9583) lr 1.7071e-02 eta 0:22:24\n",
            "epoch [53/200] batch [5/18] time 0.459 (0.629) data 0.000 (0.184) loss 0.4231 (0.2400) acc 90.6250 (93.1250) lr 1.6959e-02 eta 0:27:51\n",
            "epoch [53/200] batch [10/18] time 0.443 (0.536) data 0.000 (0.092) loss 0.2278 (0.2755) acc 93.7500 (92.5000) lr 1.6959e-02 eta 0:23:41\n",
            "epoch [53/200] batch [15/18] time 0.441 (0.505) data 0.000 (0.062) loss 0.1877 (0.2839) acc 96.8750 (93.1250) lr 1.6959e-02 eta 0:22:17\n",
            "epoch [54/200] batch [5/18] time 0.455 (0.633) data 0.000 (0.188) loss 0.3127 (0.2406) acc 90.6250 (92.5000) lr 1.6845e-02 eta 0:27:50\n",
            "epoch [54/200] batch [10/18] time 0.447 (0.538) data 0.000 (0.094) loss 0.2866 (0.2834) acc 87.5000 (91.8750) lr 1.6845e-02 eta 0:23:38\n",
            "epoch [54/200] batch [15/18] time 0.446 (0.507) data 0.000 (0.063) loss 0.1807 (0.2912) acc 100.0000 (92.2917) lr 1.6845e-02 eta 0:22:14\n",
            "epoch [55/200] batch [5/18] time 0.455 (0.623) data 0.000 (0.179) loss 0.1063 (0.2621) acc 100.0000 (92.5000) lr 1.6730e-02 eta 0:27:13\n",
            "epoch [55/200] batch [10/18] time 0.445 (0.533) data 0.000 (0.090) loss 0.7044 (0.2869) acc 84.3750 (92.1875) lr 1.6730e-02 eta 0:23:16\n",
            "epoch [55/200] batch [15/18] time 0.443 (0.503) data 0.000 (0.060) loss 0.1648 (0.2617) acc 100.0000 (93.3333) lr 1.6730e-02 eta 0:21:55\n",
            "epoch [56/200] batch [5/18] time 0.453 (0.642) data 0.000 (0.199) loss 0.2868 (0.2419) acc 93.7500 (94.3750) lr 1.6613e-02 eta 0:27:51\n",
            "epoch [56/200] batch [10/18] time 0.446 (0.543) data 0.000 (0.099) loss 0.3708 (0.2785) acc 90.6250 (93.4375) lr 1.6613e-02 eta 0:23:31\n",
            "epoch [56/200] batch [15/18] time 0.443 (0.510) data 0.000 (0.066) loss 0.1667 (0.2927) acc 96.8750 (93.1250) lr 1.6613e-02 eta 0:22:03\n",
            "epoch [57/200] batch [5/18] time 0.449 (0.615) data 0.000 (0.171) loss 0.2099 (0.2700) acc 93.7500 (91.2500) lr 1.6494e-02 eta 0:26:31\n",
            "epoch [57/200] batch [10/18] time 0.447 (0.529) data 0.000 (0.086) loss 0.2682 (0.2767) acc 90.6250 (92.5000) lr 1.6494e-02 eta 0:22:46\n",
            "epoch [57/200] batch [15/18] time 0.445 (0.501) data 0.000 (0.057) loss 0.2337 (0.2794) acc 93.7500 (92.7083) lr 1.6494e-02 eta 0:21:30\n",
            "epoch [58/200] batch [5/18] time 0.450 (0.613) data 0.000 (0.171) loss 0.1383 (0.2274) acc 100.0000 (95.6250) lr 1.6374e-02 eta 0:26:15\n",
            "epoch [58/200] batch [10/18] time 0.440 (0.527) data 0.000 (0.086) loss 0.2868 (0.2467) acc 90.6250 (94.0625) lr 1.6374e-02 eta 0:22:32\n",
            "epoch [58/200] batch [15/18] time 0.442 (0.500) data 0.000 (0.057) loss 0.2883 (0.2420) acc 96.8750 (94.1667) lr 1.6374e-02 eta 0:21:18\n",
            "epoch [59/200] batch [5/18] time 0.452 (0.613) data 0.000 (0.170) loss 0.5175 (0.3196) acc 81.2500 (88.1250) lr 1.6252e-02 eta 0:26:04\n",
            "epoch [59/200] batch [10/18] time 0.444 (0.528) data 0.000 (0.085) loss 0.2990 (0.3117) acc 93.7500 (90.0000) lr 1.6252e-02 eta 0:22:24\n",
            "epoch [59/200] batch [15/18] time 0.444 (0.500) data 0.000 (0.057) loss 0.2150 (0.3188) acc 90.6250 (89.7917) lr 1.6252e-02 eta 0:21:10\n",
            "epoch [60/200] batch [5/18] time 0.453 (0.618) data 0.000 (0.176) loss 0.2148 (0.2363) acc 96.8750 (93.7500) lr 1.6129e-02 eta 0:26:06\n",
            "epoch [60/200] batch [10/18] time 0.448 (0.531) data 0.000 (0.088) loss 0.2121 (0.2427) acc 100.0000 (94.0625) lr 1.6129e-02 eta 0:22:22\n",
            "epoch [60/200] batch [15/18] time 0.445 (0.502) data 0.000 (0.059) loss 0.1979 (0.2245) acc 93.7500 (94.5833) lr 1.6129e-02 eta 0:21:06\n",
            "epoch [61/200] batch [5/18] time 0.451 (0.622) data 0.000 (0.181) loss 0.1964 (0.2084) acc 96.8750 (93.7500) lr 1.6004e-02 eta 0:26:05\n",
            "epoch [61/200] batch [10/18] time 0.444 (0.533) data 0.000 (0.091) loss 0.1630 (0.2476) acc 93.7500 (93.1250) lr 1.6004e-02 eta 0:22:17\n",
            "epoch [61/200] batch [15/18] time 0.439 (0.503) data 0.000 (0.060) loss 0.2960 (0.2556) acc 93.7500 (93.7500) lr 1.6004e-02 eta 0:20:58\n",
            "epoch [62/200] batch [5/18] time 0.452 (0.632) data 0.000 (0.191) loss 0.2342 (0.3120) acc 90.6250 (91.2500) lr 1.5878e-02 eta 0:26:19\n",
            "epoch [62/200] batch [10/18] time 0.444 (0.538) data 0.000 (0.095) loss 0.1170 (0.2526) acc 96.8750 (92.5000) lr 1.5878e-02 eta 0:22:20\n",
            "epoch [62/200] batch [15/18] time 0.445 (0.507) data 0.000 (0.064) loss 0.5047 (0.3061) acc 90.6250 (91.4583) lr 1.5878e-02 eta 0:21:00\n",
            "epoch [63/200] batch [5/18] time 0.454 (0.609) data 0.000 (0.165) loss 0.2721 (0.2659) acc 93.7500 (93.7500) lr 1.5750e-02 eta 0:25:09\n",
            "epoch [63/200] batch [10/18] time 0.446 (0.526) data 0.000 (0.083) loss 0.3857 (0.2895) acc 84.3750 (92.1875) lr 1.5750e-02 eta 0:21:42\n",
            "epoch [63/200] batch [15/18] time 0.447 (0.499) data 0.000 (0.055) loss 0.4225 (0.2865) acc 87.5000 (91.6667) lr 1.5750e-02 eta 0:20:32\n",
            "epoch [64/200] batch [5/18] time 0.454 (0.622) data 0.000 (0.178) loss 0.3524 (0.2956) acc 90.6250 (93.1250) lr 1.5621e-02 eta 0:25:31\n",
            "epoch [64/200] batch [10/18] time 0.446 (0.533) data 0.000 (0.089) loss 0.1640 (0.2400) acc 100.0000 (94.0625) lr 1.5621e-02 eta 0:21:50\n",
            "epoch [64/200] batch [15/18] time 0.446 (0.504) data 0.000 (0.060) loss 0.2164 (0.2563) acc 96.8750 (93.5417) lr 1.5621e-02 eta 0:20:35\n",
            "epoch [65/200] batch [5/18] time 0.443 (0.621) data 0.000 (0.179) loss 0.2958 (0.3181) acc 90.6250 (91.2500) lr 1.5490e-02 eta 0:25:17\n",
            "epoch [65/200] batch [10/18] time 0.446 (0.533) data 0.000 (0.089) loss 0.3179 (0.2966) acc 90.6250 (91.8750) lr 1.5490e-02 eta 0:21:39\n",
            "epoch [65/200] batch [15/18] time 0.447 (0.503) data 0.000 (0.060) loss 0.4354 (0.2785) acc 93.7500 (92.9167) lr 1.5490e-02 eta 0:20:24\n",
            "epoch [66/200] batch [5/18] time 0.448 (0.614) data 0.000 (0.171) loss 0.1362 (0.2862) acc 93.7500 (90.6250) lr 1.5358e-02 eta 0:24:48\n",
            "epoch [66/200] batch [10/18] time 0.444 (0.528) data 0.000 (0.086) loss 0.2457 (0.2543) acc 93.7500 (92.5000) lr 1.5358e-02 eta 0:21:18\n",
            "epoch [66/200] batch [15/18] time 0.441 (0.500) data 0.000 (0.057) loss 0.2239 (0.2433) acc 96.8750 (93.1250) lr 1.5358e-02 eta 0:20:07\n",
            "epoch [67/200] batch [5/18] time 0.449 (0.608) data 0.000 (0.164) loss 0.2059 (0.2495) acc 93.7500 (93.7500) lr 1.5225e-02 eta 0:24:23\n",
            "epoch [67/200] batch [10/18] time 0.447 (0.526) data 0.000 (0.082) loss 0.2791 (0.2794) acc 90.6250 (92.5000) lr 1.5225e-02 eta 0:21:03\n",
            "epoch [67/200] batch [15/18] time 0.441 (0.498) data 0.000 (0.055) loss 0.1532 (0.2546) acc 96.8750 (93.3333) lr 1.5225e-02 eta 0:19:53\n",
            "epoch [68/200] batch [5/18] time 0.450 (0.622) data 0.000 (0.180) loss 0.0799 (0.2681) acc 100.0000 (92.5000) lr 1.5090e-02 eta 0:24:46\n",
            "epoch [68/200] batch [10/18] time 0.443 (0.533) data 0.000 (0.090) loss 0.2769 (0.2172) acc 93.7500 (94.6875) lr 1.5090e-02 eta 0:21:09\n",
            "epoch [68/200] batch [15/18] time 0.445 (0.503) data 0.000 (0.060) loss 0.1141 (0.2274) acc 100.0000 (94.5833) lr 1.5090e-02 eta 0:19:55\n",
            "epoch [69/200] batch [5/18] time 0.451 (0.630) data 0.000 (0.185) loss 0.1796 (0.2775) acc 96.8750 (93.1250) lr 1.4955e-02 eta 0:24:54\n",
            "epoch [69/200] batch [10/18] time 0.442 (0.537) data 0.000 (0.092) loss 0.3161 (0.2743) acc 96.8750 (94.0625) lr 1.4955e-02 eta 0:21:09\n",
            "epoch [69/200] batch [15/18] time 0.446 (0.505) data 0.000 (0.062) loss 0.2760 (0.2820) acc 93.7500 (93.1250) lr 1.4955e-02 eta 0:19:52\n",
            "epoch [70/200] batch [5/18] time 0.445 (0.617) data 0.000 (0.175) loss 0.4912 (0.3742) acc 84.3750 (86.8750) lr 1.4818e-02 eta 0:24:11\n",
            "epoch [70/200] batch [10/18] time 0.444 (0.530) data 0.000 (0.087) loss 0.1340 (0.2947) acc 96.8750 (90.9375) lr 1.4818e-02 eta 0:20:44\n",
            "epoch [70/200] batch [15/18] time 0.445 (0.501) data 0.000 (0.058) loss 0.2784 (0.3157) acc 90.6250 (90.4167) lr 1.4818e-02 eta 0:19:35\n",
            "epoch [71/200] batch [5/18] time 0.449 (0.617) data 0.000 (0.175) loss 0.0457 (0.2195) acc 100.0000 (93.7500) lr 1.4679e-02 eta 0:24:00\n",
            "epoch [71/200] batch [10/18] time 0.443 (0.530) data 0.000 (0.088) loss 0.3671 (0.2854) acc 90.6250 (92.1875) lr 1.4679e-02 eta 0:20:35\n",
            "epoch [71/200] batch [15/18] time 0.445 (0.501) data 0.000 (0.059) loss 0.3079 (0.2863) acc 87.5000 (91.2500) lr 1.4679e-02 eta 0:19:25\n",
            "epoch [72/200] batch [5/18] time 0.451 (0.620) data 0.000 (0.176) loss 0.3689 (0.2810) acc 90.6250 (93.7500) lr 1.4540e-02 eta 0:23:56\n",
            "epoch [72/200] batch [10/18] time 0.442 (0.532) data 0.000 (0.088) loss 0.1986 (0.3303) acc 96.8750 (92.5000) lr 1.4540e-02 eta 0:20:29\n",
            "epoch [72/200] batch [15/18] time 0.446 (0.503) data 0.000 (0.059) loss 0.4692 (0.3294) acc 90.6250 (92.9167) lr 1.4540e-02 eta 0:19:19\n",
            "epoch [73/200] batch [5/18] time 0.451 (0.615) data 0.000 (0.171) loss 0.5715 (0.3228) acc 84.3750 (91.8750) lr 1.4399e-02 eta 0:23:33\n",
            "epoch [73/200] batch [10/18] time 0.446 (0.529) data 0.000 (0.086) loss 0.3464 (0.3263) acc 90.6250 (91.8750) lr 1.4399e-02 eta 0:20:13\n",
            "epoch [73/200] batch [15/18] time 0.445 (0.500) data 0.000 (0.057) loss 0.2241 (0.2787) acc 96.8750 (93.1250) lr 1.4399e-02 eta 0:19:05\n",
            "epoch [74/200] batch [5/18] time 0.453 (0.645) data 0.000 (0.202) loss 0.0960 (0.2087) acc 100.0000 (94.3750) lr 1.4258e-02 eta 0:24:31\n",
            "epoch [74/200] batch [10/18] time 0.441 (0.544) data 0.000 (0.101) loss 0.2635 (0.2433) acc 90.6250 (94.3750) lr 1.4258e-02 eta 0:20:39\n",
            "epoch [74/200] batch [15/18] time 0.444 (0.510) data 0.000 (0.068) loss 0.1365 (0.2385) acc 96.8750 (94.5833) lr 1.4258e-02 eta 0:19:19\n",
            "epoch [75/200] batch [5/18] time 0.451 (0.625) data 0.000 (0.182) loss 0.0981 (0.1856) acc 100.0000 (95.0000) lr 1.4115e-02 eta 0:23:33\n",
            "epoch [75/200] batch [10/18] time 0.445 (0.534) data 0.000 (0.091) loss 0.4365 (0.2056) acc 90.6250 (94.6875) lr 1.4115e-02 eta 0:20:05\n",
            "epoch [75/200] batch [15/18] time 0.444 (0.504) data 0.000 (0.061) loss 0.1563 (0.2306) acc 96.8750 (93.5417) lr 1.4115e-02 eta 0:18:55\n",
            "epoch [76/200] batch [5/18] time 0.452 (0.623) data 0.000 (0.181) loss 0.4250 (0.3365) acc 84.3750 (90.6250) lr 1.3971e-02 eta 0:23:17\n",
            "epoch [76/200] batch [10/18] time 0.444 (0.533) data 0.000 (0.090) loss 0.1221 (0.2635) acc 96.8750 (93.7500) lr 1.3971e-02 eta 0:19:54\n",
            "epoch [76/200] batch [15/18] time 0.445 (0.504) data 0.000 (0.060) loss 0.0946 (0.2704) acc 100.0000 (93.3333) lr 1.3971e-02 eta 0:18:45\n",
            "epoch [77/200] batch [5/18] time 0.446 (0.627) data 0.000 (0.185) loss 0.2035 (0.2400) acc 93.7500 (92.5000) lr 1.3827e-02 eta 0:23:16\n",
            "epoch [77/200] batch [10/18] time 0.445 (0.535) data 0.000 (0.093) loss 0.1011 (0.2172) acc 100.0000 (93.7500) lr 1.3827e-02 eta 0:19:49\n",
            "epoch [77/200] batch [15/18] time 0.446 (0.505) data 0.000 (0.062) loss 0.1173 (0.2281) acc 100.0000 (93.7500) lr 1.3827e-02 eta 0:18:39\n",
            "epoch [78/200] batch [5/18] time 0.450 (0.624) data 0.000 (0.180) loss 0.1825 (0.2824) acc 93.7500 (91.2500) lr 1.3681e-02 eta 0:22:58\n",
            "epoch [78/200] batch [10/18] time 0.444 (0.533) data 0.000 (0.090) loss 0.3428 (0.2993) acc 90.6250 (91.5625) lr 1.3681e-02 eta 0:19:34\n",
            "epoch [78/200] batch [15/18] time 0.445 (0.503) data 0.000 (0.060) loss 0.4248 (0.3049) acc 87.5000 (91.4583) lr 1.3681e-02 eta 0:18:25\n",
            "epoch [79/200] batch [5/18] time 0.454 (0.618) data 0.000 (0.174) loss 0.3981 (0.3442) acc 90.6250 (91.2500) lr 1.3535e-02 eta 0:22:34\n",
            "epoch [79/200] batch [10/18] time 0.443 (0.531) data 0.000 (0.087) loss 0.2951 (0.2703) acc 90.6250 (92.8125) lr 1.3535e-02 eta 0:19:20\n",
            "epoch [79/200] batch [15/18] time 0.446 (0.502) data 0.000 (0.058) loss 0.1149 (0.2277) acc 100.0000 (94.3750) lr 1.3535e-02 eta 0:18:15\n",
            "epoch [80/200] batch [5/18] time 0.446 (0.624) data 0.000 (0.183) loss 0.2589 (0.1980) acc 93.7500 (95.0000) lr 1.3387e-02 eta 0:22:35\n",
            "epoch [80/200] batch [10/18] time 0.442 (0.534) data 0.000 (0.091) loss 0.1450 (0.2145) acc 96.8750 (94.0625) lr 1.3387e-02 eta 0:19:17\n",
            "epoch [80/200] batch [15/18] time 0.444 (0.504) data 0.000 (0.061) loss 0.4813 (0.2284) acc 84.3750 (93.3333) lr 1.3387e-02 eta 0:18:09\n",
            "epoch [81/200] batch [5/18] time 0.453 (0.619) data 0.000 (0.177) loss 0.0860 (0.1566) acc 96.8750 (95.6250) lr 1.3239e-02 eta 0:22:14\n",
            "epoch [81/200] batch [10/18] time 0.442 (0.531) data 0.000 (0.088) loss 0.2255 (0.1880) acc 93.7500 (95.3125) lr 1.3239e-02 eta 0:19:02\n",
            "epoch [81/200] batch [15/18] time 0.443 (0.502) data 0.000 (0.059) loss 0.2898 (0.2373) acc 93.7500 (93.3333) lr 1.3239e-02 eta 0:17:56\n",
            "epoch [82/200] batch [5/18] time 0.455 (0.619) data 0.000 (0.177) loss 0.2194 (0.2327) acc 96.8750 (94.3750) lr 1.3090e-02 eta 0:22:02\n",
            "epoch [82/200] batch [10/18] time 0.442 (0.531) data 0.000 (0.088) loss 0.2113 (0.2179) acc 90.6250 (94.3750) lr 1.3090e-02 eta 0:18:51\n",
            "epoch [82/200] batch [15/18] time 0.443 (0.502) data 0.000 (0.059) loss 0.3342 (0.2324) acc 90.6250 (93.7500) lr 1.3090e-02 eta 0:17:47\n",
            "epoch [83/200] batch [5/18] time 0.445 (0.619) data 0.000 (0.178) loss 0.3394 (0.2111) acc 87.5000 (91.8750) lr 1.2940e-02 eta 0:21:51\n",
            "epoch [83/200] batch [10/18] time 0.443 (0.531) data 0.000 (0.089) loss 0.5005 (0.2496) acc 84.3750 (90.9375) lr 1.2940e-02 eta 0:18:41\n",
            "epoch [83/200] batch [15/18] time 0.440 (0.502) data 0.000 (0.059) loss 0.2549 (0.2688) acc 96.8750 (91.2500) lr 1.2940e-02 eta 0:17:38\n",
            "epoch [84/200] batch [5/18] time 0.452 (0.619) data 0.000 (0.175) loss 0.1886 (0.2109) acc 90.6250 (92.5000) lr 1.2790e-02 eta 0:21:39\n",
            "epoch [84/200] batch [10/18] time 0.448 (0.532) data 0.000 (0.087) loss 0.2568 (0.2516) acc 96.8750 (92.8125) lr 1.2790e-02 eta 0:18:35\n",
            "epoch [84/200] batch [15/18] time 0.445 (0.503) data 0.000 (0.058) loss 0.3771 (0.2463) acc 81.2500 (92.0833) lr 1.2790e-02 eta 0:17:31\n",
            "epoch [85/200] batch [5/18] time 0.452 (0.632) data 0.000 (0.188) loss 0.3386 (0.2326) acc 93.7500 (93.1250) lr 1.2639e-02 eta 0:21:56\n",
            "epoch [85/200] batch [10/18] time 0.446 (0.538) data 0.000 (0.094) loss 0.1374 (0.2371) acc 96.8750 (93.4375) lr 1.2639e-02 eta 0:18:38\n",
            "epoch [85/200] batch [15/18] time 0.445 (0.507) data 0.000 (0.063) loss 0.4928 (0.2483) acc 84.3750 (93.3333) lr 1.2639e-02 eta 0:17:30\n",
            "epoch [86/200] batch [5/18] time 0.453 (0.636) data 0.000 (0.194) loss 0.2467 (0.3011) acc 93.7500 (90.0000) lr 1.2487e-02 eta 0:21:53\n",
            "epoch [86/200] batch [10/18] time 0.446 (0.539) data 0.000 (0.097) loss 0.1967 (0.2654) acc 96.8750 (92.1875) lr 1.2487e-02 eta 0:18:30\n",
            "epoch [86/200] batch [15/18] time 0.440 (0.507) data 0.000 (0.065) loss 0.1075 (0.2371) acc 96.8750 (92.9167) lr 1.2487e-02 eta 0:17:22\n",
            "epoch [87/200] batch [5/18] time 0.450 (0.615) data 0.000 (0.172) loss 0.0922 (0.2019) acc 96.8750 (95.0000) lr 1.2334e-02 eta 0:20:59\n",
            "epoch [87/200] batch [10/18] time 0.447 (0.529) data 0.000 (0.086) loss 0.1090 (0.1796) acc 100.0000 (96.8750) lr 1.2334e-02 eta 0:18:00\n",
            "epoch [87/200] batch [15/18] time 0.445 (0.501) data 0.000 (0.058) loss 0.5958 (0.2033) acc 81.2500 (96.0417) lr 1.2334e-02 eta 0:16:59\n",
            "epoch [88/200] batch [5/18] time 0.445 (0.611) data 0.000 (0.169) loss 0.1501 (0.1856) acc 96.8750 (96.8750) lr 1.2181e-02 eta 0:20:39\n",
            "epoch [88/200] batch [10/18] time 0.441 (0.526) data 0.000 (0.085) loss 0.0915 (0.1962) acc 100.0000 (95.9375) lr 1.2181e-02 eta 0:17:45\n",
            "epoch [88/200] batch [15/18] time 0.443 (0.499) data 0.000 (0.057) loss 0.6446 (0.2604) acc 84.3750 (93.7500) lr 1.2181e-02 eta 0:16:47\n",
            "epoch [89/200] batch [5/18] time 0.453 (0.618) data 0.000 (0.175) loss 0.4784 (0.3793) acc 84.3750 (88.7500) lr 1.2028e-02 eta 0:20:42\n",
            "epoch [89/200] batch [10/18] time 0.442 (0.530) data 0.000 (0.088) loss 0.1848 (0.2686) acc 96.8750 (92.8125) lr 1.2028e-02 eta 0:17:44\n",
            "epoch [89/200] batch [15/18] time 0.442 (0.501) data 0.000 (0.058) loss 0.0740 (0.2407) acc 100.0000 (94.3750) lr 1.2028e-02 eta 0:16:42\n",
            "epoch [90/200] batch [5/18] time 0.452 (0.610) data 0.000 (0.167) loss 0.1118 (0.1749) acc 96.8750 (95.6250) lr 1.1874e-02 eta 0:20:14\n",
            "epoch [90/200] batch [10/18] time 0.441 (0.526) data 0.000 (0.084) loss 0.1384 (0.2056) acc 90.6250 (94.0625) lr 1.1874e-02 eta 0:17:26\n",
            "epoch [90/200] batch [15/18] time 0.445 (0.499) data 0.000 (0.056) loss 0.1458 (0.1945) acc 96.8750 (94.3750) lr 1.1874e-02 eta 0:16:28\n",
            "epoch [91/200] batch [5/18] time 0.450 (0.611) data 0.000 (0.170) loss 0.2871 (0.2279) acc 87.5000 (93.7500) lr 1.1719e-02 eta 0:20:06\n",
            "epoch [91/200] batch [10/18] time 0.442 (0.528) data 0.000 (0.085) loss 0.4264 (0.2353) acc 90.6250 (93.7500) lr 1.1719e-02 eta 0:17:19\n",
            "epoch [91/200] batch [15/18] time 0.442 (0.500) data 0.000 (0.057) loss 0.2324 (0.2261) acc 90.6250 (93.7500) lr 1.1719e-02 eta 0:16:22\n",
            "epoch [92/200] batch [5/18] time 0.449 (0.632) data 0.000 (0.190) loss 0.3575 (0.2657) acc 90.6250 (91.8750) lr 1.1564e-02 eta 0:20:37\n",
            "epoch [92/200] batch [10/18] time 0.444 (0.538) data 0.000 (0.095) loss 0.4322 (0.2525) acc 84.3750 (92.5000) lr 1.1564e-02 eta 0:17:29\n",
            "epoch [92/200] batch [15/18] time 0.444 (0.507) data 0.000 (0.064) loss 0.3387 (0.2606) acc 84.3750 (92.9167) lr 1.1564e-02 eta 0:16:26\n",
            "epoch [93/200] batch [5/18] time 0.447 (0.620) data 0.000 (0.178) loss 0.2071 (0.2012) acc 96.8750 (95.0000) lr 1.1409e-02 eta 0:20:01\n",
            "epoch [93/200] batch [10/18] time 0.446 (0.532) data 0.000 (0.089) loss 0.0982 (0.1899) acc 96.8750 (95.0000) lr 1.1409e-02 eta 0:17:09\n",
            "epoch [93/200] batch [15/18] time 0.439 (0.502) data 0.000 (0.060) loss 0.2082 (0.2192) acc 96.8750 (94.1667) lr 1.1409e-02 eta 0:16:09\n",
            "epoch [94/200] batch [5/18] time 0.453 (0.616) data 0.000 (0.172) loss 0.1487 (0.2899) acc 96.8750 (94.3750) lr 1.1253e-02 eta 0:19:43\n",
            "epoch [94/200] batch [10/18] time 0.443 (0.529) data 0.000 (0.086) loss 0.1945 (0.2552) acc 93.7500 (94.0625) lr 1.1253e-02 eta 0:16:54\n",
            "epoch [94/200] batch [15/18] time 0.445 (0.501) data 0.000 (0.058) loss 0.3272 (0.2451) acc 93.7500 (93.9583) lr 1.1253e-02 eta 0:15:57\n",
            "epoch [95/200] batch [5/18] time 0.450 (0.627) data 0.000 (0.184) loss 0.2815 (0.2693) acc 96.8750 (95.0000) lr 1.1097e-02 eta 0:19:52\n",
            "epoch [95/200] batch [10/18] time 0.441 (0.535) data 0.000 (0.092) loss 0.1049 (0.2476) acc 96.8750 (94.0625) lr 1.1097e-02 eta 0:16:54\n",
            "epoch [95/200] batch [15/18] time 0.444 (0.504) data 0.000 (0.061) loss 0.5354 (0.2712) acc 87.5000 (93.1250) lr 1.1097e-02 eta 0:15:54\n",
            "epoch [96/200] batch [5/18] time 0.452 (0.617) data 0.000 (0.174) loss 0.2712 (0.2255) acc 96.8750 (94.3750) lr 1.0941e-02 eta 0:19:22\n",
            "epoch [96/200] batch [10/18] time 0.448 (0.531) data 0.000 (0.087) loss 0.1166 (0.2112) acc 96.8750 (94.6875) lr 1.0941e-02 eta 0:16:38\n",
            "epoch [96/200] batch [15/18] time 0.445 (0.502) data 0.000 (0.058) loss 0.2808 (0.2217) acc 93.7500 (94.3750) lr 1.0941e-02 eta 0:15:41\n",
            "epoch [97/200] batch [5/18] time 0.450 (0.637) data 0.000 (0.194) loss 0.1720 (0.2469) acc 93.7500 (93.7500) lr 1.0785e-02 eta 0:19:49\n",
            "epoch [97/200] batch [10/18] time 0.444 (0.540) data 0.000 (0.097) loss 0.1229 (0.2002) acc 96.8750 (95.3125) lr 1.0785e-02 eta 0:16:46\n",
            "epoch [97/200] batch [15/18] time 0.444 (0.508) data 0.000 (0.065) loss 0.1155 (0.1943) acc 100.0000 (95.6250) lr 1.0785e-02 eta 0:15:43\n",
            "epoch [98/200] batch [5/18] time 0.451 (0.629) data 0.000 (0.185) loss 0.4689 (0.3355) acc 87.5000 (91.2500) lr 1.0628e-02 eta 0:19:23\n",
            "epoch [98/200] batch [10/18] time 0.441 (0.536) data 0.000 (0.093) loss 0.2507 (0.2324) acc 93.7500 (94.3750) lr 1.0628e-02 eta 0:16:28\n",
            "epoch [98/200] batch [15/18] time 0.446 (0.505) data 0.000 (0.062) loss 0.2548 (0.2342) acc 93.7500 (94.1667) lr 1.0628e-02 eta 0:15:29\n",
            "epoch [99/200] batch [5/18] time 0.450 (0.620) data 0.000 (0.178) loss 0.1054 (0.1880) acc 100.0000 (96.2500) lr 1.0471e-02 eta 0:18:54\n",
            "epoch [99/200] batch [10/18] time 0.444 (0.532) data 0.000 (0.089) loss 0.2001 (0.1579) acc 96.8750 (96.5625) lr 1.0471e-02 eta 0:16:11\n",
            "epoch [99/200] batch [15/18] time 0.446 (0.502) data 0.000 (0.059) loss 0.3397 (0.1918) acc 87.5000 (95.2083) lr 1.0471e-02 eta 0:15:14\n",
            "epoch [100/200] batch [5/18] time 0.453 (0.622) data 0.000 (0.179) loss 0.3707 (0.3155) acc 90.6250 (93.1250) lr 1.0314e-02 eta 0:18:47\n",
            "epoch [100/200] batch [10/18] time 0.446 (0.533) data 0.000 (0.089) loss 0.1650 (0.2343) acc 96.8750 (95.3125) lr 1.0314e-02 eta 0:16:03\n",
            "epoch [100/200] batch [15/18] time 0.447 (0.503) data 0.000 (0.060) loss 0.4782 (0.2246) acc 87.5000 (95.2083) lr 1.0314e-02 eta 0:15:07\n",
            "epoch [101/200] batch [5/18] time 0.447 (0.618) data 0.000 (0.176) loss 0.1944 (0.2806) acc 96.8750 (93.7500) lr 1.0157e-02 eta 0:18:28\n",
            "epoch [101/200] batch [10/18] time 0.442 (0.530) data 0.000 (0.088) loss 0.1009 (0.2760) acc 96.8750 (93.1250) lr 1.0157e-02 eta 0:15:49\n",
            "epoch [101/200] batch [15/18] time 0.446 (0.501) data 0.000 (0.059) loss 0.1007 (0.2532) acc 96.8750 (93.5417) lr 1.0157e-02 eta 0:14:54\n",
            "epoch [102/200] batch [5/18] time 0.453 (0.615) data 0.000 (0.173) loss 0.2347 (0.1576) acc 96.8750 (96.2500) lr 1.0000e-02 eta 0:18:12\n",
            "epoch [102/200] batch [10/18] time 0.443 (0.530) data 0.000 (0.087) loss 0.2172 (0.2478) acc 93.7500 (93.4375) lr 1.0000e-02 eta 0:15:38\n",
            "epoch [102/200] batch [15/18] time 0.441 (0.501) data 0.000 (0.058) loss 0.0577 (0.2101) acc 100.0000 (95.0000) lr 1.0000e-02 eta 0:14:45\n",
            "epoch [103/200] batch [5/18] time 0.450 (0.606) data 0.000 (0.164) loss 0.0751 (0.1515) acc 96.8750 (96.8750) lr 9.8429e-03 eta 0:17:46\n",
            "epoch [103/200] batch [10/18] time 0.444 (0.525) data 0.000 (0.082) loss 0.1864 (0.1771) acc 93.7500 (95.9375) lr 9.8429e-03 eta 0:15:20\n",
            "epoch [103/200] batch [15/18] time 0.444 (0.498) data 0.000 (0.055) loss 0.3971 (0.1831) acc 93.7500 (96.2500) lr 9.8429e-03 eta 0:14:30\n",
            "epoch [104/200] batch [5/18] time 0.449 (0.617) data 0.000 (0.175) loss 0.2111 (0.2028) acc 93.7500 (94.3750) lr 9.6859e-03 eta 0:17:54\n",
            "epoch [104/200] batch [10/18] time 0.443 (0.530) data 0.000 (0.088) loss 0.1907 (0.2081) acc 96.8750 (93.7500) lr 9.6859e-03 eta 0:15:19\n",
            "epoch [104/200] batch [15/18] time 0.445 (0.501) data 0.000 (0.059) loss 0.2826 (0.2400) acc 93.7500 (92.5000) lr 9.6859e-03 eta 0:14:27\n",
            "epoch [105/200] batch [5/18] time 0.453 (0.626) data 0.000 (0.182) loss 0.2457 (0.2779) acc 90.6250 (91.8750) lr 9.5289e-03 eta 0:17:59\n",
            "epoch [105/200] batch [10/18] time 0.445 (0.535) data 0.000 (0.091) loss 0.1966 (0.2449) acc 93.7500 (92.5000) lr 9.5289e-03 eta 0:15:18\n",
            "epoch [105/200] batch [15/18] time 0.439 (0.504) data 0.000 (0.061) loss 0.2544 (0.2541) acc 90.6250 (92.0833) lr 9.5289e-03 eta 0:14:22\n",
            "epoch [106/200] batch [5/18] time 0.447 (0.632) data 0.000 (0.191) loss 0.3416 (0.2522) acc 87.5000 (93.1250) lr 9.3721e-03 eta 0:17:57\n",
            "epoch [106/200] batch [10/18] time 0.442 (0.537) data 0.000 (0.095) loss 0.2182 (0.2219) acc 96.8750 (94.3750) lr 9.3721e-03 eta 0:15:13\n",
            "epoch [106/200] batch [15/18] time 0.442 (0.506) data 0.000 (0.064) loss 0.2080 (0.2438) acc 96.8750 (94.3750) lr 9.3721e-03 eta 0:14:17\n",
            "epoch [107/200] batch [5/18] time 0.452 (0.620) data 0.000 (0.178) loss 0.1263 (0.1434) acc 93.7500 (96.2500) lr 9.2154e-03 eta 0:17:26\n",
            "epoch [107/200] batch [10/18] time 0.437 (0.531) data 0.001 (0.089) loss 0.0694 (0.1554) acc 100.0000 (96.8750) lr 9.2154e-03 eta 0:14:53\n",
            "epoch [107/200] batch [15/18] time 0.445 (0.502) data 0.000 (0.060) loss 0.2004 (0.1760) acc 90.6250 (96.6667) lr 9.2154e-03 eta 0:14:02\n",
            "epoch [108/200] batch [5/18] time 0.445 (0.617) data 0.000 (0.177) loss 0.1929 (0.1783) acc 96.8750 (95.0000) lr 9.0589e-03 eta 0:17:09\n",
            "epoch [108/200] batch [10/18] time 0.445 (0.530) data 0.000 (0.089) loss 0.2554 (0.2193) acc 93.7500 (93.7500) lr 9.0589e-03 eta 0:14:42\n",
            "epoch [108/200] batch [15/18] time 0.440 (0.501) data 0.000 (0.059) loss 0.0302 (0.1958) acc 100.0000 (94.3750) lr 9.0589e-03 eta 0:13:51\n",
            "epoch [109/200] batch [5/18] time 0.452 (0.629) data 0.000 (0.186) loss 0.1890 (0.1459) acc 96.8750 (96.8750) lr 8.9027e-03 eta 0:17:17\n",
            "epoch [109/200] batch [10/18] time 0.443 (0.535) data 0.000 (0.093) loss 0.1939 (0.1811) acc 96.8750 (95.3125) lr 8.9027e-03 eta 0:14:41\n",
            "epoch [109/200] batch [15/18] time 0.444 (0.505) data 0.000 (0.062) loss 0.3361 (0.2225) acc 90.6250 (93.5417) lr 8.9027e-03 eta 0:13:49\n",
            "epoch [110/200] batch [5/18] time 0.453 (0.616) data 0.000 (0.173) loss 0.3529 (0.2987) acc 90.6250 (92.5000) lr 8.7467e-03 eta 0:16:46\n",
            "epoch [110/200] batch [10/18] time 0.446 (0.529) data 0.000 (0.087) loss 0.1076 (0.2360) acc 96.8750 (94.3750) lr 8.7467e-03 eta 0:14:21\n",
            "epoch [110/200] batch [15/18] time 0.441 (0.501) data 0.000 (0.058) loss 0.3370 (0.2436) acc 87.5000 (94.3750) lr 8.7467e-03 eta 0:13:32\n",
            "epoch [111/200] batch [5/18] time 0.450 (0.618) data 0.000 (0.176) loss 0.1766 (0.2012) acc 96.8750 (93.7500) lr 8.5910e-03 eta 0:16:37\n",
            "epoch [111/200] batch [10/18] time 0.446 (0.531) data 0.000 (0.088) loss 0.1590 (0.1788) acc 96.8750 (95.6250) lr 8.5910e-03 eta 0:14:14\n",
            "epoch [111/200] batch [15/18] time 0.444 (0.502) data 0.000 (0.059) loss 0.0838 (0.1919) acc 100.0000 (95.0000) lr 8.5910e-03 eta 0:13:25\n",
            "epoch [112/200] batch [5/18] time 0.448 (0.612) data 0.000 (0.169) loss 0.3568 (0.2715) acc 84.3750 (91.2500) lr 8.4357e-03 eta 0:16:16\n",
            "epoch [112/200] batch [10/18] time 0.447 (0.528) data 0.000 (0.085) loss 0.1644 (0.2318) acc 93.7500 (93.4375) lr 8.4357e-03 eta 0:13:59\n",
            "epoch [112/200] batch [15/18] time 0.446 (0.500) data 0.000 (0.057) loss 0.2689 (0.2182) acc 93.7500 (94.1667) lr 8.4357e-03 eta 0:13:12\n",
            "epoch [113/200] batch [5/18] time 0.451 (0.628) data 0.000 (0.185) loss 0.0669 (0.1765) acc 100.0000 (97.5000) lr 8.2807e-03 eta 0:16:31\n",
            "epoch [113/200] batch [10/18] time 0.443 (0.536) data 0.000 (0.093) loss 0.3250 (0.2219) acc 90.6250 (95.3125) lr 8.2807e-03 eta 0:14:02\n",
            "epoch [113/200] batch [15/18] time 0.441 (0.505) data 0.000 (0.062) loss 0.2166 (0.2055) acc 90.6250 (95.4167) lr 8.2807e-03 eta 0:13:12\n",
            "epoch [114/200] batch [5/18] time 0.450 (0.608) data 0.000 (0.165) loss 0.3133 (0.3108) acc 90.6250 (92.5000) lr 8.1262e-03 eta 0:15:49\n",
            "epoch [114/200] batch [10/18] time 0.445 (0.526) data 0.001 (0.083) loss 0.3003 (0.2797) acc 90.6250 (92.8125) lr 8.1262e-03 eta 0:13:37\n",
            "epoch [114/200] batch [15/18] time 0.442 (0.498) data 0.000 (0.055) loss 0.2130 (0.2493) acc 93.7500 (93.3333) lr 8.1262e-03 eta 0:12:52\n",
            "epoch [115/200] batch [5/18] time 0.451 (0.615) data 0.000 (0.172) loss 0.1066 (0.1692) acc 96.8750 (96.8750) lr 7.9721e-03 eta 0:15:48\n",
            "epoch [115/200] batch [10/18] time 0.442 (0.529) data 0.000 (0.086) loss 0.1665 (0.1612) acc 93.7500 (95.9375) lr 7.9721e-03 eta 0:13:32\n",
            "epoch [115/200] batch [15/18] time 0.444 (0.500) data 0.000 (0.058) loss 0.1247 (0.1801) acc 96.8750 (95.2083) lr 7.9721e-03 eta 0:12:46\n",
            "epoch [116/200] batch [5/18] time 0.448 (0.628) data 0.000 (0.186) loss 0.3737 (0.2014) acc 90.6250 (95.0000) lr 7.8186e-03 eta 0:15:57\n",
            "epoch [116/200] batch [10/18] time 0.445 (0.535) data 0.000 (0.093) loss 0.1269 (0.2046) acc 93.7500 (94.3750) lr 7.8186e-03 eta 0:13:33\n",
            "epoch [116/200] batch [15/18] time 0.446 (0.504) data 0.000 (0.062) loss 0.4760 (0.2200) acc 81.2500 (93.9583) lr 7.8186e-03 eta 0:12:44\n",
            "epoch [117/200] batch [5/18] time 0.453 (0.624) data 0.000 (0.180) loss 0.1351 (0.2532) acc 96.8750 (93.7500) lr 7.6655e-03 eta 0:15:39\n",
            "epoch [117/200] batch [10/18] time 0.442 (0.533) data 0.000 (0.090) loss 0.2756 (0.2284) acc 93.7500 (94.3750) lr 7.6655e-03 eta 0:13:20\n",
            "epoch [117/200] batch [15/18] time 0.443 (0.503) data 0.000 (0.060) loss 0.2086 (0.2368) acc 93.7500 (94.1667) lr 7.6655e-03 eta 0:12:32\n",
            "epoch [118/200] batch [5/18] time 0.449 (0.615) data 0.000 (0.173) loss 0.3823 (0.2604) acc 96.8750 (95.0000) lr 7.5131e-03 eta 0:15:16\n",
            "epoch [118/200] batch [10/18] time 0.446 (0.530) data 0.000 (0.087) loss 0.3572 (0.2546) acc 90.6250 (94.3750) lr 7.5131e-03 eta 0:13:06\n",
            "epoch [118/200] batch [15/18] time 0.442 (0.501) data 0.000 (0.058) loss 0.3984 (0.2642) acc 90.6250 (93.9583) lr 7.5131e-03 eta 0:12:20\n",
            "epoch [119/200] batch [5/18] time 0.449 (0.624) data 0.000 (0.182) loss 0.4304 (0.3227) acc 84.3750 (90.6250) lr 7.3613e-03 eta 0:15:17\n",
            "epoch [119/200] batch [10/18] time 0.443 (0.533) data 0.000 (0.091) loss 0.4729 (0.2963) acc 84.3750 (91.2500) lr 7.3613e-03 eta 0:13:01\n",
            "epoch [119/200] batch [15/18] time 0.443 (0.503) data 0.000 (0.061) loss 0.0813 (0.2555) acc 100.0000 (93.1250) lr 7.3613e-03 eta 0:12:14\n",
            "epoch [120/200] batch [5/18] time 0.448 (0.623) data 0.000 (0.180) loss 0.0679 (0.2095) acc 100.0000 (95.0000) lr 7.2101e-03 eta 0:15:05\n",
            "epoch [120/200] batch [10/18] time 0.447 (0.534) data 0.000 (0.090) loss 0.2210 (0.2136) acc 93.7500 (94.6875) lr 7.2101e-03 eta 0:12:52\n",
            "epoch [120/200] batch [15/18] time 0.445 (0.504) data 0.000 (0.060) loss 0.1765 (0.2175) acc 93.7500 (94.3750) lr 7.2101e-03 eta 0:12:06\n",
            "epoch [121/200] batch [5/18] time 0.450 (0.622) data 0.000 (0.179) loss 0.1878 (0.1831) acc 93.7500 (96.2500) lr 7.0596e-03 eta 0:14:52\n",
            "epoch [121/200] batch [10/18] time 0.446 (0.533) data 0.000 (0.090) loss 0.1053 (0.1919) acc 96.8750 (95.3125) lr 7.0596e-03 eta 0:12:41\n",
            "epoch [121/200] batch [15/18] time 0.445 (0.503) data 0.000 (0.060) loss 0.2907 (0.1812) acc 90.6250 (95.6250) lr 7.0596e-03 eta 0:11:57\n",
            "epoch [122/200] batch [5/18] time 0.446 (0.631) data 0.000 (0.187) loss 0.1309 (0.2396) acc 96.8750 (92.5000) lr 6.9098e-03 eta 0:14:53\n",
            "epoch [122/200] batch [10/18] time 0.443 (0.536) data 0.000 (0.094) loss 0.5133 (0.2358) acc 84.3750 (92.8125) lr 6.9098e-03 eta 0:12:37\n",
            "epoch [122/200] batch [15/18] time 0.442 (0.505) data 0.000 (0.063) loss 0.1477 (0.2432) acc 93.7500 (92.0833) lr 6.9098e-03 eta 0:11:51\n",
            "epoch [123/200] batch [5/18] time 0.453 (0.614) data 0.000 (0.170) loss 0.0825 (0.1670) acc 96.8750 (96.2500) lr 6.7608e-03 eta 0:14:19\n",
            "epoch [123/200] batch [10/18] time 0.444 (0.529) data 0.000 (0.085) loss 0.2674 (0.1542) acc 90.6250 (96.5625) lr 6.7608e-03 eta 0:12:17\n",
            "epoch [123/200] batch [15/18] time 0.447 (0.500) data 0.000 (0.057) loss 0.2772 (0.1618) acc 84.3750 (95.6250) lr 6.7608e-03 eta 0:11:34\n",
            "epoch [124/200] batch [5/18] time 0.452 (0.619) data 0.000 (0.177) loss 0.2743 (0.2264) acc 93.7500 (94.3750) lr 6.6126e-03 eta 0:14:15\n",
            "epoch [124/200] batch [10/18] time 0.443 (0.531) data 0.000 (0.089) loss 0.1981 (0.2209) acc 90.6250 (94.0625) lr 6.6126e-03 eta 0:12:10\n",
            "epoch [124/200] batch [15/18] time 0.437 (0.501) data 0.001 (0.059) loss 0.1872 (0.1984) acc 93.7500 (94.7917) lr 6.6126e-03 eta 0:11:26\n",
            "epoch [125/200] batch [5/18] time 0.452 (0.627) data 0.000 (0.184) loss 0.0965 (0.2279) acc 96.8750 (93.7500) lr 6.4653e-03 eta 0:14:14\n",
            "epoch [125/200] batch [10/18] time 0.443 (0.534) data 0.000 (0.092) loss 0.3278 (0.2455) acc 90.6250 (93.7500) lr 6.4653e-03 eta 0:12:05\n",
            "epoch [125/200] batch [15/18] time 0.440 (0.504) data 0.000 (0.062) loss 0.2476 (0.2231) acc 96.8750 (94.5833) lr 6.4653e-03 eta 0:11:21\n",
            "epoch [126/200] batch [5/18] time 0.456 (0.624) data 0.000 (0.179) loss 0.1670 (0.1830) acc 96.8750 (96.2500) lr 6.3188e-03 eta 0:13:58\n",
            "epoch [126/200] batch [10/18] time 0.447 (0.534) data 0.000 (0.090) loss 0.0834 (0.1536) acc 100.0000 (96.5625) lr 6.3188e-03 eta 0:11:55\n",
            "epoch [126/200] batch [15/18] time 0.442 (0.504) data 0.000 (0.060) loss 0.2562 (0.1669) acc 93.7500 (96.0417) lr 6.3188e-03 eta 0:11:12\n",
            "epoch [127/200] batch [5/18] time 0.453 (0.615) data 0.000 (0.172) loss 0.1273 (0.1918) acc 96.8750 (95.6250) lr 6.1732e-03 eta 0:13:36\n",
            "epoch [127/200] batch [10/18] time 0.443 (0.529) data 0.000 (0.086) loss 0.2555 (0.2121) acc 90.6250 (94.0625) lr 6.1732e-03 eta 0:11:38\n",
            "epoch [127/200] batch [15/18] time 0.445 (0.500) data 0.000 (0.057) loss 0.1663 (0.2099) acc 93.7500 (93.9583) lr 6.1732e-03 eta 0:10:58\n",
            "epoch [128/200] batch [5/18] time 0.449 (0.624) data 0.000 (0.181) loss 0.1867 (0.3052) acc 96.8750 (95.6250) lr 6.0285e-03 eta 0:13:36\n",
            "epoch [128/200] batch [10/18] time 0.444 (0.533) data 0.000 (0.091) loss 0.3173 (0.2929) acc 90.6250 (93.7500) lr 6.0285e-03 eta 0:11:35\n",
            "epoch [128/200] batch [15/18] time 0.446 (0.503) data 0.000 (0.061) loss 0.2517 (0.2825) acc 93.7500 (93.3333) lr 6.0285e-03 eta 0:10:53\n",
            "epoch [129/200] batch [5/18] time 0.450 (0.626) data 0.000 (0.183) loss 0.1293 (0.1428) acc 93.7500 (96.2500) lr 5.8849e-03 eta 0:13:27\n",
            "epoch [129/200] batch [10/18] time 0.443 (0.534) data 0.000 (0.091) loss 0.0961 (0.1488) acc 96.8750 (96.2500) lr 5.8849e-03 eta 0:11:27\n",
            "epoch [129/200] batch [15/18] time 0.446 (0.504) data 0.000 (0.061) loss 0.2572 (0.1538) acc 93.7500 (96.0417) lr 5.8849e-03 eta 0:10:45\n",
            "epoch [130/200] batch [5/18] time 0.451 (0.635) data 0.000 (0.192) loss 0.2096 (0.2578) acc 90.6250 (92.5000) lr 5.7422e-03 eta 0:13:28\n",
            "epoch [130/200] batch [10/18] time 0.439 (0.539) data 0.000 (0.096) loss 0.1605 (0.2080) acc 96.8750 (94.3750) lr 5.7422e-03 eta 0:11:23\n",
            "epoch [130/200] batch [15/18] time 0.445 (0.507) data 0.000 (0.064) loss 0.4029 (0.2057) acc 90.6250 (94.5833) lr 5.7422e-03 eta 0:10:40\n",
            "epoch [131/200] batch [5/18] time 0.458 (0.622) data 0.000 (0.181) loss 0.3687 (0.1997) acc 84.3750 (93.1250) lr 5.6006e-03 eta 0:13:00\n",
            "epoch [131/200] batch [10/18] time 0.443 (0.533) data 0.000 (0.090) loss 0.4374 (0.2078) acc 90.6250 (94.3750) lr 5.6006e-03 eta 0:11:06\n",
            "epoch [131/200] batch [15/18] time 0.441 (0.503) data 0.000 (0.060) loss 0.1584 (0.2044) acc 93.7500 (94.1667) lr 5.6006e-03 eta 0:10:26\n",
            "epoch [132/200] batch [5/18] time 0.452 (0.625) data 0.000 (0.183) loss 0.0786 (0.1921) acc 96.8750 (95.6250) lr 5.4601e-03 eta 0:12:53\n",
            "epoch [132/200] batch [10/18] time 0.449 (0.534) data 0.000 (0.092) loss 0.4610 (0.2167) acc 87.5000 (94.3750) lr 5.4601e-03 eta 0:10:58\n",
            "epoch [132/200] batch [15/18] time 0.445 (0.504) data 0.000 (0.061) loss 0.1034 (0.2045) acc 96.8750 (94.5833) lr 5.4601e-03 eta 0:10:18\n",
            "epoch [133/200] batch [5/18] time 0.452 (0.631) data 0.000 (0.188) loss 0.1013 (0.1872) acc 96.8750 (94.3750) lr 5.3207e-03 eta 0:12:49\n",
            "epoch [133/200] batch [10/18] time 0.445 (0.538) data 0.000 (0.094) loss 0.3209 (0.2476) acc 90.6250 (92.5000) lr 5.3207e-03 eta 0:10:53\n",
            "epoch [133/200] batch [15/18] time 0.443 (0.506) data 0.000 (0.063) loss 0.2452 (0.2515) acc 93.7500 (92.9167) lr 5.3207e-03 eta 0:10:12\n",
            "epoch [134/200] batch [5/18] time 0.454 (0.621) data 0.000 (0.178) loss 0.2532 (0.1695) acc 90.6250 (95.6250) lr 5.1825e-03 eta 0:12:25\n",
            "epoch [134/200] batch [10/18] time 0.445 (0.532) data 0.000 (0.089) loss 0.4385 (0.2019) acc 90.6250 (95.3125) lr 5.1825e-03 eta 0:10:36\n",
            "epoch [134/200] batch [15/18] time 0.440 (0.503) data 0.000 (0.059) loss 0.1355 (0.1978) acc 96.8750 (95.4167) lr 5.1825e-03 eta 0:09:58\n",
            "epoch [135/200] batch [5/18] time 0.454 (0.612) data 0.000 (0.169) loss 0.3301 (0.1855) acc 87.5000 (94.3750) lr 5.0454e-03 eta 0:12:04\n",
            "epoch [135/200] batch [10/18] time 0.446 (0.528) data 0.000 (0.085) loss 0.0549 (0.1671) acc 100.0000 (95.6250) lr 5.0454e-03 eta 0:10:22\n",
            "epoch [135/200] batch [15/18] time 0.443 (0.500) data 0.000 (0.057) loss 0.3381 (0.1917) acc 93.7500 (95.0000) lr 5.0454e-03 eta 0:09:46\n",
            "epoch [136/200] batch [5/18] time 0.448 (0.616) data 0.000 (0.175) loss 0.1003 (0.1498) acc 100.0000 (96.8750) lr 4.9096e-03 eta 0:11:58\n",
            "epoch [136/200] batch [10/18] time 0.446 (0.531) data 0.000 (0.088) loss 0.1195 (0.2060) acc 96.8750 (94.3750) lr 4.9096e-03 eta 0:10:15\n",
            "epoch [136/200] batch [15/18] time 0.442 (0.502) data 0.000 (0.059) loss 0.3157 (0.2135) acc 93.7500 (94.3750) lr 4.9096e-03 eta 0:09:39\n",
            "epoch [137/200] batch [5/18] time 0.448 (0.617) data 0.000 (0.175) loss 0.2402 (0.2617) acc 93.7500 (92.5000) lr 4.7750e-03 eta 0:11:47\n",
            "epoch [137/200] batch [10/18] time 0.446 (0.530) data 0.000 (0.088) loss 0.1614 (0.2322) acc 96.8750 (93.4375) lr 4.7750e-03 eta 0:10:05\n",
            "epoch [137/200] batch [15/18] time 0.445 (0.502) data 0.000 (0.059) loss 0.1308 (0.2188) acc 96.8750 (94.1667) lr 4.7750e-03 eta 0:09:30\n",
            "epoch [138/200] batch [5/18] time 0.450 (0.625) data 0.000 (0.181) loss 0.3428 (0.2394) acc 93.7500 (95.0000) lr 4.6417e-03 eta 0:11:45\n",
            "epoch [138/200] batch [10/18] time 0.442 (0.534) data 0.000 (0.091) loss 0.3049 (0.2116) acc 87.5000 (95.3125) lr 4.6417e-03 eta 0:09:59\n",
            "epoch [138/200] batch [15/18] time 0.445 (0.503) data 0.000 (0.061) loss 0.1333 (0.1946) acc 96.8750 (95.4167) lr 4.6417e-03 eta 0:09:23\n",
            "epoch [139/200] batch [5/18] time 0.449 (0.616) data 0.000 (0.175) loss 0.4187 (0.2056) acc 87.5000 (95.0000) lr 4.5098e-03 eta 0:11:24\n",
            "epoch [139/200] batch [10/18] time 0.443 (0.530) data 0.000 (0.088) loss 0.1899 (0.1810) acc 93.7500 (95.6250) lr 4.5098e-03 eta 0:09:46\n",
            "epoch [139/200] batch [15/18] time 0.445 (0.501) data 0.000 (0.059) loss 0.1874 (0.1755) acc 93.7500 (95.6250) lr 4.5098e-03 eta 0:09:11\n",
            "epoch [140/200] batch [5/18] time 0.454 (0.631) data 0.000 (0.187) loss 0.2834 (0.1932) acc 93.7500 (94.3750) lr 4.3792e-03 eta 0:11:29\n",
            "epoch [140/200] batch [10/18] time 0.446 (0.537) data 0.000 (0.094) loss 0.3630 (0.1693) acc 90.6250 (95.6250) lr 4.3792e-03 eta 0:09:44\n",
            "epoch [140/200] batch [15/18] time 0.444 (0.506) data 0.000 (0.063) loss 0.1811 (0.1840) acc 96.8750 (95.8333) lr 4.3792e-03 eta 0:09:07\n",
            "epoch [141/200] batch [5/18] time 0.452 (0.630) data 0.000 (0.188) loss 0.1563 (0.1457) acc 93.7500 (96.2500) lr 4.2499e-03 eta 0:11:17\n",
            "epoch [141/200] batch [10/18] time 0.445 (0.537) data 0.000 (0.094) loss 0.1780 (0.1950) acc 96.8750 (94.3750) lr 4.2499e-03 eta 0:09:34\n",
            "epoch [141/200] batch [15/18] time 0.444 (0.505) data 0.000 (0.063) loss 0.2994 (0.2203) acc 90.6250 (94.1667) lr 4.2499e-03 eta 0:08:58\n",
            "epoch [142/200] batch [5/18] time 0.453 (0.626) data 0.000 (0.184) loss 0.4098 (0.2736) acc 87.5000 (93.7500) lr 4.1221e-03 eta 0:11:01\n",
            "epoch [142/200] batch [10/18] time 0.443 (0.535) data 0.000 (0.092) loss 0.2278 (0.2014) acc 93.7500 (95.6250) lr 4.1221e-03 eta 0:09:22\n",
            "epoch [142/200] batch [15/18] time 0.445 (0.505) data 0.000 (0.061) loss 0.1118 (0.1860) acc 96.8750 (95.6250) lr 4.1221e-03 eta 0:08:48\n",
            "epoch [143/200] batch [5/18] time 0.451 (0.623) data 0.000 (0.181) loss 0.5370 (0.2942) acc 81.2500 (91.2500) lr 3.9958e-03 eta 0:10:47\n",
            "epoch [143/200] batch [10/18] time 0.444 (0.534) data 0.000 (0.091) loss 0.0972 (0.2416) acc 100.0000 (93.1250) lr 3.9958e-03 eta 0:09:11\n",
            "epoch [143/200] batch [15/18] time 0.445 (0.503) data 0.000 (0.061) loss 0.2327 (0.2293) acc 93.7500 (93.5417) lr 3.9958e-03 eta 0:08:37\n",
            "epoch [144/200] batch [5/18] time 0.454 (0.624) data 0.000 (0.181) loss 0.1774 (0.2166) acc 96.8750 (93.1250) lr 3.8709e-03 eta 0:10:36\n",
            "epoch [144/200] batch [10/18] time 0.441 (0.534) data 0.001 (0.091) loss 0.0897 (0.1999) acc 96.8750 (94.0625) lr 3.8709e-03 eta 0:09:02\n",
            "epoch [144/200] batch [15/18] time 0.445 (0.504) data 0.000 (0.061) loss 0.2043 (0.2208) acc 93.7500 (93.5417) lr 3.8709e-03 eta 0:08:29\n",
            "epoch [145/200] batch [5/18] time 0.448 (0.619) data 0.000 (0.177) loss 0.1320 (0.1876) acc 96.8750 (93.7500) lr 3.7476e-03 eta 0:10:21\n",
            "epoch [145/200] batch [10/18] time 0.446 (0.531) data 0.000 (0.089) loss 0.0911 (0.1728) acc 100.0000 (95.0000) lr 3.7476e-03 eta 0:08:49\n",
            "epoch [145/200] batch [15/18] time 0.447 (0.502) data 0.000 (0.059) loss 0.4213 (0.1917) acc 87.5000 (94.3750) lr 3.7476e-03 eta 0:08:18\n",
            "epoch [146/200] batch [5/18] time 0.453 (0.638) data 0.000 (0.195) loss 0.3260 (0.1691) acc 93.7500 (96.2500) lr 3.6258e-03 eta 0:10:27\n",
            "epoch [146/200] batch [10/18] time 0.444 (0.540) data 0.000 (0.098) loss 0.0835 (0.1593) acc 96.8750 (95.6250) lr 3.6258e-03 eta 0:08:49\n",
            "epoch [146/200] batch [15/18] time 0.444 (0.508) data 0.000 (0.065) loss 0.1884 (0.1749) acc 96.8750 (95.4167) lr 3.6258e-03 eta 0:08:15\n",
            "epoch [147/200] batch [5/18] time 0.452 (0.617) data 0.000 (0.173) loss 0.2396 (0.1618) acc 96.8750 (96.2500) lr 3.5055e-03 eta 0:09:56\n",
            "epoch [147/200] batch [10/18] time 0.443 (0.530) data 0.000 (0.087) loss 0.1430 (0.1564) acc 96.8750 (96.5625) lr 3.5055e-03 eta 0:08:29\n",
            "epoch [147/200] batch [15/18] time 0.441 (0.501) data 0.000 (0.058) loss 0.1583 (0.1837) acc 93.7500 (95.8333) lr 3.5055e-03 eta 0:07:59\n",
            "epoch [148/200] batch [5/18] time 0.448 (0.613) data 0.000 (0.171) loss 0.4590 (0.3033) acc 84.3750 (91.2500) lr 3.3869e-03 eta 0:09:41\n",
            "epoch [148/200] batch [10/18] time 0.443 (0.528) data 0.000 (0.086) loss 0.0467 (0.2245) acc 100.0000 (94.0625) lr 3.3869e-03 eta 0:08:18\n",
            "epoch [148/200] batch [15/18] time 0.442 (0.500) data 0.000 (0.057) loss 0.0782 (0.2060) acc 100.0000 (94.7917) lr 3.3869e-03 eta 0:07:49\n",
            "epoch [149/200] batch [5/18] time 0.452 (0.647) data 0.000 (0.203) loss 0.3855 (0.2100) acc 87.5000 (94.3750) lr 3.2699e-03 eta 0:10:02\n",
            "epoch [149/200] batch [10/18] time 0.444 (0.545) data 0.000 (0.102) loss 0.2710 (0.2383) acc 90.6250 (92.8125) lr 3.2699e-03 eta 0:08:24\n",
            "epoch [149/200] batch [15/18] time 0.445 (0.511) data 0.000 (0.068) loss 0.3340 (0.2310) acc 93.7500 (93.3333) lr 3.2699e-03 eta 0:07:50\n",
            "epoch [150/200] batch [5/18] time 0.455 (0.646) data 0.000 (0.203) loss 0.1834 (0.1745) acc 96.8750 (96.2500) lr 3.1545e-03 eta 0:09:49\n",
            "epoch [150/200] batch [10/18] time 0.442 (0.544) data 0.000 (0.102) loss 0.2245 (0.1875) acc 93.7500 (96.2500) lr 3.1545e-03 eta 0:08:13\n",
            "epoch [150/200] batch [15/18] time 0.443 (0.510) data 0.000 (0.068) loss 0.1069 (0.1816) acc 96.8750 (96.0417) lr 3.1545e-03 eta 0:07:40\n",
            "epoch [151/200] batch [5/18] time 0.451 (0.622) data 0.000 (0.179) loss 0.2886 (0.1087) acc 90.6250 (98.1250) lr 3.0409e-03 eta 0:09:16\n",
            "epoch [151/200] batch [10/18] time 0.446 (0.533) data 0.001 (0.090) loss 0.0709 (0.1303) acc 96.8750 (97.1875) lr 3.0409e-03 eta 0:07:54\n",
            "epoch [151/200] batch [15/18] time 0.443 (0.504) data 0.000 (0.060) loss 0.0741 (0.1265) acc 100.0000 (97.5000) lr 3.0409e-03 eta 0:07:25\n",
            "epoch [152/200] batch [5/18] time 0.451 (0.633) data 0.000 (0.190) loss 0.0600 (0.2158) acc 96.8750 (92.5000) lr 2.9289e-03 eta 0:09:15\n",
            "epoch [152/200] batch [10/18] time 0.444 (0.538) data 0.000 (0.095) loss 0.1335 (0.2132) acc 96.8750 (93.4375) lr 2.9289e-03 eta 0:07:49\n",
            "epoch [152/200] batch [15/18] time 0.440 (0.506) data 0.000 (0.063) loss 0.3108 (0.2056) acc 90.6250 (94.3750) lr 2.9289e-03 eta 0:07:18\n",
            "epoch [153/200] batch [5/18] time 0.448 (0.612) data 0.000 (0.171) loss 0.1585 (0.1250) acc 96.8750 (97.5000) lr 2.8187e-03 eta 0:08:45\n",
            "epoch [153/200] batch [10/18] time 0.447 (0.528) data 0.000 (0.086) loss 0.3620 (0.1727) acc 87.5000 (95.6250) lr 2.8187e-03 eta 0:07:30\n",
            "epoch [153/200] batch [15/18] time 0.442 (0.499) data 0.000 (0.057) loss 0.0637 (0.1523) acc 100.0000 (96.2500) lr 2.8187e-03 eta 0:07:04\n",
            "epoch [154/200] batch [5/18] time 0.450 (0.613) data 0.000 (0.170) loss 0.1196 (0.1094) acc 96.8750 (97.5000) lr 2.7103e-03 eta 0:08:35\n",
            "epoch [154/200] batch [10/18] time 0.446 (0.527) data 0.000 (0.085) loss 0.3840 (0.1371) acc 90.6250 (96.5625) lr 2.7103e-03 eta 0:07:20\n",
            "epoch [154/200] batch [15/18] time 0.445 (0.499) data 0.000 (0.057) loss 0.2553 (0.1604) acc 90.6250 (95.8333) lr 2.7103e-03 eta 0:06:54\n",
            "epoch [155/200] batch [5/18] time 0.449 (0.619) data 0.000 (0.177) loss 0.2369 (0.1687) acc 93.7500 (96.2500) lr 2.6037e-03 eta 0:08:29\n",
            "epoch [155/200] batch [10/18] time 0.445 (0.530) data 0.000 (0.089) loss 0.1179 (0.1945) acc 96.8750 (94.6875) lr 2.6037e-03 eta 0:07:13\n",
            "epoch [155/200] batch [15/18] time 0.441 (0.501) data 0.000 (0.059) loss 0.1077 (0.2009) acc 100.0000 (94.3750) lr 2.6037e-03 eta 0:06:47\n",
            "epoch [156/200] batch [5/18] time 0.447 (0.626) data 0.000 (0.186) loss 0.1986 (0.1361) acc 93.7500 (97.5000) lr 2.4989e-03 eta 0:08:23\n",
            "epoch [156/200] batch [10/18] time 0.443 (0.533) data 0.000 (0.093) loss 0.1920 (0.1474) acc 93.7500 (96.8750) lr 2.4989e-03 eta 0:07:06\n",
            "epoch [156/200] batch [15/18] time 0.444 (0.504) data 0.000 (0.062) loss 0.3345 (0.2102) acc 93.7500 (95.0000) lr 2.4989e-03 eta 0:06:40\n",
            "epoch [157/200] batch [5/18] time 0.451 (0.618) data 0.000 (0.176) loss 0.0658 (0.1406) acc 100.0000 (96.8750) lr 2.3959e-03 eta 0:08:06\n",
            "epoch [157/200] batch [10/18] time 0.441 (0.530) data 0.000 (0.088) loss 0.1753 (0.1859) acc 96.8750 (95.9375) lr 2.3959e-03 eta 0:06:54\n",
            "epoch [157/200] batch [15/18] time 0.443 (0.501) data 0.000 (0.059) loss 0.1083 (0.1779) acc 100.0000 (96.0417) lr 2.3959e-03 eta 0:06:29\n",
            "epoch [158/200] batch [5/18] time 0.449 (0.623) data 0.000 (0.181) loss 0.1189 (0.1601) acc 96.8750 (96.8750) lr 2.2949e-03 eta 0:07:59\n",
            "epoch [158/200] batch [10/18] time 0.440 (0.533) data 0.000 (0.091) loss 0.1164 (0.1590) acc 100.0000 (97.1875) lr 2.2949e-03 eta 0:06:47\n",
            "epoch [158/200] batch [15/18] time 0.441 (0.503) data 0.000 (0.061) loss 0.2849 (0.2021) acc 90.6250 (95.6250) lr 2.2949e-03 eta 0:06:21\n",
            "epoch [159/200] batch [5/18] time 0.448 (0.610) data 0.000 (0.169) loss 0.1209 (0.1307) acc 96.8750 (96.2500) lr 2.1957e-03 eta 0:07:37\n",
            "epoch [159/200] batch [10/18] time 0.443 (0.526) data 0.000 (0.085) loss 0.3552 (0.1727) acc 87.5000 (95.6250) lr 2.1957e-03 eta 0:06:32\n",
            "epoch [159/200] batch [15/18] time 0.441 (0.498) data 0.000 (0.056) loss 0.1513 (0.1797) acc 96.8750 (95.6250) lr 2.1957e-03 eta 0:06:09\n",
            "epoch [160/200] batch [5/18] time 0.447 (0.614) data 0.000 (0.173) loss 0.2503 (0.1606) acc 93.7500 (95.6250) lr 2.0984e-03 eta 0:07:30\n",
            "epoch [160/200] batch [10/18] time 0.443 (0.529) data 0.000 (0.087) loss 0.1799 (0.1671) acc 93.7500 (95.9375) lr 2.0984e-03 eta 0:06:24\n",
            "epoch [160/200] batch [15/18] time 0.442 (0.500) data 0.000 (0.058) loss 0.1419 (0.1526) acc 96.8750 (96.2500) lr 2.0984e-03 eta 0:06:01\n",
            "epoch [161/200] batch [5/18] time 0.447 (0.618) data 0.000 (0.177) loss 0.2219 (0.1734) acc 96.8750 (95.0000) lr 2.0032e-03 eta 0:07:22\n",
            "epoch [161/200] batch [10/18] time 0.446 (0.531) data 0.000 (0.089) loss 0.2871 (0.1655) acc 93.7500 (95.9375) lr 2.0032e-03 eta 0:06:17\n",
            "epoch [161/200] batch [15/18] time 0.446 (0.502) data 0.000 (0.059) loss 0.1738 (0.1724) acc 96.8750 (95.6250) lr 2.0032e-03 eta 0:05:53\n",
            "epoch [162/200] batch [5/18] time 0.454 (0.621) data 0.000 (0.177) loss 0.2311 (0.2079) acc 96.8750 (96.2500) lr 1.9098e-03 eta 0:07:12\n",
            "epoch [162/200] batch [10/18] time 0.445 (0.532) data 0.000 (0.089) loss 0.3557 (0.2275) acc 93.7500 (95.0000) lr 1.9098e-03 eta 0:06:08\n",
            "epoch [162/200] batch [15/18] time 0.443 (0.502) data 0.000 (0.059) loss 0.4291 (0.2265) acc 87.5000 (94.5833) lr 1.9098e-03 eta 0:05:44\n",
            "epoch [163/200] batch [5/18] time 0.454 (0.642) data 0.000 (0.199) loss 0.2163 (0.1824) acc 93.7500 (95.0000) lr 1.8185e-03 eta 0:07:15\n",
            "epoch [163/200] batch [10/18] time 0.442 (0.542) data 0.000 (0.100) loss 0.1294 (0.1619) acc 96.8750 (95.9375) lr 1.8185e-03 eta 0:06:05\n",
            "epoch [163/200] batch [15/18] time 0.444 (0.509) data 0.000 (0.067) loss 0.3220 (0.1781) acc 93.7500 (95.2083) lr 1.8185e-03 eta 0:05:40\n",
            "epoch [164/200] batch [5/18] time 0.450 (0.618) data 0.000 (0.175) loss 0.0659 (0.2034) acc 100.0000 (95.0000) lr 1.7292e-03 eta 0:06:48\n",
            "epoch [164/200] batch [10/18] time 0.445 (0.531) data 0.000 (0.088) loss 0.1669 (0.2066) acc 96.8750 (94.3750) lr 1.7292e-03 eta 0:05:48\n",
            "epoch [164/200] batch [15/18] time 0.442 (0.501) data 0.000 (0.059) loss 0.1664 (0.2088) acc 96.8750 (94.3750) lr 1.7292e-03 eta 0:05:26\n",
            "epoch [165/200] batch [5/18] time 0.452 (0.608) data 0.000 (0.164) loss 0.1783 (0.2175) acc 96.8750 (93.7500) lr 1.6419e-03 eta 0:06:30\n",
            "epoch [165/200] batch [10/18] time 0.444 (0.526) data 0.000 (0.082) loss 0.3012 (0.2174) acc 87.5000 (94.0625) lr 1.6419e-03 eta 0:05:35\n",
            "epoch [165/200] batch [15/18] time 0.442 (0.498) data 0.000 (0.055) loss 0.1928 (0.2135) acc 96.8750 (94.3750) lr 1.6419e-03 eta 0:05:15\n",
            "epoch [166/200] batch [5/18] time 0.454 (0.626) data 0.000 (0.184) loss 0.0689 (0.2775) acc 100.0000 (91.2500) lr 1.5567e-03 eta 0:06:31\n",
            "epoch [166/200] batch [10/18] time 0.444 (0.534) data 0.000 (0.092) loss 0.1877 (0.2271) acc 96.8750 (93.1250) lr 1.5567e-03 eta 0:05:31\n",
            "epoch [166/200] batch [15/18] time 0.445 (0.504) data 0.000 (0.061) loss 0.2062 (0.2166) acc 93.7500 (93.5417) lr 1.5567e-03 eta 0:05:10\n",
            "epoch [167/200] batch [5/18] time 0.451 (0.642) data 0.000 (0.199) loss 0.3202 (0.2514) acc 90.6250 (93.1250) lr 1.4736e-03 eta 0:06:29\n",
            "epoch [167/200] batch [10/18] time 0.445 (0.543) data 0.000 (0.100) loss 0.2658 (0.2432) acc 87.5000 (92.1875) lr 1.4736e-03 eta 0:05:27\n",
            "epoch [167/200] batch [15/18] time 0.441 (0.510) data 0.000 (0.066) loss 0.1105 (0.2179) acc 96.8750 (93.7500) lr 1.4736e-03 eta 0:05:04\n",
            "epoch [168/200] batch [5/18] time 0.452 (0.637) data 0.000 (0.193) loss 0.2135 (0.2370) acc 96.8750 (95.0000) lr 1.3926e-03 eta 0:06:15\n",
            "epoch [168/200] batch [10/18] time 0.446 (0.541) data 0.000 (0.097) loss 0.0553 (0.1665) acc 100.0000 (96.8750) lr 1.3926e-03 eta 0:05:15\n",
            "epoch [168/200] batch [15/18] time 0.445 (0.509) data 0.000 (0.065) loss 0.2512 (0.1548) acc 93.7500 (96.6667) lr 1.3926e-03 eta 0:04:54\n",
            "epoch [169/200] batch [5/18] time 0.449 (0.622) data 0.000 (0.179) loss 0.0945 (0.1350) acc 100.0000 (97.5000) lr 1.3137e-03 eta 0:05:55\n",
            "epoch [169/200] batch [10/18] time 0.442 (0.532) data 0.000 (0.090) loss 0.1729 (0.1460) acc 93.7500 (96.8750) lr 1.3137e-03 eta 0:05:01\n",
            "epoch [169/200] batch [15/18] time 0.444 (0.503) data 0.000 (0.060) loss 0.3534 (0.1857) acc 93.7500 (95.6250) lr 1.3137e-03 eta 0:04:42\n",
            "epoch [170/200] batch [5/18] time 0.452 (0.620) data 0.000 (0.180) loss 0.1118 (0.1775) acc 96.8750 (95.0000) lr 1.2369e-03 eta 0:05:42\n",
            "epoch [170/200] batch [10/18] time 0.447 (0.532) data 0.000 (0.090) loss 0.3634 (0.1723) acc 84.3750 (95.3125) lr 1.2369e-03 eta 0:04:51\n",
            "epoch [170/200] batch [15/18] time 0.441 (0.502) data 0.000 (0.060) loss 0.2488 (0.1681) acc 93.7500 (95.8333) lr 1.2369e-03 eta 0:04:32\n",
            "epoch [171/200] batch [5/18] time 0.452 (0.617) data 0.000 (0.174) loss 0.2250 (0.1411) acc 93.7500 (96.8750) lr 1.1623e-03 eta 0:05:29\n",
            "epoch [171/200] batch [10/18] time 0.448 (0.530) data 0.000 (0.087) loss 0.1115 (0.1908) acc 96.8750 (95.6250) lr 1.1623e-03 eta 0:04:40\n",
            "epoch [171/200] batch [15/18] time 0.444 (0.501) data 0.000 (0.058) loss 0.0580 (0.1877) acc 100.0000 (95.8333) lr 1.1623e-03 eta 0:04:23\n",
            "epoch [172/200] batch [5/18] time 0.449 (0.623) data 0.000 (0.180) loss 0.1549 (0.1295) acc 100.0000 (96.8750) lr 1.0899e-03 eta 0:05:22\n",
            "epoch [172/200] batch [10/18] time 0.443 (0.533) data 0.000 (0.090) loss 0.2173 (0.1731) acc 90.6250 (95.6250) lr 1.0899e-03 eta 0:04:33\n",
            "epoch [172/200] batch [15/18] time 0.442 (0.503) data 0.000 (0.060) loss 0.0945 (0.1519) acc 100.0000 (96.4583) lr 1.0899e-03 eta 0:04:14\n",
            "epoch [173/200] batch [5/18] time 0.453 (0.627) data 0.000 (0.182) loss 0.2745 (0.2067) acc 93.7500 (93.7500) lr 1.0197e-03 eta 0:05:12\n",
            "epoch [173/200] batch [10/18] time 0.444 (0.535) data 0.000 (0.091) loss 0.0814 (0.1673) acc 100.0000 (95.3125) lr 1.0197e-03 eta 0:04:24\n",
            "epoch [173/200] batch [15/18] time 0.445 (0.505) data 0.000 (0.061) loss 0.1157 (0.1887) acc 96.8750 (95.0000) lr 1.0197e-03 eta 0:04:06\n",
            "epoch [174/200] batch [5/18] time 0.451 (0.614) data 0.000 (0.171) loss 0.2279 (0.1938) acc 93.7500 (96.2500) lr 9.5173e-04 eta 0:04:55\n",
            "epoch [174/200] batch [10/18] time 0.446 (0.530) data 0.000 (0.086) loss 0.1919 (0.1774) acc 93.7500 (96.2500) lr 9.5173e-04 eta 0:04:12\n",
            "epoch [174/200] batch [15/18] time 0.441 (0.501) data 0.000 (0.057) loss 0.3722 (0.1656) acc 87.5000 (95.8333) lr 9.5173e-04 eta 0:03:55\n",
            "epoch [175/200] batch [5/18] time 0.448 (0.630) data 0.000 (0.189) loss 0.2007 (0.1627) acc 93.7500 (95.0000) lr 8.8597e-04 eta 0:04:51\n",
            "epoch [175/200] batch [10/18] time 0.443 (0.537) data 0.000 (0.095) loss 0.1170 (0.1773) acc 93.7500 (95.0000) lr 8.8597e-04 eta 0:04:05\n",
            "epoch [175/200] batch [15/18] time 0.445 (0.506) data 0.000 (0.063) loss 0.1512 (0.1779) acc 93.7500 (95.0000) lr 8.8597e-04 eta 0:03:49\n",
            "epoch [176/200] batch [5/18] time 0.448 (0.620) data 0.000 (0.178) loss 0.3578 (0.2708) acc 93.7500 (92.5000) lr 8.2245e-04 eta 0:04:35\n",
            "epoch [176/200] batch [10/18] time 0.444 (0.532) data 0.000 (0.089) loss 0.0618 (0.1934) acc 96.8750 (94.6875) lr 8.2245e-04 eta 0:03:54\n",
            "epoch [176/200] batch [15/18] time 0.447 (0.503) data 0.000 (0.059) loss 0.0822 (0.1740) acc 96.8750 (95.2083) lr 8.2245e-04 eta 0:03:38\n",
            "epoch [177/200] batch [5/18] time 0.452 (0.614) data 0.000 (0.172) loss 0.2931 (0.1627) acc 93.7500 (95.6250) lr 7.6120e-04 eta 0:04:22\n",
            "epoch [177/200] batch [10/18] time 0.444 (0.528) data 0.000 (0.086) loss 0.3128 (0.2102) acc 90.6250 (94.3750) lr 7.6120e-04 eta 0:03:42\n",
            "epoch [177/200] batch [15/18] time 0.446 (0.500) data 0.000 (0.058) loss 0.2318 (0.1856) acc 90.6250 (95.0000) lr 7.6120e-04 eta 0:03:28\n",
            "epoch [178/200] batch [5/18] time 0.448 (0.617) data 0.001 (0.176) loss 0.0963 (0.1388) acc 96.8750 (95.6250) lr 7.0224e-04 eta 0:04:12\n",
            "epoch [178/200] batch [10/18] time 0.441 (0.530) data 0.000 (0.088) loss 0.2115 (0.1361) acc 96.8750 (96.8750) lr 7.0224e-04 eta 0:03:34\n",
            "epoch [178/200] batch [15/18] time 0.443 (0.501) data 0.000 (0.059) loss 0.1098 (0.1391) acc 96.8750 (96.6667) lr 7.0224e-04 eta 0:03:19\n",
            "epoch [179/200] batch [5/18] time 0.450 (0.618) data 0.000 (0.176) loss 0.1516 (0.1394) acc 96.8750 (96.8750) lr 6.4556e-04 eta 0:04:01\n",
            "epoch [179/200] batch [10/18] time 0.442 (0.531) data 0.000 (0.088) loss 0.1123 (0.1799) acc 100.0000 (95.6250) lr 6.4556e-04 eta 0:03:24\n",
            "epoch [179/200] batch [15/18] time 0.445 (0.501) data 0.000 (0.059) loss 0.2002 (0.1873) acc 96.8750 (95.4167) lr 6.4556e-04 eta 0:03:10\n",
            "epoch [180/200] batch [5/18] time 0.452 (0.606) data 0.000 (0.164) loss 0.1981 (0.2550) acc 93.7500 (93.7500) lr 5.9119e-04 eta 0:03:46\n",
            "epoch [180/200] batch [10/18] time 0.446 (0.525) data 0.001 (0.082) loss 0.2081 (0.2130) acc 93.7500 (95.0000) lr 5.9119e-04 eta 0:03:13\n",
            "epoch [180/200] batch [15/18] time 0.442 (0.497) data 0.000 (0.055) loss 0.1440 (0.1947) acc 96.8750 (95.2083) lr 5.9119e-04 eta 0:03:00\n",
            "epoch [181/200] batch [5/18] time 0.451 (0.632) data 0.000 (0.189) loss 0.1315 (0.1121) acc 96.8750 (98.1250) lr 5.3915e-04 eta 0:03:44\n",
            "epoch [181/200] batch [10/18] time 0.444 (0.537) data 0.000 (0.095) loss 0.1378 (0.1161) acc 96.8750 (97.1875) lr 5.3915e-04 eta 0:03:08\n",
            "epoch [181/200] batch [15/18] time 0.446 (0.506) data 0.000 (0.063) loss 0.1120 (0.1278) acc 96.8750 (97.0833) lr 5.3915e-04 eta 0:02:54\n",
            "epoch [182/200] batch [5/18] time 0.447 (0.637) data 0.000 (0.194) loss 0.1315 (0.2603) acc 96.8750 (92.5000) lr 4.8943e-04 eta 0:03:34\n",
            "epoch [182/200] batch [10/18] time 0.443 (0.540) data 0.000 (0.097) loss 0.2879 (0.2020) acc 93.7500 (94.3750) lr 4.8943e-04 eta 0:02:59\n",
            "epoch [182/200] batch [15/18] time 0.443 (0.507) data 0.000 (0.065) loss 0.2591 (0.1826) acc 90.6250 (94.5833) lr 4.8943e-04 eta 0:02:45\n",
            "epoch [183/200] batch [5/18] time 0.454 (0.622) data 0.000 (0.179) loss 0.0912 (0.2486) acc 100.0000 (95.0000) lr 4.4207e-04 eta 0:03:18\n",
            "epoch [183/200] batch [10/18] time 0.446 (0.532) data 0.000 (0.090) loss 0.1401 (0.2025) acc 96.8750 (95.0000) lr 4.4207e-04 eta 0:02:47\n",
            "epoch [183/200] batch [15/18] time 0.444 (0.503) data 0.000 (0.060) loss 0.1480 (0.2012) acc 93.7500 (94.5833) lr 4.4207e-04 eta 0:02:35\n",
            "epoch [184/200] batch [5/18] time 0.452 (0.616) data 0.000 (0.174) loss 0.1331 (0.2033) acc 96.8750 (95.6250) lr 3.9706e-04 eta 0:03:05\n",
            "epoch [184/200] batch [10/18] time 0.443 (0.530) data 0.000 (0.087) loss 0.2329 (0.1898) acc 93.7500 (95.6250) lr 3.9706e-04 eta 0:02:36\n",
            "epoch [184/200] batch [15/18] time 0.441 (0.501) data 0.000 (0.058) loss 0.2242 (0.1929) acc 90.6250 (94.7917) lr 3.9706e-04 eta 0:02:25\n",
            "epoch [185/200] batch [5/18] time 0.449 (0.617) data 0.000 (0.174) loss 0.1962 (0.1729) acc 93.7500 (95.6250) lr 3.5443e-04 eta 0:02:54\n",
            "epoch [185/200] batch [10/18] time 0.443 (0.529) data 0.000 (0.087) loss 0.1993 (0.1610) acc 93.7500 (95.9375) lr 3.5443e-04 eta 0:02:27\n",
            "epoch [185/200] batch [15/18] time 0.444 (0.501) data 0.000 (0.058) loss 0.0663 (0.1755) acc 100.0000 (96.0417) lr 3.5443e-04 eta 0:02:16\n",
            "epoch [186/200] batch [5/18] time 0.448 (0.626) data 0.000 (0.184) loss 0.1116 (0.1350) acc 100.0000 (96.8750) lr 3.1417e-04 eta 0:02:45\n",
            "epoch [186/200] batch [10/18] time 0.441 (0.535) data 0.000 (0.092) loss 0.1907 (0.2177) acc 96.8750 (94.3750) lr 3.1417e-04 eta 0:02:19\n",
            "epoch [186/200] batch [15/18] time 0.440 (0.504) data 0.000 (0.061) loss 0.1254 (0.2015) acc 96.8750 (94.7917) lr 3.1417e-04 eta 0:02:08\n",
            "epoch [187/200] batch [5/18] time 0.451 (0.626) data 0.000 (0.185) loss 0.1400 (0.1084) acc 96.8750 (97.5000) lr 2.7630e-04 eta 0:02:34\n",
            "epoch [187/200] batch [10/18] time 0.444 (0.534) data 0.000 (0.092) loss 0.3943 (0.1447) acc 93.7500 (96.5625) lr 2.7630e-04 eta 0:02:09\n",
            "epoch [187/200] batch [15/18] time 0.443 (0.504) data 0.000 (0.062) loss 0.1409 (0.1488) acc 96.8750 (96.2500) lr 2.7630e-04 eta 0:01:59\n",
            "epoch [188/200] batch [5/18] time 0.448 (0.617) data 0.000 (0.176) loss 0.0201 (0.1495) acc 100.0000 (94.3750) lr 2.4083e-04 eta 0:02:21\n",
            "epoch [188/200] batch [10/18] time 0.446 (0.530) data 0.000 (0.088) loss 0.2065 (0.1728) acc 96.8750 (94.6875) lr 2.4083e-04 eta 0:01:58\n",
            "epoch [188/200] batch [15/18] time 0.442 (0.501) data 0.000 (0.059) loss 0.2664 (0.1594) acc 93.7500 (95.6250) lr 2.4083e-04 eta 0:01:49\n",
            "epoch [189/200] batch [5/18] time 0.452 (0.612) data 0.000 (0.170) loss 0.1208 (0.1841) acc 96.8750 (94.3750) lr 2.0777e-04 eta 0:02:09\n",
            "epoch [189/200] batch [10/18] time 0.443 (0.528) data 0.000 (0.085) loss 0.2261 (0.2021) acc 93.7500 (94.6875) lr 2.0777e-04 eta 0:01:48\n",
            "epoch [189/200] batch [15/18] time 0.438 (0.499) data 0.000 (0.057) loss 0.1520 (0.1930) acc 96.8750 (94.7917) lr 2.0777e-04 eta 0:01:40\n",
            "epoch [190/200] batch [5/18] time 0.452 (0.622) data 0.000 (0.178) loss 0.1165 (0.1197) acc 96.8750 (97.5000) lr 1.7713e-04 eta 0:02:00\n",
            "epoch [190/200] batch [10/18] time 0.442 (0.532) data 0.000 (0.089) loss 0.3238 (0.1863) acc 90.6250 (95.3125) lr 1.7713e-04 eta 0:01:40\n",
            "epoch [190/200] batch [15/18] time 0.442 (0.503) data 0.000 (0.060) loss 0.0318 (0.1548) acc 100.0000 (96.0417) lr 1.7713e-04 eta 0:01:31\n",
            "epoch [191/200] batch [5/18] time 0.452 (0.628) data 0.000 (0.186) loss 0.1263 (0.1812) acc 100.0000 (96.8750) lr 1.4891e-04 eta 0:01:49\n",
            "epoch [191/200] batch [10/18] time 0.445 (0.535) data 0.000 (0.093) loss 0.2017 (0.2097) acc 96.8750 (96.2500) lr 1.4891e-04 eta 0:01:31\n",
            "epoch [191/200] batch [15/18] time 0.444 (0.504) data 0.000 (0.062) loss 0.3128 (0.2146) acc 93.7500 (95.8333) lr 1.4891e-04 eta 0:01:23\n",
            "epoch [192/200] batch [5/18] time 0.447 (0.612) data 0.000 (0.170) loss 0.0739 (0.2409) acc 100.0000 (95.0000) lr 1.2312e-04 eta 0:01:36\n",
            "epoch [192/200] batch [10/18] time 0.439 (0.527) data 0.000 (0.085) loss 0.1904 (0.2303) acc 90.6250 (94.6875) lr 1.2312e-04 eta 0:01:20\n",
            "epoch [192/200] batch [15/18] time 0.442 (0.499) data 0.000 (0.057) loss 0.1314 (0.2070) acc 93.7500 (95.2083) lr 1.2312e-04 eta 0:01:13\n",
            "epoch [193/200] batch [5/18] time 0.447 (0.622) data 0.000 (0.180) loss 0.0733 (0.1962) acc 100.0000 (95.0000) lr 9.9763e-05 eta 0:01:26\n",
            "epoch [193/200] batch [10/18] time 0.442 (0.531) data 0.000 (0.090) loss 0.2989 (0.1969) acc 96.8750 (95.6250) lr 9.9763e-05 eta 0:01:11\n",
            "epoch [193/200] batch [15/18] time 0.443 (0.501) data 0.000 (0.060) loss 0.4195 (0.1924) acc 90.6250 (96.2500) lr 9.9763e-05 eta 0:01:04\n",
            "epoch [194/200] batch [5/18] time 0.451 (0.630) data 0.000 (0.188) loss 0.1398 (0.1690) acc 96.8750 (95.0000) lr 7.8853e-05 eta 0:01:16\n",
            "epoch [194/200] batch [10/18] time 0.438 (0.536) data 0.000 (0.094) loss 0.2147 (0.1617) acc 93.7500 (95.3125) lr 7.8853e-05 eta 0:01:02\n",
            "epoch [194/200] batch [15/18] time 0.442 (0.504) data 0.000 (0.063) loss 0.2043 (0.1560) acc 93.7500 (95.2083) lr 7.8853e-05 eta 0:00:55\n",
            "epoch [195/200] batch [5/18] time 0.454 (0.624) data 0.000 (0.182) loss 0.2143 (0.1702) acc 96.8750 (96.8750) lr 6.0390e-05 eta 0:01:04\n",
            "epoch [195/200] batch [10/18] time 0.445 (0.533) data 0.000 (0.091) loss 0.0327 (0.1354) acc 100.0000 (97.1875) lr 6.0390e-05 eta 0:00:52\n",
            "epoch [195/200] batch [15/18] time 0.442 (0.503) data 0.000 (0.061) loss 0.2247 (0.1586) acc 93.7500 (96.4583) lr 6.0390e-05 eta 0:00:46\n",
            "epoch [196/200] batch [5/18] time 0.451 (0.623) data 0.000 (0.180) loss 0.0475 (0.1193) acc 100.0000 (98.1250) lr 4.4380e-05 eta 0:00:52\n",
            "epoch [196/200] batch [10/18] time 0.445 (0.534) data 0.000 (0.090) loss 0.1314 (0.1112) acc 96.8750 (97.8125) lr 4.4380e-05 eta 0:00:42\n",
            "epoch [196/200] batch [15/18] time 0.445 (0.504) data 0.000 (0.060) loss 0.1989 (0.1411) acc 96.8750 (96.4583) lr 4.4380e-05 eta 0:00:37\n",
            "epoch [197/200] batch [5/18] time 0.452 (0.627) data 0.000 (0.186) loss 0.4632 (0.1822) acc 90.6250 (95.6250) lr 3.0827e-05 eta 0:00:42\n",
            "epoch [197/200] batch [10/18] time 0.443 (0.535) data 0.000 (0.093) loss 0.2457 (0.1537) acc 93.7500 (95.9375) lr 3.0827e-05 eta 0:00:33\n",
            "epoch [197/200] batch [15/18] time 0.446 (0.504) data 0.000 (0.062) loss 0.1667 (0.1704) acc 93.7500 (95.6250) lr 3.0827e-05 eta 0:00:28\n",
            "epoch [198/200] batch [5/18] time 0.444 (0.623) data 0.000 (0.182) loss 0.0679 (0.2120) acc 100.0000 (94.3750) lr 1.9733e-05 eta 0:00:30\n",
            "epoch [198/200] batch [10/18] time 0.438 (0.533) data 0.000 (0.091) loss 0.0917 (0.2015) acc 100.0000 (94.6875) lr 1.9733e-05 eta 0:00:23\n",
            "epoch [198/200] batch [15/18] time 0.441 (0.503) data 0.000 (0.061) loss 0.2226 (0.2054) acc 93.7500 (94.7917) lr 1.9733e-05 eta 0:00:19\n",
            "epoch [199/200] batch [5/18] time 0.447 (0.632) data 0.000 (0.191) loss 0.1439 (0.1319) acc 96.8750 (97.5000) lr 1.1101e-05 eta 0:00:19\n",
            "epoch [199/200] batch [10/18] time 0.446 (0.537) data 0.000 (0.096) loss 0.1456 (0.1426) acc 96.8750 (96.5625) lr 1.1101e-05 eta 0:00:13\n",
            "epoch [199/200] batch [15/18] time 0.439 (0.506) data 0.000 (0.064) loss 0.1080 (0.1381) acc 96.8750 (96.4583) lr 1.1101e-05 eta 0:00:10\n",
            "epoch [200/200] batch [5/18] time 0.452 (0.615) data 0.000 (0.174) loss 0.1777 (0.1687) acc 96.8750 (95.6250) lr 4.9344e-06 eta 0:00:07\n",
            "epoch [200/200] batch [10/18] time 0.443 (0.529) data 0.000 (0.087) loss 0.1596 (0.1628) acc 96.8750 (95.9375) lr 4.9344e-06 eta 0:00:04\n",
            "epoch [200/200] batch [15/18] time 0.444 (0.500) data 0.000 (0.058) loss 0.3162 (0.1760) acc 90.6250 (95.4167) lr 4.9344e-06 eta 0:00:01\n",
            "Checkpoint saved to output/1207_new_init/oxford_pets/DAPT/vit_b16_16shots/seed1/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [05:34<00:00,  9.05s/it]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,237\n",
            "* accuracy: 88.2%\n",
            "* error: 11.8%\n",
            "* macro_f1: 88.0%\n",
            "Elapsed: 0:35:26\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets-16shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_init/oxford_pets/DAPT/vit_b16_16shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OXQwQTInyMf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a78b98e6-157f-4f8f-aec2-9933548531c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 07:26:08.617192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 07:26:08.637803: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 07:26:08.643630: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 07:26:08.657653: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 07:26:09.663008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8']\n",
            "output_dir: output/1207_new_init/oxford_pets/DAPT/vit_b16_8shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/oxford_pets/DAPT/vit_b16_8shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_8-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  296\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "100%|████████████████████████████████████████| 351M/351M [00:02<00:00, 151MiB/s]\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/oxford_pets/DAPT/vit_b16_8shots/seed1/tensorboard)\n",
            "epoch [1/200] batch [5/9] time 0.400 (6.017) data 0.000 (5.197) loss 0.6739 (1.2125) acc 81.2500 (60.6250) lr 1.0000e-05 eta 3:00:00\n",
            "epoch [2/200] batch [5/9] time 0.403 (0.745) data 0.000 (0.343) loss 1.0616 (0.9846) acc 65.6250 (71.2500) lr 2.0000e-02 eta 0:22:10\n",
            "epoch [3/200] batch [5/9] time 0.404 (0.565) data 0.000 (0.160) loss 0.6813 (0.8080) acc 87.5000 (78.1250) lr 1.9999e-02 eta 0:16:44\n",
            "epoch [4/200] batch [5/9] time 0.409 (0.567) data 0.000 (0.159) loss 1.0702 (0.8726) acc 71.8750 (76.2500) lr 1.9995e-02 eta 0:16:41\n",
            "epoch [5/200] batch [5/9] time 0.409 (0.569) data 0.000 (0.159) loss 0.6643 (0.6706) acc 78.1250 (83.1250) lr 1.9989e-02 eta 0:16:40\n",
            "epoch [6/200] batch [5/9] time 0.408 (0.584) data 0.000 (0.174) loss 0.6075 (0.7303) acc 78.1250 (80.0000) lr 1.9980e-02 eta 0:17:02\n",
            "epoch [7/200] batch [5/9] time 0.412 (0.560) data 0.000 (0.149) loss 0.7544 (0.6482) acc 84.3750 (83.7500) lr 1.9969e-02 eta 0:16:14\n",
            "epoch [8/200] batch [5/9] time 0.413 (0.565) data 0.000 (0.151) loss 0.7742 (0.7636) acc 84.3750 (79.3750) lr 1.9956e-02 eta 0:16:18\n",
            "epoch [9/200] batch [5/9] time 0.418 (0.587) data 0.000 (0.173) loss 0.4872 (0.5726) acc 90.6250 (85.6250) lr 1.9940e-02 eta 0:16:50\n",
            "epoch [10/200] batch [5/9] time 0.417 (0.578) data 0.000 (0.161) loss 0.4524 (0.5428) acc 87.5000 (83.1250) lr 1.9921e-02 eta 0:16:30\n",
            "epoch [11/200] batch [5/9] time 0.429 (0.583) data 0.000 (0.162) loss 0.5617 (0.5662) acc 84.3750 (85.0000) lr 1.9900e-02 eta 0:16:33\n",
            "epoch [12/200] batch [5/9] time 0.435 (0.583) data 0.000 (0.159) loss 0.4134 (0.4782) acc 93.7500 (86.8750) lr 1.9877e-02 eta 0:16:29\n",
            "epoch [13/200] batch [5/9] time 0.435 (0.575) data 0.000 (0.150) loss 0.9221 (0.5750) acc 71.8750 (82.5000) lr 1.9851e-02 eta 0:16:10\n",
            "epoch [14/200] batch [5/9] time 0.437 (0.595) data 0.000 (0.167) loss 0.4905 (0.4234) acc 87.5000 (91.8750) lr 1.9823e-02 eta 0:16:38\n",
            "epoch [15/200] batch [5/9] time 0.437 (0.592) data 0.000 (0.163) loss 0.3020 (0.3875) acc 90.6250 (86.2500) lr 1.9792e-02 eta 0:16:27\n",
            "epoch [16/200] batch [5/9] time 0.439 (0.577) data 0.000 (0.148) loss 0.6191 (0.4414) acc 84.3750 (88.7500) lr 1.9759e-02 eta 0:15:58\n",
            "epoch [17/200] batch [5/9] time 0.442 (0.589) data 0.000 (0.157) loss 0.4633 (0.4201) acc 78.1250 (86.8750) lr 1.9724e-02 eta 0:16:12\n",
            "epoch [18/200] batch [5/9] time 0.448 (0.600) data 0.000 (0.164) loss 0.4397 (0.5179) acc 84.3750 (87.5000) lr 1.9686e-02 eta 0:16:24\n",
            "epoch [19/200] batch [5/9] time 0.448 (0.601) data 0.000 (0.162) loss 0.1838 (0.3646) acc 96.8750 (90.6250) lr 1.9646e-02 eta 0:16:22\n",
            "epoch [20/200] batch [5/9] time 0.449 (0.598) data 0.000 (0.157) loss 0.3422 (0.3710) acc 93.7500 (89.3750) lr 1.9603e-02 eta 0:16:11\n",
            "epoch [21/200] batch [5/9] time 0.452 (0.596) data 0.000 (0.153) loss 0.3209 (0.4372) acc 90.6250 (88.7500) lr 1.9558e-02 eta 0:16:02\n",
            "epoch [22/200] batch [5/9] time 0.454 (0.607) data 0.000 (0.159) loss 0.2569 (0.3678) acc 93.7500 (91.2500) lr 1.9511e-02 eta 0:16:15\n",
            "epoch [23/200] batch [5/9] time 0.460 (0.616) data 0.000 (0.166) loss 0.3705 (0.3679) acc 90.6250 (91.2500) lr 1.9461e-02 eta 0:16:23\n",
            "epoch [24/200] batch [5/9] time 0.462 (0.624) data 0.000 (0.173) loss 0.2104 (0.4392) acc 96.8750 (88.1250) lr 1.9409e-02 eta 0:16:30\n",
            "epoch [25/200] batch [5/9] time 0.458 (0.617) data 0.000 (0.167) loss 0.3726 (0.4099) acc 93.7500 (90.0000) lr 1.9354e-02 eta 0:16:13\n",
            "epoch [26/200] batch [5/9] time 0.454 (0.608) data 0.000 (0.164) loss 0.4551 (0.4159) acc 84.3750 (90.0000) lr 1.9298e-02 eta 0:15:54\n",
            "epoch [27/200] batch [5/9] time 0.448 (0.595) data 0.000 (0.155) loss 0.4440 (0.3429) acc 87.5000 (92.5000) lr 1.9239e-02 eta 0:15:29\n",
            "epoch [28/200] batch [5/9] time 0.447 (0.601) data 0.000 (0.163) loss 0.2445 (0.3934) acc 93.7500 (90.6250) lr 1.9178e-02 eta 0:15:33\n",
            "epoch [29/200] batch [5/9] time 0.450 (0.604) data 0.000 (0.167) loss 0.3480 (0.4259) acc 90.6250 (88.1250) lr 1.9114e-02 eta 0:15:32\n",
            "epoch [30/200] batch [5/9] time 0.443 (0.597) data 0.000 (0.162) loss 0.3324 (0.3111) acc 93.7500 (93.7500) lr 1.9048e-02 eta 0:15:15\n",
            "epoch [31/200] batch [5/9] time 0.446 (0.591) data 0.000 (0.155) loss 0.3589 (0.3515) acc 96.8750 (90.0000) lr 1.8980e-02 eta 0:15:01\n",
            "epoch [32/200] batch [5/9] time 0.438 (0.583) data 0.000 (0.152) loss 0.1519 (0.2680) acc 96.8750 (95.0000) lr 1.8910e-02 eta 0:14:44\n",
            "epoch [33/200] batch [5/9] time 0.439 (0.589) data 0.000 (0.155) loss 0.4012 (0.4200) acc 90.6250 (90.0000) lr 1.8838e-02 eta 0:14:47\n",
            "epoch [34/200] batch [5/9] time 0.445 (0.606) data 0.000 (0.169) loss 0.6998 (0.4558) acc 84.3750 (88.7500) lr 1.8763e-02 eta 0:15:07\n",
            "epoch [35/200] batch [5/9] time 0.445 (0.594) data 0.000 (0.159) loss 0.5258 (0.4200) acc 84.3750 (88.1250) lr 1.8686e-02 eta 0:14:45\n",
            "epoch [36/200] batch [5/9] time 0.439 (0.587) data 0.000 (0.152) loss 0.2179 (0.3570) acc 96.8750 (93.1250) lr 1.8607e-02 eta 0:14:28\n",
            "epoch [37/200] batch [5/9] time 0.449 (0.601) data 0.000 (0.163) loss 0.4291 (0.2704) acc 90.6250 (93.1250) lr 1.8526e-02 eta 0:14:43\n",
            "epoch [38/200] batch [5/9] time 0.445 (0.601) data 0.000 (0.163) loss 0.1587 (0.2249) acc 100.0000 (96.2500) lr 1.8443e-02 eta 0:14:38\n",
            "epoch [39/200] batch [5/9] time 0.450 (0.600) data 0.000 (0.160) loss 0.2391 (0.3028) acc 96.8750 (95.0000) lr 1.8358e-02 eta 0:14:31\n",
            "epoch [40/200] batch [5/9] time 0.452 (0.604) data 0.000 (0.163) loss 0.3974 (0.2651) acc 90.6250 (95.0000) lr 1.8271e-02 eta 0:14:31\n",
            "epoch [41/200] batch [5/9] time 0.452 (0.598) data 0.000 (0.157) loss 0.4349 (0.4057) acc 93.7500 (89.3750) lr 1.8181e-02 eta 0:14:18\n",
            "epoch [42/200] batch [5/9] time 0.454 (0.600) data 0.000 (0.157) loss 0.6400 (0.4032) acc 75.0000 (88.7500) lr 1.8090e-02 eta 0:14:15\n",
            "epoch [43/200] batch [5/9] time 0.452 (0.603) data 0.000 (0.162) loss 0.2379 (0.3552) acc 96.8750 (91.8750) lr 1.7997e-02 eta 0:14:14\n",
            "epoch [44/200] batch [5/9] time 0.448 (0.599) data 0.000 (0.159) loss 0.2216 (0.2911) acc 90.6250 (90.0000) lr 1.7902e-02 eta 0:14:04\n",
            "epoch [45/200] batch [5/9] time 0.450 (0.604) data 0.000 (0.166) loss 0.3448 (0.2731) acc 90.6250 (92.5000) lr 1.7804e-02 eta 0:14:04\n",
            "epoch [46/200] batch [5/9] time 0.448 (0.592) data 0.000 (0.152) loss 0.2936 (0.3212) acc 96.8750 (92.5000) lr 1.7705e-02 eta 0:13:42\n",
            "epoch [47/200] batch [5/9] time 0.448 (0.593) data 0.000 (0.156) loss 0.3509 (0.2372) acc 93.7500 (95.0000) lr 1.7604e-02 eta 0:13:38\n",
            "epoch [48/200] batch [5/9] time 0.446 (0.606) data 0.000 (0.167) loss 0.3785 (0.2284) acc 90.6250 (95.6250) lr 1.7501e-02 eta 0:13:51\n",
            "epoch [49/200] batch [5/9] time 0.448 (0.592) data 0.000 (0.156) loss 0.2642 (0.2332) acc 93.7500 (93.7500) lr 1.7396e-02 eta 0:13:26\n",
            "epoch [50/200] batch [5/9] time 0.448 (0.595) data 0.000 (0.157) loss 0.1951 (0.2911) acc 93.7500 (93.1250) lr 1.7290e-02 eta 0:13:25\n",
            "epoch [51/200] batch [5/9] time 0.449 (0.608) data 0.000 (0.171) loss 0.4352 (0.3156) acc 90.6250 (91.2500) lr 1.7181e-02 eta 0:13:38\n",
            "epoch [52/200] batch [5/9] time 0.450 (0.594) data 0.000 (0.156) loss 0.3738 (0.2566) acc 84.3750 (92.5000) lr 1.7071e-02 eta 0:13:14\n",
            "epoch [53/200] batch [5/9] time 0.453 (0.601) data 0.000 (0.162) loss 0.3882 (0.3398) acc 90.6250 (92.5000) lr 1.6959e-02 eta 0:13:17\n",
            "epoch [54/200] batch [5/9] time 0.440 (0.597) data 0.000 (0.160) loss 0.1766 (0.2036) acc 96.8750 (95.0000) lr 1.6845e-02 eta 0:13:06\n",
            "epoch [55/200] batch [5/9] time 0.442 (0.600) data 0.000 (0.165) loss 0.2379 (0.2748) acc 90.6250 (94.3750) lr 1.6730e-02 eta 0:13:05\n",
            "epoch [56/200] batch [5/9] time 0.451 (0.597) data 0.000 (0.159) loss 0.1859 (0.1937) acc 93.7500 (94.3750) lr 1.6613e-02 eta 0:12:55\n",
            "epoch [57/200] batch [5/9] time 0.451 (0.605) data 0.000 (0.166) loss 0.1188 (0.1579) acc 96.8750 (95.6250) lr 1.6494e-02 eta 0:13:01\n",
            "epoch [58/200] batch [5/9] time 0.450 (0.600) data 0.000 (0.163) loss 0.1507 (0.2267) acc 96.8750 (94.3750) lr 1.6374e-02 eta 0:12:49\n",
            "epoch [59/200] batch [5/9] time 0.451 (0.605) data 0.000 (0.166) loss 0.5300 (0.3037) acc 84.3750 (91.8750) lr 1.6252e-02 eta 0:12:50\n",
            "epoch [60/200] batch [5/9] time 0.447 (0.599) data 0.000 (0.161) loss 0.2095 (0.2181) acc 96.8750 (96.2500) lr 1.6129e-02 eta 0:12:37\n",
            "epoch [61/200] batch [5/9] time 0.445 (0.587) data 0.000 (0.149) loss 0.2948 (0.2719) acc 93.7500 (91.2500) lr 1.6004e-02 eta 0:12:16\n",
            "epoch [62/200] batch [5/9] time 0.451 (0.624) data 0.000 (0.183) loss 0.2885 (0.2990) acc 93.7500 (90.6250) lr 1.5878e-02 eta 0:12:56\n",
            "epoch [63/200] batch [5/9] time 0.455 (0.590) data 0.000 (0.151) loss 0.2155 (0.2471) acc 93.7500 (92.5000) lr 1.5750e-02 eta 0:12:09\n",
            "epoch [64/200] batch [5/9] time 0.447 (0.590) data 0.000 (0.151) loss 0.2345 (0.2213) acc 96.8750 (93.1250) lr 1.5621e-02 eta 0:12:04\n",
            "epoch [65/200] batch [5/9] time 0.452 (0.599) data 0.000 (0.160) loss 0.1877 (0.1659) acc 96.8750 (96.8750) lr 1.5490e-02 eta 0:12:10\n",
            "epoch [66/200] batch [5/9] time 0.451 (0.594) data 0.000 (0.154) loss 0.1529 (0.2862) acc 96.8750 (91.8750) lr 1.5358e-02 eta 0:11:58\n",
            "epoch [67/200] batch [5/9] time 0.453 (0.590) data 0.000 (0.150) loss 0.4157 (0.2892) acc 87.5000 (93.1250) lr 1.5225e-02 eta 0:11:48\n",
            "epoch [68/200] batch [5/9] time 0.451 (0.597) data 0.000 (0.158) loss 0.2932 (0.2438) acc 90.6250 (93.1250) lr 1.5090e-02 eta 0:11:52\n",
            "epoch [69/200] batch [5/9] time 0.447 (0.594) data 0.000 (0.155) loss 0.1989 (0.2577) acc 93.7500 (90.6250) lr 1.4955e-02 eta 0:11:43\n",
            "epoch [70/200] batch [5/9] time 0.449 (0.596) data 0.000 (0.158) loss 0.3742 (0.2505) acc 90.6250 (93.7500) lr 1.4818e-02 eta 0:11:39\n",
            "epoch [71/200] batch [5/9] time 0.449 (0.596) data 0.000 (0.157) loss 0.5655 (0.2784) acc 90.6250 (93.1250) lr 1.4679e-02 eta 0:11:33\n",
            "epoch [72/200] batch [5/9] time 0.444 (0.608) data 0.000 (0.169) loss 0.4345 (0.2680) acc 87.5000 (93.1250) lr 1.4540e-02 eta 0:11:42\n",
            "epoch [73/200] batch [5/9] time 0.450 (0.601) data 0.000 (0.162) loss 0.2277 (0.2665) acc 96.8750 (93.7500) lr 1.4399e-02 eta 0:11:29\n",
            "epoch [74/200] batch [5/9] time 0.453 (0.593) data 0.000 (0.154) loss 0.1982 (0.1677) acc 96.8750 (96.8750) lr 1.4258e-02 eta 0:11:15\n",
            "epoch [75/200] batch [5/9] time 0.446 (0.595) data 0.000 (0.158) loss 0.2600 (0.2100) acc 93.7500 (94.3750) lr 1.4115e-02 eta 0:11:11\n",
            "epoch [76/200] batch [5/9] time 0.441 (0.609) data 0.000 (0.171) loss 0.2060 (0.2557) acc 93.7500 (92.5000) lr 1.3971e-02 eta 0:11:22\n",
            "epoch [77/200] batch [5/9] time 0.452 (0.596) data 0.000 (0.156) loss 0.2310 (0.3228) acc 93.7500 (91.8750) lr 1.3827e-02 eta 0:11:02\n",
            "epoch [78/200] batch [5/9] time 0.452 (0.606) data 0.000 (0.166) loss 0.2023 (0.2463) acc 90.6250 (91.2500) lr 1.3681e-02 eta 0:11:07\n",
            "epoch [79/200] batch [5/9] time 0.449 (0.614) data 0.000 (0.175) loss 0.2585 (0.3137) acc 90.6250 (90.0000) lr 1.3535e-02 eta 0:11:10\n",
            "epoch [80/200] batch [5/9] time 0.451 (0.598) data 0.000 (0.158) loss 0.3320 (0.2341) acc 90.6250 (93.1250) lr 1.3387e-02 eta 0:10:48\n",
            "epoch [81/200] batch [5/9] time 0.449 (0.600) data 0.000 (0.161) loss 0.1434 (0.2322) acc 96.8750 (95.0000) lr 1.3239e-02 eta 0:10:44\n",
            "epoch [82/200] batch [5/9] time 0.451 (0.605) data 0.000 (0.167) loss 0.2583 (0.2019) acc 96.8750 (95.6250) lr 1.3090e-02 eta 0:10:45\n",
            "epoch [83/200] batch [5/9] time 0.451 (0.586) data 0.000 (0.148) loss 0.3239 (0.2356) acc 96.8750 (95.0000) lr 1.2940e-02 eta 0:10:19\n",
            "epoch [84/200] batch [5/9] time 0.447 (0.603) data 0.000 (0.165) loss 0.4230 (0.2368) acc 90.6250 (94.3750) lr 1.2790e-02 eta 0:10:31\n",
            "epoch [85/200] batch [5/9] time 0.452 (0.603) data 0.001 (0.164) loss 0.1956 (0.2079) acc 93.7500 (93.1250) lr 1.2639e-02 eta 0:10:26\n",
            "epoch [86/200] batch [5/9] time 0.451 (0.600) data 0.000 (0.160) loss 0.3101 (0.2733) acc 90.6250 (91.8750) lr 1.2487e-02 eta 0:10:17\n",
            "epoch [87/200] batch [5/9] time 0.449 (0.607) data 0.000 (0.167) loss 0.0699 (0.1893) acc 100.0000 (96.2500) lr 1.2334e-02 eta 0:10:19\n",
            "epoch [88/200] batch [5/9] time 0.450 (0.596) data 0.000 (0.158) loss 0.1445 (0.2720) acc 96.8750 (93.7500) lr 1.2181e-02 eta 0:10:03\n",
            "epoch [89/200] batch [5/9] time 0.446 (0.590) data 0.000 (0.152) loss 0.1873 (0.3256) acc 96.8750 (92.5000) lr 1.2028e-02 eta 0:09:51\n",
            "epoch [90/200] batch [5/9] time 0.452 (0.614) data 0.000 (0.175) loss 0.1881 (0.1554) acc 96.8750 (97.5000) lr 1.1874e-02 eta 0:10:09\n",
            "epoch [91/200] batch [5/9] time 0.445 (0.592) data 0.000 (0.153) loss 0.1424 (0.1490) acc 96.8750 (95.6250) lr 1.1719e-02 eta 0:09:43\n",
            "epoch [92/200] batch [5/9] time 0.444 (0.607) data 0.000 (0.168) loss 0.2881 (0.2687) acc 93.7500 (93.1250) lr 1.1564e-02 eta 0:09:52\n",
            "epoch [93/200] batch [5/9] time 0.451 (0.614) data 0.000 (0.174) loss 0.1325 (0.2335) acc 96.8750 (93.1250) lr 1.1409e-02 eta 0:09:53\n",
            "epoch [94/200] batch [5/9] time 0.448 (0.601) data 0.000 (0.163) loss 0.1575 (0.1513) acc 96.8750 (96.8750) lr 1.1253e-02 eta 0:09:35\n",
            "epoch [95/200] batch [5/9] time 0.447 (0.590) data 0.000 (0.152) loss 0.4867 (0.2500) acc 84.3750 (94.3750) lr 1.1097e-02 eta 0:09:19\n",
            "epoch [96/200] batch [5/9] time 0.450 (0.586) data 0.000 (0.149) loss 0.3617 (0.2399) acc 87.5000 (95.0000) lr 1.0941e-02 eta 0:09:11\n",
            "epoch [97/200] batch [5/9] time 0.448 (0.592) data 0.000 (0.152) loss 0.1649 (0.1960) acc 96.8750 (96.2500) lr 1.0785e-02 eta 0:09:11\n",
            "epoch [98/200] batch [5/9] time 0.451 (0.605) data 0.000 (0.166) loss 0.1819 (0.2372) acc 96.8750 (93.7500) lr 1.0628e-02 eta 0:09:17\n",
            "epoch [99/200] batch [5/9] time 0.450 (0.607) data 0.000 (0.170) loss 0.1242 (0.1399) acc 96.8750 (96.8750) lr 1.0471e-02 eta 0:09:14\n",
            "epoch [100/200] batch [5/9] time 0.449 (0.591) data 0.000 (0.154) loss 0.1459 (0.1581) acc 96.8750 (97.5000) lr 1.0314e-02 eta 0:08:54\n",
            "epoch [101/200] batch [5/9] time 0.448 (0.600) data 0.000 (0.161) loss 0.2821 (0.2533) acc 93.7500 (94.3750) lr 1.0157e-02 eta 0:08:57\n",
            "epoch [102/200] batch [5/9] time 0.448 (0.598) data 0.000 (0.159) loss 0.4740 (0.2500) acc 87.5000 (93.7500) lr 1.0000e-02 eta 0:08:49\n",
            "epoch [103/200] batch [5/9] time 0.447 (0.594) data 0.000 (0.155) loss 0.2277 (0.2110) acc 96.8750 (95.0000) lr 9.8429e-03 eta 0:08:40\n",
            "epoch [104/200] batch [5/9] time 0.447 (0.603) data 0.000 (0.163) loss 0.1394 (0.2132) acc 96.8750 (96.2500) lr 9.6859e-03 eta 0:08:43\n",
            "epoch [105/200] batch [5/9] time 0.449 (0.596) data 0.000 (0.158) loss 0.3724 (0.2829) acc 90.6250 (93.1250) lr 9.5289e-03 eta 0:08:31\n",
            "epoch [106/200] batch [5/9] time 0.449 (0.592) data 0.000 (0.152) loss 0.3016 (0.1348) acc 93.7500 (98.7500) lr 9.3721e-03 eta 0:08:22\n",
            "epoch [107/200] batch [5/9] time 0.451 (0.608) data 0.000 (0.169) loss 0.3768 (0.1634) acc 93.7500 (97.5000) lr 9.2154e-03 eta 0:08:30\n",
            "epoch [108/200] batch [5/9] time 0.449 (0.598) data 0.000 (0.158) loss 0.2421 (0.1630) acc 93.7500 (96.2500) lr 9.0589e-03 eta 0:08:17\n",
            "epoch [109/200] batch [5/9] time 0.449 (0.592) data 0.000 (0.152) loss 0.0736 (0.2174) acc 100.0000 (95.0000) lr 8.9027e-03 eta 0:08:07\n",
            "epoch [110/200] batch [5/9] time 0.448 (0.603) data 0.000 (0.164) loss 0.2822 (0.2683) acc 90.6250 (92.5000) lr 8.7467e-03 eta 0:08:10\n",
            "epoch [111/200] batch [5/9] time 0.451 (0.590) data 0.000 (0.150) loss 0.2225 (0.1740) acc 96.8750 (96.2500) lr 8.5910e-03 eta 0:07:54\n",
            "epoch [112/200] batch [5/9] time 0.448 (0.600) data 0.000 (0.162) loss 0.3134 (0.2521) acc 90.6250 (93.7500) lr 8.4357e-03 eta 0:07:57\n",
            "epoch [113/200] batch [5/9] time 0.453 (0.602) data 0.000 (0.164) loss 0.2375 (0.1574) acc 93.7500 (95.6250) lr 8.2807e-03 eta 0:07:54\n",
            "epoch [114/200] batch [5/9] time 0.444 (0.614) data 0.000 (0.177) loss 0.0634 (0.1641) acc 100.0000 (96.8750) lr 8.1262e-03 eta 0:07:58\n",
            "epoch [115/200] batch [5/9] time 0.449 (0.624) data 0.000 (0.186) loss 0.0984 (0.1274) acc 100.0000 (97.5000) lr 7.9721e-03 eta 0:08:00\n",
            "epoch [116/200] batch [5/9] time 0.447 (0.591) data 0.000 (0.153) loss 0.2455 (0.1968) acc 90.6250 (93.7500) lr 7.8186e-03 eta 0:07:29\n",
            "epoch [117/200] batch [5/9] time 0.449 (0.585) data 0.000 (0.146) loss 0.6712 (0.3237) acc 78.1250 (91.2500) lr 7.6655e-03 eta 0:07:19\n",
            "epoch [118/200] batch [5/9] time 0.452 (0.603) data 0.000 (0.163) loss 0.2739 (0.2364) acc 93.7500 (95.0000) lr 7.5131e-03 eta 0:07:27\n",
            "epoch [119/200] batch [5/9] time 0.448 (0.603) data 0.000 (0.164) loss 0.3994 (0.2205) acc 90.6250 (95.6250) lr 7.3613e-03 eta 0:07:21\n",
            "epoch [120/200] batch [5/9] time 0.452 (0.599) data 0.000 (0.160) loss 0.3783 (0.1522) acc 90.6250 (96.8750) lr 7.2101e-03 eta 0:07:13\n",
            "epoch [121/200] batch [5/9] time 0.451 (0.615) data 0.000 (0.176) loss 0.3512 (0.1836) acc 90.6250 (95.0000) lr 7.0596e-03 eta 0:07:19\n",
            "epoch [122/200] batch [5/9] time 0.449 (0.592) data 0.000 (0.154) loss 0.3461 (0.1597) acc 87.5000 (95.6250) lr 6.9098e-03 eta 0:06:57\n",
            "epoch [123/200] batch [5/9] time 0.452 (0.603) data 0.000 (0.164) loss 0.2404 (0.2134) acc 96.8750 (95.0000) lr 6.7608e-03 eta 0:07:00\n",
            "epoch [124/200] batch [5/9] time 0.443 (0.600) data 0.000 (0.163) loss 0.4205 (0.2045) acc 84.3750 (93.7500) lr 6.6126e-03 eta 0:06:52\n",
            "epoch [125/200] batch [5/9] time 0.448 (0.590) data 0.000 (0.150) loss 0.1770 (0.1931) acc 96.8750 (96.2500) lr 6.4653e-03 eta 0:06:40\n",
            "epoch [126/200] batch [5/9] time 0.446 (0.598) data 0.000 (0.160) loss 0.3052 (0.1907) acc 93.7500 (94.3750) lr 6.3188e-03 eta 0:06:40\n",
            "epoch [127/200] batch [5/9] time 0.449 (0.601) data 0.000 (0.163) loss 0.0949 (0.1807) acc 100.0000 (96.2500) lr 6.1732e-03 eta 0:06:36\n",
            "epoch [128/200] batch [5/9] time 0.449 (0.600) data 0.000 (0.161) loss 0.4229 (0.1936) acc 93.7500 (95.6250) lr 6.0285e-03 eta 0:06:31\n",
            "epoch [129/200] batch [5/9] time 0.450 (0.609) data 0.000 (0.171) loss 0.1276 (0.1945) acc 96.8750 (95.6250) lr 5.8849e-03 eta 0:06:31\n",
            "epoch [130/200] batch [5/9] time 0.451 (0.599) data 0.000 (0.160) loss 0.0904 (0.1467) acc 100.0000 (97.5000) lr 5.7422e-03 eta 0:06:19\n",
            "epoch [131/200] batch [5/9] time 0.448 (0.596) data 0.000 (0.157) loss 0.2423 (0.1185) acc 93.7500 (98.1250) lr 5.6006e-03 eta 0:06:12\n",
            "epoch [132/200] batch [5/9] time 0.448 (0.612) data 0.000 (0.175) loss 0.3457 (0.2321) acc 96.8750 (95.0000) lr 5.4601e-03 eta 0:06:17\n",
            "epoch [133/200] batch [5/9] time 0.451 (0.588) data 0.000 (0.150) loss 0.1780 (0.1352) acc 96.8750 (96.2500) lr 5.3207e-03 eta 0:05:56\n",
            "epoch [134/200] batch [5/9] time 0.450 (0.589) data 0.000 (0.150) loss 0.2399 (0.1276) acc 90.6250 (96.8750) lr 5.1825e-03 eta 0:05:52\n",
            "epoch [135/200] batch [5/9] time 0.446 (0.616) data 0.000 (0.177) loss 0.1578 (0.1174) acc 93.7500 (98.1250) lr 5.0454e-03 eta 0:06:02\n",
            "epoch [136/200] batch [5/9] time 0.451 (0.592) data 0.000 (0.153) loss 0.0717 (0.1876) acc 100.0000 (94.3750) lr 4.9096e-03 eta 0:05:43\n",
            "epoch [137/200] batch [5/9] time 0.447 (0.596) data 0.000 (0.157) loss 0.0977 (0.1327) acc 100.0000 (97.5000) lr 4.7750e-03 eta 0:05:40\n",
            "epoch [138/200] batch [5/9] time 0.451 (0.612) data 0.000 (0.172) loss 0.0910 (0.1200) acc 100.0000 (97.5000) lr 4.6417e-03 eta 0:05:43\n",
            "epoch [139/200] batch [5/9] time 0.449 (0.590) data 0.000 (0.151) loss 0.0850 (0.1270) acc 96.8750 (96.8750) lr 4.5098e-03 eta 0:05:26\n",
            "epoch [140/200] batch [5/9] time 0.448 (0.598) data 0.000 (0.161) loss 0.2176 (0.1658) acc 96.8750 (96.8750) lr 4.3792e-03 eta 0:05:25\n",
            "epoch [141/200] batch [5/9] time 0.450 (0.586) data 0.000 (0.148) loss 0.2099 (0.2044) acc 93.7500 (94.3750) lr 4.2499e-03 eta 0:05:13\n",
            "epoch [142/200] batch [5/9] time 0.444 (0.589) data 0.000 (0.153) loss 0.3579 (0.1654) acc 84.3750 (95.0000) lr 4.1221e-03 eta 0:05:09\n",
            "epoch [143/200] batch [5/9] time 0.448 (0.600) data 0.000 (0.161) loss 0.1292 (0.1938) acc 100.0000 (95.0000) lr 3.9958e-03 eta 0:05:10\n",
            "epoch [144/200] batch [5/9] time 0.442 (0.587) data 0.000 (0.151) loss 0.0947 (0.1284) acc 100.0000 (98.1250) lr 3.8709e-03 eta 0:04:58\n",
            "epoch [145/200] batch [5/9] time 0.447 (0.596) data 0.000 (0.157) loss 0.2838 (0.1541) acc 93.7500 (95.6250) lr 3.7476e-03 eta 0:04:57\n",
            "epoch [146/200] batch [5/9] time 0.450 (0.612) data 0.000 (0.174) loss 0.1965 (0.1472) acc 93.7500 (95.6250) lr 3.6258e-03 eta 0:05:00\n",
            "epoch [147/200] batch [5/9] time 0.448 (0.607) data 0.000 (0.170) loss 0.0580 (0.1631) acc 100.0000 (96.2500) lr 3.5055e-03 eta 0:04:51\n",
            "epoch [148/200] batch [5/9] time 0.445 (0.592) data 0.000 (0.155) loss 0.1928 (0.1735) acc 96.8750 (95.6250) lr 3.3869e-03 eta 0:04:39\n",
            "epoch [149/200] batch [5/9] time 0.449 (0.608) data 0.000 (0.170) loss 0.2988 (0.1669) acc 90.6250 (95.6250) lr 3.2699e-03 eta 0:04:41\n",
            "epoch [150/200] batch [5/9] time 0.447 (0.591) data 0.000 (0.154) loss 0.0704 (0.1929) acc 100.0000 (95.6250) lr 3.1545e-03 eta 0:04:28\n",
            "epoch [151/200] batch [5/9] time 0.452 (0.593) data 0.000 (0.154) loss 0.2440 (0.1697) acc 96.8750 (96.2500) lr 3.0409e-03 eta 0:04:23\n",
            "epoch [152/200] batch [5/9] time 0.451 (0.593) data 0.000 (0.155) loss 0.2941 (0.2318) acc 93.7500 (95.0000) lr 2.9289e-03 eta 0:04:18\n",
            "epoch [153/200] batch [5/9] time 0.452 (0.604) data 0.000 (0.165) loss 0.2941 (0.1999) acc 93.7500 (95.0000) lr 2.8187e-03 eta 0:04:18\n",
            "epoch [154/200] batch [5/9] time 0.448 (0.589) data 0.000 (0.150) loss 0.2484 (0.2198) acc 90.6250 (93.1250) lr 2.7103e-03 eta 0:04:06\n",
            "epoch [155/200] batch [5/9] time 0.446 (0.598) data 0.000 (0.160) loss 0.2038 (0.1210) acc 96.8750 (98.1250) lr 2.6037e-03 eta 0:04:04\n",
            "epoch [156/200] batch [5/9] time 0.452 (0.596) data 0.000 (0.156) loss 0.1724 (0.1868) acc 93.7500 (95.0000) lr 2.4989e-03 eta 0:03:58\n",
            "epoch [157/200] batch [5/9] time 0.443 (0.623) data 0.000 (0.185) loss 0.0837 (0.2099) acc 100.0000 (94.3750) lr 2.3959e-03 eta 0:04:03\n",
            "epoch [158/200] batch [5/9] time 0.450 (0.596) data 0.000 (0.156) loss 0.0986 (0.2078) acc 96.8750 (95.6250) lr 2.2949e-03 eta 0:03:47\n",
            "epoch [159/200] batch [5/9] time 0.451 (0.596) data 0.000 (0.157) loss 0.1178 (0.1729) acc 100.0000 (95.6250) lr 2.1957e-03 eta 0:03:42\n",
            "epoch [160/200] batch [5/9] time 0.446 (0.604) data 0.000 (0.166) loss 0.1625 (0.1235) acc 96.8750 (97.5000) lr 2.0984e-03 eta 0:03:39\n",
            "epoch [161/200] batch [5/9] time 0.450 (0.596) data 0.000 (0.157) loss 0.0730 (0.1549) acc 96.8750 (95.6250) lr 2.0032e-03 eta 0:03:31\n",
            "epoch [162/200] batch [5/9] time 0.446 (0.599) data 0.000 (0.159) loss 0.2629 (0.2253) acc 96.8750 (93.7500) lr 1.9098e-03 eta 0:03:27\n",
            "epoch [163/200] batch [5/9] time 0.447 (0.606) data 0.000 (0.168) loss 0.1681 (0.1211) acc 93.7500 (96.8750) lr 1.8185e-03 eta 0:03:24\n",
            "epoch [164/200] batch [5/9] time 0.450 (0.589) data 0.000 (0.150) loss 0.1514 (0.1625) acc 96.8750 (97.5000) lr 1.7292e-03 eta 0:03:13\n",
            "epoch [165/200] batch [5/9] time 0.447 (0.597) data 0.000 (0.160) loss 0.2074 (0.2334) acc 93.7500 (93.1250) lr 1.6419e-03 eta 0:03:10\n",
            "epoch [166/200] batch [5/9] time 0.446 (0.617) data 0.000 (0.182) loss 0.2458 (0.1712) acc 96.8750 (96.2500) lr 1.5567e-03 eta 0:03:11\n",
            "epoch [167/200] batch [5/9] time 0.450 (0.592) data 0.000 (0.153) loss 0.2442 (0.1472) acc 96.8750 (98.7500) lr 1.4736e-03 eta 0:02:58\n",
            "epoch [168/200] batch [5/9] time 0.443 (0.606) data 0.000 (0.168) loss 0.1832 (0.1457) acc 100.0000 (97.5000) lr 1.3926e-03 eta 0:02:56\n",
            "epoch [169/200] batch [5/9] time 0.447 (0.605) data 0.000 (0.168) loss 0.0313 (0.1043) acc 100.0000 (97.5000) lr 1.3137e-03 eta 0:02:51\n",
            "epoch [170/200] batch [5/9] time 0.448 (0.591) data 0.000 (0.154) loss 0.0544 (0.1496) acc 100.0000 (97.5000) lr 1.2369e-03 eta 0:02:42\n",
            "epoch [171/200] batch [5/9] time 0.451 (0.597) data 0.000 (0.159) loss 0.0642 (0.1997) acc 100.0000 (95.6250) lr 1.1623e-03 eta 0:02:38\n",
            "epoch [172/200] batch [5/9] time 0.445 (0.599) data 0.001 (0.162) loss 0.1218 (0.1514) acc 96.8750 (96.8750) lr 1.0899e-03 eta 0:02:33\n",
            "epoch [173/200] batch [5/9] time 0.451 (0.596) data 0.000 (0.159) loss 0.1959 (0.1265) acc 96.8750 (98.1250) lr 1.0197e-03 eta 0:02:27\n",
            "epoch [174/200] batch [5/9] time 0.447 (0.600) data 0.000 (0.163) loss 0.2065 (0.1721) acc 96.8750 (95.6250) lr 9.5173e-04 eta 0:02:22\n",
            "epoch [175/200] batch [5/9] time 0.443 (0.589) data 0.000 (0.151) loss 0.1423 (0.2117) acc 96.8750 (93.7500) lr 8.8597e-04 eta 0:02:14\n",
            "epoch [176/200] batch [5/9] time 0.450 (0.610) data 0.000 (0.171) loss 0.1821 (0.1770) acc 96.8750 (96.8750) lr 8.2245e-04 eta 0:02:14\n",
            "epoch [177/200] batch [5/9] time 0.446 (0.604) data 0.000 (0.165) loss 0.1715 (0.1415) acc 96.8750 (96.8750) lr 7.6120e-04 eta 0:02:07\n",
            "epoch [178/200] batch [5/9] time 0.447 (0.592) data 0.000 (0.155) loss 0.1225 (0.1759) acc 96.8750 (95.6250) lr 7.0224e-04 eta 0:01:59\n",
            "epoch [179/200] batch [5/9] time 0.449 (0.591) data 0.000 (0.152) loss 0.3792 (0.1653) acc 87.5000 (96.2500) lr 6.4556e-04 eta 0:01:53\n",
            "epoch [180/200] batch [5/9] time 0.449 (0.609) data 0.000 (0.172) loss 0.1324 (0.1664) acc 96.8750 (95.6250) lr 5.9119e-04 eta 0:01:52\n",
            "epoch [181/200] batch [5/9] time 0.445 (0.603) data 0.000 (0.166) loss 0.2791 (0.1392) acc 93.7500 (96.8750) lr 5.3915e-04 eta 0:01:45\n",
            "epoch [182/200] batch [5/9] time 0.448 (0.603) data 0.000 (0.164) loss 0.1056 (0.1862) acc 96.8750 (95.6250) lr 4.8943e-04 eta 0:01:40\n",
            "epoch [183/200] batch [5/9] time 0.451 (0.604) data 0.000 (0.166) loss 0.0981 (0.1579) acc 100.0000 (96.2500) lr 4.4207e-04 eta 0:01:34\n",
            "epoch [184/200] batch [5/9] time 0.448 (0.591) data 0.000 (0.152) loss 0.2282 (0.1791) acc 93.7500 (95.0000) lr 3.9706e-04 eta 0:01:27\n",
            "epoch [185/200] batch [5/9] time 0.451 (0.615) data 0.000 (0.177) loss 0.3094 (0.1553) acc 93.7500 (96.2500) lr 3.5443e-04 eta 0:01:25\n",
            "epoch [186/200] batch [5/9] time 0.447 (0.602) data 0.000 (0.164) loss 0.1646 (0.1334) acc 96.8750 (97.5000) lr 3.1417e-04 eta 0:01:18\n",
            "epoch [187/200] batch [5/9] time 0.452 (0.595) data 0.000 (0.156) loss 0.2611 (0.1714) acc 93.7500 (96.8750) lr 2.7630e-04 eta 0:01:11\n",
            "epoch [188/200] batch [5/9] time 0.450 (0.608) data 0.000 (0.170) loss 0.0775 (0.0893) acc 100.0000 (99.3750) lr 2.4083e-04 eta 0:01:08\n",
            "epoch [189/200] batch [5/9] time 0.449 (0.599) data 0.000 (0.161) loss 0.1081 (0.1542) acc 100.0000 (95.6250) lr 2.0777e-04 eta 0:01:01\n",
            "epoch [190/200] batch [5/9] time 0.448 (0.589) data 0.000 (0.150) loss 0.1547 (0.1310) acc 96.8750 (98.1250) lr 1.7713e-04 eta 0:00:55\n",
            "epoch [191/200] batch [5/9] time 0.445 (0.605) data 0.000 (0.168) loss 0.1546 (0.1601) acc 96.8750 (96.2500) lr 1.4891e-04 eta 0:00:51\n",
            "epoch [192/200] batch [5/9] time 0.450 (0.595) data 0.000 (0.157) loss 0.1047 (0.1717) acc 96.8750 (95.6250) lr 1.2312e-04 eta 0:00:45\n",
            "epoch [193/200] batch [5/9] time 0.448 (0.606) data 0.000 (0.168) loss 0.0513 (0.2155) acc 100.0000 (93.7500) lr 9.9763e-05 eta 0:00:40\n",
            "epoch [194/200] batch [5/9] time 0.447 (0.618) data 0.000 (0.181) loss 0.0584 (0.1537) acc 100.0000 (95.6250) lr 7.8853e-05 eta 0:00:35\n",
            "epoch [195/200] batch [5/9] time 0.447 (0.607) data 0.000 (0.169) loss 0.3568 (0.1808) acc 90.6250 (96.2500) lr 6.0390e-05 eta 0:00:29\n",
            "epoch [196/200] batch [5/9] time 0.448 (0.606) data 0.000 (0.168) loss 0.1000 (0.1490) acc 100.0000 (98.1250) lr 4.4380e-05 eta 0:00:24\n",
            "epoch [197/200] batch [5/9] time 0.446 (0.593) data 0.000 (0.157) loss 0.4430 (0.2494) acc 90.6250 (95.0000) lr 3.0827e-05 eta 0:00:18\n",
            "epoch [198/200] batch [5/9] time 0.448 (0.604) data 0.000 (0.167) loss 0.1239 (0.1237) acc 96.8750 (98.1250) lr 1.9733e-05 eta 0:00:13\n",
            "epoch [199/200] batch [5/9] time 0.443 (0.595) data 0.000 (0.158) loss 0.2154 (0.1461) acc 93.7500 (96.8750) lr 1.1101e-05 eta 0:00:07\n",
            "epoch [200/200] batch [5/9] time 0.448 (0.592) data 0.000 (0.153) loss 0.2869 (0.1977) acc 93.7500 (96.2500) lr 4.9344e-06 eta 0:00:02\n",
            "Checkpoint saved to output/1207_new_init/oxford_pets/DAPT/vit_b16_8shots/seed1/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [06:17<00:00, 10.20s/it]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,178\n",
            "* accuracy: 86.6%\n",
            "* error: 13.4%\n",
            "* macro_f1: 86.5%\n",
            "Elapsed: 0:23:05\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets-8shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_init/oxford_pets/DAPT/vit_b16_8shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DFIrtg4_yP3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a71c97-e0e2-44df-86a1-44ed8155ee25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 07:51:53.158269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 07:51:53.177856: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 07:51:53.183650: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 07:51:53.197896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 07:51:54.188876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4']\n",
            "output_dir: output/1207_new_init/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_4-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  148\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1/tensorboard)\n",
            "epoch [1/100] batch [1/4] time 19.827 (19.827) data 18.480 (18.480) loss 1.1612 (1.1612) acc 78.1250 (78.1250) lr 1.0000e-05 eta 2:11:51\n",
            "epoch [1/100] batch [2/4] time 3.076 (11.452) data 2.602 (10.541) loss 0.7975 (0.9793) acc 84.3750 (81.2500) lr 1.0000e-05 eta 1:15:57\n",
            "epoch [1/100] batch [3/4] time 0.437 (7.780) data 0.000 (7.027) loss 1.3370 (1.0986) acc 62.5000 (75.0000) lr 1.0000e-05 eta 0:51:28\n",
            "epoch [1/100] batch [4/4] time 0.443 (5.946) data 0.000 (5.271) loss 0.9386 (1.0586) acc 68.7500 (73.4375) lr 2.0000e-02 eta 0:39:14\n",
            "epoch [2/100] batch [1/4] time 3.333 (3.333) data 2.842 (2.842) loss 1.0948 (1.0948) acc 65.6250 (65.6250) lr 2.0000e-02 eta 0:21:56\n",
            "epoch [2/100] batch [2/4] time 1.250 (2.292) data 0.820 (1.831) loss 0.8850 (0.9899) acc 75.0000 (70.3125) lr 2.0000e-02 eta 0:15:03\n",
            "epoch [2/100] batch [3/4] time 0.444 (1.676) data 0.000 (1.221) loss 0.9296 (0.9698) acc 84.3750 (75.0000) lr 2.0000e-02 eta 0:10:58\n",
            "epoch [2/100] batch [4/4] time 0.450 (1.370) data 0.000 (0.916) loss 0.6046 (0.8785) acc 81.2500 (76.5625) lr 1.9995e-02 eta 0:08:56\n",
            "epoch [3/100] batch [1/4] time 1.860 (1.860) data 1.427 (1.427) loss 1.0317 (1.0317) acc 71.8750 (71.8750) lr 1.9995e-02 eta 0:12:07\n",
            "epoch [3/100] batch [2/4] time 0.450 (1.155) data 0.001 (0.714) loss 0.9130 (0.9723) acc 71.8750 (71.8750) lr 1.9995e-02 eta 0:07:30\n",
            "epoch [3/100] batch [3/4] time 0.455 (0.922) data 0.000 (0.476) loss 1.0484 (0.9977) acc 65.6250 (69.7917) lr 1.9995e-02 eta 0:05:58\n",
            "epoch [3/100] batch [4/4] time 0.440 (0.801) data 0.000 (0.357) loss 0.5892 (0.8956) acc 78.1250 (71.8750) lr 1.9980e-02 eta 0:05:10\n",
            "epoch [4/100] batch [1/4] time 1.026 (1.026) data 0.589 (0.589) loss 0.8077 (0.8077) acc 75.0000 (75.0000) lr 1.9980e-02 eta 0:06:37\n",
            "epoch [4/100] batch [2/4] time 0.456 (0.741) data 0.001 (0.295) loss 0.7675 (0.7876) acc 78.1250 (76.5625) lr 1.9980e-02 eta 0:04:46\n",
            "epoch [4/100] batch [3/4] time 0.450 (0.644) data 0.000 (0.197) loss 0.6716 (0.7489) acc 71.8750 (75.0000) lr 1.9980e-02 eta 0:04:07\n",
            "epoch [4/100] batch [4/4] time 0.446 (0.595) data 0.000 (0.148) loss 0.8668 (0.7784) acc 75.0000 (75.0000) lr 1.9956e-02 eta 0:03:48\n",
            "epoch [5/100] batch [1/4] time 1.029 (1.029) data 0.590 (0.590) loss 0.6053 (0.6053) acc 81.2500 (81.2500) lr 1.9956e-02 eta 0:06:34\n",
            "epoch [5/100] batch [2/4] time 0.455 (0.742) data 0.001 (0.295) loss 1.0695 (0.8374) acc 71.8750 (76.5625) lr 1.9956e-02 eta 0:04:43\n",
            "epoch [5/100] batch [3/4] time 0.459 (0.648) data 0.000 (0.197) loss 0.7178 (0.7975) acc 81.2500 (78.1250) lr 1.9956e-02 eta 0:04:06\n",
            "epoch [5/100] batch [4/4] time 0.450 (0.598) data 0.000 (0.148) loss 0.5211 (0.7284) acc 84.3750 (79.6875) lr 1.9921e-02 eta 0:03:47\n",
            "epoch [6/100] batch [1/4] time 1.044 (1.044) data 0.603 (0.603) loss 0.5818 (0.5818) acc 81.2500 (81.2500) lr 1.9921e-02 eta 0:06:35\n",
            "epoch [6/100] batch [2/4] time 0.459 (0.752) data 0.001 (0.302) loss 1.0680 (0.8249) acc 68.7500 (75.0000) lr 1.9921e-02 eta 0:04:44\n",
            "epoch [6/100] batch [3/4] time 0.455 (0.653) data 0.000 (0.201) loss 0.6416 (0.7638) acc 84.3750 (78.1250) lr 1.9921e-02 eta 0:04:06\n",
            "epoch [6/100] batch [4/4] time 0.456 (0.604) data 0.000 (0.151) loss 0.8984 (0.7975) acc 71.8750 (76.5625) lr 1.9877e-02 eta 0:03:46\n",
            "epoch [7/100] batch [1/4] time 1.009 (1.009) data 0.566 (0.566) loss 0.9223 (0.9223) acc 78.1250 (78.1250) lr 1.9877e-02 eta 0:06:18\n",
            "epoch [7/100] batch [2/4] time 0.454 (0.731) data 0.001 (0.283) loss 0.7415 (0.8319) acc 78.1250 (78.1250) lr 1.9877e-02 eta 0:04:33\n",
            "epoch [7/100] batch [3/4] time 0.465 (0.643) data 0.000 (0.189) loss 0.7873 (0.8170) acc 78.1250 (78.1250) lr 1.9877e-02 eta 0:03:59\n",
            "epoch [7/100] batch [4/4] time 0.453 (0.595) data 0.000 (0.142) loss 0.7921 (0.8108) acc 71.8750 (76.5625) lr 1.9823e-02 eta 0:03:41\n",
            "epoch [8/100] batch [1/4] time 1.041 (1.041) data 0.596 (0.596) loss 0.4732 (0.4732) acc 87.5000 (87.5000) lr 1.9823e-02 eta 0:06:26\n",
            "epoch [8/100] batch [2/4] time 0.466 (0.753) data 0.001 (0.298) loss 0.3496 (0.4114) acc 90.6250 (89.0625) lr 1.9823e-02 eta 0:04:38\n",
            "epoch [8/100] batch [3/4] time 0.458 (0.655) data 0.000 (0.199) loss 1.0810 (0.6346) acc 71.8750 (83.3333) lr 1.9823e-02 eta 0:04:01\n",
            "epoch [8/100] batch [4/4] time 0.451 (0.604) data 0.000 (0.149) loss 0.5804 (0.6211) acc 81.2500 (82.8125) lr 1.9759e-02 eta 0:03:42\n",
            "epoch [9/100] batch [1/4] time 1.016 (1.016) data 0.573 (0.573) loss 0.5560 (0.5560) acc 84.3750 (84.3750) lr 1.9759e-02 eta 0:06:13\n",
            "epoch [9/100] batch [2/4] time 0.463 (0.740) data 0.001 (0.287) loss 0.4516 (0.5038) acc 87.5000 (85.9375) lr 1.9759e-02 eta 0:04:30\n",
            "epoch [9/100] batch [3/4] time 0.456 (0.645) data 0.000 (0.191) loss 0.5504 (0.5193) acc 87.5000 (86.4583) lr 1.9759e-02 eta 0:03:55\n",
            "epoch [9/100] batch [4/4] time 0.450 (0.596) data 0.000 (0.144) loss 0.5215 (0.5199) acc 93.7500 (88.2812) lr 1.9686e-02 eta 0:03:37\n",
            "epoch [10/100] batch [1/4] time 1.020 (1.020) data 0.577 (0.577) loss 0.5864 (0.5864) acc 84.3750 (84.3750) lr 1.9686e-02 eta 0:06:10\n",
            "epoch [10/100] batch [2/4] time 0.458 (0.739) data 0.001 (0.289) loss 0.4972 (0.5418) acc 81.2500 (82.8125) lr 1.9686e-02 eta 0:04:27\n",
            "epoch [10/100] batch [3/4] time 0.456 (0.644) data 0.000 (0.193) loss 0.7947 (0.6261) acc 78.1250 (81.2500) lr 1.9686e-02 eta 0:03:52\n",
            "epoch [10/100] batch [4/4] time 0.450 (0.596) data 0.000 (0.145) loss 0.3774 (0.5639) acc 87.5000 (82.8125) lr 1.9603e-02 eta 0:03:34\n",
            "epoch [11/100] batch [1/4] time 1.024 (1.024) data 0.584 (0.584) loss 0.5480 (0.5480) acc 87.5000 (87.5000) lr 1.9603e-02 eta 0:06:07\n",
            "epoch [11/100] batch [2/4] time 0.458 (0.741) data 0.001 (0.292) loss 0.3770 (0.4625) acc 90.6250 (89.0625) lr 1.9603e-02 eta 0:04:25\n",
            "epoch [11/100] batch [3/4] time 0.455 (0.646) data 0.000 (0.195) loss 0.4731 (0.4660) acc 87.5000 (88.5417) lr 1.9603e-02 eta 0:03:50\n",
            "epoch [11/100] batch [4/4] time 0.451 (0.597) data 0.000 (0.146) loss 0.5678 (0.4915) acc 84.3750 (87.5000) lr 1.9511e-02 eta 0:03:32\n",
            "epoch [12/100] batch [1/4] time 1.010 (1.010) data 0.573 (0.573) loss 0.4857 (0.4857) acc 90.6250 (90.6250) lr 1.9511e-02 eta 0:05:58\n",
            "epoch [12/100] batch [2/4] time 0.456 (0.733) data 0.001 (0.287) loss 0.2935 (0.3896) acc 96.8750 (93.7500) lr 1.9511e-02 eta 0:04:19\n",
            "epoch [12/100] batch [3/4] time 0.449 (0.638) data 0.000 (0.191) loss 0.2907 (0.3567) acc 93.7500 (93.7500) lr 1.9511e-02 eta 0:03:45\n",
            "epoch [12/100] batch [4/4] time 0.445 (0.590) data 0.000 (0.144) loss 0.4984 (0.3921) acc 90.6250 (92.9688) lr 1.9409e-02 eta 0:03:27\n",
            "epoch [13/100] batch [1/4] time 1.030 (1.030) data 0.592 (0.592) loss 0.5737 (0.5737) acc 81.2500 (81.2500) lr 1.9409e-02 eta 0:06:01\n",
            "epoch [13/100] batch [2/4] time 0.455 (0.742) data 0.001 (0.297) loss 0.5527 (0.5632) acc 87.5000 (84.3750) lr 1.9409e-02 eta 0:04:19\n",
            "epoch [13/100] batch [3/4] time 0.454 (0.646) data 0.000 (0.198) loss 0.4129 (0.5131) acc 93.7500 (87.5000) lr 1.9409e-02 eta 0:03:45\n",
            "epoch [13/100] batch [4/4] time 0.446 (0.596) data 0.000 (0.148) loss 0.5305 (0.5175) acc 78.1250 (85.1562) lr 1.9298e-02 eta 0:03:27\n",
            "epoch [14/100] batch [1/4] time 1.076 (1.076) data 0.638 (0.638) loss 0.3569 (0.3569) acc 93.7500 (93.7500) lr 1.9298e-02 eta 0:06:13\n",
            "epoch [14/100] batch [2/4] time 0.440 (0.758) data 0.001 (0.319) loss 0.4020 (0.3794) acc 90.6250 (92.1875) lr 1.9298e-02 eta 0:04:22\n",
            "epoch [14/100] batch [3/4] time 0.449 (0.655) data 0.000 (0.213) loss 0.6891 (0.4826) acc 78.1250 (87.5000) lr 1.9298e-02 eta 0:03:45\n",
            "epoch [14/100] batch [4/4] time 0.436 (0.600) data 0.000 (0.160) loss 0.5860 (0.5085) acc 87.5000 (87.5000) lr 1.9178e-02 eta 0:03:26\n",
            "epoch [15/100] batch [1/4] time 1.010 (1.010) data 0.576 (0.576) loss 0.6367 (0.6367) acc 84.3750 (84.3750) lr 1.9178e-02 eta 0:05:46\n",
            "epoch [15/100] batch [2/4] time 0.439 (0.724) data 0.001 (0.288) loss 0.4555 (0.5461) acc 87.5000 (85.9375) lr 1.9178e-02 eta 0:04:07\n",
            "epoch [15/100] batch [3/4] time 0.452 (0.633) data 0.000 (0.192) loss 0.4905 (0.5276) acc 87.5000 (86.4583) lr 1.9178e-02 eta 0:03:35\n",
            "epoch [15/100] batch [4/4] time 0.441 (0.585) data 0.001 (0.144) loss 0.3816 (0.4911) acc 90.6250 (87.5000) lr 1.9048e-02 eta 0:03:18\n",
            "epoch [16/100] batch [1/4] time 1.006 (1.006) data 0.577 (0.577) loss 0.8795 (0.8795) acc 68.7500 (68.7500) lr 1.9048e-02 eta 0:05:41\n",
            "epoch [16/100] batch [2/4] time 0.442 (0.724) data 0.001 (0.289) loss 0.5943 (0.7369) acc 81.2500 (75.0000) lr 1.9048e-02 eta 0:04:04\n",
            "epoch [16/100] batch [3/4] time 0.448 (0.632) data 0.000 (0.193) loss 0.5766 (0.6835) acc 81.2500 (77.0833) lr 1.9048e-02 eta 0:03:33\n",
            "epoch [16/100] batch [4/4] time 0.440 (0.584) data 0.000 (0.145) loss 0.4062 (0.6142) acc 84.3750 (78.9062) lr 1.8910e-02 eta 0:03:16\n",
            "epoch [17/100] batch [1/4] time 1.033 (1.033) data 0.600 (0.600) loss 0.4791 (0.4791) acc 84.3750 (84.3750) lr 1.8910e-02 eta 0:05:45\n",
            "epoch [17/100] batch [2/4] time 0.437 (0.735) data 0.001 (0.300) loss 0.3699 (0.4245) acc 90.6250 (87.5000) lr 1.8910e-02 eta 0:04:05\n",
            "epoch [17/100] batch [3/4] time 0.450 (0.640) data 0.000 (0.200) loss 0.5915 (0.4802) acc 87.5000 (87.5000) lr 1.8910e-02 eta 0:03:33\n",
            "epoch [17/100] batch [4/4] time 0.438 (0.590) data 0.000 (0.150) loss 0.5666 (0.5018) acc 78.1250 (85.1562) lr 1.8763e-02 eta 0:03:15\n",
            "epoch [18/100] batch [1/4] time 0.974 (0.974) data 0.547 (0.547) loss 0.4525 (0.4525) acc 87.5000 (87.5000) lr 1.8763e-02 eta 0:05:22\n",
            "epoch [18/100] batch [2/4] time 0.438 (0.706) data 0.001 (0.274) loss 0.6353 (0.5439) acc 84.3750 (85.9375) lr 1.8763e-02 eta 0:03:52\n",
            "epoch [18/100] batch [3/4] time 0.448 (0.620) data 0.000 (0.183) loss 0.3950 (0.4943) acc 93.7500 (88.5417) lr 1.8763e-02 eta 0:03:23\n",
            "epoch [18/100] batch [4/4] time 0.435 (0.574) data 0.000 (0.137) loss 0.4164 (0.4748) acc 96.8750 (90.6250) lr 1.8607e-02 eta 0:03:08\n",
            "epoch [19/100] batch [1/4] time 1.045 (1.045) data 0.615 (0.615) loss 0.3865 (0.3865) acc 93.7500 (93.7500) lr 1.8607e-02 eta 0:05:41\n",
            "epoch [19/100] batch [2/4] time 0.436 (0.740) data 0.001 (0.308) loss 0.3232 (0.3549) acc 93.7500 (93.7500) lr 1.8607e-02 eta 0:04:01\n",
            "epoch [19/100] batch [3/4] time 0.446 (0.642) data 0.000 (0.205) loss 0.3058 (0.3385) acc 96.8750 (94.7917) lr 1.8607e-02 eta 0:03:28\n",
            "epoch [19/100] batch [4/4] time 0.431 (0.590) data 0.000 (0.154) loss 0.7281 (0.4359) acc 84.3750 (92.1875) lr 1.8443e-02 eta 0:03:11\n",
            "epoch [20/100] batch [1/4] time 1.021 (1.021) data 0.592 (0.592) loss 0.4917 (0.4917) acc 87.5000 (87.5000) lr 1.8443e-02 eta 0:05:29\n",
            "epoch [20/100] batch [2/4] time 0.431 (0.726) data 0.001 (0.296) loss 0.1266 (0.3091) acc 100.0000 (93.7500) lr 1.8443e-02 eta 0:03:53\n",
            "epoch [20/100] batch [3/4] time 0.443 (0.632) data 0.000 (0.198) loss 0.2904 (0.3029) acc 90.6250 (92.7083) lr 1.8443e-02 eta 0:03:22\n",
            "epoch [20/100] batch [4/4] time 0.434 (0.582) data 0.000 (0.148) loss 0.5271 (0.3590) acc 84.3750 (90.6250) lr 1.8271e-02 eta 0:03:06\n",
            "epoch [21/100] batch [1/4] time 1.010 (1.010) data 0.584 (0.584) loss 0.1946 (0.1946) acc 93.7500 (93.7500) lr 1.8271e-02 eta 0:05:22\n",
            "epoch [21/100] batch [2/4] time 0.433 (0.721) data 0.001 (0.292) loss 0.3849 (0.2898) acc 87.5000 (90.6250) lr 1.8271e-02 eta 0:03:49\n",
            "epoch [21/100] batch [3/4] time 0.445 (0.629) data 0.000 (0.195) loss 0.4096 (0.3297) acc 87.5000 (89.5833) lr 1.8271e-02 eta 0:03:19\n",
            "epoch [21/100] batch [4/4] time 0.432 (0.580) data 0.000 (0.146) loss 0.5562 (0.3863) acc 87.5000 (89.0625) lr 1.8090e-02 eta 0:03:03\n",
            "epoch [22/100] batch [1/4] time 1.009 (1.009) data 0.581 (0.581) loss 0.3140 (0.3140) acc 93.7500 (93.7500) lr 1.8090e-02 eta 0:05:17\n",
            "epoch [22/100] batch [2/4] time 0.431 (0.720) data 0.001 (0.291) loss 0.4085 (0.3613) acc 87.5000 (90.6250) lr 1.8090e-02 eta 0:03:46\n",
            "epoch [22/100] batch [3/4] time 0.442 (0.628) data 0.000 (0.194) loss 0.5943 (0.4389) acc 84.3750 (88.5417) lr 1.8090e-02 eta 0:03:16\n",
            "epoch [22/100] batch [4/4] time 0.429 (0.578) data 0.000 (0.146) loss 0.3140 (0.4077) acc 93.7500 (89.8438) lr 1.7902e-02 eta 0:03:00\n",
            "epoch [23/100] batch [1/4] time 1.036 (1.036) data 0.612 (0.612) loss 0.2617 (0.2617) acc 100.0000 (100.0000) lr 1.7902e-02 eta 0:05:22\n",
            "epoch [23/100] batch [2/4] time 0.431 (0.733) data 0.001 (0.306) loss 0.3418 (0.3017) acc 87.5000 (93.7500) lr 1.7902e-02 eta 0:03:47\n",
            "epoch [23/100] batch [3/4] time 0.448 (0.638) data 0.000 (0.204) loss 0.2358 (0.2798) acc 93.7500 (93.7500) lr 1.7902e-02 eta 0:03:17\n",
            "epoch [23/100] batch [4/4] time 0.430 (0.586) data 0.000 (0.153) loss 0.4123 (0.3129) acc 90.6250 (92.9688) lr 1.7705e-02 eta 0:03:00\n",
            "epoch [24/100] batch [1/4] time 1.038 (1.038) data 0.617 (0.617) loss 0.3690 (0.3690) acc 87.5000 (87.5000) lr 1.7705e-02 eta 0:05:18\n",
            "epoch [24/100] batch [2/4] time 0.427 (0.732) data 0.001 (0.309) loss 0.3785 (0.3738) acc 90.6250 (89.0625) lr 1.7705e-02 eta 0:03:44\n",
            "epoch [24/100] batch [3/4] time 0.449 (0.638) data 0.000 (0.206) loss 0.5676 (0.4384) acc 81.2500 (86.4583) lr 1.7705e-02 eta 0:03:14\n",
            "epoch [24/100] batch [4/4] time 0.434 (0.587) data 0.000 (0.154) loss 0.5339 (0.4623) acc 90.6250 (87.5000) lr 1.7501e-02 eta 0:02:58\n",
            "epoch [25/100] batch [1/4] time 1.037 (1.037) data 0.613 (0.613) loss 0.4262 (0.4262) acc 87.5000 (87.5000) lr 1.7501e-02 eta 0:05:14\n",
            "epoch [25/100] batch [2/4] time 0.430 (0.733) data 0.001 (0.307) loss 0.7643 (0.5952) acc 75.0000 (81.2500) lr 1.7501e-02 eta 0:03:41\n",
            "epoch [25/100] batch [3/4] time 0.440 (0.636) data 0.000 (0.205) loss 0.1580 (0.4495) acc 93.7500 (85.4167) lr 1.7501e-02 eta 0:03:11\n",
            "epoch [25/100] batch [4/4] time 0.428 (0.584) data 0.000 (0.154) loss 0.3176 (0.4165) acc 90.6250 (86.7188) lr 1.7290e-02 eta 0:02:55\n",
            "epoch [26/100] batch [1/4] time 1.019 (1.019) data 0.594 (0.594) loss 0.1096 (0.1096) acc 100.0000 (100.0000) lr 1.7290e-02 eta 0:05:04\n",
            "epoch [26/100] batch [2/4] time 0.430 (0.724) data 0.001 (0.297) loss 0.3986 (0.2541) acc 90.6250 (95.3125) lr 1.7290e-02 eta 0:03:35\n",
            "epoch [26/100] batch [3/4] time 0.446 (0.632) data 0.000 (0.198) loss 0.4581 (0.3221) acc 78.1250 (89.5833) lr 1.7290e-02 eta 0:03:07\n",
            "epoch [26/100] batch [4/4] time 0.430 (0.581) data 0.000 (0.149) loss 0.2466 (0.3032) acc 100.0000 (92.1875) lr 1.7071e-02 eta 0:02:52\n",
            "epoch [27/100] batch [1/4] time 1.003 (1.003) data 0.578 (0.578) loss 0.2502 (0.2502) acc 93.7500 (93.7500) lr 1.7071e-02 eta 0:04:55\n",
            "epoch [27/100] batch [2/4] time 0.429 (0.716) data 0.001 (0.289) loss 0.3271 (0.2886) acc 93.7500 (93.7500) lr 1.7071e-02 eta 0:03:30\n",
            "epoch [27/100] batch [3/4] time 0.447 (0.626) data 0.000 (0.193) loss 0.3258 (0.3010) acc 90.6250 (92.7083) lr 1.7071e-02 eta 0:03:03\n",
            "epoch [27/100] batch [4/4] time 0.430 (0.577) data 0.000 (0.145) loss 0.3291 (0.3080) acc 93.7500 (92.9688) lr 1.6845e-02 eta 0:02:48\n",
            "epoch [28/100] batch [1/4] time 0.974 (0.974) data 0.545 (0.545) loss 0.3740 (0.3740) acc 90.6250 (90.6250) lr 1.6845e-02 eta 0:04:43\n",
            "epoch [28/100] batch [2/4] time 0.428 (0.701) data 0.001 (0.273) loss 0.4599 (0.4170) acc 93.7500 (92.1875) lr 1.6845e-02 eta 0:03:23\n",
            "epoch [28/100] batch [3/4] time 0.445 (0.616) data 0.000 (0.182) loss 0.2797 (0.3712) acc 90.6250 (91.6667) lr 1.6845e-02 eta 0:02:57\n",
            "epoch [28/100] batch [4/4] time 0.432 (0.570) data 0.000 (0.137) loss 0.4962 (0.4025) acc 90.6250 (91.4062) lr 1.6613e-02 eta 0:02:44\n",
            "epoch [29/100] batch [1/4] time 1.013 (1.013) data 0.584 (0.584) loss 0.1208 (0.1208) acc 100.0000 (100.0000) lr 1.6613e-02 eta 0:04:50\n",
            "epoch [29/100] batch [2/4] time 0.430 (0.722) data 0.001 (0.292) loss 0.3255 (0.2231) acc 93.7500 (96.8750) lr 1.6613e-02 eta 0:03:26\n",
            "epoch [29/100] batch [3/4] time 0.445 (0.629) data 0.000 (0.195) loss 0.7429 (0.3964) acc 78.1250 (90.6250) lr 1.6613e-02 eta 0:02:59\n",
            "epoch [29/100] batch [4/4] time 0.428 (0.579) data 0.000 (0.146) loss 0.3750 (0.3911) acc 90.6250 (90.6250) lr 1.6374e-02 eta 0:02:44\n",
            "epoch [30/100] batch [1/4] time 1.047 (1.047) data 0.620 (0.620) loss 0.1943 (0.1943) acc 96.8750 (96.8750) lr 1.6374e-02 eta 0:04:56\n",
            "epoch [30/100] batch [2/4] time 0.428 (0.738) data 0.001 (0.310) loss 0.2172 (0.2058) acc 96.8750 (96.8750) lr 1.6374e-02 eta 0:03:27\n",
            "epoch [30/100] batch [3/4] time 0.440 (0.638) data 0.000 (0.207) loss 0.2027 (0.2048) acc 96.8750 (96.8750) lr 1.6374e-02 eta 0:02:59\n",
            "epoch [30/100] batch [4/4] time 0.429 (0.586) data 0.000 (0.155) loss 0.2493 (0.2159) acc 96.8750 (96.8750) lr 1.6129e-02 eta 0:02:44\n",
            "epoch [31/100] batch [1/4] time 1.009 (1.009) data 0.583 (0.583) loss 0.5900 (0.5900) acc 84.3750 (84.3750) lr 1.6129e-02 eta 0:04:41\n",
            "epoch [31/100] batch [2/4] time 0.432 (0.721) data 0.001 (0.292) loss 0.1894 (0.3897) acc 93.7500 (89.0625) lr 1.6129e-02 eta 0:03:20\n",
            "epoch [31/100] batch [3/4] time 0.449 (0.630) data 0.000 (0.195) loss 0.2933 (0.3576) acc 96.8750 (91.6667) lr 1.6129e-02 eta 0:02:54\n",
            "epoch [31/100] batch [4/4] time 0.432 (0.581) data 0.000 (0.146) loss 0.1778 (0.3126) acc 93.7500 (92.1875) lr 1.5878e-02 eta 0:02:40\n",
            "epoch [32/100] batch [1/4] time 1.016 (1.016) data 0.590 (0.590) loss 0.1535 (0.1535) acc 100.0000 (100.0000) lr 1.5878e-02 eta 0:04:39\n",
            "epoch [32/100] batch [2/4] time 0.432 (0.724) data 0.000 (0.295) loss 0.3643 (0.2589) acc 93.7500 (96.8750) lr 1.5878e-02 eta 0:03:18\n",
            "epoch [32/100] batch [3/4] time 0.446 (0.631) data 0.000 (0.197) loss 0.2422 (0.2533) acc 96.8750 (96.8750) lr 1.5878e-02 eta 0:02:52\n",
            "epoch [32/100] batch [4/4] time 0.431 (0.581) data 0.000 (0.148) loss 0.1949 (0.2387) acc 93.7500 (96.0938) lr 1.5621e-02 eta 0:02:38\n",
            "epoch [33/100] batch [1/4] time 0.999 (0.999) data 0.572 (0.572) loss 0.3784 (0.3784) acc 93.7500 (93.7500) lr 1.5621e-02 eta 0:04:30\n",
            "epoch [33/100] batch [2/4] time 0.431 (0.715) data 0.001 (0.286) loss 0.2714 (0.3249) acc 93.7500 (93.7500) lr 1.5621e-02 eta 0:03:13\n",
            "epoch [33/100] batch [3/4] time 0.445 (0.625) data 0.000 (0.191) loss 0.2982 (0.3160) acc 90.6250 (92.7083) lr 1.5621e-02 eta 0:02:48\n",
            "epoch [33/100] batch [4/4] time 0.434 (0.577) data 0.000 (0.143) loss 0.2781 (0.3065) acc 96.8750 (93.7500) lr 1.5358e-02 eta 0:02:34\n",
            "epoch [34/100] batch [1/4] time 0.985 (0.985) data 0.557 (0.557) loss 0.4485 (0.4485) acc 90.6250 (90.6250) lr 1.5358e-02 eta 0:04:23\n",
            "epoch [34/100] batch [2/4] time 0.436 (0.711) data 0.001 (0.279) loss 0.5275 (0.4880) acc 87.5000 (89.0625) lr 1.5358e-02 eta 0:03:09\n",
            "epoch [34/100] batch [3/4] time 0.445 (0.622) data 0.000 (0.186) loss 0.2938 (0.4232) acc 96.8750 (91.6667) lr 1.5358e-02 eta 0:02:44\n",
            "epoch [34/100] batch [4/4] time 0.434 (0.575) data 0.000 (0.140) loss 0.3198 (0.3974) acc 84.3750 (89.8438) lr 1.5090e-02 eta 0:02:31\n",
            "epoch [35/100] batch [1/4] time 1.029 (1.029) data 0.604 (0.604) loss 0.4137 (0.4137) acc 93.7500 (93.7500) lr 1.5090e-02 eta 0:04:30\n",
            "epoch [35/100] batch [2/4] time 0.439 (0.734) data 0.001 (0.302) loss 0.3133 (0.3635) acc 90.6250 (92.1875) lr 1.5090e-02 eta 0:03:12\n",
            "epoch [35/100] batch [3/4] time 0.447 (0.638) data 0.000 (0.202) loss 0.2759 (0.3343) acc 93.7500 (92.7083) lr 1.5090e-02 eta 0:02:46\n",
            "epoch [35/100] batch [4/4] time 0.436 (0.588) data 0.000 (0.151) loss 0.3823 (0.3463) acc 90.6250 (92.1875) lr 1.4818e-02 eta 0:02:32\n",
            "epoch [36/100] batch [1/4] time 1.023 (1.023) data 0.591 (0.591) loss 0.6301 (0.6301) acc 78.1250 (78.1250) lr 1.4818e-02 eta 0:04:24\n",
            "epoch [36/100] batch [2/4] time 0.435 (0.729) data 0.001 (0.296) loss 0.2056 (0.4179) acc 96.8750 (87.5000) lr 1.4818e-02 eta 0:03:07\n",
            "epoch [36/100] batch [3/4] time 0.445 (0.634) data 0.000 (0.197) loss 0.3142 (0.3833) acc 90.6250 (88.5417) lr 1.4818e-02 eta 0:02:42\n",
            "epoch [36/100] batch [4/4] time 0.435 (0.584) data 0.000 (0.148) loss 0.2251 (0.3438) acc 96.8750 (90.6250) lr 1.4540e-02 eta 0:02:29\n",
            "epoch [37/100] batch [1/4] time 1.019 (1.019) data 0.588 (0.588) loss 0.1893 (0.1893) acc 96.8750 (96.8750) lr 1.4540e-02 eta 0:04:19\n",
            "epoch [37/100] batch [2/4] time 0.438 (0.729) data 0.001 (0.294) loss 0.2297 (0.2095) acc 96.8750 (96.8750) lr 1.4540e-02 eta 0:03:05\n",
            "epoch [37/100] batch [3/4] time 0.447 (0.635) data 0.000 (0.196) loss 0.1798 (0.1996) acc 96.8750 (96.8750) lr 1.4540e-02 eta 0:02:40\n",
            "epoch [37/100] batch [4/4] time 0.438 (0.586) data 0.000 (0.147) loss 0.2684 (0.2168) acc 96.8750 (96.8750) lr 1.4258e-02 eta 0:02:27\n",
            "epoch [38/100] batch [1/4] time 1.017 (1.017) data 0.586 (0.586) loss 0.1952 (0.1952) acc 93.7500 (93.7500) lr 1.4258e-02 eta 0:04:15\n",
            "epoch [38/100] batch [2/4] time 0.436 (0.726) data 0.001 (0.293) loss 0.1134 (0.1543) acc 96.8750 (95.3125) lr 1.4258e-02 eta 0:03:01\n",
            "epoch [38/100] batch [3/4] time 0.451 (0.635) data 0.000 (0.196) loss 0.7305 (0.3464) acc 84.3750 (91.6667) lr 1.4258e-02 eta 0:02:38\n",
            "epoch [38/100] batch [4/4] time 0.435 (0.585) data 0.000 (0.147) loss 0.3191 (0.3395) acc 93.7500 (92.1875) lr 1.3971e-02 eta 0:02:25\n",
            "epoch [39/100] batch [1/4] time 1.011 (1.011) data 0.579 (0.579) loss 0.3225 (0.3225) acc 93.7500 (93.7500) lr 1.3971e-02 eta 0:04:09\n",
            "epoch [39/100] batch [2/4] time 0.439 (0.725) data 0.001 (0.290) loss 0.2575 (0.2900) acc 93.7500 (93.7500) lr 1.3971e-02 eta 0:02:58\n",
            "epoch [39/100] batch [3/4] time 0.445 (0.632) data 0.000 (0.193) loss 0.3747 (0.3182) acc 93.7500 (93.7500) lr 1.3971e-02 eta 0:02:34\n",
            "epoch [39/100] batch [4/4] time 0.436 (0.583) data 0.000 (0.145) loss 0.2694 (0.3060) acc 93.7500 (93.7500) lr 1.3681e-02 eta 0:02:22\n",
            "epoch [40/100] batch [1/4] time 0.991 (0.991) data 0.560 (0.560) loss 0.1966 (0.1966) acc 96.8750 (96.8750) lr 1.3681e-02 eta 0:04:00\n",
            "epoch [40/100] batch [2/4] time 0.437 (0.714) data 0.001 (0.280) loss 0.1335 (0.1650) acc 100.0000 (98.4375) lr 1.3681e-02 eta 0:02:52\n",
            "epoch [40/100] batch [3/4] time 0.446 (0.625) data 0.000 (0.187) loss 0.4185 (0.2495) acc 87.5000 (94.7917) lr 1.3681e-02 eta 0:02:30\n",
            "epoch [40/100] batch [4/4] time 0.442 (0.579) data 0.000 (0.140) loss 0.2511 (0.2499) acc 96.8750 (95.3125) lr 1.3387e-02 eta 0:02:18\n",
            "epoch [41/100] batch [1/4] time 1.071 (1.071) data 0.641 (0.641) loss 0.3301 (0.3301) acc 90.6250 (90.6250) lr 1.3387e-02 eta 0:04:15\n",
            "epoch [41/100] batch [2/4] time 0.436 (0.753) data 0.001 (0.321) loss 0.2422 (0.2861) acc 90.6250 (90.6250) lr 1.3387e-02 eta 0:02:59\n",
            "epoch [41/100] batch [3/4] time 0.449 (0.652) data 0.000 (0.214) loss 0.2086 (0.2603) acc 93.7500 (91.6667) lr 1.3387e-02 eta 0:02:34\n",
            "epoch [41/100] batch [4/4] time 0.435 (0.598) data 0.000 (0.161) loss 0.0953 (0.2190) acc 100.0000 (93.7500) lr 1.3090e-02 eta 0:02:21\n",
            "epoch [42/100] batch [1/4] time 1.035 (1.035) data 0.602 (0.602) loss 0.2795 (0.2795) acc 96.8750 (96.8750) lr 1.3090e-02 eta 0:04:03\n",
            "epoch [42/100] batch [2/4] time 0.439 (0.737) data 0.001 (0.301) loss 0.2861 (0.2828) acc 90.6250 (93.7500) lr 1.3090e-02 eta 0:02:52\n",
            "epoch [42/100] batch [3/4] time 0.459 (0.644) data 0.000 (0.201) loss 0.2888 (0.2848) acc 93.7500 (93.7500) lr 1.3090e-02 eta 0:02:30\n",
            "epoch [42/100] batch [4/4] time 0.437 (0.592) data 0.000 (0.151) loss 0.2692 (0.2809) acc 90.6250 (92.9688) lr 1.2790e-02 eta 0:02:17\n",
            "epoch [43/100] batch [1/4] time 0.999 (0.999) data 0.565 (0.565) loss 0.3312 (0.3312) acc 90.6250 (90.6250) lr 1.2790e-02 eta 0:03:50\n",
            "epoch [43/100] batch [2/4] time 0.436 (0.717) data 0.001 (0.283) loss 0.3839 (0.3576) acc 87.5000 (89.0625) lr 1.2790e-02 eta 0:02:44\n",
            "epoch [43/100] batch [3/4] time 0.447 (0.627) data 0.000 (0.189) loss 0.7697 (0.4949) acc 78.1250 (85.4167) lr 1.2790e-02 eta 0:02:23\n",
            "epoch [43/100] batch [4/4] time 0.442 (0.581) data 0.000 (0.142) loss 0.2037 (0.4221) acc 96.8750 (88.2812) lr 1.2487e-02 eta 0:02:12\n",
            "epoch [44/100] batch [1/4] time 0.993 (0.993) data 0.559 (0.559) loss 0.2002 (0.2002) acc 96.8750 (96.8750) lr 1.2487e-02 eta 0:03:45\n",
            "epoch [44/100] batch [2/4] time 0.440 (0.716) data 0.001 (0.280) loss 0.2180 (0.2091) acc 100.0000 (98.4375) lr 1.2487e-02 eta 0:02:41\n",
            "epoch [44/100] batch [3/4] time 0.450 (0.628) data 0.000 (0.187) loss 0.4879 (0.3020) acc 87.5000 (94.7917) lr 1.2487e-02 eta 0:02:21\n",
            "epoch [44/100] batch [4/4] time 0.437 (0.580) data 0.000 (0.140) loss 0.1543 (0.2651) acc 96.8750 (95.3125) lr 1.2181e-02 eta 0:02:09\n",
            "epoch [45/100] batch [1/4] time 1.037 (1.037) data 0.606 (0.606) loss 0.1607 (0.1607) acc 96.8750 (96.8750) lr 1.2181e-02 eta 0:03:51\n",
            "epoch [45/100] batch [2/4] time 0.440 (0.739) data 0.001 (0.304) loss 0.2562 (0.2084) acc 93.7500 (95.3125) lr 1.2181e-02 eta 0:02:43\n",
            "epoch [45/100] batch [3/4] time 0.450 (0.643) data 0.000 (0.202) loss 0.5127 (0.3099) acc 90.6250 (93.7500) lr 1.2181e-02 eta 0:02:22\n",
            "epoch [45/100] batch [4/4] time 0.438 (0.591) data 0.000 (0.152) loss 0.3183 (0.3120) acc 93.7500 (93.7500) lr 1.1874e-02 eta 0:02:10\n",
            "epoch [46/100] batch [1/4] time 1.044 (1.044) data 0.613 (0.613) loss 0.1908 (0.1908) acc 93.7500 (93.7500) lr 1.1874e-02 eta 0:03:48\n",
            "epoch [46/100] batch [2/4] time 0.441 (0.743) data 0.001 (0.307) loss 0.2397 (0.2153) acc 93.7500 (93.7500) lr 1.1874e-02 eta 0:02:41\n",
            "epoch [46/100] batch [3/4] time 0.456 (0.647) data 0.000 (0.205) loss 0.1895 (0.2067) acc 96.8750 (94.7917) lr 1.1874e-02 eta 0:02:20\n",
            "epoch [46/100] batch [4/4] time 0.438 (0.595) data 0.000 (0.154) loss 0.1558 (0.1940) acc 93.7500 (94.5312) lr 1.1564e-02 eta 0:02:08\n",
            "epoch [47/100] batch [1/4] time 1.022 (1.022) data 0.589 (0.589) loss 0.3729 (0.3729) acc 90.6250 (90.6250) lr 1.1564e-02 eta 0:03:39\n",
            "epoch [47/100] batch [2/4] time 0.439 (0.731) data 0.000 (0.295) loss 0.1887 (0.2808) acc 96.8750 (93.7500) lr 1.1564e-02 eta 0:02:36\n",
            "epoch [47/100] batch [3/4] time 0.444 (0.635) data 0.000 (0.197) loss 0.2845 (0.2820) acc 93.7500 (93.7500) lr 1.1564e-02 eta 0:02:15\n",
            "epoch [47/100] batch [4/4] time 0.440 (0.586) data 0.000 (0.148) loss 0.4568 (0.3257) acc 87.5000 (92.1875) lr 1.1253e-02 eta 0:02:04\n",
            "epoch [48/100] batch [1/4] time 1.020 (1.020) data 0.586 (0.586) loss 0.5835 (0.5835) acc 87.5000 (87.5000) lr 1.1253e-02 eta 0:03:35\n",
            "epoch [48/100] batch [2/4] time 0.439 (0.730) data 0.001 (0.293) loss 0.2626 (0.4231) acc 96.8750 (92.1875) lr 1.1253e-02 eta 0:02:33\n",
            "epoch [48/100] batch [3/4] time 0.446 (0.635) data 0.000 (0.196) loss 0.2942 (0.3801) acc 93.7500 (92.7083) lr 1.1253e-02 eta 0:02:12\n",
            "epoch [48/100] batch [4/4] time 0.442 (0.587) data 0.000 (0.147) loss 0.1803 (0.3302) acc 93.7500 (92.9688) lr 1.0941e-02 eta 0:02:02\n",
            "epoch [49/100] batch [1/4] time 1.016 (1.016) data 0.589 (0.589) loss 0.3120 (0.3120) acc 90.6250 (90.6250) lr 1.0941e-02 eta 0:03:30\n",
            "epoch [49/100] batch [2/4] time 0.437 (0.727) data 0.001 (0.295) loss 0.2188 (0.2654) acc 93.7500 (92.1875) lr 1.0941e-02 eta 0:02:29\n",
            "epoch [49/100] batch [3/4] time 0.448 (0.634) data 0.000 (0.197) loss 0.5102 (0.3470) acc 87.5000 (90.6250) lr 1.0941e-02 eta 0:02:09\n",
            "epoch [49/100] batch [4/4] time 0.439 (0.585) data 0.000 (0.148) loss 0.3722 (0.3533) acc 90.6250 (90.6250) lr 1.0628e-02 eta 0:01:59\n",
            "epoch [50/100] batch [1/4] time 0.992 (0.992) data 0.559 (0.559) loss 0.2349 (0.2349) acc 96.8750 (96.8750) lr 1.0628e-02 eta 0:03:21\n",
            "epoch [50/100] batch [2/4] time 0.439 (0.715) data 0.001 (0.280) loss 0.2411 (0.2380) acc 96.8750 (96.8750) lr 1.0628e-02 eta 0:02:24\n",
            "epoch [50/100] batch [3/4] time 0.446 (0.625) data 0.000 (0.187) loss 0.2955 (0.2572) acc 93.7500 (95.8333) lr 1.0628e-02 eta 0:02:05\n",
            "epoch [50/100] batch [4/4] time 0.435 (0.578) data 0.000 (0.140) loss 0.2468 (0.2546) acc 96.8750 (96.0938) lr 1.0314e-02 eta 0:01:55\n",
            "epoch [51/100] batch [1/4] time 0.986 (0.986) data 0.555 (0.555) loss 0.2733 (0.2733) acc 93.7500 (93.7500) lr 1.0314e-02 eta 0:03:16\n",
            "epoch [51/100] batch [2/4] time 0.436 (0.711) data 0.001 (0.278) loss 0.3645 (0.3189) acc 93.7500 (93.7500) lr 1.0314e-02 eta 0:02:20\n",
            "epoch [51/100] batch [3/4] time 0.458 (0.627) data 0.000 (0.185) loss 0.1858 (0.2745) acc 96.8750 (94.7917) lr 1.0314e-02 eta 0:02:03\n",
            "epoch [51/100] batch [4/4] time 0.437 (0.579) data 0.000 (0.139) loss 0.5320 (0.3389) acc 87.5000 (92.9688) lr 1.0000e-02 eta 0:01:53\n",
            "epoch [52/100] batch [1/4] time 1.044 (1.044) data 0.616 (0.616) loss 0.4658 (0.4658) acc 84.3750 (84.3750) lr 1.0000e-02 eta 0:03:23\n",
            "epoch [52/100] batch [2/4] time 0.438 (0.741) data 0.001 (0.308) loss 0.2358 (0.3508) acc 96.8750 (90.6250) lr 1.0000e-02 eta 0:02:23\n",
            "epoch [52/100] batch [3/4] time 0.448 (0.644) data 0.000 (0.206) loss 0.2941 (0.3319) acc 93.7500 (91.6667) lr 1.0000e-02 eta 0:02:04\n",
            "epoch [52/100] batch [4/4] time 0.433 (0.591) data 0.000 (0.154) loss 0.1368 (0.2831) acc 100.0000 (93.7500) lr 9.6859e-03 eta 0:01:53\n",
            "epoch [53/100] batch [1/4] time 1.016 (1.016) data 0.584 (0.584) loss 0.3634 (0.3634) acc 93.7500 (93.7500) lr 9.6859e-03 eta 0:03:14\n",
            "epoch [53/100] batch [2/4] time 0.436 (0.726) data 0.000 (0.292) loss 0.2107 (0.2871) acc 96.8750 (95.3125) lr 9.6859e-03 eta 0:02:17\n",
            "epoch [53/100] batch [3/4] time 0.444 (0.632) data 0.000 (0.195) loss 0.2736 (0.2826) acc 90.6250 (93.7500) lr 9.6859e-03 eta 0:01:59\n",
            "epoch [53/100] batch [4/4] time 0.437 (0.583) data 0.000 (0.146) loss 0.3390 (0.2967) acc 90.6250 (92.9688) lr 9.3721e-03 eta 0:01:49\n",
            "epoch [54/100] batch [1/4] time 0.992 (0.992) data 0.563 (0.563) loss 0.3393 (0.3393) acc 90.6250 (90.6250) lr 9.3721e-03 eta 0:03:05\n",
            "epoch [54/100] batch [2/4] time 0.439 (0.716) data 0.001 (0.282) loss 0.1956 (0.2674) acc 96.8750 (93.7500) lr 9.3721e-03 eta 0:02:13\n",
            "epoch [54/100] batch [3/4] time 0.447 (0.626) data 0.000 (0.188) loss 0.2459 (0.2603) acc 93.7500 (93.7500) lr 9.3721e-03 eta 0:01:55\n",
            "epoch [54/100] batch [4/4] time 0.436 (0.579) data 0.000 (0.141) loss 0.3539 (0.2837) acc 87.5000 (92.1875) lr 9.0589e-03 eta 0:01:46\n",
            "epoch [55/100] batch [1/4] time 1.024 (1.024) data 0.593 (0.593) loss 0.4096 (0.4096) acc 87.5000 (87.5000) lr 9.0589e-03 eta 0:03:07\n",
            "epoch [55/100] batch [2/4] time 0.435 (0.729) data 0.001 (0.297) loss 0.1007 (0.2552) acc 100.0000 (93.7500) lr 9.0589e-03 eta 0:02:12\n",
            "epoch [55/100] batch [3/4] time 0.444 (0.634) data 0.000 (0.198) loss 0.1631 (0.2245) acc 96.8750 (94.7917) lr 9.0589e-03 eta 0:01:54\n",
            "epoch [55/100] batch [4/4] time 0.434 (0.584) data 0.000 (0.149) loss 0.4181 (0.2729) acc 90.6250 (93.7500) lr 8.7467e-03 eta 0:01:45\n",
            "epoch [56/100] batch [1/4] time 1.014 (1.014) data 0.589 (0.589) loss 0.1876 (0.1876) acc 93.7500 (93.7500) lr 8.7467e-03 eta 0:03:01\n",
            "epoch [56/100] batch [2/4] time 0.435 (0.724) data 0.000 (0.295) loss 0.2982 (0.2429) acc 93.7500 (93.7500) lr 8.7467e-03 eta 0:02:08\n",
            "epoch [56/100] batch [3/4] time 0.446 (0.632) data 0.000 (0.196) loss 0.2349 (0.2402) acc 96.8750 (94.7917) lr 8.7467e-03 eta 0:01:51\n",
            "epoch [56/100] batch [4/4] time 0.432 (0.582) data 0.000 (0.147) loss 0.5152 (0.3090) acc 90.6250 (93.7500) lr 8.4357e-03 eta 0:01:42\n",
            "epoch [57/100] batch [1/4] time 1.060 (1.060) data 0.630 (0.630) loss 0.2607 (0.2607) acc 93.7500 (93.7500) lr 8.4357e-03 eta 0:03:05\n",
            "epoch [57/100] batch [2/4] time 0.438 (0.749) data 0.000 (0.315) loss 0.1821 (0.2214) acc 96.8750 (95.3125) lr 8.4357e-03 eta 0:02:10\n",
            "epoch [57/100] batch [3/4] time 0.447 (0.649) data 0.000 (0.210) loss 0.2717 (0.2382) acc 93.7500 (94.7917) lr 8.4357e-03 eta 0:01:52\n",
            "epoch [57/100] batch [4/4] time 0.432 (0.594) data 0.000 (0.158) loss 0.2235 (0.2345) acc 93.7500 (94.5312) lr 8.1262e-03 eta 0:01:42\n",
            "epoch [58/100] batch [1/4] time 1.034 (1.034) data 0.606 (0.606) loss 0.3100 (0.3100) acc 90.6250 (90.6250) lr 8.1262e-03 eta 0:02:56\n",
            "epoch [58/100] batch [2/4] time 0.436 (0.735) data 0.000 (0.303) loss 0.1082 (0.2091) acc 100.0000 (95.3125) lr 8.1262e-03 eta 0:02:04\n",
            "epoch [58/100] batch [3/4] time 0.446 (0.639) data 0.000 (0.202) loss 0.2294 (0.2158) acc 96.8750 (95.8333) lr 8.1262e-03 eta 0:01:47\n",
            "epoch [58/100] batch [4/4] time 0.436 (0.588) data 0.000 (0.152) loss 0.1768 (0.2061) acc 93.7500 (95.3125) lr 7.8186e-03 eta 0:01:38\n",
            "epoch [59/100] batch [1/4] time 0.986 (0.986) data 0.556 (0.556) loss 0.1718 (0.1718) acc 96.8750 (96.8750) lr 7.8186e-03 eta 0:02:44\n",
            "epoch [59/100] batch [2/4] time 0.436 (0.711) data 0.001 (0.278) loss 0.4248 (0.2983) acc 87.5000 (92.1875) lr 7.8186e-03 eta 0:01:57\n",
            "epoch [59/100] batch [3/4] time 0.445 (0.622) data 0.000 (0.186) loss 0.2898 (0.2955) acc 93.7500 (92.7083) lr 7.8186e-03 eta 0:01:42\n",
            "epoch [59/100] batch [4/4] time 0.434 (0.575) data 0.000 (0.139) loss 0.1319 (0.2546) acc 100.0000 (94.5312) lr 7.5131e-03 eta 0:01:34\n",
            "epoch [60/100] batch [1/4] time 1.018 (1.018) data 0.590 (0.590) loss 0.2012 (0.2012) acc 93.7500 (93.7500) lr 7.5131e-03 eta 0:02:45\n",
            "epoch [60/100] batch [2/4] time 0.433 (0.726) data 0.001 (0.295) loss 0.4889 (0.3451) acc 90.6250 (92.1875) lr 7.5131e-03 eta 0:01:57\n",
            "epoch [60/100] batch [3/4] time 0.447 (0.633) data 0.000 (0.197) loss 0.1856 (0.2919) acc 93.7500 (92.7083) lr 7.5131e-03 eta 0:01:41\n",
            "epoch [60/100] batch [4/4] time 0.436 (0.584) data 0.000 (0.148) loss 0.3349 (0.3027) acc 90.6250 (92.1875) lr 7.2101e-03 eta 0:01:33\n",
            "epoch [61/100] batch [1/4] time 0.994 (0.994) data 0.564 (0.564) loss 0.1406 (0.1406) acc 96.8750 (96.8750) lr 7.2101e-03 eta 0:02:37\n",
            "epoch [61/100] batch [2/4] time 0.435 (0.714) data 0.001 (0.282) loss 0.1855 (0.1631) acc 96.8750 (96.8750) lr 7.2101e-03 eta 0:01:52\n",
            "epoch [61/100] batch [3/4] time 0.447 (0.625) data 0.000 (0.188) loss 0.2723 (0.1995) acc 93.7500 (95.8333) lr 7.2101e-03 eta 0:01:38\n",
            "epoch [61/100] batch [4/4] time 0.434 (0.578) data 0.000 (0.141) loss 0.3294 (0.2320) acc 90.6250 (94.5312) lr 6.9098e-03 eta 0:01:30\n",
            "epoch [62/100] batch [1/4] time 1.015 (1.015) data 0.585 (0.585) loss 0.2605 (0.2605) acc 96.8750 (96.8750) lr 6.9098e-03 eta 0:02:37\n",
            "epoch [62/100] batch [2/4] time 0.437 (0.726) data 0.001 (0.293) loss 0.4617 (0.3611) acc 81.2500 (89.0625) lr 6.9098e-03 eta 0:01:51\n",
            "epoch [62/100] batch [3/4] time 0.443 (0.631) data 0.000 (0.195) loss 0.3274 (0.3499) acc 93.7500 (90.6250) lr 6.9098e-03 eta 0:01:36\n",
            "epoch [62/100] batch [4/4] time 0.432 (0.582) data 0.000 (0.147) loss 0.1645 (0.3035) acc 96.8750 (92.1875) lr 6.6126e-03 eta 0:01:28\n",
            "epoch [63/100] batch [1/4] time 1.048 (1.048) data 0.623 (0.623) loss 0.2529 (0.2529) acc 90.6250 (90.6250) lr 6.6126e-03 eta 0:02:38\n",
            "epoch [63/100] batch [2/4] time 0.431 (0.739) data 0.001 (0.312) loss 0.1945 (0.2237) acc 96.8750 (93.7500) lr 6.6126e-03 eta 0:01:50\n",
            "epoch [63/100] batch [3/4] time 0.445 (0.641) data 0.000 (0.208) loss 0.1479 (0.1985) acc 96.8750 (94.7917) lr 6.6126e-03 eta 0:01:35\n",
            "epoch [63/100] batch [4/4] time 0.434 (0.590) data 0.000 (0.156) loss 0.1696 (0.1912) acc 96.8750 (95.3125) lr 6.3188e-03 eta 0:01:27\n",
            "epoch [64/100] batch [1/4] time 1.028 (1.028) data 0.600 (0.600) loss 0.2253 (0.2253) acc 96.8750 (96.8750) lr 6.3188e-03 eta 0:02:31\n",
            "epoch [64/100] batch [2/4] time 0.433 (0.731) data 0.001 (0.300) loss 0.1043 (0.1648) acc 100.0000 (98.4375) lr 6.3188e-03 eta 0:01:46\n",
            "epoch [64/100] batch [3/4] time 0.447 (0.636) data 0.000 (0.200) loss 0.3793 (0.2363) acc 93.7500 (96.8750) lr 6.3188e-03 eta 0:01:32\n",
            "epoch [64/100] batch [4/4] time 0.433 (0.585) data 0.000 (0.150) loss 0.0914 (0.2001) acc 100.0000 (97.6562) lr 6.0285e-03 eta 0:01:24\n",
            "epoch [65/100] batch [1/4] time 1.002 (1.002) data 0.574 (0.574) loss 0.1997 (0.1997) acc 96.8750 (96.8750) lr 6.0285e-03 eta 0:02:23\n",
            "epoch [65/100] batch [2/4] time 0.437 (0.719) data 0.001 (0.287) loss 0.1670 (0.1833) acc 100.0000 (98.4375) lr 6.0285e-03 eta 0:01:42\n",
            "epoch [65/100] batch [3/4] time 0.440 (0.626) data 0.000 (0.192) loss 0.3228 (0.2298) acc 90.6250 (95.8333) lr 6.0285e-03 eta 0:01:28\n",
            "epoch [65/100] batch [4/4] time 0.434 (0.578) data 0.000 (0.144) loss 0.3968 (0.2716) acc 93.7500 (95.3125) lr 5.7422e-03 eta 0:01:20\n",
            "epoch [66/100] batch [1/4] time 1.027 (1.027) data 0.602 (0.602) loss 0.1010 (0.1010) acc 100.0000 (100.0000) lr 5.7422e-03 eta 0:02:22\n",
            "epoch [66/100] batch [2/4] time 0.434 (0.731) data 0.001 (0.301) loss 0.1537 (0.1274) acc 96.8750 (98.4375) lr 5.7422e-03 eta 0:01:40\n",
            "epoch [66/100] batch [3/4] time 0.448 (0.636) data 0.000 (0.201) loss 0.1181 (0.1243) acc 100.0000 (98.9583) lr 5.7422e-03 eta 0:01:27\n",
            "epoch [66/100] batch [4/4] time 0.432 (0.585) data 0.000 (0.151) loss 0.3902 (0.1908) acc 90.6250 (96.8750) lr 5.4601e-03 eta 0:01:19\n",
            "epoch [67/100] batch [1/4] time 1.032 (1.032) data 0.605 (0.605) loss 0.3496 (0.3496) acc 87.5000 (87.5000) lr 5.4601e-03 eta 0:02:19\n",
            "epoch [67/100] batch [2/4] time 0.432 (0.732) data 0.001 (0.303) loss 0.3015 (0.3256) acc 93.7500 (90.6250) lr 5.4601e-03 eta 0:01:38\n",
            "epoch [67/100] batch [3/4] time 0.450 (0.638) data 0.000 (0.202) loss 0.1632 (0.2714) acc 100.0000 (93.7500) lr 5.4601e-03 eta 0:01:24\n",
            "epoch [67/100] batch [4/4] time 0.433 (0.587) data 0.000 (0.152) loss 0.2384 (0.2632) acc 93.7500 (93.7500) lr 5.1825e-03 eta 0:01:17\n",
            "epoch [68/100] batch [1/4] time 1.053 (1.053) data 0.628 (0.628) loss 0.0740 (0.0740) acc 100.0000 (100.0000) lr 5.1825e-03 eta 0:02:17\n",
            "epoch [68/100] batch [2/4] time 0.435 (0.744) data 0.000 (0.314) loss 0.3068 (0.1904) acc 93.7500 (96.8750) lr 5.1825e-03 eta 0:01:36\n",
            "epoch [68/100] batch [3/4] time 0.445 (0.644) data 0.001 (0.210) loss 0.1527 (0.1778) acc 96.8750 (96.8750) lr 5.1825e-03 eta 0:01:23\n",
            "epoch [68/100] batch [4/4] time 0.433 (0.591) data 0.000 (0.157) loss 0.1693 (0.1757) acc 96.8750 (96.8750) lr 4.9096e-03 eta 0:01:15\n",
            "epoch [69/100] batch [1/4] time 1.044 (1.044) data 0.616 (0.616) loss 0.3539 (0.3539) acc 90.6250 (90.6250) lr 4.9096e-03 eta 0:02:12\n",
            "epoch [69/100] batch [2/4] time 0.435 (0.740) data 0.001 (0.308) loss 0.4418 (0.3979) acc 84.3750 (87.5000) lr 4.9096e-03 eta 0:01:33\n",
            "epoch [69/100] batch [3/4] time 0.439 (0.639) data 0.000 (0.206) loss 0.5397 (0.4452) acc 87.5000 (87.5000) lr 4.9096e-03 eta 0:01:19\n",
            "epoch [69/100] batch [4/4] time 0.433 (0.588) data 0.000 (0.154) loss 0.1627 (0.3746) acc 93.7500 (89.0625) lr 4.6417e-03 eta 0:01:12\n",
            "epoch [70/100] batch [1/4] time 0.998 (0.998) data 0.569 (0.569) loss 0.3402 (0.3402) acc 90.6250 (90.6250) lr 4.6417e-03 eta 0:02:02\n",
            "epoch [70/100] batch [2/4] time 0.436 (0.717) data 0.001 (0.285) loss 0.3399 (0.3401) acc 90.6250 (90.6250) lr 4.6417e-03 eta 0:01:27\n",
            "epoch [70/100] batch [3/4] time 0.446 (0.626) data 0.000 (0.190) loss 0.4593 (0.3798) acc 87.5000 (89.5833) lr 4.6417e-03 eta 0:01:15\n",
            "epoch [70/100] batch [4/4] time 0.432 (0.578) data 0.000 (0.143) loss 0.1480 (0.3219) acc 96.8750 (91.4062) lr 4.3792e-03 eta 0:01:09\n",
            "epoch [71/100] batch [1/4] time 1.038 (1.038) data 0.611 (0.611) loss 0.3731 (0.3731) acc 93.7500 (93.7500) lr 4.3792e-03 eta 0:02:03\n",
            "epoch [71/100] batch [2/4] time 0.435 (0.736) data 0.001 (0.306) loss 0.3230 (0.3480) acc 90.6250 (92.1875) lr 4.3792e-03 eta 0:01:26\n",
            "epoch [71/100] batch [3/4] time 0.447 (0.640) data 0.000 (0.204) loss 0.0951 (0.2637) acc 96.8750 (93.7500) lr 4.3792e-03 eta 0:01:14\n",
            "epoch [71/100] batch [4/4] time 0.436 (0.589) data 0.000 (0.153) loss 0.2856 (0.2692) acc 96.8750 (94.5312) lr 4.1221e-03 eta 0:01:08\n",
            "epoch [72/100] batch [1/4] time 1.012 (1.012) data 0.584 (0.584) loss 0.1820 (0.1820) acc 96.8750 (96.8750) lr 4.1221e-03 eta 0:01:56\n",
            "epoch [72/100] batch [2/4] time 0.436 (0.724) data 0.001 (0.292) loss 0.1055 (0.1438) acc 100.0000 (98.4375) lr 4.1221e-03 eta 0:01:22\n",
            "epoch [72/100] batch [3/4] time 0.444 (0.630) data 0.000 (0.195) loss 0.3422 (0.2099) acc 93.7500 (96.8750) lr 4.1221e-03 eta 0:01:11\n",
            "epoch [72/100] batch [4/4] time 0.438 (0.582) data 0.000 (0.146) loss 0.2953 (0.2313) acc 93.7500 (96.0938) lr 3.8709e-03 eta 0:01:05\n",
            "epoch [73/100] batch [1/4] time 1.038 (1.038) data 0.611 (0.611) loss 0.4096 (0.4096) acc 90.6250 (90.6250) lr 3.8709e-03 eta 0:01:55\n",
            "epoch [73/100] batch [2/4] time 0.435 (0.737) data 0.001 (0.306) loss 0.5011 (0.4553) acc 87.5000 (89.0625) lr 3.8709e-03 eta 0:01:21\n",
            "epoch [73/100] batch [3/4] time 0.445 (0.639) data 0.000 (0.204) loss 0.2473 (0.3860) acc 90.6250 (89.5833) lr 3.8709e-03 eta 0:01:09\n",
            "epoch [73/100] batch [4/4] time 0.433 (0.588) data 0.000 (0.153) loss 0.2977 (0.3639) acc 90.6250 (89.8438) lr 3.6258e-03 eta 0:01:03\n",
            "epoch [74/100] batch [1/4] time 1.030 (1.030) data 0.601 (0.601) loss 0.1050 (0.1050) acc 96.8750 (96.8750) lr 3.6258e-03 eta 0:01:50\n",
            "epoch [74/100] batch [2/4] time 0.437 (0.733) data 0.001 (0.301) loss 0.1352 (0.1201) acc 100.0000 (98.4375) lr 3.6258e-03 eta 0:01:17\n",
            "epoch [74/100] batch [3/4] time 0.442 (0.636) data 0.000 (0.201) loss 0.2948 (0.1783) acc 93.7500 (96.8750) lr 3.6258e-03 eta 0:01:06\n",
            "epoch [74/100] batch [4/4] time 0.432 (0.585) data 0.000 (0.151) loss 0.3384 (0.2183) acc 90.6250 (95.3125) lr 3.3869e-03 eta 0:01:00\n",
            "epoch [75/100] batch [1/4] time 0.987 (0.987) data 0.557 (0.557) loss 0.1708 (0.1708) acc 96.8750 (96.8750) lr 3.3869e-03 eta 0:01:41\n",
            "epoch [75/100] batch [2/4] time 0.437 (0.712) data 0.001 (0.279) loss 0.2718 (0.2213) acc 93.7500 (95.3125) lr 3.3869e-03 eta 0:01:12\n",
            "epoch [75/100] batch [3/4] time 0.445 (0.623) data 0.000 (0.186) loss 0.5149 (0.3192) acc 90.6250 (93.7500) lr 3.3869e-03 eta 0:01:02\n",
            "epoch [75/100] batch [4/4] time 0.434 (0.576) data 0.000 (0.140) loss 0.1032 (0.2652) acc 100.0000 (95.3125) lr 3.1545e-03 eta 0:00:57\n",
            "epoch [76/100] batch [1/4] time 0.987 (0.987) data 0.560 (0.560) loss 0.2599 (0.2599) acc 93.7500 (93.7500) lr 3.1545e-03 eta 0:01:37\n",
            "epoch [76/100] batch [2/4] time 0.434 (0.711) data 0.001 (0.280) loss 0.3259 (0.2929) acc 90.6250 (92.1875) lr 3.1545e-03 eta 0:01:09\n",
            "epoch [76/100] batch [3/4] time 0.452 (0.624) data 0.000 (0.187) loss 0.2666 (0.2841) acc 93.7500 (92.7083) lr 3.1545e-03 eta 0:01:00\n",
            "epoch [76/100] batch [4/4] time 0.436 (0.577) data 0.000 (0.140) loss 0.2712 (0.2809) acc 90.6250 (92.1875) lr 2.9289e-03 eta 0:00:55\n",
            "epoch [77/100] batch [1/4] time 0.979 (0.979) data 0.553 (0.553) loss 0.1097 (0.1097) acc 100.0000 (100.0000) lr 2.9289e-03 eta 0:01:33\n",
            "epoch [77/100] batch [2/4] time 0.435 (0.707) data 0.001 (0.277) loss 0.1996 (0.1546) acc 100.0000 (100.0000) lr 2.9289e-03 eta 0:01:06\n",
            "epoch [77/100] batch [3/4] time 0.449 (0.621) data 0.000 (0.185) loss 0.2114 (0.1736) acc 96.8750 (98.9583) lr 2.9289e-03 eta 0:00:57\n",
            "epoch [77/100] batch [4/4] time 0.433 (0.574) data 0.000 (0.139) loss 0.1423 (0.1657) acc 100.0000 (99.2188) lr 2.7103e-03 eta 0:00:52\n",
            "epoch [78/100] batch [1/4] time 1.022 (1.022) data 0.592 (0.592) loss 0.4111 (0.4111) acc 93.7500 (93.7500) lr 2.7103e-03 eta 0:01:32\n",
            "epoch [78/100] batch [2/4] time 0.435 (0.728) data 0.001 (0.296) loss 0.3190 (0.3651) acc 90.6250 (92.1875) lr 2.7103e-03 eta 0:01:05\n",
            "epoch [78/100] batch [3/4] time 0.450 (0.636) data 0.000 (0.198) loss 0.2339 (0.3214) acc 93.7500 (92.7083) lr 2.7103e-03 eta 0:00:56\n",
            "epoch [78/100] batch [4/4] time 0.435 (0.585) data 0.000 (0.148) loss 0.1274 (0.2729) acc 96.8750 (93.7500) lr 2.4989e-03 eta 0:00:51\n",
            "epoch [79/100] batch [1/4] time 1.022 (1.022) data 0.592 (0.592) loss 0.1371 (0.1371) acc 96.8750 (96.8750) lr 2.4989e-03 eta 0:01:28\n",
            "epoch [79/100] batch [2/4] time 0.431 (0.726) data 0.001 (0.297) loss 0.1562 (0.1466) acc 96.8750 (96.8750) lr 2.4989e-03 eta 0:01:02\n",
            "epoch [79/100] batch [3/4] time 0.450 (0.634) data 0.000 (0.198) loss 0.1691 (0.1541) acc 96.8750 (96.8750) lr 2.4989e-03 eta 0:00:53\n",
            "epoch [79/100] batch [4/4] time 0.437 (0.585) data 0.000 (0.148) loss 0.1014 (0.1409) acc 96.8750 (96.8750) lr 2.2949e-03 eta 0:00:49\n",
            "epoch [80/100] batch [1/4] time 1.023 (1.023) data 0.594 (0.594) loss 0.3077 (0.3077) acc 93.7500 (93.7500) lr 2.2949e-03 eta 0:01:24\n",
            "epoch [80/100] batch [2/4] time 0.441 (0.732) data 0.001 (0.297) loss 0.1216 (0.2147) acc 100.0000 (96.8750) lr 2.2949e-03 eta 0:01:00\n",
            "epoch [80/100] batch [3/4] time 0.446 (0.637) data 0.000 (0.198) loss 0.2103 (0.2132) acc 93.7500 (95.8333) lr 2.2949e-03 eta 0:00:51\n",
            "epoch [80/100] batch [4/4] time 0.436 (0.586) data 0.000 (0.149) loss 0.1477 (0.1968) acc 96.8750 (96.0938) lr 2.0984e-03 eta 0:00:46\n",
            "epoch [81/100] batch [1/4] time 1.026 (1.026) data 0.598 (0.598) loss 0.2687 (0.2687) acc 93.7500 (93.7500) lr 2.0984e-03 eta 0:01:21\n",
            "epoch [81/100] batch [2/4] time 0.439 (0.733) data 0.000 (0.299) loss 0.3797 (0.3242) acc 90.6250 (92.1875) lr 2.0984e-03 eta 0:00:57\n",
            "epoch [81/100] batch [3/4] time 0.448 (0.638) data 0.000 (0.200) loss 0.3421 (0.3302) acc 90.6250 (91.6667) lr 2.0984e-03 eta 0:00:49\n",
            "epoch [81/100] batch [4/4] time 0.435 (0.587) data 0.000 (0.150) loss 0.1872 (0.2944) acc 96.8750 (92.9688) lr 1.9098e-03 eta 0:00:44\n",
            "epoch [82/100] batch [1/4] time 1.046 (1.046) data 0.616 (0.616) loss 0.0934 (0.0934) acc 100.0000 (100.0000) lr 1.9098e-03 eta 0:01:18\n",
            "epoch [82/100] batch [2/4] time 0.434 (0.740) data 0.001 (0.308) loss 0.1377 (0.1155) acc 96.8750 (98.4375) lr 1.9098e-03 eta 0:00:54\n",
            "epoch [82/100] batch [3/4] time 0.445 (0.642) data 0.000 (0.205) loss 0.2933 (0.1748) acc 90.6250 (95.8333) lr 1.9098e-03 eta 0:00:46\n",
            "epoch [82/100] batch [4/4] time 0.440 (0.591) data 0.001 (0.154) loss 0.0701 (0.1486) acc 100.0000 (96.8750) lr 1.7292e-03 eta 0:00:42\n",
            "epoch [83/100] batch [1/4] time 1.008 (1.008) data 0.574 (0.574) loss 0.2447 (0.2447) acc 93.7500 (93.7500) lr 1.7292e-03 eta 0:01:11\n",
            "epoch [83/100] batch [2/4] time 0.431 (0.719) data 0.001 (0.287) loss 0.2037 (0.2242) acc 96.8750 (95.3125) lr 1.7292e-03 eta 0:00:50\n",
            "epoch [83/100] batch [3/4] time 0.446 (0.628) data 0.000 (0.192) loss 0.2171 (0.2218) acc 93.7500 (94.7917) lr 1.7292e-03 eta 0:00:43\n",
            "epoch [83/100] batch [4/4] time 0.441 (0.581) data 0.000 (0.144) loss 0.2453 (0.2277) acc 93.7500 (94.5312) lr 1.5567e-03 eta 0:00:39\n",
            "epoch [84/100] batch [1/4] time 1.038 (1.038) data 0.608 (0.608) loss 0.1171 (0.1171) acc 96.8750 (96.8750) lr 1.5567e-03 eta 0:01:09\n",
            "epoch [84/100] batch [2/4] time 0.442 (0.740) data 0.000 (0.304) loss 0.2380 (0.1775) acc 90.6250 (93.7500) lr 1.5567e-03 eta 0:00:48\n",
            "epoch [84/100] batch [3/4] time 0.446 (0.642) data 0.000 (0.203) loss 0.1512 (0.1687) acc 100.0000 (95.8333) lr 1.5567e-03 eta 0:00:41\n",
            "epoch [84/100] batch [4/4] time 0.434 (0.590) data 0.000 (0.152) loss 0.0540 (0.1401) acc 100.0000 (96.8750) lr 1.3926e-03 eta 0:00:37\n",
            "epoch [85/100] batch [1/4] time 1.033 (1.033) data 0.599 (0.599) loss 0.2179 (0.2179) acc 96.8750 (96.8750) lr 1.3926e-03 eta 0:01:05\n",
            "epoch [85/100] batch [2/4] time 0.438 (0.735) data 0.001 (0.300) loss 0.1569 (0.1874) acc 96.8750 (96.8750) lr 1.3926e-03 eta 0:00:45\n",
            "epoch [85/100] batch [3/4] time 0.447 (0.639) data 0.000 (0.200) loss 0.0816 (0.1521) acc 100.0000 (97.9167) lr 1.3926e-03 eta 0:00:38\n",
            "epoch [85/100] batch [4/4] time 0.438 (0.589) data 0.000 (0.150) loss 0.2991 (0.1889) acc 93.7500 (96.8750) lr 1.2369e-03 eta 0:00:35\n",
            "epoch [86/100] batch [1/4] time 1.001 (1.001) data 0.571 (0.571) loss 0.4497 (0.4497) acc 90.6250 (90.6250) lr 1.2369e-03 eta 0:00:59\n",
            "epoch [86/100] batch [2/4] time 0.435 (0.718) data 0.000 (0.286) loss 0.3606 (0.4052) acc 93.7500 (92.1875) lr 1.2369e-03 eta 0:00:41\n",
            "epoch [86/100] batch [3/4] time 0.451 (0.629) data 0.000 (0.191) loss 0.3492 (0.3865) acc 90.6250 (91.6667) lr 1.2369e-03 eta 0:00:35\n",
            "epoch [86/100] batch [4/4] time 0.435 (0.580) data 0.000 (0.143) loss 0.2268 (0.3466) acc 93.7500 (92.1875) lr 1.0899e-03 eta 0:00:32\n",
            "epoch [87/100] batch [1/4] time 1.004 (1.004) data 0.575 (0.575) loss 0.0788 (0.0788) acc 100.0000 (100.0000) lr 1.0899e-03 eta 0:00:55\n",
            "epoch [87/100] batch [2/4] time 0.435 (0.719) data 0.000 (0.288) loss 0.2008 (0.1398) acc 93.7500 (96.8750) lr 1.0899e-03 eta 0:00:38\n",
            "epoch [87/100] batch [3/4] time 0.449 (0.629) data 0.000 (0.192) loss 0.3167 (0.1988) acc 93.7500 (95.8333) lr 1.0899e-03 eta 0:00:33\n",
            "epoch [87/100] batch [4/4] time 0.437 (0.581) data 0.000 (0.144) loss 0.3142 (0.2276) acc 87.5000 (93.7500) lr 9.5173e-04 eta 0:00:30\n",
            "epoch [88/100] batch [1/4] time 1.018 (1.018) data 0.585 (0.585) loss 0.2265 (0.2265) acc 93.7500 (93.7500) lr 9.5173e-04 eta 0:00:51\n",
            "epoch [88/100] batch [2/4] time 0.442 (0.730) data 0.001 (0.293) loss 0.3240 (0.2752) acc 96.8750 (95.3125) lr 9.5173e-04 eta 0:00:36\n",
            "epoch [88/100] batch [3/4] time 0.449 (0.636) data 0.000 (0.195) loss 0.2964 (0.2823) acc 93.7500 (94.7917) lr 9.5173e-04 eta 0:00:31\n",
            "epoch [88/100] batch [4/4] time 0.438 (0.586) data 0.000 (0.147) loss 0.3787 (0.3064) acc 84.3750 (92.1875) lr 8.2245e-04 eta 0:00:28\n",
            "epoch [89/100] batch [1/4] time 1.035 (1.035) data 0.605 (0.605) loss 0.2402 (0.2402) acc 96.8750 (96.8750) lr 8.2245e-04 eta 0:00:48\n",
            "epoch [89/100] batch [2/4] time 0.436 (0.736) data 0.001 (0.303) loss 0.2632 (0.2517) acc 90.6250 (93.7500) lr 8.2245e-04 eta 0:00:33\n",
            "epoch [89/100] batch [3/4] time 0.444 (0.639) data 0.000 (0.202) loss 0.3628 (0.2887) acc 84.3750 (90.6250) lr 8.2245e-04 eta 0:00:28\n",
            "epoch [89/100] batch [4/4] time 0.434 (0.588) data 0.000 (0.152) loss 0.4663 (0.3331) acc 90.6250 (90.6250) lr 7.0224e-04 eta 0:00:25\n",
            "epoch [90/100] batch [1/4] time 1.034 (1.034) data 0.606 (0.606) loss 0.0804 (0.0804) acc 100.0000 (100.0000) lr 7.0224e-04 eta 0:00:44\n",
            "epoch [90/100] batch [2/4] time 0.437 (0.735) data 0.001 (0.303) loss 0.2659 (0.1731) acc 96.8750 (98.4375) lr 7.0224e-04 eta 0:00:30\n",
            "epoch [90/100] batch [3/4] time 0.446 (0.639) data 0.000 (0.202) loss 0.1420 (0.1627) acc 100.0000 (98.9583) lr 7.0224e-04 eta 0:00:26\n",
            "epoch [90/100] batch [4/4] time 0.436 (0.588) data 0.000 (0.152) loss 0.1372 (0.1564) acc 96.8750 (98.4375) lr 5.9119e-04 eta 0:00:23\n",
            "epoch [91/100] batch [1/4] time 1.006 (1.006) data 0.578 (0.578) loss 0.3931 (0.3931) acc 90.6250 (90.6250) lr 5.9119e-04 eta 0:00:39\n",
            "epoch [91/100] batch [2/4] time 0.437 (0.721) data 0.001 (0.289) loss 0.3689 (0.3810) acc 90.6250 (90.6250) lr 5.9119e-04 eta 0:00:27\n",
            "epoch [91/100] batch [3/4] time 0.444 (0.629) data 0.000 (0.193) loss 0.2367 (0.3329) acc 93.7500 (91.6667) lr 5.9119e-04 eta 0:00:23\n",
            "epoch [91/100] batch [4/4] time 0.431 (0.579) data 0.000 (0.145) loss 0.1033 (0.2755) acc 100.0000 (93.7500) lr 4.8943e-04 eta 0:00:20\n",
            "epoch [92/100] batch [1/4] time 1.014 (1.014) data 0.587 (0.587) loss 0.3081 (0.3081) acc 90.6250 (90.6250) lr 4.8943e-04 eta 0:00:35\n",
            "epoch [92/100] batch [2/4] time 0.436 (0.725) data 0.001 (0.294) loss 0.1567 (0.2324) acc 96.8750 (93.7500) lr 4.8943e-04 eta 0:00:24\n",
            "epoch [92/100] batch [3/4] time 0.446 (0.632) data 0.000 (0.196) loss 0.1964 (0.2204) acc 96.8750 (94.7917) lr 4.8943e-04 eta 0:00:20\n",
            "epoch [92/100] batch [4/4] time 0.433 (0.582) data 0.000 (0.147) loss 0.1268 (0.1970) acc 100.0000 (96.0938) lr 3.9706e-04 eta 0:00:18\n",
            "epoch [93/100] batch [1/4] time 1.035 (1.035) data 0.608 (0.608) loss 0.3252 (0.3252) acc 93.7500 (93.7500) lr 3.9706e-04 eta 0:00:32\n",
            "epoch [93/100] batch [2/4] time 0.435 (0.735) data 0.000 (0.304) loss 0.2781 (0.3016) acc 93.7500 (93.7500) lr 3.9706e-04 eta 0:00:22\n",
            "epoch [93/100] batch [3/4] time 0.443 (0.638) data 0.000 (0.203) loss 0.2922 (0.2985) acc 90.6250 (92.7083) lr 3.9706e-04 eta 0:00:18\n",
            "epoch [93/100] batch [4/4] time 0.433 (0.586) data 0.000 (0.152) loss 0.2309 (0.2816) acc 96.8750 (93.7500) lr 3.1417e-04 eta 0:00:16\n",
            "epoch [94/100] batch [1/4] time 0.989 (0.989) data 0.560 (0.560) loss 0.3423 (0.3423) acc 90.6250 (90.6250) lr 3.1417e-04 eta 0:00:26\n",
            "epoch [94/100] batch [2/4] time 0.434 (0.712) data 0.001 (0.280) loss 0.1526 (0.2474) acc 93.7500 (92.1875) lr 3.1417e-04 eta 0:00:18\n",
            "epoch [94/100] batch [3/4] time 0.446 (0.623) data 0.000 (0.187) loss 0.1806 (0.2251) acc 96.8750 (93.7500) lr 3.1417e-04 eta 0:00:15\n",
            "epoch [94/100] batch [4/4] time 0.431 (0.575) data 0.000 (0.140) loss 0.2314 (0.2267) acc 93.7500 (93.7500) lr 2.4083e-04 eta 0:00:13\n",
            "epoch [95/100] batch [1/4] time 1.037 (1.037) data 0.610 (0.610) loss 0.1361 (0.1361) acc 100.0000 (100.0000) lr 2.4083e-04 eta 0:00:23\n",
            "epoch [95/100] batch [2/4] time 0.436 (0.737) data 0.001 (0.305) loss 0.2221 (0.1791) acc 93.7500 (96.8750) lr 2.4083e-04 eta 0:00:16\n",
            "epoch [95/100] batch [3/4] time 0.445 (0.639) data 0.000 (0.204) loss 0.1883 (0.1822) acc 96.8750 (96.8750) lr 2.4083e-04 eta 0:00:13\n",
            "epoch [95/100] batch [4/4] time 0.434 (0.588) data 0.000 (0.153) loss 0.2534 (0.2000) acc 90.6250 (95.3125) lr 1.7713e-04 eta 0:00:11\n",
            "epoch [96/100] batch [1/4] time 0.982 (0.982) data 0.550 (0.550) loss 0.1362 (0.1362) acc 100.0000 (100.0000) lr 1.7713e-04 eta 0:00:18\n",
            "epoch [96/100] batch [2/4] time 0.434 (0.708) data 0.001 (0.276) loss 0.3066 (0.2214) acc 93.7500 (96.8750) lr 1.7713e-04 eta 0:00:12\n",
            "epoch [96/100] batch [3/4] time 0.446 (0.621) data 0.000 (0.184) loss 0.1276 (0.1901) acc 96.8750 (96.8750) lr 1.7713e-04 eta 0:00:10\n",
            "epoch [96/100] batch [4/4] time 0.431 (0.573) data 0.000 (0.138) loss 0.2746 (0.2113) acc 87.5000 (94.5312) lr 1.2312e-04 eta 0:00:09\n",
            "epoch [97/100] batch [1/4] time 0.999 (0.999) data 0.571 (0.571) loss 0.2966 (0.2966) acc 93.7500 (93.7500) lr 1.2312e-04 eta 0:00:14\n",
            "epoch [97/100] batch [2/4] time 0.433 (0.716) data 0.000 (0.286) loss 0.1889 (0.2428) acc 96.8750 (95.3125) lr 1.2312e-04 eta 0:00:10\n",
            "epoch [97/100] batch [3/4] time 0.445 (0.626) data 0.000 (0.191) loss 0.0802 (0.1886) acc 100.0000 (96.8750) lr 1.2312e-04 eta 0:00:08\n",
            "epoch [97/100] batch [4/4] time 0.432 (0.577) data 0.000 (0.143) loss 0.1612 (0.1817) acc 96.8750 (96.8750) lr 7.8853e-05 eta 0:00:06\n",
            "epoch [98/100] batch [1/4] time 0.991 (0.991) data 0.562 (0.562) loss 0.3813 (0.3813) acc 90.6250 (90.6250) lr 7.8853e-05 eta 0:00:10\n",
            "epoch [98/100] batch [2/4] time 0.435 (0.713) data 0.000 (0.281) loss 0.2572 (0.3192) acc 96.8750 (93.7500) lr 7.8853e-05 eta 0:00:07\n",
            "epoch [98/100] batch [3/4] time 0.443 (0.623) data 0.000 (0.188) loss 0.1519 (0.2635) acc 96.8750 (94.7917) lr 7.8853e-05 eta 0:00:05\n",
            "epoch [98/100] batch [4/4] time 0.433 (0.575) data 0.000 (0.141) loss 0.3261 (0.2791) acc 93.7500 (94.5312) lr 4.4380e-05 eta 0:00:04\n",
            "epoch [99/100] batch [1/4] time 0.993 (0.993) data 0.567 (0.567) loss 0.3095 (0.3095) acc 93.7500 (93.7500) lr 4.4380e-05 eta 0:00:06\n",
            "epoch [99/100] batch [2/4] time 0.432 (0.713) data 0.000 (0.284) loss 0.1741 (0.2418) acc 96.8750 (95.3125) lr 4.4380e-05 eta 0:00:04\n",
            "epoch [99/100] batch [3/4] time 0.450 (0.625) data 0.000 (0.189) loss 0.3305 (0.2714) acc 90.6250 (93.7500) lr 4.4380e-05 eta 0:00:03\n",
            "epoch [99/100] batch [4/4] time 0.429 (0.576) data 0.000 (0.142) loss 0.1938 (0.2520) acc 96.8750 (94.5312) lr 1.9733e-05 eta 0:00:02\n",
            "epoch [100/100] batch [1/4] time 1.003 (1.003) data 0.576 (0.576) loss 0.1154 (0.1154) acc 100.0000 (100.0000) lr 1.9733e-05 eta 0:00:03\n",
            "epoch [100/100] batch [2/4] time 0.430 (0.717) data 0.001 (0.288) loss 0.2189 (0.1672) acc 93.7500 (96.8750) lr 1.9733e-05 eta 0:00:01\n",
            "epoch [100/100] batch [3/4] time 0.445 (0.626) data 0.000 (0.192) loss 0.1129 (0.1491) acc 100.0000 (97.9167) lr 1.9733e-05 eta 0:00:00\n",
            "epoch [100/100] batch [4/4] time 0.432 (0.578) data 0.000 (0.144) loss 0.1914 (0.1597) acc 93.7500 (96.8750) lr 4.9344e-06 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:19<00:00,  1.89it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,268\n",
            "* accuracy: 89.1%\n",
            "* error: 10.9%\n",
            "* macro_f1: 88.9%\n",
            "Elapsed: 0:04:45\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets-4shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_init/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YT8gJIg_yV1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a880cb0-d37c-4f98-c1a7-840c8265d3e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 07:56:56.502150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 07:56:56.521434: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 07:56:56.527213: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 07:56:56.541759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 07:56:57.523432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2']\n",
            "output_dir: output/1207_new_init/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_2-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  74\n",
            "# val      74\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1/tensorboard)\n",
            "epoch [1/100] batch [1/2] time 21.932 (21.932) data 20.587 (20.587) loss 0.8995 (0.8995) acc 68.7500 (68.7500) lr 1.0000e-05 eta 1:12:44\n",
            "epoch [1/100] batch [2/2] time 0.421 (11.176) data 0.000 (10.294) loss 1.5499 (1.2247) acc 59.3750 (64.0625) lr 2.0000e-02 eta 0:36:52\n",
            "epoch [2/100] batch [1/2] time 3.294 (3.294) data 2.820 (2.820) loss 1.3028 (1.3028) acc 68.7500 (68.7500) lr 2.0000e-02 eta 0:10:48\n",
            "epoch [2/100] batch [2/2] time 0.728 (2.011) data 0.307 (1.563) loss 1.4884 (1.3956) acc 43.7500 (56.2500) lr 1.9995e-02 eta 0:06:34\n",
            "epoch [3/100] batch [1/2] time 0.939 (0.939) data 0.520 (0.520) loss 1.0594 (1.0594) acc 78.1250 (78.1250) lr 1.9995e-02 eta 0:03:03\n",
            "epoch [3/100] batch [2/2] time 0.428 (0.684) data 0.000 (0.260) loss 0.6374 (0.8484) acc 84.3750 (81.2500) lr 1.9980e-02 eta 0:02:12\n",
            "epoch [4/100] batch [1/2] time 1.021 (1.021) data 0.598 (0.598) loss 0.8458 (0.8458) acc 75.0000 (75.0000) lr 1.9980e-02 eta 0:03:17\n",
            "epoch [4/100] batch [2/2] time 0.430 (0.726) data 0.001 (0.299) loss 1.0928 (0.9693) acc 62.5000 (68.7500) lr 1.9956e-02 eta 0:02:19\n",
            "epoch [5/100] batch [1/2] time 0.983 (0.983) data 0.561 (0.561) loss 0.9147 (0.9147) acc 65.6250 (65.6250) lr 1.9956e-02 eta 0:03:07\n",
            "epoch [5/100] batch [2/2] time 0.429 (0.706) data 0.001 (0.281) loss 1.0217 (0.9682) acc 65.6250 (65.6250) lr 1.9921e-02 eta 0:02:14\n",
            "epoch [6/100] batch [1/2] time 1.019 (1.019) data 0.594 (0.594) loss 0.9647 (0.9647) acc 65.6250 (65.6250) lr 1.9921e-02 eta 0:03:12\n",
            "epoch [6/100] batch [2/2] time 0.430 (0.724) data 0.000 (0.297) loss 0.5728 (0.7687) acc 84.3750 (75.0000) lr 1.9877e-02 eta 0:02:16\n",
            "epoch [7/100] batch [1/2] time 0.940 (0.940) data 0.516 (0.516) loss 1.0833 (1.0833) acc 62.5000 (62.5000) lr 1.9877e-02 eta 0:02:55\n",
            "epoch [7/100] batch [2/2] time 0.432 (0.686) data 0.001 (0.258) loss 0.9230 (1.0031) acc 78.1250 (70.3125) lr 1.9823e-02 eta 0:02:07\n",
            "epoch [8/100] batch [1/2] time 0.924 (0.924) data 0.500 (0.500) loss 0.8223 (0.8223) acc 75.0000 (75.0000) lr 1.9823e-02 eta 0:02:50\n",
            "epoch [8/100] batch [2/2] time 0.433 (0.679) data 0.001 (0.250) loss 0.4184 (0.6203) acc 93.7500 (84.3750) lr 1.9759e-02 eta 0:02:04\n",
            "epoch [9/100] batch [1/2] time 0.904 (0.904) data 0.480 (0.480) loss 0.7014 (0.7014) acc 84.3750 (84.3750) lr 1.9759e-02 eta 0:02:45\n",
            "epoch [9/100] batch [2/2] time 0.436 (0.670) data 0.001 (0.240) loss 0.5661 (0.6338) acc 87.5000 (85.9375) lr 1.9686e-02 eta 0:02:01\n",
            "epoch [10/100] batch [1/2] time 0.936 (0.936) data 0.510 (0.510) loss 0.5125 (0.5125) acc 87.5000 (87.5000) lr 1.9686e-02 eta 0:02:49\n",
            "epoch [10/100] batch [2/2] time 0.435 (0.686) data 0.001 (0.255) loss 0.8725 (0.6925) acc 68.7500 (78.1250) lr 1.9603e-02 eta 0:02:03\n",
            "epoch [11/100] batch [1/2] time 0.930 (0.930) data 0.537 (0.537) loss 0.5448 (0.5448) acc 87.5000 (87.5000) lr 1.9603e-02 eta 0:02:46\n",
            "epoch [11/100] batch [2/2] time 0.433 (0.681) data 0.000 (0.269) loss 0.7127 (0.6287) acc 81.2500 (84.3750) lr 1.9511e-02 eta 0:02:01\n",
            "epoch [12/100] batch [1/2] time 1.027 (1.027) data 0.585 (0.585) loss 0.5197 (0.5197) acc 90.6250 (90.6250) lr 1.9511e-02 eta 0:03:01\n",
            "epoch [12/100] batch [2/2] time 0.436 (0.731) data 0.001 (0.293) loss 0.7584 (0.6390) acc 84.3750 (87.5000) lr 1.9409e-02 eta 0:02:08\n",
            "epoch [13/100] batch [1/2] time 1.058 (1.058) data 0.628 (0.628) loss 0.6567 (0.6567) acc 84.3750 (84.3750) lr 1.9409e-02 eta 0:03:05\n",
            "epoch [13/100] batch [2/2] time 0.439 (0.749) data 0.001 (0.314) loss 0.6483 (0.6525) acc 84.3750 (84.3750) lr 1.9298e-02 eta 0:02:10\n",
            "epoch [14/100] batch [1/2] time 1.002 (1.002) data 0.569 (0.569) loss 0.3431 (0.3431) acc 93.7500 (93.7500) lr 1.9298e-02 eta 0:02:53\n",
            "epoch [14/100] batch [2/2] time 0.443 (0.722) data 0.001 (0.285) loss 0.7038 (0.5234) acc 75.0000 (84.3750) lr 1.9178e-02 eta 0:02:04\n",
            "epoch [15/100] batch [1/2] time 0.990 (0.990) data 0.558 (0.558) loss 0.6011 (0.6011) acc 87.5000 (87.5000) lr 1.9178e-02 eta 0:02:49\n",
            "epoch [15/100] batch [2/2] time 0.440 (0.715) data 0.000 (0.279) loss 0.5990 (0.6001) acc 90.6250 (89.0625) lr 1.9048e-02 eta 0:02:01\n",
            "epoch [16/100] batch [1/2] time 0.883 (0.883) data 0.453 (0.453) loss 0.7092 (0.7092) acc 78.1250 (78.1250) lr 1.9048e-02 eta 0:02:29\n",
            "epoch [16/100] batch [2/2] time 0.443 (0.663) data 0.000 (0.227) loss 0.4700 (0.5896) acc 93.7500 (85.9375) lr 1.8910e-02 eta 0:01:51\n",
            "epoch [17/100] batch [1/2] time 0.943 (0.943) data 0.511 (0.511) loss 0.3229 (0.3229) acc 96.8750 (96.8750) lr 1.8910e-02 eta 0:02:37\n",
            "epoch [17/100] batch [2/2] time 0.442 (0.692) data 0.001 (0.256) loss 0.3924 (0.3576) acc 90.6250 (93.7500) lr 1.8763e-02 eta 0:01:54\n",
            "epoch [18/100] batch [1/2] time 0.970 (0.970) data 0.536 (0.536) loss 0.4767 (0.4767) acc 81.2500 (81.2500) lr 1.8763e-02 eta 0:02:40\n",
            "epoch [18/100] batch [2/2] time 0.453 (0.712) data 0.001 (0.268) loss 0.4324 (0.4545) acc 87.5000 (84.3750) lr 1.8607e-02 eta 0:01:56\n",
            "epoch [19/100] batch [1/2] time 0.983 (0.983) data 0.546 (0.546) loss 0.4039 (0.4039) acc 90.6250 (90.6250) lr 1.8607e-02 eta 0:02:40\n",
            "epoch [19/100] batch [2/2] time 0.442 (0.712) data 0.001 (0.273) loss 0.3396 (0.3718) acc 93.7500 (92.1875) lr 1.8443e-02 eta 0:01:55\n",
            "epoch [20/100] batch [1/2] time 0.929 (0.929) data 0.496 (0.496) loss 0.4752 (0.4752) acc 93.7500 (93.7500) lr 1.8443e-02 eta 0:02:29\n",
            "epoch [20/100] batch [2/2] time 0.448 (0.689) data 0.001 (0.248) loss 0.3422 (0.4087) acc 90.6250 (92.1875) lr 1.8271e-02 eta 0:01:50\n",
            "epoch [21/100] batch [1/2] time 0.942 (0.942) data 0.507 (0.507) loss 0.3025 (0.3025) acc 93.7500 (93.7500) lr 1.8271e-02 eta 0:02:29\n",
            "epoch [21/100] batch [2/2] time 0.445 (0.693) data 0.001 (0.254) loss 0.7364 (0.5194) acc 81.2500 (87.5000) lr 1.8090e-02 eta 0:01:49\n",
            "epoch [22/100] batch [1/2] time 0.959 (0.959) data 0.522 (0.522) loss 0.4889 (0.4889) acc 87.5000 (87.5000) lr 1.8090e-02 eta 0:02:30\n",
            "epoch [22/100] batch [2/2] time 0.453 (0.706) data 0.001 (0.261) loss 0.6272 (0.5580) acc 84.3750 (85.9375) lr 1.7902e-02 eta 0:01:50\n",
            "epoch [23/100] batch [1/2] time 1.035 (1.035) data 0.598 (0.598) loss 0.2691 (0.2691) acc 96.8750 (96.8750) lr 1.7902e-02 eta 0:02:40\n",
            "epoch [23/100] batch [2/2] time 0.442 (0.738) data 0.001 (0.299) loss 0.3622 (0.3156) acc 93.7500 (95.3125) lr 1.7705e-02 eta 0:01:53\n",
            "epoch [24/100] batch [1/2] time 0.948 (0.948) data 0.515 (0.515) loss 0.4577 (0.4577) acc 87.5000 (87.5000) lr 1.7705e-02 eta 0:02:25\n",
            "epoch [24/100] batch [2/2] time 0.455 (0.701) data 0.001 (0.258) loss 0.5264 (0.4920) acc 87.5000 (87.5000) lr 1.7501e-02 eta 0:01:46\n",
            "epoch [25/100] batch [1/2] time 1.002 (1.002) data 0.563 (0.563) loss 0.2655 (0.2655) acc 93.7500 (93.7500) lr 1.7501e-02 eta 0:02:31\n",
            "epoch [25/100] batch [2/2] time 0.445 (0.724) data 0.000 (0.281) loss 0.4807 (0.3731) acc 87.5000 (90.6250) lr 1.7290e-02 eta 0:01:48\n",
            "epoch [26/100] batch [1/2] time 0.946 (0.946) data 0.509 (0.509) loss 0.1208 (0.1208) acc 100.0000 (100.0000) lr 1.7290e-02 eta 0:02:20\n",
            "epoch [26/100] batch [2/2] time 0.457 (0.701) data 0.000 (0.255) loss 0.4771 (0.2990) acc 87.5000 (93.7500) lr 1.7071e-02 eta 0:01:43\n",
            "epoch [27/100] batch [1/2] time 0.951 (0.951) data 0.513 (0.513) loss 0.4311 (0.4311) acc 84.3750 (84.3750) lr 1.7071e-02 eta 0:02:19\n",
            "epoch [27/100] batch [2/2] time 0.455 (0.703) data 0.000 (0.257) loss 0.5283 (0.4797) acc 81.2500 (82.8125) lr 1.6845e-02 eta 0:01:42\n",
            "epoch [28/100] batch [1/2] time 0.969 (0.969) data 0.529 (0.529) loss 0.2899 (0.2899) acc 93.7500 (93.7500) lr 1.6845e-02 eta 0:02:20\n",
            "epoch [28/100] batch [2/2] time 0.445 (0.707) data 0.001 (0.265) loss 0.5012 (0.3956) acc 84.3750 (89.0625) lr 1.6613e-02 eta 0:01:41\n",
            "epoch [29/100] batch [1/2] time 0.935 (0.935) data 0.496 (0.496) loss 0.3299 (0.3299) acc 93.7500 (93.7500) lr 1.6613e-02 eta 0:02:13\n",
            "epoch [29/100] batch [2/2] time 0.454 (0.695) data 0.001 (0.248) loss 0.3332 (0.3315) acc 93.7500 (93.7500) lr 1.6374e-02 eta 0:01:38\n",
            "epoch [30/100] batch [1/2] time 0.951 (0.951) data 0.513 (0.513) loss 0.2691 (0.2691) acc 96.8750 (96.8750) lr 1.6374e-02 eta 0:02:14\n",
            "epoch [30/100] batch [2/2] time 0.456 (0.703) data 0.001 (0.257) loss 0.3953 (0.3322) acc 93.7500 (95.3125) lr 1.6129e-02 eta 0:01:38\n",
            "epoch [31/100] batch [1/2] time 1.006 (1.006) data 0.569 (0.569) loss 0.3715 (0.3715) acc 84.3750 (84.3750) lr 1.6129e-02 eta 0:02:19\n",
            "epoch [31/100] batch [2/2] time 0.452 (0.729) data 0.001 (0.285) loss 0.3233 (0.3474) acc 93.7500 (89.0625) lr 1.5878e-02 eta 0:01:40\n",
            "epoch [32/100] batch [1/2] time 1.006 (1.006) data 0.571 (0.571) loss 0.4702 (0.4702) acc 87.5000 (87.5000) lr 1.5878e-02 eta 0:02:17\n",
            "epoch [32/100] batch [2/2] time 0.447 (0.726) data 0.001 (0.286) loss 0.3769 (0.4236) acc 87.5000 (87.5000) lr 1.5621e-02 eta 0:01:38\n",
            "epoch [33/100] batch [1/2] time 0.989 (0.989) data 0.555 (0.555) loss 0.1432 (0.1432) acc 96.8750 (96.8750) lr 1.5621e-02 eta 0:02:13\n",
            "epoch [33/100] batch [2/2] time 0.447 (0.718) data 0.001 (0.278) loss 0.5834 (0.3633) acc 87.5000 (92.1875) lr 1.5358e-02 eta 0:01:36\n",
            "epoch [34/100] batch [1/2] time 0.984 (0.984) data 0.550 (0.550) loss 0.3842 (0.3842) acc 87.5000 (87.5000) lr 1.5358e-02 eta 0:02:10\n",
            "epoch [34/100] batch [2/2] time 0.444 (0.714) data 0.001 (0.275) loss 0.2356 (0.3099) acc 96.8750 (92.1875) lr 1.5090e-02 eta 0:01:34\n",
            "epoch [35/100] batch [1/2] time 0.918 (0.918) data 0.484 (0.484) loss 0.6717 (0.6717) acc 81.2500 (81.2500) lr 1.5090e-02 eta 0:02:00\n",
            "epoch [35/100] batch [2/2] time 0.447 (0.683) data 0.001 (0.242) loss 0.3523 (0.5120) acc 90.6250 (85.9375) lr 1.4818e-02 eta 0:01:28\n",
            "epoch [36/100] batch [1/2] time 0.970 (0.970) data 0.539 (0.539) loss 0.5320 (0.5320) acc 87.5000 (87.5000) lr 1.4818e-02 eta 0:02:05\n",
            "epoch [36/100] batch [2/2] time 0.440 (0.705) data 0.000 (0.270) loss 0.2289 (0.3804) acc 100.0000 (93.7500) lr 1.4540e-02 eta 0:01:30\n",
            "epoch [37/100] batch [1/2] time 0.937 (0.937) data 0.503 (0.503) loss 0.3006 (0.3006) acc 93.7500 (93.7500) lr 1.4540e-02 eta 0:01:59\n",
            "epoch [37/100] batch [2/2] time 0.437 (0.687) data 0.001 (0.252) loss 0.3423 (0.3214) acc 96.8750 (95.3125) lr 1.4258e-02 eta 0:01:26\n",
            "epoch [38/100] batch [1/2] time 0.969 (0.969) data 0.539 (0.539) loss 0.2145 (0.2145) acc 93.7500 (93.7500) lr 1.4258e-02 eta 0:02:01\n",
            "epoch [38/100] batch [2/2] time 0.437 (0.703) data 0.001 (0.270) loss 0.3932 (0.3038) acc 87.5000 (90.6250) lr 1.3971e-02 eta 0:01:27\n",
            "epoch [39/100] batch [1/2] time 0.989 (0.989) data 0.560 (0.560) loss 0.1814 (0.1814) acc 96.8750 (96.8750) lr 1.3971e-02 eta 0:02:01\n",
            "epoch [39/100] batch [2/2] time 0.436 (0.713) data 0.001 (0.280) loss 0.3898 (0.2856) acc 90.6250 (93.7500) lr 1.3681e-02 eta 0:01:26\n",
            "epoch [40/100] batch [1/2] time 0.981 (0.981) data 0.553 (0.553) loss 0.4548 (0.4548) acc 90.6250 (90.6250) lr 1.3681e-02 eta 0:01:58\n",
            "epoch [40/100] batch [2/2] time 0.438 (0.709) data 0.001 (0.277) loss 0.2702 (0.3625) acc 90.6250 (90.6250) lr 1.3387e-02 eta 0:01:25\n",
            "epoch [41/100] batch [1/2] time 1.002 (1.002) data 0.572 (0.572) loss 0.4851 (0.4851) acc 90.6250 (90.6250) lr 1.3387e-02 eta 0:01:59\n",
            "epoch [41/100] batch [2/2] time 0.435 (0.718) data 0.001 (0.286) loss 0.3126 (0.3989) acc 96.8750 (93.7500) lr 1.3090e-02 eta 0:01:24\n",
            "epoch [42/100] batch [1/2] time 0.971 (0.971) data 0.542 (0.542) loss 0.4424 (0.4424) acc 90.6250 (90.6250) lr 1.3090e-02 eta 0:01:53\n",
            "epoch [42/100] batch [2/2] time 0.436 (0.703) data 0.001 (0.271) loss 0.3780 (0.4102) acc 93.7500 (92.1875) lr 1.2790e-02 eta 0:01:21\n",
            "epoch [43/100] batch [1/2] time 0.950 (0.950) data 0.523 (0.523) loss 0.2740 (0.2740) acc 93.7500 (93.7500) lr 1.2790e-02 eta 0:01:49\n",
            "epoch [43/100] batch [2/2] time 0.433 (0.691) data 0.001 (0.262) loss 0.3375 (0.3057) acc 93.7500 (93.7500) lr 1.2487e-02 eta 0:01:18\n",
            "epoch [44/100] batch [1/2] time 0.964 (0.964) data 0.538 (0.538) loss 0.1872 (0.1872) acc 96.8750 (96.8750) lr 1.2487e-02 eta 0:01:48\n",
            "epoch [44/100] batch [2/2] time 0.435 (0.699) data 0.001 (0.269) loss 0.2769 (0.2320) acc 90.6250 (93.7500) lr 1.2181e-02 eta 0:01:18\n",
            "epoch [45/100] batch [1/2] time 0.987 (0.987) data 0.559 (0.559) loss 0.3048 (0.3048) acc 93.7500 (93.7500) lr 1.2181e-02 eta 0:01:49\n",
            "epoch [45/100] batch [2/2] time 0.431 (0.709) data 0.001 (0.280) loss 0.1950 (0.2499) acc 96.8750 (95.3125) lr 1.1874e-02 eta 0:01:17\n",
            "epoch [46/100] batch [1/2] time 0.947 (0.947) data 0.519 (0.519) loss 0.1829 (0.1829) acc 96.8750 (96.8750) lr 1.1874e-02 eta 0:01:43\n",
            "epoch [46/100] batch [2/2] time 0.434 (0.691) data 0.001 (0.260) loss 0.8394 (0.5112) acc 78.1250 (87.5000) lr 1.1564e-02 eta 0:01:14\n",
            "epoch [47/100] batch [1/2] time 0.950 (0.950) data 0.524 (0.524) loss 0.2987 (0.2987) acc 93.7500 (93.7500) lr 1.1564e-02 eta 0:01:41\n",
            "epoch [47/100] batch [2/2] time 0.427 (0.688) data 0.001 (0.262) loss 0.1344 (0.2165) acc 96.8750 (95.3125) lr 1.1253e-02 eta 0:01:12\n",
            "epoch [48/100] batch [1/2] time 0.978 (0.978) data 0.551 (0.551) loss 0.1535 (0.1535) acc 96.8750 (96.8750) lr 1.1253e-02 eta 0:01:42\n",
            "epoch [48/100] batch [2/2] time 0.431 (0.705) data 0.001 (0.276) loss 0.2600 (0.2067) acc 96.8750 (96.8750) lr 1.0941e-02 eta 0:01:13\n",
            "epoch [49/100] batch [1/2] time 0.954 (0.954) data 0.529 (0.529) loss 0.4515 (0.4515) acc 90.6250 (90.6250) lr 1.0941e-02 eta 0:01:38\n",
            "epoch [49/100] batch [2/2] time 0.431 (0.693) data 0.001 (0.265) loss 0.4200 (0.4358) acc 87.5000 (89.0625) lr 1.0628e-02 eta 0:01:10\n",
            "epoch [50/100] batch [1/2] time 1.025 (1.025) data 0.600 (0.600) loss 0.1305 (0.1305) acc 96.8750 (96.8750) lr 1.0628e-02 eta 0:01:43\n",
            "epoch [50/100] batch [2/2] time 0.429 (0.727) data 0.001 (0.300) loss 0.1466 (0.1385) acc 96.8750 (96.8750) lr 1.0314e-02 eta 0:01:12\n",
            "epoch [51/100] batch [1/2] time 0.953 (0.953) data 0.527 (0.527) loss 0.4058 (0.4058) acc 87.5000 (87.5000) lr 1.0314e-02 eta 0:01:34\n",
            "epoch [51/100] batch [2/2] time 0.431 (0.692) data 0.001 (0.264) loss 0.4185 (0.4122) acc 93.7500 (90.6250) lr 1.0000e-02 eta 0:01:07\n",
            "epoch [52/100] batch [1/2] time 0.950 (0.950) data 0.523 (0.523) loss 0.2135 (0.2135) acc 93.7500 (93.7500) lr 1.0000e-02 eta 0:01:32\n",
            "epoch [52/100] batch [2/2] time 0.430 (0.690) data 0.001 (0.262) loss 0.3872 (0.3003) acc 87.5000 (90.6250) lr 9.6859e-03 eta 0:01:06\n",
            "epoch [53/100] batch [1/2] time 0.993 (0.993) data 0.566 (0.566) loss 0.5026 (0.5026) acc 93.7500 (93.7500) lr 9.6859e-03 eta 0:01:34\n",
            "epoch [53/100] batch [2/2] time 0.428 (0.711) data 0.000 (0.283) loss 0.3089 (0.4057) acc 93.7500 (93.7500) lr 9.3721e-03 eta 0:01:06\n",
            "epoch [54/100] batch [1/2] time 0.931 (0.931) data 0.508 (0.508) loss 0.2174 (0.2174) acc 96.8750 (96.8750) lr 9.3721e-03 eta 0:01:26\n",
            "epoch [54/100] batch [2/2] time 0.431 (0.681) data 0.000 (0.254) loss 0.2148 (0.2161) acc 96.8750 (96.8750) lr 9.0589e-03 eta 0:01:02\n",
            "epoch [55/100] batch [1/2] time 0.910 (0.910) data 0.483 (0.483) loss 0.2711 (0.2711) acc 90.6250 (90.6250) lr 9.0589e-03 eta 0:01:22\n",
            "epoch [55/100] batch [2/2] time 0.429 (0.669) data 0.000 (0.242) loss 0.1254 (0.1983) acc 100.0000 (95.3125) lr 8.7467e-03 eta 0:01:00\n",
            "epoch [56/100] batch [1/2] time 0.961 (0.961) data 0.537 (0.537) loss 0.1883 (0.1883) acc 96.8750 (96.8750) lr 8.7467e-03 eta 0:01:25\n",
            "epoch [56/100] batch [2/2] time 0.430 (0.696) data 0.000 (0.269) loss 0.2571 (0.2227) acc 93.7500 (95.3125) lr 8.4357e-03 eta 0:01:01\n",
            "epoch [57/100] batch [1/2] time 0.937 (0.937) data 0.511 (0.511) loss 0.1769 (0.1769) acc 93.7500 (93.7500) lr 8.4357e-03 eta 0:01:21\n",
            "epoch [57/100] batch [2/2] time 0.428 (0.683) data 0.000 (0.256) loss 0.3412 (0.2591) acc 90.6250 (92.1875) lr 8.1262e-03 eta 0:00:58\n",
            "epoch [58/100] batch [1/2] time 0.948 (0.948) data 0.525 (0.525) loss 0.2465 (0.2465) acc 93.7500 (93.7500) lr 8.1262e-03 eta 0:01:20\n",
            "epoch [58/100] batch [2/2] time 0.432 (0.690) data 0.001 (0.263) loss 0.1918 (0.2192) acc 100.0000 (96.8750) lr 7.8186e-03 eta 0:00:57\n",
            "epoch [59/100] batch [1/2] time 0.949 (0.949) data 0.526 (0.526) loss 0.2300 (0.2300) acc 93.7500 (93.7500) lr 7.8186e-03 eta 0:01:18\n",
            "epoch [59/100] batch [2/2] time 0.433 (0.691) data 0.001 (0.263) loss 0.4638 (0.3469) acc 87.5000 (90.6250) lr 7.5131e-03 eta 0:00:56\n",
            "epoch [60/100] batch [1/2] time 0.935 (0.935) data 0.511 (0.511) loss 0.2405 (0.2405) acc 96.8750 (96.8750) lr 7.5131e-03 eta 0:01:15\n",
            "epoch [60/100] batch [2/2] time 0.430 (0.683) data 0.001 (0.256) loss 0.1627 (0.2016) acc 96.8750 (96.8750) lr 7.2101e-03 eta 0:00:54\n",
            "epoch [61/100] batch [1/2] time 0.968 (0.968) data 0.546 (0.546) loss 0.2650 (0.2650) acc 93.7500 (93.7500) lr 7.2101e-03 eta 0:01:16\n",
            "epoch [61/100] batch [2/2] time 0.430 (0.699) data 0.000 (0.273) loss 0.3114 (0.2882) acc 93.7500 (93.7500) lr 6.9098e-03 eta 0:00:54\n",
            "epoch [62/100] batch [1/2] time 0.947 (0.947) data 0.521 (0.521) loss 0.2256 (0.2256) acc 96.8750 (96.8750) lr 6.9098e-03 eta 0:01:12\n",
            "epoch [62/100] batch [2/2] time 0.428 (0.688) data 0.000 (0.261) loss 0.4652 (0.3454) acc 84.3750 (90.6250) lr 6.6126e-03 eta 0:00:52\n",
            "epoch [63/100] batch [1/2] time 0.926 (0.926) data 0.502 (0.502) loss 0.3903 (0.3903) acc 84.3750 (84.3750) lr 6.6126e-03 eta 0:01:09\n",
            "epoch [63/100] batch [2/2] time 0.428 (0.677) data 0.000 (0.251) loss 0.2175 (0.3039) acc 93.7500 (89.0625) lr 6.3188e-03 eta 0:00:50\n",
            "epoch [64/100] batch [1/2] time 0.928 (0.928) data 0.501 (0.501) loss 0.2768 (0.2768) acc 93.7500 (93.7500) lr 6.3188e-03 eta 0:01:07\n",
            "epoch [64/100] batch [2/2] time 0.431 (0.679) data 0.000 (0.250) loss 0.3869 (0.3319) acc 90.6250 (92.1875) lr 6.0285e-03 eta 0:00:48\n",
            "epoch [65/100] batch [1/2] time 0.944 (0.944) data 0.520 (0.520) loss 0.2357 (0.2357) acc 90.6250 (90.6250) lr 6.0285e-03 eta 0:01:07\n",
            "epoch [65/100] batch [2/2] time 0.433 (0.688) data 0.000 (0.260) loss 0.3852 (0.3104) acc 84.3750 (87.5000) lr 5.7422e-03 eta 0:00:48\n",
            "epoch [66/100] batch [1/2] time 0.908 (0.908) data 0.480 (0.480) loss 0.2394 (0.2394) acc 93.7500 (93.7500) lr 5.7422e-03 eta 0:01:02\n",
            "epoch [66/100] batch [2/2] time 0.433 (0.670) data 0.000 (0.240) loss 0.4359 (0.3377) acc 93.7500 (93.7500) lr 5.4601e-03 eta 0:00:45\n",
            "epoch [67/100] batch [1/2] time 0.932 (0.932) data 0.503 (0.503) loss 0.1939 (0.1939) acc 96.8750 (96.8750) lr 5.4601e-03 eta 0:01:02\n",
            "epoch [67/100] batch [2/2] time 0.430 (0.681) data 0.001 (0.252) loss 0.1670 (0.1804) acc 96.8750 (96.8750) lr 5.1825e-03 eta 0:00:44\n",
            "epoch [68/100] batch [1/2] time 0.932 (0.932) data 0.505 (0.505) loss 0.0893 (0.0893) acc 100.0000 (100.0000) lr 5.1825e-03 eta 0:01:00\n",
            "epoch [68/100] batch [2/2] time 0.431 (0.682) data 0.001 (0.253) loss 0.1682 (0.1288) acc 96.8750 (98.4375) lr 4.9096e-03 eta 0:00:43\n",
            "epoch [69/100] batch [1/2] time 0.973 (0.973) data 0.546 (0.546) loss 0.1739 (0.1739) acc 100.0000 (100.0000) lr 4.9096e-03 eta 0:01:01\n",
            "epoch [69/100] batch [2/2] time 0.436 (0.704) data 0.001 (0.273) loss 0.3714 (0.2726) acc 93.7500 (96.8750) lr 4.6417e-03 eta 0:00:43\n",
            "epoch [70/100] batch [1/2] time 0.934 (0.934) data 0.506 (0.506) loss 0.1114 (0.1114) acc 100.0000 (100.0000) lr 4.6417e-03 eta 0:00:56\n",
            "epoch [70/100] batch [2/2] time 0.436 (0.685) data 0.000 (0.253) loss 0.3864 (0.2489) acc 87.5000 (93.7500) lr 4.3792e-03 eta 0:00:41\n",
            "epoch [71/100] batch [1/2] time 0.913 (0.913) data 0.486 (0.486) loss 0.1931 (0.1931) acc 96.8750 (96.8750) lr 4.3792e-03 eta 0:00:53\n",
            "epoch [71/100] batch [2/2] time 0.435 (0.674) data 0.000 (0.243) loss 0.1580 (0.1755) acc 93.7500 (95.3125) lr 4.1221e-03 eta 0:00:39\n",
            "epoch [72/100] batch [1/2] time 0.941 (0.941) data 0.510 (0.510) loss 0.2086 (0.2086) acc 90.6250 (90.6250) lr 4.1221e-03 eta 0:00:53\n",
            "epoch [72/100] batch [2/2] time 0.434 (0.688) data 0.000 (0.255) loss 0.1584 (0.1835) acc 96.8750 (93.7500) lr 3.8709e-03 eta 0:00:38\n",
            "epoch [73/100] batch [1/2] time 0.951 (0.951) data 0.522 (0.522) loss 0.1409 (0.1409) acc 100.0000 (100.0000) lr 3.8709e-03 eta 0:00:52\n",
            "epoch [73/100] batch [2/2] time 0.437 (0.694) data 0.000 (0.261) loss 0.1792 (0.1601) acc 93.7500 (96.8750) lr 3.6258e-03 eta 0:00:37\n",
            "epoch [74/100] batch [1/2] time 0.956 (0.956) data 0.526 (0.526) loss 0.1962 (0.1962) acc 96.8750 (96.8750) lr 3.6258e-03 eta 0:00:50\n",
            "epoch [74/100] batch [2/2] time 0.436 (0.696) data 0.001 (0.263) loss 0.0596 (0.1279) acc 100.0000 (98.4375) lr 3.3869e-03 eta 0:00:36\n",
            "epoch [75/100] batch [1/2] time 0.956 (0.956) data 0.524 (0.524) loss 0.2378 (0.2378) acc 96.8750 (96.8750) lr 3.3869e-03 eta 0:00:48\n",
            "epoch [75/100] batch [2/2] time 0.438 (0.697) data 0.000 (0.262) loss 0.2182 (0.2280) acc 93.7500 (95.3125) lr 3.1545e-03 eta 0:00:34\n",
            "epoch [76/100] batch [1/2] time 0.953 (0.953) data 0.523 (0.523) loss 0.1639 (0.1639) acc 93.7500 (93.7500) lr 3.1545e-03 eta 0:00:46\n",
            "epoch [76/100] batch [2/2] time 0.442 (0.698) data 0.001 (0.262) loss 0.2115 (0.1877) acc 96.8750 (95.3125) lr 2.9289e-03 eta 0:00:33\n",
            "epoch [77/100] batch [1/2] time 0.957 (0.957) data 0.529 (0.529) loss 0.0719 (0.0719) acc 100.0000 (100.0000) lr 2.9289e-03 eta 0:00:45\n",
            "epoch [77/100] batch [2/2] time 0.445 (0.701) data 0.001 (0.265) loss 0.1438 (0.1079) acc 100.0000 (100.0000) lr 2.7103e-03 eta 0:00:32\n",
            "epoch [78/100] batch [1/2] time 0.976 (0.976) data 0.544 (0.544) loss 0.1737 (0.1737) acc 93.7500 (93.7500) lr 2.7103e-03 eta 0:00:43\n",
            "epoch [78/100] batch [2/2] time 0.441 (0.709) data 0.000 (0.272) loss 0.4042 (0.2890) acc 87.5000 (90.6250) lr 2.4989e-03 eta 0:00:31\n",
            "epoch [79/100] batch [1/2] time 0.908 (0.908) data 0.476 (0.476) loss 0.1036 (0.1036) acc 100.0000 (100.0000) lr 2.4989e-03 eta 0:00:39\n",
            "epoch [79/100] batch [2/2] time 0.442 (0.675) data 0.000 (0.238) loss 0.2094 (0.1565) acc 93.7500 (96.8750) lr 2.2949e-03 eta 0:00:28\n",
            "epoch [80/100] batch [1/2] time 0.917 (0.917) data 0.485 (0.485) loss 0.2831 (0.2831) acc 93.7500 (93.7500) lr 2.2949e-03 eta 0:00:37\n",
            "epoch [80/100] batch [2/2] time 0.439 (0.678) data 0.000 (0.243) loss 0.1431 (0.2131) acc 96.8750 (95.3125) lr 2.0984e-03 eta 0:00:27\n",
            "epoch [81/100] batch [1/2] time 0.922 (0.922) data 0.492 (0.492) loss 0.3041 (0.3041) acc 90.6250 (90.6250) lr 2.0984e-03 eta 0:00:35\n",
            "epoch [81/100] batch [2/2] time 0.439 (0.680) data 0.000 (0.246) loss 0.3397 (0.3219) acc 93.7500 (92.1875) lr 1.9098e-03 eta 0:00:25\n",
            "epoch [82/100] batch [1/2] time 0.923 (0.923) data 0.490 (0.490) loss 0.2022 (0.2022) acc 96.8750 (96.8750) lr 1.9098e-03 eta 0:00:34\n",
            "epoch [82/100] batch [2/2] time 0.447 (0.685) data 0.000 (0.245) loss 0.2552 (0.2287) acc 93.7500 (95.3125) lr 1.7292e-03 eta 0:00:24\n",
            "epoch [83/100] batch [1/2] time 0.977 (0.977) data 0.545 (0.545) loss 0.6084 (0.6084) acc 78.1250 (78.1250) lr 1.7292e-03 eta 0:00:34\n",
            "epoch [83/100] batch [2/2] time 0.442 (0.709) data 0.000 (0.273) loss 0.2271 (0.4178) acc 96.8750 (87.5000) lr 1.5567e-03 eta 0:00:24\n",
            "epoch [84/100] batch [1/2] time 0.955 (0.955) data 0.526 (0.526) loss 0.2545 (0.2545) acc 93.7500 (93.7500) lr 1.5567e-03 eta 0:00:31\n",
            "epoch [84/100] batch [2/2] time 0.436 (0.696) data 0.000 (0.263) loss 0.1975 (0.2260) acc 93.7500 (93.7500) lr 1.3926e-03 eta 0:00:22\n",
            "epoch [85/100] batch [1/2] time 0.971 (0.971) data 0.540 (0.540) loss 0.3252 (0.3252) acc 87.5000 (87.5000) lr 1.3926e-03 eta 0:00:30\n",
            "epoch [85/100] batch [2/2] time 0.438 (0.704) data 0.001 (0.270) loss 0.0650 (0.1951) acc 100.0000 (93.7500) lr 1.2369e-03 eta 0:00:21\n",
            "epoch [86/100] batch [1/2] time 0.948 (0.948) data 0.516 (0.516) loss 0.3691 (0.3691) acc 93.7500 (93.7500) lr 1.2369e-03 eta 0:00:27\n",
            "epoch [86/100] batch [2/2] time 0.436 (0.692) data 0.001 (0.258) loss 0.1701 (0.2696) acc 96.8750 (95.3125) lr 1.0899e-03 eta 0:00:19\n",
            "epoch [87/100] batch [1/2] time 0.953 (0.953) data 0.526 (0.526) loss 0.1767 (0.1767) acc 100.0000 (100.0000) lr 1.0899e-03 eta 0:00:25\n",
            "epoch [87/100] batch [2/2] time 0.435 (0.694) data 0.000 (0.263) loss 0.1040 (0.1403) acc 100.0000 (100.0000) lr 9.5173e-04 eta 0:00:18\n",
            "epoch [88/100] batch [1/2] time 0.929 (0.929) data 0.501 (0.501) loss 0.1984 (0.1984) acc 96.8750 (96.8750) lr 9.5173e-04 eta 0:00:23\n",
            "epoch [88/100] batch [2/2] time 0.437 (0.683) data 0.000 (0.251) loss 0.2826 (0.2405) acc 90.6250 (93.7500) lr 8.2245e-04 eta 0:00:16\n",
            "epoch [89/100] batch [1/2] time 0.933 (0.933) data 0.504 (0.504) loss 0.2092 (0.2092) acc 90.6250 (90.6250) lr 8.2245e-04 eta 0:00:21\n",
            "epoch [89/100] batch [2/2] time 0.437 (0.685) data 0.001 (0.253) loss 0.2285 (0.2189) acc 93.7500 (92.1875) lr 7.0224e-04 eta 0:00:15\n",
            "epoch [90/100] batch [1/2] time 1.010 (1.010) data 0.580 (0.580) loss 0.1200 (0.1200) acc 96.8750 (96.8750) lr 7.0224e-04 eta 0:00:21\n",
            "epoch [90/100] batch [2/2] time 0.437 (0.724) data 0.001 (0.290) loss 0.1674 (0.1437) acc 93.7500 (95.3125) lr 5.9119e-04 eta 0:00:14\n",
            "epoch [91/100] batch [1/2] time 0.938 (0.938) data 0.508 (0.508) loss 0.2261 (0.2261) acc 93.7500 (93.7500) lr 5.9119e-04 eta 0:00:17\n",
            "epoch [91/100] batch [2/2] time 0.438 (0.688) data 0.001 (0.254) loss 0.4398 (0.3329) acc 84.3750 (89.0625) lr 4.8943e-04 eta 0:00:12\n",
            "epoch [92/100] batch [1/2] time 0.933 (0.933) data 0.502 (0.502) loss 0.2197 (0.2197) acc 93.7500 (93.7500) lr 4.8943e-04 eta 0:00:15\n",
            "epoch [92/100] batch [2/2] time 0.437 (0.685) data 0.001 (0.251) loss 0.2425 (0.2311) acc 96.8750 (95.3125) lr 3.9706e-04 eta 0:00:10\n",
            "epoch [93/100] batch [1/2] time 0.960 (0.960) data 0.532 (0.532) loss 0.1980 (0.1980) acc 96.8750 (96.8750) lr 3.9706e-04 eta 0:00:14\n",
            "epoch [93/100] batch [2/2] time 0.434 (0.697) data 0.001 (0.266) loss 0.1225 (0.1602) acc 96.8750 (96.8750) lr 3.1417e-04 eta 0:00:09\n",
            "epoch [94/100] batch [1/2] time 0.974 (0.974) data 0.544 (0.544) loss 0.4633 (0.4633) acc 90.6250 (90.6250) lr 3.1417e-04 eta 0:00:12\n",
            "epoch [94/100] batch [2/2] time 0.437 (0.706) data 0.001 (0.272) loss 0.2836 (0.3734) acc 90.6250 (90.6250) lr 2.4083e-04 eta 0:00:08\n",
            "epoch [95/100] batch [1/2] time 1.019 (1.019) data 0.592 (0.592) loss 0.1085 (0.1085) acc 100.0000 (100.0000) lr 2.4083e-04 eta 0:00:11\n",
            "epoch [95/100] batch [2/2] time 0.436 (0.727) data 0.001 (0.296) loss 0.2714 (0.1899) acc 93.7500 (96.8750) lr 1.7713e-04 eta 0:00:07\n",
            "epoch [96/100] batch [1/2] time 0.990 (0.990) data 0.561 (0.561) loss 0.2129 (0.2129) acc 93.7500 (93.7500) lr 1.7713e-04 eta 0:00:08\n",
            "epoch [96/100] batch [2/2] time 0.435 (0.712) data 0.000 (0.281) loss 0.1090 (0.1609) acc 100.0000 (96.8750) lr 1.2312e-04 eta 0:00:05\n",
            "epoch [97/100] batch [1/2] time 0.964 (0.964) data 0.535 (0.535) loss 0.0885 (0.0885) acc 100.0000 (100.0000) lr 1.2312e-04 eta 0:00:06\n",
            "epoch [97/100] batch [2/2] time 0.439 (0.702) data 0.001 (0.268) loss 0.1864 (0.1374) acc 96.8750 (98.4375) lr 7.8853e-05 eta 0:00:04\n",
            "epoch [98/100] batch [1/2] time 0.955 (0.955) data 0.528 (0.528) loss 0.0984 (0.0984) acc 100.0000 (100.0000) lr 7.8853e-05 eta 0:00:04\n",
            "epoch [98/100] batch [2/2] time 0.434 (0.695) data 0.001 (0.264) loss 0.3461 (0.2223) acc 87.5000 (93.7500) lr 4.4380e-05 eta 0:00:02\n",
            "epoch [99/100] batch [1/2] time 0.955 (0.955) data 0.525 (0.525) loss 0.3632 (0.3632) acc 90.6250 (90.6250) lr 4.4380e-05 eta 0:00:02\n",
            "epoch [99/100] batch [2/2] time 0.431 (0.693) data 0.001 (0.263) loss 0.1544 (0.2588) acc 96.8750 (93.7500) lr 1.9733e-05 eta 0:00:01\n",
            "epoch [100/100] batch [1/2] time 0.978 (0.978) data 0.548 (0.548) loss 0.2933 (0.2933) acc 93.7500 (93.7500) lr 1.9733e-05 eta 0:00:00\n",
            "epoch [100/100] batch [2/2] time 0.433 (0.705) data 0.001 (0.274) loss 0.1548 (0.2241) acc 96.8750 (95.3125) lr 4.9344e-06 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:19<00:00,  1.90it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 2,939\n",
            "* accuracy: 80.1%\n",
            "* error: 19.9%\n",
            "* macro_f1: 78.3%\n",
            "Elapsed: 0:03:09\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets-2shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_init/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ElhLl8RMya3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63d2e5d3-5c61-4728-aa2f-8926595fe2c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 08:00:23.516518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 08:00:23.536857: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 08:00:23.542963: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 08:00:23.557663: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 08:00:24.554149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/1207_new_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  37\n",
            "# val      37\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
            "epoch [1/50] batch [1/1] time 21.355 (21.355) data 19.990 (19.990) loss 1.1925 (1.1925) acc 71.8750 (71.8750) lr 2.0000e-02 eta 0:17:26\n",
            "epoch [2/50] batch [1/1] time 4.021 (4.021) data 3.521 (3.521) loss 0.8551 (0.8551) acc 75.0000 (75.0000) lr 1.9980e-02 eta 0:03:13\n",
            "epoch [3/50] batch [1/1] time 0.948 (0.948) data 0.529 (0.529) loss 1.2558 (1.2558) acc 71.8750 (71.8750) lr 1.9921e-02 eta 0:00:44\n",
            "epoch [4/50] batch [1/1] time 0.895 (0.895) data 0.472 (0.472) loss 1.1324 (1.1324) acc 62.5000 (62.5000) lr 1.9823e-02 eta 0:00:41\n",
            "epoch [5/50] batch [1/1] time 0.906 (0.906) data 0.484 (0.484) loss 0.7559 (0.7559) acc 81.2500 (81.2500) lr 1.9686e-02 eta 0:00:40\n",
            "epoch [6/50] batch [1/1] time 0.911 (0.911) data 0.486 (0.486) loss 0.9608 (0.9608) acc 78.1250 (78.1250) lr 1.9511e-02 eta 0:00:40\n",
            "epoch [7/50] batch [1/1] time 0.900 (0.900) data 0.478 (0.478) loss 0.7376 (0.7376) acc 84.3750 (84.3750) lr 1.9298e-02 eta 0:00:38\n",
            "epoch [8/50] batch [1/1] time 0.908 (0.908) data 0.484 (0.484) loss 0.9901 (0.9901) acc 71.8750 (71.8750) lr 1.9048e-02 eta 0:00:38\n",
            "epoch [9/50] batch [1/1] time 0.911 (0.911) data 0.490 (0.490) loss 0.5232 (0.5232) acc 87.5000 (87.5000) lr 1.8763e-02 eta 0:00:37\n",
            "epoch [10/50] batch [1/1] time 0.925 (0.925) data 0.503 (0.503) loss 0.5734 (0.5734) acc 87.5000 (87.5000) lr 1.8443e-02 eta 0:00:37\n",
            "epoch [11/50] batch [1/1] time 0.947 (0.947) data 0.526 (0.526) loss 0.7973 (0.7973) acc 84.3750 (84.3750) lr 1.8090e-02 eta 0:00:36\n",
            "epoch [12/50] batch [1/1] time 0.936 (0.936) data 0.513 (0.513) loss 0.8234 (0.8234) acc 84.3750 (84.3750) lr 1.7705e-02 eta 0:00:35\n",
            "epoch [13/50] batch [1/1] time 0.917 (0.917) data 0.489 (0.489) loss 0.5586 (0.5586) acc 81.2500 (81.2500) lr 1.7290e-02 eta 0:00:33\n",
            "epoch [14/50] batch [1/1] time 0.903 (0.903) data 0.477 (0.477) loss 0.4735 (0.4735) acc 87.5000 (87.5000) lr 1.6845e-02 eta 0:00:32\n",
            "epoch [15/50] batch [1/1] time 0.912 (0.912) data 0.487 (0.487) loss 0.6017 (0.6017) acc 84.3750 (84.3750) lr 1.6374e-02 eta 0:00:31\n",
            "epoch [16/50] batch [1/1] time 0.912 (0.912) data 0.485 (0.485) loss 0.6206 (0.6206) acc 78.1250 (78.1250) lr 1.5878e-02 eta 0:00:31\n",
            "epoch [17/50] batch [1/1] time 0.913 (0.913) data 0.490 (0.490) loss 0.6254 (0.6254) acc 84.3750 (84.3750) lr 1.5358e-02 eta 0:00:30\n",
            "epoch [18/50] batch [1/1] time 0.947 (0.947) data 0.519 (0.519) loss 0.5647 (0.5647) acc 84.3750 (84.3750) lr 1.4818e-02 eta 0:00:30\n",
            "epoch [19/50] batch [1/1] time 0.911 (0.911) data 0.484 (0.484) loss 0.5577 (0.5577) acc 84.3750 (84.3750) lr 1.4258e-02 eta 0:00:28\n",
            "epoch [20/50] batch [1/1] time 0.921 (0.921) data 0.494 (0.494) loss 0.6269 (0.6269) acc 84.3750 (84.3750) lr 1.3681e-02 eta 0:00:27\n",
            "epoch [21/50] batch [1/1] time 0.866 (0.866) data 0.469 (0.469) loss 0.4012 (0.4012) acc 93.7500 (93.7500) lr 1.3090e-02 eta 0:00:25\n",
            "epoch [22/50] batch [1/1] time 0.928 (0.928) data 0.498 (0.498) loss 0.9133 (0.9133) acc 78.1250 (78.1250) lr 1.2487e-02 eta 0:00:25\n",
            "epoch [23/50] batch [1/1] time 0.940 (0.940) data 0.511 (0.511) loss 0.4215 (0.4215) acc 87.5000 (87.5000) lr 1.1874e-02 eta 0:00:25\n",
            "epoch [24/50] batch [1/1] time 0.931 (0.931) data 0.503 (0.503) loss 0.5996 (0.5996) acc 81.2500 (81.2500) lr 1.1253e-02 eta 0:00:24\n",
            "epoch [25/50] batch [1/1] time 0.960 (0.960) data 0.528 (0.528) loss 0.7118 (0.7118) acc 81.2500 (81.2500) lr 1.0628e-02 eta 0:00:24\n",
            "epoch [26/50] batch [1/1] time 0.911 (0.911) data 0.477 (0.477) loss 0.3123 (0.3123) acc 96.8750 (96.8750) lr 1.0000e-02 eta 0:00:21\n",
            "epoch [27/50] batch [1/1] time 0.923 (0.923) data 0.494 (0.494) loss 0.4719 (0.4719) acc 90.6250 (90.6250) lr 9.3721e-03 eta 0:00:21\n",
            "epoch [28/50] batch [1/1] time 0.905 (0.905) data 0.475 (0.475) loss 0.3942 (0.3942) acc 87.5000 (87.5000) lr 8.7467e-03 eta 0:00:19\n",
            "epoch [29/50] batch [1/1] time 0.898 (0.898) data 0.465 (0.465) loss 0.2649 (0.2649) acc 100.0000 (100.0000) lr 8.1262e-03 eta 0:00:18\n",
            "epoch [30/50] batch [1/1] time 0.936 (0.936) data 0.502 (0.502) loss 0.2732 (0.2732) acc 100.0000 (100.0000) lr 7.5131e-03 eta 0:00:18\n",
            "epoch [31/50] batch [1/1] time 0.898 (0.898) data 0.463 (0.463) loss 0.7563 (0.7563) acc 84.3750 (84.3750) lr 6.9098e-03 eta 0:00:17\n",
            "epoch [32/50] batch [1/1] time 0.925 (0.925) data 0.494 (0.494) loss 0.4938 (0.4938) acc 84.3750 (84.3750) lr 6.3188e-03 eta 0:00:16\n",
            "epoch [33/50] batch [1/1] time 0.906 (0.906) data 0.474 (0.474) loss 0.4246 (0.4246) acc 96.8750 (96.8750) lr 5.7422e-03 eta 0:00:15\n",
            "epoch [34/50] batch [1/1] time 0.911 (0.911) data 0.475 (0.475) loss 0.3315 (0.3315) acc 87.5000 (87.5000) lr 5.1825e-03 eta 0:00:14\n",
            "epoch [35/50] batch [1/1] time 0.934 (0.934) data 0.493 (0.493) loss 0.6364 (0.6364) acc 84.3750 (84.3750) lr 4.6417e-03 eta 0:00:14\n",
            "epoch [36/50] batch [1/1] time 0.932 (0.932) data 0.497 (0.497) loss 0.2337 (0.2337) acc 96.8750 (96.8750) lr 4.1221e-03 eta 0:00:13\n",
            "epoch [37/50] batch [1/1] time 0.940 (0.940) data 0.505 (0.505) loss 0.4340 (0.4340) acc 93.7500 (93.7500) lr 3.6258e-03 eta 0:00:12\n",
            "epoch [38/50] batch [1/1] time 0.926 (0.926) data 0.489 (0.489) loss 0.6559 (0.6559) acc 87.5000 (87.5000) lr 3.1545e-03 eta 0:00:11\n",
            "epoch [39/50] batch [1/1] time 0.903 (0.903) data 0.469 (0.469) loss 0.2187 (0.2187) acc 96.8750 (96.8750) lr 2.7103e-03 eta 0:00:09\n",
            "epoch [40/50] batch [1/1] time 0.925 (0.925) data 0.487 (0.487) loss 0.3563 (0.3563) acc 96.8750 (96.8750) lr 2.2949e-03 eta 0:00:09\n",
            "epoch [41/50] batch [1/1] time 0.914 (0.914) data 0.478 (0.478) loss 0.2740 (0.2740) acc 93.7500 (93.7500) lr 1.9098e-03 eta 0:00:08\n",
            "epoch [42/50] batch [1/1] time 0.930 (0.930) data 0.491 (0.491) loss 0.4773 (0.4773) acc 87.5000 (87.5000) lr 1.5567e-03 eta 0:00:07\n",
            "epoch [43/50] batch [1/1] time 0.892 (0.892) data 0.454 (0.454) loss 0.3135 (0.3135) acc 93.7500 (93.7500) lr 1.2369e-03 eta 0:00:06\n",
            "epoch [44/50] batch [1/1] time 0.945 (0.945) data 0.508 (0.508) loss 0.3350 (0.3350) acc 93.7500 (93.7500) lr 9.5173e-04 eta 0:00:05\n",
            "epoch [45/50] batch [1/1] time 0.915 (0.915) data 0.478 (0.478) loss 0.4856 (0.4856) acc 93.7500 (93.7500) lr 7.0224e-04 eta 0:00:04\n",
            "epoch [46/50] batch [1/1] time 0.923 (0.923) data 0.486 (0.486) loss 0.4773 (0.4773) acc 90.6250 (90.6250) lr 4.8943e-04 eta 0:00:03\n",
            "epoch [47/50] batch [1/1] time 0.945 (0.945) data 0.510 (0.510) loss 0.2920 (0.2920) acc 96.8750 (96.8750) lr 3.1417e-04 eta 0:00:02\n",
            "epoch [48/50] batch [1/1] time 0.925 (0.925) data 0.488 (0.488) loss 0.1482 (0.1482) acc 100.0000 (100.0000) lr 1.7713e-04 eta 0:00:01\n",
            "epoch [49/50] batch [1/1] time 0.946 (0.946) data 0.507 (0.507) loss 0.1688 (0.1688) acc 96.8750 (96.8750) lr 7.8853e-05 eta 0:00:00\n",
            "epoch [50/50] batch [1/1] time 0.934 (0.934) data 0.496 (0.496) loss 0.4101 (0.4101) acc 90.6250 (90.6250) lr 1.9733e-05 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:19<00:00,  1.87it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,229\n",
            "* accuracy: 88.0%\n",
            "* error: 12.0%\n",
            "* macro_f1: 87.8%\n",
            "Elapsed: 0:01:33\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/1207_new_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#oxford_pets-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/test/1207_new_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O98QDt6wSwH2",
        "outputId": "d1909fb6-ed60-4b85-a162-1b9fd040b879"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 08:57:52.122052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 08:57:52.141436: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 08:57:52.147593: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 08:57:52.161891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 08:57:53.168448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/test/1207_new_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/test/1207_new_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  37\n",
            "# val      37\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "         [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "         [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "         ...,\n",
            "         [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "         [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "         [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]],\n",
            "\n",
            "        [[-0.0005, -0.0112, -0.0217,  ..., -0.0049,  0.0059,  0.0067],\n",
            "         [ 0.0006, -0.0186, -0.0251,  ..., -0.0035,  0.0054, -0.0018],\n",
            "         [ 0.0017, -0.0073, -0.0235,  ..., -0.0021,  0.0049,  0.0061],\n",
            "         ...,\n",
            "         [ 0.0010, -0.0121, -0.0159,  ..., -0.0129, -0.0021,  0.0048],\n",
            "         [ 0.0078, -0.0110, -0.0108,  ...,  0.0007,  0.0015, -0.0017],\n",
            "         [ 0.0023, -0.0060, -0.0173,  ...,  0.0006,  0.0159,  0.0043]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/test/1207_new_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
            "epoch [1/50] batch [1/1] time 1.891 (1.891) data 0.593 (0.593) loss 1.1925 (1.1925) acc 71.8750 (71.8750) lr 2.0000e-02 eta 0:01:32\n",
            "epoch [2/50] batch [1/1] time 0.841 (0.841) data 0.448 (0.448) loss 0.8551 (0.8551) acc 75.0000 (75.0000) lr 1.9980e-02 eta 0:00:40\n",
            "epoch [3/50] batch [1/1] time 0.882 (0.882) data 0.490 (0.490) loss 1.2558 (1.2558) acc 71.8750 (71.8750) lr 1.9921e-02 eta 0:00:41\n",
            "epoch [4/50] batch [1/1] time 0.852 (0.852) data 0.461 (0.461) loss 1.1324 (1.1324) acc 62.5000 (62.5000) lr 1.9823e-02 eta 0:00:39\n",
            "epoch [5/50] batch [1/1] time 0.863 (0.863) data 0.472 (0.472) loss 0.7564 (0.7564) acc 81.2500 (81.2500) lr 1.9686e-02 eta 0:00:38\n",
            "epoch [6/50] batch [1/1] time 0.873 (0.873) data 0.482 (0.482) loss 0.9599 (0.9599) acc 78.1250 (78.1250) lr 1.9511e-02 eta 0:00:38\n",
            "epoch [7/50] batch [1/1] time 0.844 (0.844) data 0.453 (0.453) loss 0.7372 (0.7372) acc 84.3750 (84.3750) lr 1.9298e-02 eta 0:00:36\n",
            "epoch [8/50] batch [1/1] time 0.860 (0.860) data 0.466 (0.466) loss 0.9887 (0.9887) acc 71.8750 (71.8750) lr 1.9048e-02 eta 0:00:36\n",
            "epoch [9/50] batch [1/1] time 0.841 (0.841) data 0.449 (0.449) loss 0.5227 (0.5227) acc 87.5000 (87.5000) lr 1.8763e-02 eta 0:00:34\n",
            "epoch [10/50] batch [1/1] time 0.867 (0.867) data 0.475 (0.475) loss 0.5720 (0.5720) acc 87.5000 (87.5000) lr 1.8443e-02 eta 0:00:34\n",
            "epoch [11/50] batch [1/1] time 0.856 (0.856) data 0.465 (0.465) loss 0.7968 (0.7968) acc 84.3750 (84.3750) lr 1.8090e-02 eta 0:00:33\n",
            "epoch [12/50] batch [1/1] time 0.847 (0.847) data 0.456 (0.456) loss 0.8215 (0.8215) acc 84.3750 (84.3750) lr 1.7705e-02 eta 0:00:32\n",
            "epoch [13/50] batch [1/1] time 0.862 (0.862) data 0.470 (0.470) loss 0.5571 (0.5571) acc 81.2500 (81.2500) lr 1.7290e-02 eta 0:00:31\n",
            "epoch [14/50] batch [1/1] time 0.850 (0.850) data 0.449 (0.449) loss 0.4720 (0.4720) acc 87.5000 (87.5000) lr 1.6845e-02 eta 0:00:30\n",
            "epoch [15/50] batch [1/1] time 0.876 (0.876) data 0.484 (0.484) loss 0.6022 (0.6022) acc 84.3750 (84.3750) lr 1.6374e-02 eta 0:00:30\n",
            "epoch [16/50] batch [1/1] time 0.856 (0.856) data 0.463 (0.463) loss 0.6206 (0.6206) acc 78.1250 (78.1250) lr 1.5878e-02 eta 0:00:29\n",
            "epoch [17/50] batch [1/1] time 0.898 (0.898) data 0.505 (0.505) loss 0.6263 (0.6263) acc 84.3750 (84.3750) lr 1.5358e-02 eta 0:00:29\n",
            "epoch [18/50] batch [1/1] time 0.881 (0.881) data 0.486 (0.486) loss 0.5642 (0.5642) acc 84.3750 (84.3750) lr 1.4818e-02 eta 0:00:28\n",
            "epoch [19/50] batch [1/1] time 0.868 (0.868) data 0.473 (0.473) loss 0.5577 (0.5577) acc 84.3750 (84.3750) lr 1.4258e-02 eta 0:00:26\n",
            "epoch [20/50] batch [1/1] time 0.854 (0.854) data 0.462 (0.462) loss 0.6259 (0.6259) acc 84.3750 (84.3750) lr 1.3681e-02 eta 0:00:25\n",
            "epoch [21/50] batch [1/1] time 0.860 (0.860) data 0.468 (0.468) loss 0.4024 (0.4024) acc 93.7500 (93.7500) lr 1.3090e-02 eta 0:00:24\n",
            "epoch [22/50] batch [1/1] time 0.873 (0.873) data 0.479 (0.479) loss 0.9108 (0.9108) acc 78.1250 (78.1250) lr 1.2487e-02 eta 0:00:24\n",
            "epoch [23/50] batch [1/1] time 0.844 (0.844) data 0.449 (0.449) loss 0.4224 (0.4224) acc 87.5000 (87.5000) lr 1.1874e-02 eta 0:00:22\n",
            "epoch [24/50] batch [1/1] time 0.848 (0.848) data 0.453 (0.453) loss 0.5996 (0.5996) acc 78.1250 (78.1250) lr 1.1253e-02 eta 0:00:22\n",
            "epoch [25/50] batch [1/1] time 0.875 (0.875) data 0.482 (0.482) loss 0.7103 (0.7103) acc 81.2500 (81.2500) lr 1.0628e-02 eta 0:00:21\n",
            "epoch [26/50] batch [1/1] time 0.834 (0.834) data 0.439 (0.439) loss 0.3123 (0.3123) acc 93.7500 (93.7500) lr 1.0000e-02 eta 0:00:20\n",
            "epoch [27/50] batch [1/1] time 0.857 (0.857) data 0.463 (0.463) loss 0.4719 (0.4719) acc 90.6250 (90.6250) lr 9.3721e-03 eta 0:00:19\n",
            "epoch [28/50] batch [1/1] time 0.861 (0.861) data 0.467 (0.467) loss 0.3947 (0.3947) acc 87.5000 (87.5000) lr 8.7467e-03 eta 0:00:18\n",
            "epoch [29/50] batch [1/1] time 0.856 (0.856) data 0.460 (0.460) loss 0.2652 (0.2652) acc 100.0000 (100.0000) lr 8.1262e-03 eta 0:00:17\n",
            "epoch [30/50] batch [1/1] time 0.891 (0.891) data 0.495 (0.495) loss 0.2737 (0.2737) acc 100.0000 (100.0000) lr 7.5131e-03 eta 0:00:17\n",
            "epoch [31/50] batch [1/1] time 0.876 (0.876) data 0.479 (0.479) loss 0.7567 (0.7567) acc 84.3750 (84.3750) lr 6.9098e-03 eta 0:00:16\n",
            "epoch [32/50] batch [1/1] time 0.923 (0.923) data 0.532 (0.532) loss 0.4935 (0.4935) acc 84.3750 (84.3750) lr 6.3188e-03 eta 0:00:16\n",
            "epoch [33/50] batch [1/1] time 0.855 (0.855) data 0.461 (0.461) loss 0.4256 (0.4256) acc 96.8750 (96.8750) lr 5.7422e-03 eta 0:00:14\n",
            "epoch [34/50] batch [1/1] time 0.848 (0.848) data 0.453 (0.453) loss 0.3328 (0.3328) acc 87.5000 (87.5000) lr 5.1825e-03 eta 0:00:13\n",
            "epoch [35/50] batch [1/1] time 0.852 (0.852) data 0.459 (0.459) loss 0.6355 (0.6355) acc 84.3750 (84.3750) lr 4.6417e-03 eta 0:00:12\n",
            "epoch [36/50] batch [1/1] time 0.849 (0.849) data 0.454 (0.454) loss 0.2341 (0.2341) acc 96.8750 (96.8750) lr 4.1221e-03 eta 0:00:11\n",
            "epoch [37/50] batch [1/1] time 0.870 (0.870) data 0.475 (0.475) loss 0.4335 (0.4335) acc 93.7500 (93.7500) lr 3.6258e-03 eta 0:00:11\n",
            "epoch [38/50] batch [1/1] time 0.845 (0.845) data 0.449 (0.449) loss 0.6559 (0.6559) acc 87.5000 (87.5000) lr 3.1545e-03 eta 0:00:10\n",
            "epoch [39/50] batch [1/1] time 0.847 (0.847) data 0.453 (0.453) loss 0.2187 (0.2187) acc 96.8750 (96.8750) lr 2.7103e-03 eta 0:00:09\n",
            "epoch [40/50] batch [1/1] time 0.859 (0.859) data 0.465 (0.465) loss 0.3565 (0.3565) acc 96.8750 (96.8750) lr 2.2949e-03 eta 0:00:08\n",
            "epoch [41/50] batch [1/1] time 0.849 (0.849) data 0.453 (0.453) loss 0.2747 (0.2747) acc 93.7500 (93.7500) lr 1.9098e-03 eta 0:00:07\n",
            "epoch [42/50] batch [1/1] time 0.877 (0.877) data 0.481 (0.481) loss 0.4771 (0.4771) acc 87.5000 (87.5000) lr 1.5567e-03 eta 0:00:07\n",
            "epoch [43/50] batch [1/1] time 0.874 (0.874) data 0.477 (0.477) loss 0.3135 (0.3135) acc 93.7500 (93.7500) lr 1.2369e-03 eta 0:00:06\n",
            "epoch [44/50] batch [1/1] time 0.896 (0.896) data 0.499 (0.499) loss 0.3340 (0.3340) acc 93.7500 (93.7500) lr 9.5173e-04 eta 0:00:05\n",
            "epoch [45/50] batch [1/1] time 0.924 (0.924) data 0.527 (0.527) loss 0.4858 (0.4858) acc 93.7500 (93.7500) lr 7.0224e-04 eta 0:00:04\n",
            "epoch [46/50] batch [1/1] time 0.873 (0.873) data 0.478 (0.478) loss 0.4763 (0.4763) acc 90.6250 (90.6250) lr 4.8943e-04 eta 0:00:03\n",
            "epoch [47/50] batch [1/1] time 0.868 (0.868) data 0.473 (0.473) loss 0.2927 (0.2927) acc 96.8750 (96.8750) lr 3.1417e-04 eta 0:00:02\n",
            "epoch [48/50] batch [1/1] time 0.872 (0.872) data 0.473 (0.473) loss 0.1477 (0.1477) acc 100.0000 (100.0000) lr 1.7713e-04 eta 0:00:01\n",
            "epoch [49/50] batch [1/1] time 0.860 (0.860) data 0.463 (0.463) loss 0.1687 (0.1687) acc 96.8750 (96.8750) lr 7.8853e-05 eta 0:00:00\n",
            "epoch [50/50] batch [1/1] time 0.854 (0.854) data 0.454 (0.454) loss 0.4101 (0.4101) acc 90.6250 (90.6250) lr 1.9733e-05 eta 0:00:00\n",
            "Checkpoint saved to output/test/1207_new_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 211, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 151, in main\n",
            "    trainer.train()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 385, in train\n",
            "    super().train(self.start_epoch, self.max_epoch)\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 251, in train\n",
            "    self.after_train()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 403, in after_train\n",
            "    plot_losses(total_losses, image_losses, text_losses)\n",
            "NameError: name 'plot_losses' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different_init"
      ],
      "metadata": {
        "id": "xBAE02MiyTzF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828d5bf0-3400-4ce1-e4e0-1684ca6552d3",
        "id": "bOi_SZn8yWJe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 10:23:04.248748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 10:23:04.268577: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 10:23:04.274805: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 10:23:04.288901: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 10:23:05.261689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16']\n",
            "output_dir: output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_16shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_16shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_16-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  592\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "        [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "        [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "        ...,\n",
            "        [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "        [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "        [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-5.2644e-04, -1.1168e-02, -2.1661e-02,  ..., -4.8596e-03,\n",
            "           5.8912e-03,  6.7493e-03],\n",
            "         [ 5.8841e-04, -1.8584e-02, -2.5060e-02,  ..., -3.4987e-03,\n",
            "           5.4105e-03, -1.7861e-03],\n",
            "         [ 1.7309e-03, -7.2606e-03, -2.3481e-02,  ..., -2.1107e-03,\n",
            "           4.9070e-03,  6.0702e-03],\n",
            "         ...,\n",
            "         [ 1.0223e-03, -1.2080e-02, -1.5868e-02,  ..., -1.2882e-02,\n",
            "          -2.0567e-03,  4.8362e-03],\n",
            "         [ 7.8468e-03, -1.1038e-02, -1.0770e-02,  ...,  7.4988e-04,\n",
            "           1.5215e-03, -1.6774e-03],\n",
            "         [ 2.3028e-03, -5.9798e-03, -1.7329e-02,  ...,  5.5724e-04,\n",
            "           1.5857e-02,  4.3269e-03]],\n",
            "\n",
            "        [[ 6.0997e-03,  1.7106e-03, -2.1188e-02,  ...,  2.5523e-03,\n",
            "           2.1776e-03,  7.7525e-03],\n",
            "         [ 6.9275e-03, -6.8095e-03, -1.3799e-02,  ..., -3.6580e-03,\n",
            "           5.6513e-03,  1.9980e-03],\n",
            "         [ 6.2942e-04, -7.5124e-03, -9.4807e-03,  ...,  9.2128e-03,\n",
            "           1.0227e-02,  3.9989e-03],\n",
            "         ...,\n",
            "         [-8.9265e-04, -1.5108e-02, -1.9400e-02,  ..., -3.9164e-03,\n",
            "           4.4435e-03, -3.3369e-04],\n",
            "         [ 9.0713e-03, -1.0043e-02, -2.8737e-02,  ..., -4.9359e-03,\n",
            "           9.1699e-03,  5.4980e-03],\n",
            "         [-8.9653e-05, -7.9620e-03, -2.2422e-02,  ..., -2.5861e-03,\n",
            "           2.1928e-03,  6.9095e-03]],\n",
            "\n",
            "        [[ 2.2506e-04, -1.2583e-02, -1.8306e-02,  ..., -1.1051e-02,\n",
            "           9.2462e-03,  8.1454e-03],\n",
            "         [-9.4987e-04, -1.2930e-02, -2.0111e-02,  ..., -9.7730e-03,\n",
            "           5.3495e-03,  1.8893e-03],\n",
            "         [-2.7428e-03, -8.8699e-03, -1.4280e-02,  ...,  5.0471e-03,\n",
            "           5.3924e-03,  1.0839e-02],\n",
            "         ...,\n",
            "         [ 7.2136e-03, -9.1107e-03, -1.5424e-02,  ...,  1.9115e-03,\n",
            "           6.7415e-03, -8.7823e-04],\n",
            "         [-4.8866e-03, -9.1675e-03, -1.5351e-02,  ..., -5.0427e-03,\n",
            "           8.2830e-03,  1.1029e-02],\n",
            "         [-4.4785e-03, -1.2034e-02, -1.6046e-02,  ..., -1.3892e-03,\n",
            "           1.1470e-02, -4.6686e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1066e-02, -2.1956e-03, -1.7727e-02,  ..., -1.0564e-03,\n",
            "           1.2302e-02,  5.0345e-03],\n",
            "         [ 2.7537e-03, -4.1800e-04, -1.4596e-02,  ..., -1.1298e-03,\n",
            "           6.9791e-03,  3.2216e-03],\n",
            "         [ 3.3331e-03, -1.0128e-02, -1.2788e-02,  ..., -1.2218e-02,\n",
            "           2.8146e-03,  4.3975e-03],\n",
            "         ...,\n",
            "         [ 5.6838e-04, -7.7658e-04, -3.2178e-02,  ..., -6.8700e-03,\n",
            "           4.0220e-03, -3.8805e-04],\n",
            "         [ 9.2410e-04, -7.9189e-03, -1.0762e-02,  ..., -4.6880e-03,\n",
            "           5.9761e-03,  4.5386e-03],\n",
            "         [-2.8230e-04, -1.1029e-02, -1.4386e-02,  ..., -1.5611e-04,\n",
            "           1.0764e-02,  4.3288e-03]],\n",
            "\n",
            "        [[ 5.6610e-03, -6.1458e-03, -2.3435e-02,  ..., -8.1288e-03,\n",
            "           9.4217e-03,  1.0564e-02],\n",
            "         [ 1.7781e-03, -9.1155e-03, -7.3978e-03,  ...,  3.8989e-03,\n",
            "           8.7808e-03, -3.2376e-03],\n",
            "         [ 6.8626e-03, -1.4937e-02, -2.3481e-02,  ...,  1.5681e-03,\n",
            "           1.1096e-02,  2.1168e-03],\n",
            "         ...,\n",
            "         [ 2.3734e-03, -1.0926e-02, -2.4415e-02,  ..., -8.3730e-03,\n",
            "           1.5094e-02,  1.2127e-03],\n",
            "         [ 2.2802e-03, -9.2195e-03, -2.1869e-02,  ..., -2.9556e-03,\n",
            "           8.4852e-03,  4.9182e-03],\n",
            "         [-8.3199e-03, -8.2483e-03, -1.6881e-02,  ..., -4.5459e-03,\n",
            "           6.2860e-03,  5.5418e-04]],\n",
            "\n",
            "        [[ 1.3695e-03, -1.2808e-02, -1.6318e-02,  ...,  3.0864e-03,\n",
            "           4.0411e-03,  6.7378e-03],\n",
            "         [ 4.1695e-03, -1.2923e-02, -2.3946e-02,  ...,  2.9491e-03,\n",
            "           1.2264e-02,  2.7028e-03],\n",
            "         [-3.1091e-04, -1.2759e-02, -1.5500e-02,  ...,  1.6597e-03,\n",
            "           2.8471e-03, -5.2213e-03],\n",
            "         ...,\n",
            "         [ 9.3452e-05, -4.8621e-03, -1.9077e-02,  ..., -5.4662e-03,\n",
            "           5.2866e-03,  2.2937e-03],\n",
            "         [ 5.1670e-03, -1.0069e-02, -1.7373e-02,  ...,  2.5828e-03,\n",
            "           8.2563e-03,  1.3529e-03],\n",
            "         [ 3.2577e-03, -1.7065e-02, -1.7372e-02,  ..., -3.7200e-03,\n",
            "           4.5885e-03, -3.4474e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4769\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_16shots/seed1/tensorboard)\n",
            "epoch [1/200] batch [5/18] time 0.402 (5.493) data 0.000 (4.895) loss 0.8654 (1.1069) acc 68.7500 (65.0000) lr 1.0000e-05 eta 5:29:07\n",
            "epoch [1/200] batch [10/18] time 8.367 (4.927) data 7.841 (4.409) loss 1.2175 (1.2088) acc 62.5000 (64.0625) lr 1.0000e-05 eta 4:54:47\n",
            "epoch [1/200] batch [15/18] time 0.406 (3.419) data 0.000 (2.939) loss 1.3630 (1.2081) acc 62.5000 (65.0000) lr 1.0000e-05 eta 3:24:16\n",
            "epoch [2/200] batch [5/18] time 0.408 (0.539) data 0.000 (0.132) loss 1.1891 (0.9847) acc 75.0000 (76.8750) lr 2.0000e-02 eta 0:32:09\n",
            "epoch [2/200] batch [10/18] time 0.409 (0.475) data 0.000 (0.066) loss 0.8007 (0.9305) acc 81.2500 (75.6250) lr 2.0000e-02 eta 0:28:15\n",
            "epoch [2/200] batch [15/18] time 0.410 (0.453) data 0.000 (0.044) loss 1.0241 (0.9953) acc 71.8750 (72.9167) lr 2.0000e-02 eta 0:26:57\n",
            "epoch [3/200] batch [5/18] time 0.410 (0.579) data 0.000 (0.166) loss 0.5084 (0.7520) acc 84.3750 (77.5000) lr 1.9999e-02 eta 0:34:19\n",
            "epoch [3/200] batch [10/18] time 0.413 (0.495) data 0.000 (0.083) loss 0.8110 (0.7654) acc 75.0000 (77.5000) lr 1.9999e-02 eta 0:29:20\n",
            "epoch [3/200] batch [15/18] time 0.415 (0.468) data 0.000 (0.056) loss 0.5948 (0.8173) acc 84.3750 (76.0417) lr 1.9999e-02 eta 0:27:41\n",
            "epoch [4/200] batch [5/18] time 0.412 (0.614) data 0.000 (0.198) loss 0.8092 (0.7668) acc 81.2500 (79.3750) lr 1.9995e-02 eta 0:36:15\n",
            "epoch [4/200] batch [10/18] time 0.412 (0.515) data 0.000 (0.099) loss 0.6122 (0.7991) acc 81.2500 (78.7500) lr 1.9995e-02 eta 0:30:21\n",
            "epoch [4/200] batch [15/18] time 0.418 (0.483) data 0.000 (0.066) loss 0.4971 (0.7255) acc 81.2500 (80.2083) lr 1.9995e-02 eta 0:28:24\n",
            "epoch [5/200] batch [5/18] time 0.422 (0.596) data 0.000 (0.177) loss 1.1324 (0.8648) acc 62.5000 (72.5000) lr 1.9989e-02 eta 0:35:00\n",
            "epoch [5/200] batch [10/18] time 0.419 (0.508) data 0.000 (0.088) loss 0.7762 (0.7793) acc 87.5000 (76.8750) lr 1.9989e-02 eta 0:29:47\n",
            "epoch [5/200] batch [15/18] time 0.422 (0.479) data 0.000 (0.059) loss 0.4906 (0.7433) acc 90.6250 (78.1250) lr 1.9989e-02 eta 0:28:02\n",
            "epoch [6/200] batch [5/18] time 0.424 (0.615) data 0.000 (0.190) loss 0.5335 (0.7603) acc 90.6250 (79.3750) lr 1.9980e-02 eta 0:35:54\n",
            "epoch [6/200] batch [10/18] time 0.422 (0.519) data 0.000 (0.095) loss 0.5401 (0.7319) acc 78.1250 (78.7500) lr 1.9980e-02 eta 0:30:14\n",
            "epoch [6/200] batch [15/18] time 0.422 (0.486) data 0.000 (0.064) loss 0.4351 (0.7012) acc 90.6250 (80.4167) lr 1.9980e-02 eta 0:28:19\n",
            "epoch [7/200] batch [5/18] time 0.434 (0.599) data 0.000 (0.173) loss 0.4518 (0.6264) acc 90.6250 (85.0000) lr 1.9969e-02 eta 0:34:50\n",
            "epoch [7/200] batch [10/18] time 0.433 (0.513) data 0.001 (0.087) loss 0.6492 (0.5951) acc 75.0000 (84.3750) lr 1.9969e-02 eta 0:29:46\n",
            "epoch [7/200] batch [15/18] time 0.435 (0.485) data 0.000 (0.058) loss 0.7507 (0.6202) acc 75.0000 (82.7083) lr 1.9969e-02 eta 0:28:04\n",
            "epoch [8/200] batch [5/18] time 0.437 (0.617) data 0.000 (0.186) loss 0.7159 (0.6509) acc 75.0000 (81.8750) lr 1.9956e-02 eta 0:35:38\n",
            "epoch [8/200] batch [10/18] time 0.435 (0.525) data 0.000 (0.093) loss 0.9097 (0.6237) acc 68.7500 (81.2500) lr 1.9956e-02 eta 0:30:17\n",
            "epoch [8/200] batch [15/18] time 0.431 (0.494) data 0.000 (0.062) loss 0.4300 (0.6249) acc 81.2500 (81.6667) lr 1.9956e-02 eta 0:28:29\n",
            "epoch [9/200] batch [5/18] time 0.445 (0.628) data 0.000 (0.191) loss 0.3493 (0.5251) acc 93.7500 (88.7500) lr 1.9940e-02 eta 0:36:06\n",
            "epoch [9/200] batch [10/18] time 0.438 (0.533) data 0.000 (0.096) loss 0.5455 (0.5254) acc 87.5000 (86.8750) lr 1.9940e-02 eta 0:30:35\n",
            "epoch [9/200] batch [15/18] time 0.439 (0.501) data 0.000 (0.064) loss 0.6107 (0.5784) acc 71.8750 (84.3750) lr 1.9940e-02 eta 0:28:43\n",
            "epoch [10/200] batch [5/18] time 0.452 (0.621) data 0.000 (0.180) loss 0.4779 (0.5492) acc 90.6250 (85.6250) lr 1.9921e-02 eta 0:35:32\n",
            "epoch [10/200] batch [10/18] time 0.444 (0.532) data 0.000 (0.090) loss 0.4117 (0.5666) acc 84.3750 (84.0625) lr 1.9921e-02 eta 0:30:22\n",
            "epoch [10/200] batch [15/18] time 0.447 (0.504) data 0.000 (0.060) loss 0.3640 (0.5710) acc 90.6250 (83.1250) lr 1.9921e-02 eta 0:28:44\n",
            "epoch [11/200] batch [5/18] time 0.459 (0.632) data 0.000 (0.182) loss 0.6154 (0.5808) acc 84.3750 (83.7500) lr 1.9900e-02 eta 0:35:57\n",
            "epoch [11/200] batch [10/18] time 0.446 (0.541) data 0.000 (0.091) loss 0.1841 (0.4934) acc 100.0000 (86.8750) lr 1.9900e-02 eta 0:30:43\n",
            "epoch [11/200] batch [15/18] time 0.449 (0.511) data 0.000 (0.061) loss 0.4426 (0.5264) acc 84.3750 (85.2083) lr 1.9900e-02 eta 0:28:59\n",
            "epoch [12/200] batch [5/18] time 0.456 (0.632) data 0.000 (0.182) loss 0.4577 (0.5069) acc 84.3750 (83.7500) lr 1.9877e-02 eta 0:35:46\n",
            "epoch [12/200] batch [10/18] time 0.447 (0.540) data 0.000 (0.091) loss 0.4939 (0.5503) acc 87.5000 (84.0625) lr 1.9877e-02 eta 0:30:30\n",
            "epoch [12/200] batch [15/18] time 0.449 (0.508) data 0.000 (0.061) loss 0.7083 (0.5611) acc 78.1250 (82.9167) lr 1.9877e-02 eta 0:28:41\n",
            "epoch [13/200] batch [5/18] time 0.453 (0.617) data 0.000 (0.175) loss 0.5836 (0.5970) acc 84.3750 (80.6250) lr 1.9851e-02 eta 0:34:46\n",
            "epoch [13/200] batch [10/18] time 0.443 (0.530) data 0.000 (0.088) loss 0.4365 (0.6020) acc 90.6250 (81.5625) lr 1.9851e-02 eta 0:29:47\n",
            "epoch [13/200] batch [15/18] time 0.440 (0.500) data 0.000 (0.059) loss 0.4326 (0.5763) acc 90.6250 (82.5000) lr 1.9851e-02 eta 0:28:05\n",
            "epoch [14/200] batch [5/18] time 0.442 (0.623) data 0.000 (0.186) loss 0.2312 (0.4073) acc 90.6250 (86.8750) lr 1.9823e-02 eta 0:34:52\n",
            "epoch [14/200] batch [10/18] time 0.444 (0.531) data 0.000 (0.093) loss 0.5422 (0.4955) acc 84.3750 (85.6250) lr 1.9823e-02 eta 0:29:41\n",
            "epoch [14/200] batch [15/18] time 0.435 (0.498) data 0.000 (0.062) loss 0.7208 (0.5304) acc 81.2500 (85.6250) lr 1.9823e-02 eta 0:27:50\n",
            "epoch [15/200] batch [5/18] time 0.443 (0.630) data 0.000 (0.195) loss 0.4888 (0.5348) acc 81.2500 (83.7500) lr 1.9792e-02 eta 0:35:05\n",
            "epoch [15/200] batch [10/18] time 0.434 (0.532) data 0.000 (0.098) loss 0.6734 (0.4997) acc 84.3750 (85.6250) lr 1.9792e-02 eta 0:29:36\n",
            "epoch [15/200] batch [15/18] time 0.437 (0.499) data 0.000 (0.065) loss 0.4347 (0.4981) acc 87.5000 (86.2500) lr 1.9792e-02 eta 0:27:44\n",
            "epoch [16/200] batch [5/18] time 0.441 (0.601) data 0.000 (0.167) loss 0.3581 (0.5344) acc 87.5000 (81.8750) lr 1.9759e-02 eta 0:33:17\n",
            "epoch [16/200] batch [10/18] time 0.439 (0.517) data 0.000 (0.083) loss 0.3566 (0.5079) acc 90.6250 (84.3750) lr 1.9759e-02 eta 0:28:37\n",
            "epoch [16/200] batch [15/18] time 0.436 (0.490) data 0.000 (0.056) loss 0.1960 (0.4537) acc 90.6250 (85.6250) lr 1.9759e-02 eta 0:27:02\n",
            "epoch [17/200] batch [5/18] time 0.445 (0.621) data 0.000 (0.187) loss 0.5705 (0.4417) acc 84.3750 (88.7500) lr 1.9724e-02 eta 0:34:12\n",
            "epoch [17/200] batch [10/18] time 0.431 (0.527) data 0.001 (0.093) loss 0.4113 (0.5131) acc 87.5000 (87.1875) lr 1.9724e-02 eta 0:29:00\n",
            "epoch [17/200] batch [15/18] time 0.434 (0.497) data 0.000 (0.062) loss 0.1222 (0.4816) acc 100.0000 (87.2917) lr 1.9724e-02 eta 0:27:18\n",
            "epoch [18/200] batch [5/18] time 0.446 (0.614) data 0.000 (0.178) loss 0.4106 (0.4802) acc 87.5000 (84.3750) lr 1.9686e-02 eta 0:33:39\n",
            "epoch [18/200] batch [10/18] time 0.437 (0.525) data 0.000 (0.089) loss 0.5508 (0.5183) acc 90.6250 (85.3125) lr 1.9686e-02 eta 0:28:43\n",
            "epoch [18/200] batch [15/18] time 0.432 (0.495) data 0.000 (0.060) loss 0.2274 (0.5245) acc 96.8750 (85.8333) lr 1.9686e-02 eta 0:27:02\n",
            "epoch [19/200] batch [5/18] time 0.451 (0.604) data 0.000 (0.165) loss 0.3811 (0.4469) acc 90.6250 (86.2500) lr 1.9646e-02 eta 0:32:55\n",
            "epoch [19/200] batch [10/18] time 0.441 (0.521) data 0.000 (0.083) loss 0.4762 (0.4529) acc 87.5000 (86.2500) lr 1.9646e-02 eta 0:28:21\n",
            "epoch [19/200] batch [15/18] time 0.439 (0.493) data 0.000 (0.055) loss 0.5741 (0.4586) acc 81.2500 (87.2917) lr 1.9646e-02 eta 0:26:48\n",
            "epoch [20/200] batch [5/18] time 0.449 (0.614) data 0.000 (0.175) loss 0.3236 (0.3708) acc 90.6250 (90.0000) lr 1.9603e-02 eta 0:33:16\n",
            "epoch [20/200] batch [10/18] time 0.441 (0.527) data 0.000 (0.088) loss 0.6633 (0.4462) acc 78.1250 (87.5000) lr 1.9603e-02 eta 0:28:30\n",
            "epoch [20/200] batch [15/18] time 0.442 (0.498) data 0.000 (0.059) loss 0.5263 (0.4192) acc 90.6250 (89.3750) lr 1.9603e-02 eta 0:26:54\n",
            "epoch [21/200] batch [5/18] time 0.452 (0.629) data 0.000 (0.188) loss 0.5605 (0.4094) acc 81.2500 (88.7500) lr 1.9558e-02 eta 0:33:54\n",
            "epoch [21/200] batch [10/18] time 0.440 (0.535) data 0.000 (0.094) loss 0.6410 (0.4509) acc 84.3750 (86.5625) lr 1.9558e-02 eta 0:28:47\n",
            "epoch [21/200] batch [15/18] time 0.446 (0.504) data 0.000 (0.063) loss 0.4905 (0.4271) acc 81.2500 (87.2917) lr 1.9558e-02 eta 0:27:05\n",
            "epoch [22/200] batch [5/18] time 0.449 (0.620) data 0.000 (0.178) loss 0.4197 (0.5174) acc 87.5000 (86.8750) lr 1.9511e-02 eta 0:33:13\n",
            "epoch [22/200] batch [10/18] time 0.439 (0.530) data 0.000 (0.089) loss 0.2312 (0.4841) acc 96.8750 (87.1875) lr 1.9511e-02 eta 0:28:22\n",
            "epoch [22/200] batch [15/18] time 0.436 (0.500) data 0.000 (0.060) loss 0.2632 (0.4640) acc 90.6250 (86.8750) lr 1.9511e-02 eta 0:26:43\n",
            "epoch [23/200] batch [5/18] time 0.451 (0.613) data 0.000 (0.172) loss 0.3717 (0.3950) acc 84.3750 (89.3750) lr 1.9461e-02 eta 0:32:40\n",
            "epoch [23/200] batch [10/18] time 0.439 (0.526) data 0.000 (0.086) loss 0.4155 (0.4218) acc 90.6250 (88.7500) lr 1.9461e-02 eta 0:28:01\n",
            "epoch [23/200] batch [15/18] time 0.439 (0.498) data 0.000 (0.058) loss 0.4140 (0.4258) acc 84.3750 (89.1667) lr 1.9461e-02 eta 0:26:27\n",
            "epoch [24/200] batch [5/18] time 0.444 (0.600) data 0.000 (0.162) loss 0.4067 (0.5261) acc 93.7500 (87.5000) lr 1.9409e-02 eta 0:31:49\n",
            "epoch [24/200] batch [10/18] time 0.443 (0.521) data 0.000 (0.081) loss 0.5220 (0.4458) acc 90.6250 (89.0625) lr 1.9409e-02 eta 0:27:33\n",
            "epoch [24/200] batch [15/18] time 0.439 (0.494) data 0.000 (0.054) loss 0.3716 (0.4844) acc 90.6250 (86.6667) lr 1.9409e-02 eta 0:26:05\n",
            "epoch [25/200] batch [5/18] time 0.451 (0.618) data 0.000 (0.178) loss 0.6902 (0.4910) acc 84.3750 (86.8750) lr 1.9354e-02 eta 0:32:34\n",
            "epoch [25/200] batch [10/18] time 0.441 (0.529) data 0.000 (0.089) loss 0.3251 (0.4711) acc 90.6250 (87.8125) lr 1.9354e-02 eta 0:27:49\n",
            "epoch [25/200] batch [15/18] time 0.439 (0.499) data 0.000 (0.060) loss 0.2646 (0.4282) acc 93.7500 (88.1250) lr 1.9354e-02 eta 0:26:11\n",
            "epoch [26/200] batch [5/18] time 0.450 (0.613) data 0.000 (0.174) loss 0.3471 (0.3701) acc 84.3750 (87.5000) lr 1.9298e-02 eta 0:32:08\n",
            "epoch [26/200] batch [10/18] time 0.437 (0.525) data 0.000 (0.087) loss 0.6174 (0.4216) acc 75.0000 (86.2500) lr 1.9298e-02 eta 0:27:29\n",
            "epoch [26/200] batch [15/18] time 0.440 (0.496) data 0.000 (0.058) loss 0.2312 (0.4242) acc 90.6250 (87.2917) lr 1.9298e-02 eta 0:25:55\n",
            "epoch [27/200] batch [5/18] time 0.446 (0.617) data 0.000 (0.180) loss 0.4009 (0.3179) acc 90.6250 (92.5000) lr 1.9239e-02 eta 0:32:07\n",
            "epoch [27/200] batch [10/18] time 0.439 (0.526) data 0.000 (0.090) loss 0.3715 (0.3688) acc 84.3750 (89.6875) lr 1.9239e-02 eta 0:27:23\n",
            "epoch [27/200] batch [15/18] time 0.437 (0.497) data 0.000 (0.060) loss 0.7054 (0.3817) acc 84.3750 (89.5833) lr 1.9239e-02 eta 0:25:50\n",
            "epoch [28/200] batch [5/18] time 0.445 (0.616) data 0.000 (0.179) loss 0.2584 (0.4266) acc 90.6250 (88.7500) lr 1.9178e-02 eta 0:31:54\n",
            "epoch [28/200] batch [10/18] time 0.436 (0.526) data 0.000 (0.090) loss 0.6641 (0.4439) acc 81.2500 (88.4375) lr 1.9178e-02 eta 0:27:14\n",
            "epoch [28/200] batch [15/18] time 0.435 (0.497) data 0.000 (0.060) loss 0.2400 (0.4386) acc 90.6250 (87.9167) lr 1.9178e-02 eta 0:25:38\n",
            "epoch [29/200] batch [5/18] time 0.451 (0.619) data 0.000 (0.182) loss 0.7003 (0.4494) acc 81.2500 (88.7500) lr 1.9114e-02 eta 0:31:54\n",
            "epoch [29/200] batch [10/18] time 0.438 (0.528) data 0.000 (0.091) loss 0.4911 (0.4311) acc 90.6250 (89.0625) lr 1.9114e-02 eta 0:27:09\n",
            "epoch [29/200] batch [15/18] time 0.440 (0.499) data 0.000 (0.061) loss 0.5339 (0.3988) acc 78.1250 (89.3750) lr 1.9114e-02 eta 0:25:36\n",
            "epoch [30/200] batch [5/18] time 0.447 (0.609) data 0.000 (0.170) loss 0.2768 (0.4214) acc 93.7500 (89.3750) lr 1.9048e-02 eta 0:31:11\n",
            "epoch [30/200] batch [10/18] time 0.441 (0.524) data 0.000 (0.085) loss 0.2967 (0.3997) acc 90.6250 (89.0625) lr 1.9048e-02 eta 0:26:46\n",
            "epoch [30/200] batch [15/18] time 0.439 (0.495) data 0.000 (0.057) loss 0.2870 (0.3700) acc 96.8750 (90.8333) lr 1.9048e-02 eta 0:25:15\n",
            "epoch [31/200] batch [5/18] time 0.446 (0.613) data 0.000 (0.175) loss 0.5924 (0.3531) acc 81.2500 (90.0000) lr 1.8980e-02 eta 0:31:12\n",
            "epoch [31/200] batch [10/18] time 0.441 (0.525) data 0.000 (0.088) loss 0.3839 (0.4314) acc 84.3750 (87.8125) lr 1.8980e-02 eta 0:26:41\n",
            "epoch [31/200] batch [15/18] time 0.442 (0.496) data 0.000 (0.059) loss 0.4011 (0.3940) acc 87.5000 (88.7500) lr 1.8980e-02 eta 0:25:10\n",
            "epoch [32/200] batch [5/18] time 0.450 (0.608) data 0.000 (0.168) loss 0.3065 (0.3352) acc 90.6250 (90.6250) lr 1.8910e-02 eta 0:30:45\n",
            "epoch [32/200] batch [10/18] time 0.434 (0.523) data 0.000 (0.084) loss 0.2084 (0.2919) acc 93.7500 (93.1250) lr 1.8910e-02 eta 0:26:24\n",
            "epoch [32/200] batch [15/18] time 0.439 (0.495) data 0.000 (0.056) loss 0.1933 (0.2847) acc 96.8750 (92.9167) lr 1.8910e-02 eta 0:24:57\n",
            "epoch [33/200] batch [5/18] time 0.448 (0.621) data 0.000 (0.182) loss 0.4274 (0.4083) acc 84.3750 (88.7500) lr 1.8838e-02 eta 0:31:15\n",
            "epoch [33/200] batch [10/18] time 0.440 (0.530) data 0.000 (0.091) loss 0.5161 (0.4507) acc 84.3750 (87.5000) lr 1.8838e-02 eta 0:26:37\n",
            "epoch [33/200] batch [15/18] time 0.436 (0.499) data 0.000 (0.061) loss 0.2663 (0.3854) acc 93.7500 (89.5833) lr 1.8838e-02 eta 0:25:02\n",
            "epoch [34/200] batch [5/18] time 0.446 (0.611) data 0.000 (0.173) loss 0.3242 (0.3541) acc 93.7500 (91.8750) lr 1.8763e-02 eta 0:30:34\n",
            "epoch [34/200] batch [10/18] time 0.441 (0.526) data 0.000 (0.087) loss 0.4991 (0.3182) acc 87.5000 (93.4375) lr 1.8763e-02 eta 0:26:15\n",
            "epoch [34/200] batch [15/18] time 0.440 (0.497) data 0.000 (0.058) loss 0.5649 (0.3961) acc 84.3750 (90.0000) lr 1.8763e-02 eta 0:24:47\n",
            "epoch [35/200] batch [5/18] time 0.450 (0.622) data 0.000 (0.183) loss 0.2833 (0.3400) acc 93.7500 (91.2500) lr 1.8686e-02 eta 0:30:55\n",
            "epoch [35/200] batch [10/18] time 0.438 (0.531) data 0.000 (0.092) loss 0.3331 (0.3653) acc 87.5000 (90.0000) lr 1.8686e-02 eta 0:26:20\n",
            "epoch [35/200] batch [15/18] time 0.440 (0.500) data 0.000 (0.061) loss 0.5670 (0.3720) acc 84.3750 (90.0000) lr 1.8686e-02 eta 0:24:47\n",
            "epoch [36/200] batch [5/18] time 0.447 (0.622) data 0.000 (0.183) loss 0.6294 (0.4502) acc 84.3750 (89.3750) lr 1.8607e-02 eta 0:30:44\n",
            "epoch [36/200] batch [10/18] time 0.443 (0.531) data 0.000 (0.092) loss 0.4369 (0.4057) acc 90.6250 (90.3125) lr 1.8607e-02 eta 0:26:12\n",
            "epoch [36/200] batch [15/18] time 0.438 (0.501) data 0.000 (0.061) loss 0.1659 (0.3994) acc 93.7500 (90.2083) lr 1.8607e-02 eta 0:24:40\n",
            "epoch [37/200] batch [5/18] time 0.451 (0.612) data 0.000 (0.172) loss 0.2677 (0.2232) acc 93.7500 (95.6250) lr 1.8526e-02 eta 0:30:03\n",
            "epoch [37/200] batch [10/18] time 0.442 (0.526) data 0.000 (0.086) loss 0.4465 (0.3065) acc 90.6250 (93.7500) lr 1.8526e-02 eta 0:25:47\n",
            "epoch [37/200] batch [15/18] time 0.439 (0.497) data 0.000 (0.057) loss 0.1489 (0.3336) acc 100.0000 (92.0833) lr 1.8526e-02 eta 0:24:19\n",
            "epoch [38/200] batch [5/18] time 0.446 (0.626) data 0.000 (0.188) loss 0.5055 (0.3859) acc 90.6250 (90.6250) lr 1.8443e-02 eta 0:30:33\n",
            "epoch [38/200] batch [10/18] time 0.440 (0.533) data 0.000 (0.094) loss 0.2490 (0.3733) acc 90.6250 (89.6875) lr 1.8443e-02 eta 0:25:57\n",
            "epoch [38/200] batch [15/18] time 0.438 (0.501) data 0.000 (0.063) loss 0.2530 (0.3453) acc 90.6250 (91.0417) lr 1.8443e-02 eta 0:24:23\n",
            "epoch [39/200] batch [5/18] time 0.446 (0.619) data 0.000 (0.181) loss 0.4210 (0.2642) acc 84.3750 (95.0000) lr 1.8358e-02 eta 0:30:01\n",
            "epoch [39/200] batch [10/18] time 0.441 (0.529) data 0.000 (0.090) loss 0.3140 (0.2596) acc 90.6250 (93.1250) lr 1.8358e-02 eta 0:25:36\n",
            "epoch [39/200] batch [15/18] time 0.439 (0.499) data 0.000 (0.060) loss 0.1448 (0.2621) acc 100.0000 (93.3333) lr 1.8358e-02 eta 0:24:07\n",
            "epoch [40/200] batch [5/18] time 0.452 (0.598) data 0.000 (0.157) loss 0.5942 (0.3081) acc 81.2500 (93.1250) lr 1.8271e-02 eta 0:28:48\n",
            "epoch [40/200] batch [10/18] time 0.444 (0.518) data 0.000 (0.079) loss 0.4784 (0.3178) acc 87.5000 (92.8125) lr 1.8271e-02 eta 0:24:56\n",
            "epoch [40/200] batch [15/18] time 0.433 (0.492) data 0.000 (0.053) loss 0.3785 (0.3499) acc 93.7500 (91.2500) lr 1.8271e-02 eta 0:23:38\n",
            "epoch [41/200] batch [5/18] time 0.448 (0.615) data 0.000 (0.175) loss 0.2474 (0.3394) acc 93.7500 (90.0000) lr 1.8181e-02 eta 0:29:27\n",
            "epoch [41/200] batch [10/18] time 0.440 (0.527) data 0.000 (0.088) loss 0.5004 (0.3996) acc 87.5000 (88.7500) lr 1.8181e-02 eta 0:25:11\n",
            "epoch [41/200] batch [15/18] time 0.442 (0.498) data 0.000 (0.059) loss 0.2147 (0.3393) acc 96.8750 (91.2500) lr 1.8181e-02 eta 0:23:47\n",
            "epoch [42/200] batch [5/18] time 0.451 (0.621) data 0.000 (0.181) loss 0.3375 (0.3427) acc 93.7500 (90.6250) lr 1.8090e-02 eta 0:29:34\n",
            "epoch [42/200] batch [10/18] time 0.439 (0.530) data 0.000 (0.091) loss 0.4829 (0.3943) acc 87.5000 (89.3750) lr 1.8090e-02 eta 0:25:11\n",
            "epoch [42/200] batch [15/18] time 0.439 (0.499) data 0.000 (0.060) loss 0.1943 (0.3834) acc 93.7500 (90.2083) lr 1.8090e-02 eta 0:23:41\n",
            "epoch [43/200] batch [5/18] time 0.449 (0.623) data 0.000 (0.182) loss 0.1029 (0.2739) acc 100.0000 (93.7500) lr 1.7997e-02 eta 0:29:28\n",
            "epoch [43/200] batch [10/18] time 0.438 (0.531) data 0.000 (0.091) loss 0.3196 (0.3436) acc 93.7500 (89.0625) lr 1.7997e-02 eta 0:25:06\n",
            "epoch [43/200] batch [15/18] time 0.437 (0.501) data 0.000 (0.061) loss 0.2817 (0.3154) acc 93.7500 (90.6250) lr 1.7997e-02 eta 0:23:37\n",
            "epoch [44/200] batch [5/18] time 0.450 (0.609) data 0.000 (0.169) loss 0.5893 (0.3175) acc 78.1250 (90.6250) lr 1.7902e-02 eta 0:28:38\n",
            "epoch [44/200] batch [10/18] time 0.441 (0.524) data 0.000 (0.084) loss 0.4313 (0.3521) acc 87.5000 (90.0000) lr 1.7902e-02 eta 0:24:35\n",
            "epoch [44/200] batch [15/18] time 0.439 (0.497) data 0.000 (0.056) loss 0.5360 (0.3671) acc 84.3750 (88.9583) lr 1.7902e-02 eta 0:23:15\n",
            "epoch [45/200] batch [5/18] time 0.450 (0.613) data 0.000 (0.173) loss 0.3699 (0.2523) acc 90.6250 (94.3750) lr 1.7804e-02 eta 0:28:38\n",
            "epoch [45/200] batch [10/18] time 0.439 (0.526) data 0.000 (0.087) loss 0.1868 (0.2412) acc 93.7500 (94.0625) lr 1.7804e-02 eta 0:24:31\n",
            "epoch [45/200] batch [15/18] time 0.436 (0.497) data 0.000 (0.058) loss 0.2552 (0.2780) acc 93.7500 (93.3333) lr 1.7804e-02 eta 0:23:07\n",
            "epoch [46/200] batch [5/18] time 0.452 (0.609) data 0.000 (0.169) loss 0.1740 (0.2599) acc 90.6250 (91.2500) lr 1.7705e-02 eta 0:28:15\n",
            "epoch [46/200] batch [10/18] time 0.442 (0.524) data 0.000 (0.085) loss 0.3810 (0.2848) acc 90.6250 (89.6875) lr 1.7705e-02 eta 0:24:16\n",
            "epoch [46/200] batch [15/18] time 0.438 (0.495) data 0.000 (0.057) loss 0.3627 (0.2865) acc 90.6250 (90.8333) lr 1.7705e-02 eta 0:22:54\n",
            "epoch [47/200] batch [5/18] time 0.449 (0.615) data 0.000 (0.175) loss 0.3635 (0.3610) acc 90.6250 (90.0000) lr 1.7604e-02 eta 0:28:21\n",
            "epoch [47/200] batch [10/18] time 0.439 (0.527) data 0.000 (0.088) loss 0.4990 (0.3651) acc 84.3750 (90.0000) lr 1.7604e-02 eta 0:24:15\n",
            "epoch [47/200] batch [15/18] time 0.439 (0.498) data 0.000 (0.059) loss 0.3096 (0.3125) acc 93.7500 (91.6667) lr 1.7604e-02 eta 0:22:52\n",
            "epoch [48/200] batch [5/18] time 0.448 (0.630) data 0.000 (0.192) loss 0.2524 (0.2357) acc 90.6250 (93.7500) lr 1.7501e-02 eta 0:28:51\n",
            "epoch [48/200] batch [10/18] time 0.439 (0.535) data 0.000 (0.096) loss 0.2873 (0.2757) acc 90.6250 (91.5625) lr 1.7501e-02 eta 0:24:27\n",
            "epoch [48/200] batch [15/18] time 0.439 (0.503) data 0.000 (0.064) loss 0.5044 (0.3121) acc 84.3750 (91.0417) lr 1.7501e-02 eta 0:22:58\n",
            "epoch [49/200] batch [5/18] time 0.447 (0.601) data 0.000 (0.161) loss 0.4833 (0.3691) acc 87.5000 (90.0000) lr 1.7396e-02 eta 0:27:20\n",
            "epoch [49/200] batch [10/18] time 0.442 (0.521) data 0.000 (0.081) loss 0.3002 (0.3797) acc 90.6250 (89.0625) lr 1.7396e-02 eta 0:23:39\n",
            "epoch [49/200] batch [15/18] time 0.442 (0.494) data 0.000 (0.054) loss 0.5018 (0.3640) acc 78.1250 (89.3750) lr 1.7396e-02 eta 0:22:23\n",
            "epoch [50/200] batch [5/18] time 0.450 (0.614) data 0.000 (0.175) loss 0.0770 (0.3118) acc 100.0000 (91.8750) lr 1.7290e-02 eta 0:27:46\n",
            "epoch [50/200] batch [10/18] time 0.439 (0.527) data 0.000 (0.088) loss 0.1555 (0.3297) acc 96.8750 (90.0000) lr 1.7290e-02 eta 0:23:45\n",
            "epoch [50/200] batch [15/18] time 0.437 (0.497) data 0.000 (0.059) loss 0.3965 (0.3456) acc 87.5000 (89.5833) lr 1.7290e-02 eta 0:22:23\n",
            "epoch [51/200] batch [5/18] time 0.446 (0.625) data 0.000 (0.187) loss 0.1274 (0.3343) acc 96.8750 (90.6250) lr 1.7181e-02 eta 0:28:03\n",
            "epoch [51/200] batch [10/18] time 0.438 (0.531) data 0.000 (0.093) loss 0.5191 (0.3534) acc 84.3750 (89.3750) lr 1.7181e-02 eta 0:23:49\n",
            "epoch [51/200] batch [15/18] time 0.434 (0.500) data 0.000 (0.062) loss 0.1525 (0.3654) acc 100.0000 (89.7917) lr 1.7181e-02 eta 0:22:23\n",
            "epoch [52/200] batch [5/18] time 0.447 (0.614) data 0.000 (0.175) loss 0.2673 (0.2432) acc 93.7500 (94.3750) lr 1.7071e-02 eta 0:27:22\n",
            "epoch [52/200] batch [10/18] time 0.439 (0.526) data 0.000 (0.088) loss 0.2419 (0.2318) acc 90.6250 (94.6875) lr 1.7071e-02 eta 0:23:25\n",
            "epoch [52/200] batch [15/18] time 0.435 (0.497) data 0.000 (0.059) loss 0.0890 (0.2629) acc 100.0000 (93.7500) lr 1.7071e-02 eta 0:22:05\n",
            "epoch [53/200] batch [5/18] time 0.450 (0.617) data 0.000 (0.180) loss 0.4148 (0.2522) acc 90.6250 (93.1250) lr 1.6959e-02 eta 0:27:21\n",
            "epoch [53/200] batch [10/18] time 0.440 (0.528) data 0.000 (0.090) loss 0.2521 (0.2919) acc 90.6250 (91.8750) lr 1.6959e-02 eta 0:23:21\n",
            "epoch [53/200] batch [15/18] time 0.436 (0.498) data 0.000 (0.060) loss 0.1893 (0.2995) acc 96.8750 (92.5000) lr 1.6959e-02 eta 0:22:00\n",
            "epoch [54/200] batch [5/18] time 0.449 (0.626) data 0.000 (0.186) loss 0.3440 (0.2629) acc 90.6250 (91.8750) lr 1.6845e-02 eta 0:27:32\n",
            "epoch [54/200] batch [10/18] time 0.440 (0.532) data 0.000 (0.093) loss 0.3078 (0.2965) acc 84.3750 (90.9375) lr 1.6845e-02 eta 0:23:23\n",
            "epoch [54/200] batch [15/18] time 0.442 (0.502) data 0.000 (0.062) loss 0.1818 (0.3023) acc 100.0000 (91.4583) lr 1.6845e-02 eta 0:22:00\n",
            "epoch [55/200] batch [5/18] time 0.447 (0.611) data 0.000 (0.173) loss 0.1140 (0.2573) acc 100.0000 (95.0000) lr 1.6730e-02 eta 0:26:43\n",
            "epoch [55/200] batch [10/18] time 0.441 (0.524) data 0.000 (0.086) loss 0.7197 (0.2863) acc 84.3750 (93.7500) lr 1.6730e-02 eta 0:22:52\n",
            "epoch [55/200] batch [15/18] time 0.438 (0.496) data 0.000 (0.058) loss 0.1590 (0.2643) acc 100.0000 (94.1667) lr 1.6730e-02 eta 0:21:36\n",
            "epoch [56/200] batch [5/18] time 0.447 (0.613) data 0.000 (0.175) loss 0.2704 (0.2406) acc 93.7500 (94.3750) lr 1.6613e-02 eta 0:26:37\n",
            "epoch [56/200] batch [10/18] time 0.441 (0.527) data 0.000 (0.088) loss 0.3519 (0.2709) acc 90.6250 (93.4375) lr 1.6613e-02 eta 0:22:49\n",
            "epoch [56/200] batch [15/18] time 0.441 (0.498) data 0.000 (0.059) loss 0.1727 (0.2905) acc 96.8750 (92.9167) lr 1.6613e-02 eta 0:21:31\n",
            "epoch [57/200] batch [5/18] time 0.453 (0.620) data 0.000 (0.180) loss 0.2835 (0.2943) acc 93.7500 (91.2500) lr 1.6494e-02 eta 0:26:44\n",
            "epoch [57/200] batch [10/18] time 0.437 (0.529) data 0.000 (0.090) loss 0.2971 (0.2974) acc 87.5000 (91.8750) lr 1.6494e-02 eta 0:22:46\n",
            "epoch [57/200] batch [15/18] time 0.442 (0.500) data 0.000 (0.060) loss 0.2156 (0.2952) acc 93.7500 (92.2917) lr 1.6494e-02 eta 0:21:27\n",
            "epoch [58/200] batch [5/18] time 0.450 (0.613) data 0.000 (0.176) loss 0.1468 (0.2432) acc 100.0000 (95.6250) lr 1.6374e-02 eta 0:26:15\n",
            "epoch [58/200] batch [10/18] time 0.437 (0.527) data 0.000 (0.088) loss 0.2647 (0.2524) acc 93.7500 (94.0625) lr 1.6374e-02 eta 0:22:30\n",
            "epoch [58/200] batch [15/18] time 0.439 (0.498) data 0.000 (0.059) loss 0.3232 (0.2469) acc 93.7500 (93.7500) lr 1.6374e-02 eta 0:21:13\n",
            "epoch [59/200] batch [5/18] time 0.452 (0.604) data 0.000 (0.164) loss 0.5233 (0.3339) acc 81.2500 (90.6250) lr 1.6252e-02 eta 0:25:41\n",
            "epoch [59/200] batch [10/18] time 0.439 (0.522) data 0.000 (0.082) loss 0.3253 (0.3251) acc 90.6250 (91.2500) lr 1.6252e-02 eta 0:22:08\n",
            "epoch [59/200] batch [15/18] time 0.434 (0.494) data 0.000 (0.055) loss 0.2565 (0.3317) acc 90.6250 (90.8333) lr 1.6252e-02 eta 0:20:55\n",
            "epoch [60/200] batch [5/18] time 0.449 (0.610) data 0.000 (0.171) loss 0.2149 (0.2358) acc 96.8750 (93.7500) lr 1.6129e-02 eta 0:25:44\n",
            "epoch [60/200] batch [10/18] time 0.443 (0.525) data 0.000 (0.086) loss 0.2586 (0.2479) acc 93.7500 (93.7500) lr 1.6129e-02 eta 0:22:06\n",
            "epoch [60/200] batch [15/18] time 0.436 (0.497) data 0.000 (0.057) loss 0.2135 (0.2302) acc 96.8750 (94.5833) lr 1.6129e-02 eta 0:20:52\n",
            "epoch [61/200] batch [5/18] time 0.450 (0.617) data 0.001 (0.178) loss 0.2034 (0.2193) acc 96.8750 (92.5000) lr 1.6004e-02 eta 0:25:52\n",
            "epoch [61/200] batch [10/18] time 0.436 (0.528) data 0.000 (0.089) loss 0.1915 (0.2645) acc 93.7500 (92.1875) lr 1.6004e-02 eta 0:22:05\n",
            "epoch [61/200] batch [15/18] time 0.438 (0.498) data 0.000 (0.060) loss 0.3179 (0.2690) acc 93.7500 (93.1250) lr 1.6004e-02 eta 0:20:48\n",
            "epoch [62/200] batch [5/18] time 0.447 (0.613) data 0.000 (0.175) loss 0.1950 (0.3042) acc 90.6250 (90.6250) lr 1.5878e-02 eta 0:25:31\n",
            "epoch [62/200] batch [10/18] time 0.435 (0.525) data 0.000 (0.088) loss 0.1269 (0.2624) acc 96.8750 (92.1875) lr 1.5878e-02 eta 0:21:49\n",
            "epoch [62/200] batch [15/18] time 0.438 (0.497) data 0.000 (0.058) loss 0.5288 (0.3126) acc 87.5000 (91.2500) lr 1.5878e-02 eta 0:20:35\n",
            "epoch [63/200] batch [5/18] time 0.445 (0.614) data 0.000 (0.178) loss 0.2747 (0.2685) acc 93.7500 (92.5000) lr 1.5750e-02 eta 0:25:23\n",
            "epoch [63/200] batch [10/18] time 0.437 (0.527) data 0.000 (0.089) loss 0.3723 (0.2982) acc 90.6250 (91.8750) lr 1.5750e-02 eta 0:21:42\n",
            "epoch [63/200] batch [15/18] time 0.440 (0.497) data 0.000 (0.059) loss 0.4202 (0.2977) acc 84.3750 (91.0417) lr 1.5750e-02 eta 0:20:27\n",
            "epoch [64/200] batch [5/18] time 0.450 (0.616) data 0.000 (0.177) loss 0.3250 (0.2963) acc 90.6250 (92.5000) lr 1.5621e-02 eta 0:25:15\n",
            "epoch [64/200] batch [10/18] time 0.436 (0.527) data 0.000 (0.088) loss 0.1792 (0.2454) acc 93.7500 (93.7500) lr 1.5621e-02 eta 0:21:33\n",
            "epoch [64/200] batch [15/18] time 0.433 (0.497) data 0.000 (0.059) loss 0.2197 (0.2641) acc 96.8750 (93.3333) lr 1.5621e-02 eta 0:20:17\n",
            "epoch [65/200] batch [5/18] time 0.450 (0.603) data 0.000 (0.165) loss 0.3227 (0.3195) acc 90.6250 (91.2500) lr 1.5490e-02 eta 0:24:32\n",
            "epoch [65/200] batch [10/18] time 0.440 (0.521) data 0.000 (0.083) loss 0.3578 (0.3070) acc 90.6250 (92.8125) lr 1.5490e-02 eta 0:21:10\n",
            "epoch [65/200] batch [15/18] time 0.439 (0.493) data 0.000 (0.055) loss 0.4520 (0.2853) acc 93.7500 (93.7500) lr 1.5490e-02 eta 0:19:59\n",
            "epoch [66/200] batch [5/18] time 0.446 (0.609) data 0.000 (0.171) loss 0.1320 (0.3072) acc 93.7500 (91.2500) lr 1.5358e-02 eta 0:24:35\n",
            "epoch [66/200] batch [10/18] time 0.438 (0.523) data 0.000 (0.086) loss 0.2569 (0.2703) acc 93.7500 (92.5000) lr 1.5358e-02 eta 0:21:05\n",
            "epoch [66/200] batch [15/18] time 0.433 (0.494) data 0.000 (0.057) loss 0.2305 (0.2585) acc 93.7500 (92.7083) lr 1.5358e-02 eta 0:19:53\n",
            "epoch [67/200] batch [5/18] time 0.448 (0.606) data 0.000 (0.168) loss 0.1966 (0.2429) acc 93.7500 (93.7500) lr 1.5225e-02 eta 0:24:19\n",
            "epoch [67/200] batch [10/18] time 0.436 (0.522) data 0.000 (0.084) loss 0.3075 (0.2831) acc 90.6250 (92.1875) lr 1.5225e-02 eta 0:20:53\n",
            "epoch [67/200] batch [15/18] time 0.439 (0.494) data 0.000 (0.056) loss 0.1443 (0.2561) acc 96.8750 (93.1250) lr 1.5225e-02 eta 0:19:44\n",
            "epoch [68/200] batch [5/18] time 0.445 (0.607) data 0.000 (0.169) loss 0.0896 (0.2739) acc 100.0000 (93.1250) lr 1.5090e-02 eta 0:24:10\n",
            "epoch [68/200] batch [10/18] time 0.439 (0.522) data 0.000 (0.085) loss 0.2823 (0.2241) acc 93.7500 (95.3125) lr 1.5090e-02 eta 0:20:45\n",
            "epoch [68/200] batch [15/18] time 0.439 (0.495) data 0.000 (0.057) loss 0.1188 (0.2317) acc 100.0000 (95.2083) lr 1.5090e-02 eta 0:19:36\n",
            "epoch [69/200] batch [5/18] time 0.447 (0.619) data 0.000 (0.181) loss 0.1767 (0.2880) acc 96.8750 (92.5000) lr 1.4955e-02 eta 0:24:27\n",
            "epoch [69/200] batch [10/18] time 0.438 (0.528) data 0.000 (0.090) loss 0.3830 (0.2812) acc 93.7500 (93.4375) lr 1.4955e-02 eta 0:20:49\n",
            "epoch [69/200] batch [15/18] time 0.442 (0.499) data 0.000 (0.060) loss 0.2842 (0.2870) acc 90.6250 (92.2917) lr 1.4955e-02 eta 0:19:36\n",
            "epoch [70/200] batch [5/18] time 0.452 (0.611) data 0.000 (0.171) loss 0.4743 (0.3718) acc 90.6250 (87.5000) lr 1.4818e-02 eta 0:23:56\n",
            "epoch [70/200] batch [10/18] time 0.442 (0.525) data 0.000 (0.085) loss 0.1306 (0.2919) acc 100.0000 (91.5625) lr 1.4818e-02 eta 0:20:32\n",
            "epoch [70/200] batch [15/18] time 0.437 (0.496) data 0.000 (0.057) loss 0.2700 (0.3229) acc 93.7500 (90.8333) lr 1.4818e-02 eta 0:19:21\n",
            "epoch [71/200] batch [5/18] time 0.446 (0.603) data 0.000 (0.165) loss 0.0467 (0.2060) acc 100.0000 (94.3750) lr 1.4679e-02 eta 0:23:28\n",
            "epoch [71/200] batch [10/18] time 0.441 (0.521) data 0.000 (0.083) loss 0.3640 (0.2873) acc 93.7500 (92.8125) lr 1.4679e-02 eta 0:20:14\n",
            "epoch [71/200] batch [15/18] time 0.438 (0.494) data 0.000 (0.055) loss 0.3249 (0.2962) acc 87.5000 (91.4583) lr 1.4679e-02 eta 0:19:07\n",
            "epoch [72/200] batch [5/18] time 0.446 (0.628) data 0.000 (0.191) loss 0.3685 (0.2970) acc 93.7500 (93.7500) lr 1.4540e-02 eta 0:24:14\n",
            "epoch [72/200] batch [10/18] time 0.442 (0.533) data 0.000 (0.095) loss 0.2453 (0.3412) acc 96.8750 (92.8125) lr 1.4540e-02 eta 0:20:33\n",
            "epoch [72/200] batch [15/18] time 0.438 (0.502) data 0.000 (0.064) loss 0.4409 (0.3333) acc 87.5000 (92.9167) lr 1.4540e-02 eta 0:19:17\n",
            "epoch [73/200] batch [5/18] time 0.453 (0.609) data 0.000 (0.170) loss 0.5818 (0.3427) acc 84.3750 (90.6250) lr 1.4399e-02 eta 0:23:19\n",
            "epoch [73/200] batch [10/18] time 0.439 (0.525) data 0.000 (0.085) loss 0.2973 (0.3298) acc 93.7500 (91.8750) lr 1.4399e-02 eta 0:20:03\n",
            "epoch [73/200] batch [15/18] time 0.438 (0.496) data 0.000 (0.057) loss 0.2892 (0.2847) acc 90.6250 (93.1250) lr 1.4399e-02 eta 0:18:55\n",
            "epoch [74/200] batch [5/18] time 0.442 (0.623) data 0.000 (0.185) loss 0.1168 (0.2281) acc 100.0000 (94.3750) lr 1.4258e-02 eta 0:23:41\n",
            "epoch [74/200] batch [10/18] time 0.440 (0.531) data 0.000 (0.093) loss 0.2674 (0.2600) acc 87.5000 (93.4375) lr 1.4258e-02 eta 0:20:08\n",
            "epoch [74/200] batch [15/18] time 0.436 (0.500) data 0.000 (0.062) loss 0.1373 (0.2478) acc 96.8750 (94.1667) lr 1.4258e-02 eta 0:18:56\n",
            "epoch [75/200] batch [5/18] time 0.445 (0.606) data 0.000 (0.169) loss 0.0957 (0.1959) acc 100.0000 (95.0000) lr 1.4115e-02 eta 0:22:51\n",
            "epoch [75/200] batch [10/18] time 0.439 (0.522) data 0.000 (0.085) loss 0.4755 (0.2102) acc 87.5000 (94.6875) lr 1.4115e-02 eta 0:19:38\n",
            "epoch [75/200] batch [15/18] time 0.438 (0.494) data 0.000 (0.056) loss 0.1827 (0.2410) acc 93.7500 (93.1250) lr 1.4115e-02 eta 0:18:33\n",
            "epoch [76/200] batch [5/18] time 0.446 (0.615) data 0.000 (0.177) loss 0.4873 (0.3522) acc 84.3750 (91.2500) lr 1.3971e-02 eta 0:23:01\n",
            "epoch [76/200] batch [10/18] time 0.437 (0.527) data 0.000 (0.089) loss 0.0947 (0.2834) acc 96.8750 (92.8125) lr 1.3971e-02 eta 0:19:40\n",
            "epoch [76/200] batch [15/18] time 0.437 (0.498) data 0.000 (0.059) loss 0.1305 (0.2848) acc 100.0000 (92.9167) lr 1.3971e-02 eta 0:18:31\n",
            "epoch [77/200] batch [5/18] time 0.450 (0.619) data 0.000 (0.180) loss 0.2021 (0.2451) acc 93.7500 (91.8750) lr 1.3827e-02 eta 0:22:57\n",
            "epoch [77/200] batch [10/18] time 0.440 (0.529) data 0.000 (0.090) loss 0.1028 (0.2198) acc 100.0000 (93.4375) lr 1.3827e-02 eta 0:19:34\n",
            "epoch [77/200] batch [15/18] time 0.436 (0.499) data 0.000 (0.060) loss 0.1396 (0.2367) acc 96.8750 (93.1250) lr 1.3827e-02 eta 0:18:25\n",
            "epoch [78/200] batch [5/18] time 0.449 (0.621) data 0.000 (0.181) loss 0.1934 (0.2882) acc 93.7500 (91.2500) lr 1.3681e-02 eta 0:22:50\n",
            "epoch [78/200] batch [10/18] time 0.438 (0.530) data 0.000 (0.091) loss 0.3456 (0.3060) acc 90.6250 (91.5625) lr 1.3681e-02 eta 0:19:28\n",
            "epoch [78/200] batch [15/18] time 0.439 (0.500) data 0.000 (0.061) loss 0.4125 (0.3041) acc 87.5000 (91.2500) lr 1.3681e-02 eta 0:18:19\n",
            "epoch [79/200] batch [5/18] time 0.450 (0.613) data 0.000 (0.172) loss 0.4178 (0.3608) acc 90.6250 (90.0000) lr 1.3535e-02 eta 0:22:23\n",
            "epoch [79/200] batch [10/18] time 0.441 (0.526) data 0.000 (0.086) loss 0.2974 (0.2801) acc 90.6250 (92.8125) lr 1.3535e-02 eta 0:19:09\n",
            "epoch [79/200] batch [15/18] time 0.441 (0.497) data 0.000 (0.058) loss 0.1151 (0.2388) acc 100.0000 (93.9583) lr 1.3535e-02 eta 0:18:04\n",
            "epoch [80/200] batch [5/18] time 0.448 (0.625) data 0.000 (0.187) loss 0.2533 (0.1938) acc 93.7500 (96.8750) lr 1.3387e-02 eta 0:22:38\n",
            "epoch [80/200] batch [10/18] time 0.441 (0.532) data 0.000 (0.094) loss 0.1708 (0.2139) acc 96.8750 (95.6250) lr 1.3387e-02 eta 0:19:14\n",
            "epoch [80/200] batch [15/18] time 0.438 (0.501) data 0.000 (0.062) loss 0.4783 (0.2263) acc 84.3750 (94.7917) lr 1.3387e-02 eta 0:18:04\n",
            "epoch [81/200] batch [5/18] time 0.448 (0.613) data 0.000 (0.175) loss 0.0784 (0.1615) acc 96.8750 (95.6250) lr 1.3239e-02 eta 0:22:01\n",
            "epoch [81/200] batch [10/18] time 0.439 (0.526) data 0.000 (0.088) loss 0.2255 (0.1991) acc 93.7500 (94.6875) lr 1.3239e-02 eta 0:18:50\n",
            "epoch [81/200] batch [15/18] time 0.439 (0.497) data 0.000 (0.059) loss 0.3200 (0.2506) acc 93.7500 (93.1250) lr 1.3239e-02 eta 0:17:46\n",
            "epoch [82/200] batch [5/18] time 0.452 (0.605) data 0.000 (0.167) loss 0.2143 (0.2338) acc 93.7500 (95.0000) lr 1.3090e-02 eta 0:21:33\n",
            "epoch [82/200] batch [10/18] time 0.436 (0.522) data 0.000 (0.084) loss 0.2275 (0.2148) acc 90.6250 (94.6875) lr 1.3090e-02 eta 0:18:32\n",
            "epoch [82/200] batch [15/18] time 0.433 (0.494) data 0.000 (0.056) loss 0.3659 (0.2333) acc 93.7500 (94.3750) lr 1.3090e-02 eta 0:17:29\n",
            "epoch [83/200] batch [5/18] time 0.452 (0.613) data 0.000 (0.175) loss 0.3813 (0.2254) acc 90.6250 (92.5000) lr 1.2940e-02 eta 0:21:38\n",
            "epoch [83/200] batch [10/18] time 0.438 (0.525) data 0.000 (0.087) loss 0.4552 (0.2557) acc 84.3750 (92.5000) lr 1.2940e-02 eta 0:18:30\n",
            "epoch [83/200] batch [15/18] time 0.443 (0.497) data 0.000 (0.058) loss 0.2709 (0.2728) acc 96.8750 (92.5000) lr 1.2940e-02 eta 0:17:27\n",
            "epoch [84/200] batch [5/18] time 0.450 (0.615) data 0.000 (0.176) loss 0.1890 (0.2092) acc 90.6250 (92.5000) lr 1.2790e-02 eta 0:21:32\n",
            "epoch [84/200] batch [10/18] time 0.441 (0.527) data 0.000 (0.088) loss 0.2488 (0.2521) acc 93.7500 (92.5000) lr 1.2790e-02 eta 0:18:25\n",
            "epoch [84/200] batch [15/18] time 0.433 (0.498) data 0.000 (0.059) loss 0.4283 (0.2515) acc 84.3750 (91.6667) lr 1.2790e-02 eta 0:17:21\n",
            "epoch [85/200] batch [5/18] time 0.449 (0.625) data 0.000 (0.187) loss 0.3372 (0.2285) acc 93.7500 (93.7500) lr 1.2639e-02 eta 0:21:41\n",
            "epoch [85/200] batch [10/18] time 0.436 (0.532) data 0.000 (0.094) loss 0.1414 (0.2361) acc 96.8750 (93.4375) lr 1.2639e-02 eta 0:18:26\n",
            "epoch [85/200] batch [15/18] time 0.440 (0.502) data 0.000 (0.063) loss 0.4805 (0.2478) acc 84.3750 (93.1250) lr 1.2639e-02 eta 0:17:20\n",
            "epoch [86/200] batch [5/18] time 0.450 (0.626) data 0.000 (0.186) loss 0.3182 (0.3342) acc 93.7500 (90.0000) lr 1.2487e-02 eta 0:21:32\n",
            "epoch [86/200] batch [10/18] time 0.442 (0.532) data 0.000 (0.093) loss 0.2102 (0.2784) acc 93.7500 (92.5000) lr 1.2487e-02 eta 0:18:16\n",
            "epoch [86/200] batch [15/18] time 0.438 (0.502) data 0.000 (0.062) loss 0.1305 (0.2468) acc 93.7500 (93.1250) lr 1.2487e-02 eta 0:17:10\n",
            "epoch [87/200] batch [5/18] time 0.451 (0.626) data 0.000 (0.187) loss 0.0897 (0.1942) acc 100.0000 (96.2500) lr 1.2334e-02 eta 0:21:21\n",
            "epoch [87/200] batch [10/18] time 0.443 (0.532) data 0.000 (0.094) loss 0.1263 (0.1787) acc 100.0000 (96.8750) lr 1.2334e-02 eta 0:18:07\n",
            "epoch [87/200] batch [15/18] time 0.438 (0.501) data 0.000 (0.062) loss 0.6680 (0.2111) acc 81.2500 (96.0417) lr 1.2334e-02 eta 0:17:01\n",
            "epoch [88/200] batch [5/18] time 0.449 (0.603) data 0.000 (0.164) loss 0.1555 (0.1916) acc 93.7500 (95.0000) lr 1.2181e-02 eta 0:20:24\n",
            "epoch [88/200] batch [10/18] time 0.439 (0.521) data 0.000 (0.082) loss 0.0764 (0.2068) acc 100.0000 (95.6250) lr 1.2181e-02 eta 0:17:35\n",
            "epoch [88/200] batch [15/18] time 0.442 (0.494) data 0.000 (0.055) loss 0.6539 (0.2737) acc 84.3750 (93.3333) lr 1.2181e-02 eta 0:16:37\n",
            "epoch [89/200] batch [5/18] time 0.445 (0.620) data 0.000 (0.183) loss 0.4803 (0.3806) acc 87.5000 (88.7500) lr 1.2028e-02 eta 0:20:47\n",
            "epoch [89/200] batch [10/18] time 0.440 (0.529) data 0.000 (0.091) loss 0.1971 (0.2741) acc 96.8750 (92.8125) lr 1.2028e-02 eta 0:17:42\n",
            "epoch [89/200] batch [15/18] time 0.437 (0.499) data 0.000 (0.061) loss 0.0669 (0.2439) acc 100.0000 (93.9583) lr 1.2028e-02 eta 0:16:39\n",
            "epoch [90/200] batch [5/18] time 0.447 (0.616) data 0.000 (0.178) loss 0.1182 (0.1640) acc 96.8750 (95.6250) lr 1.1874e-02 eta 0:20:28\n",
            "epoch [90/200] batch [10/18] time 0.439 (0.528) data 0.000 (0.089) loss 0.1587 (0.1965) acc 90.6250 (94.6875) lr 1.1874e-02 eta 0:17:29\n",
            "epoch [90/200] batch [15/18] time 0.440 (0.498) data 0.000 (0.060) loss 0.1742 (0.1877) acc 93.7500 (95.0000) lr 1.1874e-02 eta 0:16:28\n",
            "epoch [91/200] batch [5/18] time 0.449 (0.614) data 0.000 (0.174) loss 0.2577 (0.2137) acc 90.6250 (95.0000) lr 1.1719e-02 eta 0:20:12\n",
            "epoch [91/200] batch [10/18] time 0.438 (0.525) data 0.000 (0.087) loss 0.3883 (0.2267) acc 90.6250 (94.3750) lr 1.1719e-02 eta 0:17:15\n",
            "epoch [91/200] batch [15/18] time 0.437 (0.496) data 0.000 (0.058) loss 0.2339 (0.2206) acc 90.6250 (94.1667) lr 1.1719e-02 eta 0:16:14\n",
            "epoch [92/200] batch [5/18] time 0.448 (0.605) data 0.000 (0.168) loss 0.4120 (0.2924) acc 90.6250 (91.8750) lr 1.1564e-02 eta 0:19:43\n",
            "epoch [92/200] batch [10/18] time 0.436 (0.520) data 0.000 (0.084) loss 0.4134 (0.2645) acc 87.5000 (92.8125) lr 1.1564e-02 eta 0:16:55\n",
            "epoch [92/200] batch [15/18] time 0.437 (0.493) data 0.000 (0.056) loss 0.3660 (0.2755) acc 87.5000 (93.3333) lr 1.1564e-02 eta 0:15:59\n",
            "epoch [93/200] batch [5/18] time 0.442 (0.617) data 0.000 (0.182) loss 0.1748 (0.1932) acc 96.8750 (94.3750) lr 1.1409e-02 eta 0:19:56\n",
            "epoch [93/200] batch [10/18] time 0.438 (0.525) data 0.000 (0.091) loss 0.1174 (0.1878) acc 100.0000 (95.3125) lr 1.1409e-02 eta 0:16:55\n",
            "epoch [93/200] batch [15/18] time 0.433 (0.495) data 0.000 (0.061) loss 0.2321 (0.2169) acc 96.8750 (94.5833) lr 1.1409e-02 eta 0:15:54\n",
            "epoch [94/200] batch [5/18] time 0.444 (0.603) data 0.000 (0.169) loss 0.1577 (0.3003) acc 96.8750 (93.7500) lr 1.1253e-02 eta 0:19:18\n",
            "epoch [94/200] batch [10/18] time 0.435 (0.519) data 0.000 (0.084) loss 0.2149 (0.2564) acc 90.6250 (93.4375) lr 1.1253e-02 eta 0:16:33\n",
            "epoch [94/200] batch [15/18] time 0.434 (0.491) data 0.000 (0.056) loss 0.3142 (0.2457) acc 90.6250 (92.9167) lr 1.1253e-02 eta 0:15:37\n",
            "epoch [95/200] batch [5/18] time 0.444 (0.615) data 0.000 (0.181) loss 0.2825 (0.2769) acc 96.8750 (93.7500) lr 1.1097e-02 eta 0:19:31\n",
            "epoch [95/200] batch [10/18] time 0.435 (0.523) data 0.000 (0.091) loss 0.0922 (0.2491) acc 96.8750 (92.8125) lr 1.1097e-02 eta 0:16:33\n",
            "epoch [95/200] batch [15/18] time 0.433 (0.493) data 0.000 (0.061) loss 0.6013 (0.2812) acc 90.6250 (92.0833) lr 1.1097e-02 eta 0:15:33\n",
            "epoch [96/200] batch [5/18] time 0.438 (0.620) data 0.000 (0.188) loss 0.2871 (0.2396) acc 93.7500 (93.7500) lr 1.0941e-02 eta 0:19:28\n",
            "epoch [96/200] batch [10/18] time 0.432 (0.526) data 0.000 (0.094) loss 0.0964 (0.2228) acc 100.0000 (93.4375) lr 1.0941e-02 eta 0:16:28\n",
            "epoch [96/200] batch [15/18] time 0.434 (0.494) data 0.000 (0.063) loss 0.2774 (0.2308) acc 93.7500 (93.5417) lr 1.0941e-02 eta 0:15:26\n",
            "epoch [97/200] batch [5/18] time 0.439 (0.604) data 0.000 (0.174) loss 0.1661 (0.2557) acc 96.8750 (94.3750) lr 1.0785e-02 eta 0:18:48\n",
            "epoch [97/200] batch [10/18] time 0.437 (0.517) data 0.000 (0.087) loss 0.1494 (0.2069) acc 93.7500 (95.6250) lr 1.0785e-02 eta 0:16:02\n",
            "epoch [97/200] batch [15/18] time 0.433 (0.488) data 0.000 (0.058) loss 0.1214 (0.1970) acc 96.8750 (95.4167) lr 1.0785e-02 eta 0:15:06\n",
            "epoch [98/200] batch [5/18] time 0.434 (0.607) data 0.000 (0.178) loss 0.4824 (0.3352) acc 87.5000 (91.2500) lr 1.0628e-02 eta 0:18:42\n",
            "epoch [98/200] batch [10/18] time 0.434 (0.519) data 0.000 (0.089) loss 0.2687 (0.2395) acc 93.7500 (94.0625) lr 1.0628e-02 eta 0:15:56\n",
            "epoch [98/200] batch [15/18] time 0.433 (0.489) data 0.000 (0.059) loss 0.2564 (0.2406) acc 90.6250 (93.3333) lr 1.0628e-02 eta 0:14:59\n",
            "epoch [99/200] batch [5/18] time 0.438 (0.609) data 0.000 (0.178) loss 0.1093 (0.1990) acc 100.0000 (95.6250) lr 1.0471e-02 eta 0:18:35\n",
            "epoch [99/200] batch [10/18] time 0.434 (0.520) data 0.000 (0.089) loss 0.1991 (0.1626) acc 96.8750 (96.5625) lr 1.0471e-02 eta 0:15:49\n",
            "epoch [99/200] batch [15/18] time 0.431 (0.490) data 0.000 (0.060) loss 0.3054 (0.1924) acc 87.5000 (95.4167) lr 1.0471e-02 eta 0:14:51\n",
            "epoch [100/200] batch [5/18] time 0.441 (0.601) data 0.000 (0.170) loss 0.2755 (0.3072) acc 90.6250 (91.8750) lr 1.0314e-02 eta 0:18:09\n",
            "epoch [100/200] batch [10/18] time 0.433 (0.515) data 0.000 (0.085) loss 0.1402 (0.2276) acc 96.8750 (94.3750) lr 1.0314e-02 eta 0:15:30\n",
            "epoch [100/200] batch [15/18] time 0.431 (0.486) data 0.000 (0.057) loss 0.4874 (0.2169) acc 87.5000 (95.2083) lr 1.0314e-02 eta 0:14:36\n",
            "epoch [101/200] batch [5/18] time 0.439 (0.610) data 0.000 (0.179) loss 0.1848 (0.2652) acc 96.8750 (94.3750) lr 1.0157e-02 eta 0:18:14\n",
            "epoch [101/200] batch [10/18] time 0.428 (0.519) data 0.000 (0.090) loss 0.1079 (0.2778) acc 93.7500 (92.8125) lr 1.0157e-02 eta 0:15:29\n",
            "epoch [101/200] batch [15/18] time 0.429 (0.489) data 0.000 (0.060) loss 0.0950 (0.2546) acc 96.8750 (93.1250) lr 1.0157e-02 eta 0:14:33\n",
            "epoch [102/200] batch [5/18] time 0.443 (0.608) data 0.000 (0.177) loss 0.2157 (0.1554) acc 96.8750 (96.2500) lr 1.0000e-02 eta 0:18:00\n",
            "epoch [102/200] batch [10/18] time 0.431 (0.518) data 0.000 (0.088) loss 0.2429 (0.2479) acc 90.6250 (93.7500) lr 1.0000e-02 eta 0:15:18\n",
            "epoch [102/200] batch [15/18] time 0.427 (0.488) data 0.000 (0.059) loss 0.0611 (0.2141) acc 100.0000 (95.2083) lr 1.0000e-02 eta 0:14:23\n",
            "epoch [103/200] batch [5/18] time 0.438 (0.600) data 0.000 (0.170) loss 0.0680 (0.1423) acc 100.0000 (97.5000) lr 9.8429e-03 eta 0:17:35\n",
            "epoch [103/200] batch [10/18] time 0.431 (0.514) data 0.000 (0.085) loss 0.1770 (0.1717) acc 96.8750 (96.8750) lr 9.8429e-03 eta 0:15:02\n",
            "epoch [103/200] batch [15/18] time 0.427 (0.486) data 0.000 (0.057) loss 0.4138 (0.1792) acc 93.7500 (96.6667) lr 9.8429e-03 eta 0:14:09\n",
            "epoch [104/200] batch [5/18] time 0.438 (0.604) data 0.000 (0.175) loss 0.2344 (0.2039) acc 90.6250 (94.3750) lr 9.6859e-03 eta 0:17:32\n",
            "epoch [104/200] batch [10/18] time 0.433 (0.516) data 0.000 (0.088) loss 0.2079 (0.2121) acc 93.7500 (94.0625) lr 9.6859e-03 eta 0:14:55\n",
            "epoch [104/200] batch [15/18] time 0.430 (0.487) data 0.000 (0.059) loss 0.2394 (0.2433) acc 93.7500 (92.9167) lr 9.6859e-03 eta 0:14:02\n",
            "epoch [105/200] batch [5/18] time 0.436 (0.614) data 0.000 (0.185) loss 0.2342 (0.2874) acc 93.7500 (92.5000) lr 9.5289e-03 eta 0:17:38\n",
            "epoch [105/200] batch [10/18] time 0.434 (0.521) data 0.000 (0.093) loss 0.1606 (0.2568) acc 93.7500 (93.1250) lr 9.5289e-03 eta 0:14:54\n",
            "epoch [105/200] batch [15/18] time 0.431 (0.490) data 0.000 (0.062) loss 0.2965 (0.2685) acc 87.5000 (92.5000) lr 9.5289e-03 eta 0:13:58\n",
            "epoch [106/200] batch [5/18] time 0.438 (0.610) data 0.000 (0.181) loss 0.3650 (0.2462) acc 87.5000 (93.1250) lr 9.3721e-03 eta 0:17:20\n",
            "epoch [106/200] batch [10/18] time 0.433 (0.520) data 0.000 (0.091) loss 0.2244 (0.2313) acc 93.7500 (93.4375) lr 9.3721e-03 eta 0:14:43\n",
            "epoch [106/200] batch [15/18] time 0.436 (0.490) data 0.000 (0.061) loss 0.1790 (0.2449) acc 96.8750 (93.7500) lr 9.3721e-03 eta 0:13:50\n",
            "epoch [107/200] batch [5/18] time 0.443 (0.608) data 0.000 (0.175) loss 0.1165 (0.1548) acc 96.8750 (96.8750) lr 9.2154e-03 eta 0:17:05\n",
            "epoch [107/200] batch [10/18] time 0.440 (0.520) data 0.000 (0.087) loss 0.0766 (0.1711) acc 100.0000 (96.2500) lr 9.2154e-03 eta 0:14:35\n",
            "epoch [107/200] batch [15/18] time 0.439 (0.492) data 0.000 (0.058) loss 0.2223 (0.1910) acc 90.6250 (95.6250) lr 9.2154e-03 eta 0:13:45\n",
            "epoch [108/200] batch [5/18] time 0.445 (0.615) data 0.000 (0.177) loss 0.1654 (0.1782) acc 96.8750 (95.0000) lr 9.0589e-03 eta 0:17:05\n",
            "epoch [108/200] batch [10/18] time 0.445 (0.528) data 0.000 (0.089) loss 0.2749 (0.2227) acc 93.7500 (93.4375) lr 9.0589e-03 eta 0:14:39\n",
            "epoch [108/200] batch [15/18] time 0.437 (0.499) data 0.000 (0.059) loss 0.0339 (0.2015) acc 100.0000 (94.3750) lr 9.0589e-03 eta 0:13:48\n",
            "epoch [109/200] batch [5/18] time 0.462 (0.629) data 0.000 (0.185) loss 0.1756 (0.1514) acc 96.8750 (96.2500) lr 8.9027e-03 eta 0:17:18\n",
            "epoch [109/200] batch [10/18] time 0.453 (0.539) data 0.000 (0.092) loss 0.2127 (0.1882) acc 96.8750 (95.3125) lr 8.9027e-03 eta 0:14:47\n",
            "epoch [109/200] batch [15/18] time 0.447 (0.509) data 0.000 (0.062) loss 0.3744 (0.2311) acc 87.5000 (93.1250) lr 8.9027e-03 eta 0:13:55\n",
            "epoch [110/200] batch [5/18] time 0.458 (0.611) data 0.000 (0.162) loss 0.3420 (0.3054) acc 90.6250 (91.8750) lr 8.7467e-03 eta 0:16:37\n",
            "epoch [110/200] batch [10/18] time 0.444 (0.528) data 0.000 (0.081) loss 0.1022 (0.2462) acc 96.8750 (92.8125) lr 8.7467e-03 eta 0:14:19\n",
            "epoch [110/200] batch [15/18] time 0.445 (0.501) data 0.000 (0.054) loss 0.3549 (0.2551) acc 87.5000 (93.1250) lr 8.7467e-03 eta 0:13:32\n",
            "epoch [111/200] batch [5/18] time 0.452 (0.610) data 0.000 (0.168) loss 0.1541 (0.2002) acc 96.8750 (94.3750) lr 8.5910e-03 eta 0:16:25\n",
            "epoch [111/200] batch [10/18] time 0.444 (0.526) data 0.000 (0.084) loss 0.1448 (0.1714) acc 93.7500 (95.6250) lr 8.5910e-03 eta 0:14:07\n",
            "epoch [111/200] batch [15/18] time 0.443 (0.499) data 0.000 (0.056) loss 0.0917 (0.1838) acc 100.0000 (95.0000) lr 8.5910e-03 eta 0:13:20\n",
            "epoch [112/200] batch [5/18] time 0.452 (0.612) data 0.000 (0.172) loss 0.3876 (0.2802) acc 87.5000 (91.8750) lr 8.4357e-03 eta 0:16:18\n",
            "epoch [112/200] batch [10/18] time 0.441 (0.526) data 0.000 (0.086) loss 0.1983 (0.2381) acc 90.6250 (93.4375) lr 8.4357e-03 eta 0:13:57\n",
            "epoch [112/200] batch [15/18] time 0.435 (0.497) data 0.000 (0.057) loss 0.2732 (0.2267) acc 93.7500 (94.1667) lr 8.4357e-03 eta 0:13:08\n",
            "epoch [113/200] batch [5/18] time 0.442 (0.618) data 0.000 (0.182) loss 0.0708 (0.1786) acc 100.0000 (95.0000) lr 8.2807e-03 eta 0:16:15\n",
            "epoch [113/200] batch [10/18] time 0.439 (0.528) data 0.000 (0.091) loss 0.3754 (0.2237) acc 87.5000 (93.4375) lr 8.2807e-03 eta 0:13:50\n",
            "epoch [113/200] batch [15/18] time 0.434 (0.497) data 0.000 (0.061) loss 0.1877 (0.2072) acc 100.0000 (94.5833) lr 8.2807e-03 eta 0:12:59\n",
            "epoch [114/200] batch [5/18] time 0.449 (0.616) data 0.000 (0.179) loss 0.2931 (0.3056) acc 90.6250 (93.1250) lr 8.1262e-03 eta 0:16:01\n",
            "epoch [114/200] batch [10/18] time 0.437 (0.525) data 0.000 (0.090) loss 0.2889 (0.2748) acc 90.6250 (94.0625) lr 8.1262e-03 eta 0:13:37\n",
            "epoch [114/200] batch [15/18] time 0.433 (0.495) data 0.000 (0.060) loss 0.2186 (0.2454) acc 93.7500 (94.1667) lr 8.1262e-03 eta 0:12:48\n",
            "epoch [115/200] batch [5/18] time 0.443 (0.605) data 0.000 (0.169) loss 0.0886 (0.1626) acc 100.0000 (96.8750) lr 7.9721e-03 eta 0:15:32\n",
            "epoch [115/200] batch [10/18] time 0.436 (0.520) data 0.001 (0.085) loss 0.1614 (0.1661) acc 93.7500 (95.9375) lr 7.9721e-03 eta 0:13:19\n",
            "epoch [115/200] batch [15/18] time 0.432 (0.492) data 0.000 (0.057) loss 0.1097 (0.1823) acc 96.8750 (95.4167) lr 7.9721e-03 eta 0:12:33\n",
            "epoch [116/200] batch [5/18] time 0.448 (0.616) data 0.000 (0.179) loss 0.3608 (0.1940) acc 90.6250 (94.3750) lr 7.8186e-03 eta 0:15:39\n",
            "epoch [116/200] batch [10/18] time 0.437 (0.526) data 0.000 (0.090) loss 0.1149 (0.2049) acc 96.8750 (93.7500) lr 7.8186e-03 eta 0:13:19\n",
            "epoch [116/200] batch [15/18] time 0.439 (0.497) data 0.000 (0.060) loss 0.4590 (0.2218) acc 78.1250 (93.1250) lr 7.8186e-03 eta 0:12:32\n",
            "epoch [117/200] batch [5/18] time 0.448 (0.606) data 0.000 (0.168) loss 0.1274 (0.2553) acc 100.0000 (93.7500) lr 7.6655e-03 eta 0:15:12\n",
            "epoch [117/200] batch [10/18] time 0.437 (0.521) data 0.000 (0.084) loss 0.2732 (0.2318) acc 93.7500 (94.3750) lr 7.6655e-03 eta 0:13:02\n",
            "epoch [117/200] batch [15/18] time 0.439 (0.494) data 0.000 (0.056) loss 0.2207 (0.2450) acc 93.7500 (94.3750) lr 7.6655e-03 eta 0:12:18\n",
            "epoch [118/200] batch [5/18] time 0.447 (0.603) data 0.000 (0.165) loss 0.3494 (0.2815) acc 96.8750 (94.3750) lr 7.5131e-03 eta 0:14:58\n",
            "epoch [118/200] batch [10/18] time 0.441 (0.521) data 0.000 (0.082) loss 0.3465 (0.2628) acc 90.6250 (94.0625) lr 7.5131e-03 eta 0:12:53\n",
            "epoch [118/200] batch [15/18] time 0.441 (0.494) data 0.000 (0.055) loss 0.3846 (0.2690) acc 90.6250 (93.5417) lr 7.5131e-03 eta 0:12:10\n",
            "epoch [119/200] batch [5/18] time 0.446 (0.626) data 0.000 (0.186) loss 0.4190 (0.3184) acc 87.5000 (91.8750) lr 7.3613e-03 eta 0:15:20\n",
            "epoch [119/200] batch [10/18] time 0.442 (0.534) data 0.000 (0.093) loss 0.3938 (0.2912) acc 87.5000 (92.1875) lr 7.3613e-03 eta 0:13:02\n",
            "epoch [119/200] batch [15/18] time 0.436 (0.502) data 0.000 (0.062) loss 0.0828 (0.2486) acc 100.0000 (93.7500) lr 7.3613e-03 eta 0:12:13\n",
            "epoch [120/200] batch [5/18] time 0.449 (0.610) data 0.000 (0.170) loss 0.0774 (0.2096) acc 100.0000 (95.0000) lr 7.2101e-03 eta 0:14:45\n",
            "epoch [120/200] batch [10/18] time 0.441 (0.525) data 0.000 (0.085) loss 0.2036 (0.2120) acc 93.7500 (94.6875) lr 7.2101e-03 eta 0:12:39\n",
            "epoch [120/200] batch [15/18] time 0.439 (0.497) data 0.000 (0.057) loss 0.1798 (0.2195) acc 90.6250 (94.1667) lr 7.2101e-03 eta 0:11:56\n",
            "epoch [121/200] batch [5/18] time 0.454 (0.623) data 0.000 (0.183) loss 0.2205 (0.1931) acc 93.7500 (96.2500) lr 7.0596e-03 eta 0:14:53\n",
            "epoch [121/200] batch [10/18] time 0.443 (0.531) data 0.000 (0.092) loss 0.0704 (0.1976) acc 100.0000 (95.3125) lr 7.0596e-03 eta 0:12:39\n",
            "epoch [121/200] batch [15/18] time 0.442 (0.501) data 0.000 (0.061) loss 0.3237 (0.1929) acc 87.5000 (95.2083) lr 7.0596e-03 eta 0:11:53\n",
            "epoch [122/200] batch [5/18] time 0.445 (0.613) data 0.000 (0.175) loss 0.1398 (0.2361) acc 96.8750 (93.1250) lr 6.9098e-03 eta 0:14:28\n",
            "epoch [122/200] batch [10/18] time 0.438 (0.525) data 0.000 (0.088) loss 0.4565 (0.2280) acc 84.3750 (93.1250) lr 6.9098e-03 eta 0:12:21\n",
            "epoch [122/200] batch [15/18] time 0.438 (0.497) data 0.000 (0.059) loss 0.1242 (0.2420) acc 96.8750 (92.7083) lr 6.9098e-03 eta 0:11:38\n",
            "epoch [123/200] batch [5/18] time 0.447 (0.611) data 0.000 (0.172) loss 0.0783 (0.1658) acc 96.8750 (95.6250) lr 6.7608e-03 eta 0:14:15\n",
            "epoch [123/200] batch [10/18] time 0.443 (0.524) data 0.000 (0.086) loss 0.2629 (0.1601) acc 90.6250 (96.2500) lr 6.7608e-03 eta 0:12:10\n",
            "epoch [123/200] batch [15/18] time 0.436 (0.495) data 0.000 (0.058) loss 0.2810 (0.1648) acc 87.5000 (95.6250) lr 6.7608e-03 eta 0:11:27\n",
            "epoch [124/200] batch [5/18] time 0.445 (0.608) data 0.000 (0.170) loss 0.2749 (0.2317) acc 93.7500 (93.7500) lr 6.6126e-03 eta 0:13:59\n",
            "epoch [124/200] batch [10/18] time 0.440 (0.523) data 0.000 (0.085) loss 0.2044 (0.2223) acc 90.6250 (94.0625) lr 6.6126e-03 eta 0:11:59\n",
            "epoch [124/200] batch [15/18] time 0.437 (0.495) data 0.000 (0.057) loss 0.1822 (0.2074) acc 93.7500 (93.9583) lr 6.6126e-03 eta 0:11:18\n",
            "epoch [125/200] batch [5/18] time 0.445 (0.611) data 0.000 (0.173) loss 0.1189 (0.2408) acc 96.8750 (94.3750) lr 6.4653e-03 eta 0:13:52\n",
            "epoch [125/200] batch [10/18] time 0.435 (0.524) data 0.000 (0.087) loss 0.3342 (0.2552) acc 90.6250 (93.7500) lr 6.4653e-03 eta 0:11:51\n",
            "epoch [125/200] batch [15/18] time 0.436 (0.495) data 0.000 (0.058) loss 0.2368 (0.2275) acc 96.8750 (94.5833) lr 6.4653e-03 eta 0:11:09\n",
            "epoch [126/200] batch [5/18] time 0.447 (0.614) data 0.000 (0.177) loss 0.1813 (0.1891) acc 93.7500 (95.0000) lr 6.3188e-03 eta 0:13:45\n",
            "epoch [126/200] batch [10/18] time 0.435 (0.525) data 0.000 (0.089) loss 0.1020 (0.1551) acc 100.0000 (96.5625) lr 6.3188e-03 eta 0:11:44\n",
            "epoch [126/200] batch [15/18] time 0.433 (0.496) data 0.000 (0.059) loss 0.2902 (0.1700) acc 90.6250 (95.6250) lr 6.3188e-03 eta 0:11:02\n",
            "epoch [127/200] batch [5/18] time 0.448 (0.613) data 0.000 (0.174) loss 0.1394 (0.1948) acc 96.8750 (95.0000) lr 6.1732e-03 eta 0:13:33\n",
            "epoch [127/200] batch [10/18] time 0.438 (0.525) data 0.000 (0.087) loss 0.2633 (0.2168) acc 90.6250 (93.7500) lr 6.1732e-03 eta 0:11:33\n",
            "epoch [127/200] batch [15/18] time 0.436 (0.496) data 0.000 (0.058) loss 0.1789 (0.2133) acc 93.7500 (93.7500) lr 6.1732e-03 eta 0:10:53\n",
            "epoch [128/200] batch [5/18] time 0.444 (0.606) data 0.000 (0.169) loss 0.1603 (0.3182) acc 96.8750 (94.3750) lr 6.0285e-03 eta 0:13:13\n",
            "epoch [128/200] batch [10/18] time 0.444 (0.522) data 0.000 (0.085) loss 0.3061 (0.2899) acc 90.6250 (93.4375) lr 6.0285e-03 eta 0:11:21\n",
            "epoch [128/200] batch [15/18] time 0.439 (0.495) data 0.000 (0.057) loss 0.2794 (0.2849) acc 93.7500 (92.9167) lr 6.0285e-03 eta 0:10:42\n",
            "epoch [129/200] batch [5/18] time 0.448 (0.622) data 0.000 (0.184) loss 0.1499 (0.1562) acc 93.7500 (96.2500) lr 5.8849e-03 eta 0:13:23\n",
            "epoch [129/200] batch [10/18] time 0.436 (0.530) data 0.000 (0.092) loss 0.0962 (0.1600) acc 96.8750 (96.2500) lr 5.8849e-03 eta 0:11:21\n",
            "epoch [129/200] batch [15/18] time 0.436 (0.500) data 0.000 (0.061) loss 0.2833 (0.1635) acc 90.6250 (95.8333) lr 5.8849e-03 eta 0:10:40\n",
            "epoch [130/200] batch [5/18] time 0.446 (0.630) data 0.000 (0.192) loss 0.1767 (0.2419) acc 96.8750 (94.3750) lr 5.7422e-03 eta 0:13:21\n",
            "epoch [130/200] batch [10/18] time 0.438 (0.533) data 0.000 (0.096) loss 0.1432 (0.2007) acc 96.8750 (94.6875) lr 5.7422e-03 eta 0:11:16\n",
            "epoch [130/200] batch [15/18] time 0.436 (0.502) data 0.000 (0.064) loss 0.4520 (0.2061) acc 90.6250 (95.0000) lr 5.7422e-03 eta 0:10:33\n",
            "epoch [131/200] batch [5/18] time 0.450 (0.609) data 0.000 (0.171) loss 0.3707 (0.2010) acc 90.6250 (95.0000) lr 5.6006e-03 eta 0:12:44\n",
            "epoch [131/200] batch [10/18] time 0.438 (0.524) data 0.000 (0.085) loss 0.3875 (0.2051) acc 93.7500 (95.3125) lr 5.6006e-03 eta 0:10:54\n",
            "epoch [131/200] batch [15/18] time 0.439 (0.495) data 0.000 (0.057) loss 0.1406 (0.1972) acc 96.8750 (95.2083) lr 5.6006e-03 eta 0:10:16\n",
            "epoch [132/200] batch [5/18] time 0.448 (0.624) data 0.000 (0.184) loss 0.0984 (0.1974) acc 96.8750 (94.3750) lr 5.4601e-03 eta 0:12:51\n",
            "epoch [132/200] batch [10/18] time 0.441 (0.531) data 0.000 (0.092) loss 0.4336 (0.2179) acc 90.6250 (93.7500) lr 5.4601e-03 eta 0:10:54\n",
            "epoch [132/200] batch [15/18] time 0.439 (0.500) data 0.000 (0.062) loss 0.1176 (0.2075) acc 96.8750 (94.1667) lr 5.4601e-03 eta 0:10:13\n",
            "epoch [133/200] batch [5/18] time 0.449 (0.622) data 0.000 (0.183) loss 0.0993 (0.1890) acc 96.8750 (94.3750) lr 5.3207e-03 eta 0:12:38\n",
            "epoch [133/200] batch [10/18] time 0.439 (0.530) data 0.000 (0.092) loss 0.3279 (0.2576) acc 90.6250 (91.2500) lr 5.3207e-03 eta 0:10:43\n",
            "epoch [133/200] batch [15/18] time 0.436 (0.500) data 0.000 (0.061) loss 0.2566 (0.2548) acc 90.6250 (92.0833) lr 5.3207e-03 eta 0:10:04\n",
            "epoch [134/200] batch [5/18] time 0.444 (0.618) data 0.000 (0.180) loss 0.2786 (0.1880) acc 87.5000 (95.0000) lr 5.1825e-03 eta 0:12:21\n",
            "epoch [134/200] batch [10/18] time 0.444 (0.529) data 0.000 (0.090) loss 0.4038 (0.2019) acc 93.7500 (95.3125) lr 5.1825e-03 eta 0:10:32\n",
            "epoch [134/200] batch [15/18] time 0.438 (0.499) data 0.000 (0.060) loss 0.1603 (0.2046) acc 93.7500 (95.0000) lr 5.1825e-03 eta 0:09:53\n",
            "epoch [135/200] batch [5/18] time 0.446 (0.614) data 0.000 (0.177) loss 0.3730 (0.1984) acc 87.5000 (94.3750) lr 5.0454e-03 eta 0:12:06\n",
            "epoch [135/200] batch [10/18] time 0.440 (0.526) data 0.000 (0.089) loss 0.0589 (0.1751) acc 100.0000 (95.6250) lr 5.0454e-03 eta 0:10:19\n",
            "epoch [135/200] batch [15/18] time 0.441 (0.497) data 0.000 (0.059) loss 0.3195 (0.1932) acc 93.7500 (95.2083) lr 5.0454e-03 eta 0:09:42\n",
            "epoch [136/200] batch [5/18] time 0.450 (0.614) data 0.000 (0.177) loss 0.0984 (0.1591) acc 100.0000 (96.8750) lr 4.9096e-03 eta 0:11:55\n",
            "epoch [136/200] batch [10/18] time 0.443 (0.527) data 0.000 (0.089) loss 0.1294 (0.2092) acc 96.8750 (94.6875) lr 4.9096e-03 eta 0:10:11\n",
            "epoch [136/200] batch [15/18] time 0.440 (0.497) data 0.000 (0.059) loss 0.3657 (0.2234) acc 90.6250 (93.9583) lr 4.9096e-03 eta 0:09:34\n",
            "epoch [137/200] batch [5/18] time 0.444 (0.613) data 0.000 (0.174) loss 0.2381 (0.2634) acc 93.7500 (93.1250) lr 4.7750e-03 eta 0:11:43\n",
            "epoch [137/200] batch [10/18] time 0.441 (0.526) data 0.000 (0.087) loss 0.1839 (0.2360) acc 96.8750 (93.4375) lr 4.7750e-03 eta 0:10:00\n",
            "epoch [137/200] batch [15/18] time 0.435 (0.496) data 0.000 (0.058) loss 0.1384 (0.2226) acc 96.8750 (94.1667) lr 4.7750e-03 eta 0:09:24\n",
            "epoch [138/200] batch [5/18] time 0.447 (0.625) data 0.000 (0.188) loss 0.3391 (0.2603) acc 90.6250 (93.7500) lr 4.6417e-03 eta 0:11:46\n",
            "epoch [138/200] batch [10/18] time 0.444 (0.532) data 0.000 (0.094) loss 0.3261 (0.2263) acc 87.5000 (94.6875) lr 4.6417e-03 eta 0:09:57\n",
            "epoch [138/200] batch [15/18] time 0.435 (0.500) data 0.000 (0.063) loss 0.1522 (0.2104) acc 96.8750 (94.7917) lr 4.6417e-03 eta 0:09:19\n",
            "epoch [139/200] batch [5/18] time 0.451 (0.605) data 0.000 (0.167) loss 0.3791 (0.2018) acc 87.5000 (93.1250) lr 4.5098e-03 eta 0:11:12\n",
            "epoch [139/200] batch [10/18] time 0.439 (0.521) data 0.000 (0.083) loss 0.2021 (0.1853) acc 93.7500 (94.0625) lr 4.5098e-03 eta 0:09:35\n",
            "epoch [139/200] batch [15/18] time 0.438 (0.493) data 0.000 (0.056) loss 0.1973 (0.1764) acc 93.7500 (94.1667) lr 4.5098e-03 eta 0:09:02\n",
            "epoch [140/200] batch [5/18] time 0.450 (0.616) data 0.000 (0.180) loss 0.2631 (0.1728) acc 93.7500 (95.0000) lr 4.3792e-03 eta 0:11:13\n",
            "epoch [140/200] batch [10/18] time 0.436 (0.526) data 0.000 (0.090) loss 0.3483 (0.1579) acc 87.5000 (95.0000) lr 4.3792e-03 eta 0:09:32\n",
            "epoch [140/200] batch [15/18] time 0.434 (0.496) data 0.000 (0.060) loss 0.1870 (0.1821) acc 96.8750 (95.2083) lr 4.3792e-03 eta 0:08:57\n",
            "epoch [141/200] batch [5/18] time 0.440 (0.612) data 0.000 (0.178) loss 0.1594 (0.1551) acc 93.7500 (95.6250) lr 4.2499e-03 eta 0:10:58\n",
            "epoch [141/200] batch [10/18] time 0.438 (0.525) data 0.000 (0.089) loss 0.1743 (0.1996) acc 96.8750 (93.7500) lr 4.2499e-03 eta 0:09:21\n",
            "epoch [141/200] batch [15/18] time 0.440 (0.496) data 0.000 (0.059) loss 0.3012 (0.2245) acc 90.6250 (93.5417) lr 4.2499e-03 eta 0:08:48\n",
            "epoch [142/200] batch [5/18] time 0.450 (0.619) data 0.000 (0.182) loss 0.4255 (0.2605) acc 90.6250 (95.0000) lr 4.1221e-03 eta 0:10:53\n",
            "epoch [142/200] batch [10/18] time 0.439 (0.528) data 0.000 (0.091) loss 0.2496 (0.1989) acc 93.7500 (95.9375) lr 4.1221e-03 eta 0:09:15\n",
            "epoch [142/200] batch [15/18] time 0.438 (0.499) data 0.000 (0.061) loss 0.1231 (0.1883) acc 96.8750 (95.8333) lr 4.1221e-03 eta 0:08:42\n",
            "epoch [143/200] batch [5/18] time 0.447 (0.607) data 0.000 (0.168) loss 0.4760 (0.2804) acc 84.3750 (91.2500) lr 3.9958e-03 eta 0:10:31\n",
            "epoch [143/200] batch [10/18] time 0.439 (0.522) data 0.000 (0.084) loss 0.1053 (0.2306) acc 100.0000 (93.4375) lr 3.9958e-03 eta 0:08:59\n",
            "epoch [143/200] batch [15/18] time 0.440 (0.494) data 0.000 (0.056) loss 0.2436 (0.2264) acc 93.7500 (93.7500) lr 3.9958e-03 eta 0:08:28\n",
            "epoch [144/200] batch [5/18] time 0.448 (0.624) data 0.000 (0.185) loss 0.1662 (0.2103) acc 96.8750 (93.1250) lr 3.8709e-03 eta 0:10:36\n",
            "epoch [144/200] batch [10/18] time 0.439 (0.531) data 0.000 (0.093) loss 0.0886 (0.1940) acc 96.8750 (94.3750) lr 3.8709e-03 eta 0:08:59\n",
            "epoch [144/200] batch [15/18] time 0.441 (0.501) data 0.000 (0.062) loss 0.2097 (0.2144) acc 93.7500 (93.9583) lr 3.8709e-03 eta 0:08:26\n",
            "epoch [145/200] batch [5/18] time 0.449 (0.613) data 0.000 (0.174) loss 0.1440 (0.2070) acc 96.8750 (93.7500) lr 3.7476e-03 eta 0:10:14\n",
            "epoch [145/200] batch [10/18] time 0.443 (0.526) data 0.000 (0.087) loss 0.0872 (0.1833) acc 100.0000 (94.6875) lr 3.7476e-03 eta 0:08:45\n",
            "epoch [145/200] batch [15/18] time 0.439 (0.497) data 0.000 (0.058) loss 0.4645 (0.2017) acc 87.5000 (94.1667) lr 3.7476e-03 eta 0:08:13\n",
            "epoch [146/200] batch [5/18] time 0.448 (0.606) data 0.000 (0.167) loss 0.3450 (0.1773) acc 90.6250 (95.6250) lr 3.6258e-03 eta 0:09:57\n",
            "epoch [146/200] batch [10/18] time 0.435 (0.522) data 0.000 (0.084) loss 0.0700 (0.1659) acc 96.8750 (95.6250) lr 3.6258e-03 eta 0:08:31\n",
            "epoch [146/200] batch [15/18] time 0.434 (0.494) data 0.000 (0.056) loss 0.2089 (0.1816) acc 93.7500 (95.2083) lr 3.6258e-03 eta 0:08:01\n",
            "epoch [147/200] batch [5/18] time 0.447 (0.612) data 0.000 (0.175) loss 0.2376 (0.1710) acc 93.7500 (96.2500) lr 3.5055e-03 eta 0:09:52\n",
            "epoch [147/200] batch [10/18] time 0.440 (0.525) data 0.000 (0.087) loss 0.1633 (0.1668) acc 93.7500 (96.2500) lr 3.5055e-03 eta 0:08:24\n",
            "epoch [147/200] batch [15/18] time 0.434 (0.496) data 0.000 (0.058) loss 0.1429 (0.1924) acc 96.8750 (95.6250) lr 3.5055e-03 eta 0:07:54\n",
            "epoch [148/200] batch [5/18] time 0.446 (0.604) data 0.000 (0.167) loss 0.4078 (0.3000) acc 87.5000 (91.2500) lr 3.3869e-03 eta 0:09:33\n",
            "epoch [148/200] batch [10/18] time 0.433 (0.520) data 0.000 (0.084) loss 0.0469 (0.2226) acc 100.0000 (94.0625) lr 3.3869e-03 eta 0:08:10\n",
            "epoch [148/200] batch [15/18] time 0.437 (0.492) data 0.000 (0.056) loss 0.0802 (0.2070) acc 100.0000 (94.7917) lr 3.3869e-03 eta 0:07:42\n",
            "epoch [149/200] batch [5/18] time 0.443 (0.622) data 0.000 (0.187) loss 0.4177 (0.2201) acc 90.6250 (95.6250) lr 3.2699e-03 eta 0:09:39\n",
            "epoch [149/200] batch [10/18] time 0.435 (0.529) data 0.000 (0.093) loss 0.2646 (0.2352) acc 93.7500 (94.6875) lr 3.2699e-03 eta 0:08:09\n",
            "epoch [149/200] batch [15/18] time 0.433 (0.499) data 0.000 (0.062) loss 0.3254 (0.2285) acc 93.7500 (94.5833) lr 3.2699e-03 eta 0:07:39\n",
            "epoch [150/200] batch [5/18] time 0.440 (0.623) data 0.000 (0.188) loss 0.1940 (0.1772) acc 96.8750 (96.2500) lr 3.1545e-03 eta 0:09:29\n",
            "epoch [150/200] batch [10/18] time 0.435 (0.530) data 0.000 (0.094) loss 0.2431 (0.1970) acc 90.6250 (95.6250) lr 3.1545e-03 eta 0:08:01\n",
            "epoch [150/200] batch [15/18] time 0.434 (0.499) data 0.000 (0.063) loss 0.0997 (0.1826) acc 96.8750 (95.6250) lr 3.1545e-03 eta 0:07:30\n",
            "epoch [151/200] batch [5/18] time 0.449 (0.612) data 0.000 (0.174) loss 0.2773 (0.1132) acc 90.6250 (97.5000) lr 3.0409e-03 eta 0:09:07\n",
            "epoch [151/200] batch [10/18] time 0.440 (0.525) data 0.000 (0.087) loss 0.0771 (0.1331) acc 96.8750 (96.5625) lr 3.0409e-03 eta 0:07:47\n",
            "epoch [151/200] batch [15/18] time 0.439 (0.497) data 0.000 (0.058) loss 0.1193 (0.1326) acc 93.7500 (96.8750) lr 3.0409e-03 eta 0:07:19\n",
            "epoch [152/200] batch [5/18] time 0.442 (0.615) data 0.000 (0.179) loss 0.0437 (0.2192) acc 100.0000 (93.7500) lr 2.9289e-03 eta 0:08:59\n",
            "epoch [152/200] batch [10/18] time 0.440 (0.526) data 0.000 (0.089) loss 0.1192 (0.2188) acc 96.8750 (93.7500) lr 2.9289e-03 eta 0:07:38\n",
            "epoch [152/200] batch [15/18] time 0.441 (0.497) data 0.000 (0.060) loss 0.3151 (0.2108) acc 90.6250 (94.5833) lr 2.9289e-03 eta 0:07:10\n",
            "epoch [153/200] batch [5/18] time 0.449 (0.623) data 0.000 (0.185) loss 0.1700 (0.1335) acc 96.8750 (96.8750) lr 2.8187e-03 eta 0:08:54\n",
            "epoch [153/200] batch [10/18] time 0.442 (0.531) data 0.000 (0.092) loss 0.3327 (0.1800) acc 87.5000 (95.3125) lr 2.8187e-03 eta 0:07:33\n",
            "epoch [153/200] batch [15/18] time 0.433 (0.500) data 0.000 (0.062) loss 0.0766 (0.1550) acc 100.0000 (95.8333) lr 2.8187e-03 eta 0:07:04\n",
            "epoch [154/200] batch [5/18] time 0.447 (0.604) data 0.000 (0.165) loss 0.1474 (0.1260) acc 96.8750 (96.8750) lr 2.7103e-03 eta 0:08:28\n",
            "epoch [154/200] batch [10/18] time 0.442 (0.522) data 0.000 (0.083) loss 0.3427 (0.1398) acc 87.5000 (95.9375) lr 2.7103e-03 eta 0:07:16\n",
            "epoch [154/200] batch [15/18] time 0.439 (0.494) data 0.000 (0.055) loss 0.2404 (0.1634) acc 90.6250 (95.4167) lr 2.7103e-03 eta 0:06:50\n",
            "epoch [155/200] batch [5/18] time 0.447 (0.614) data 0.000 (0.176) loss 0.2565 (0.1679) acc 93.7500 (96.2500) lr 2.6037e-03 eta 0:08:25\n",
            "epoch [155/200] batch [10/18] time 0.439 (0.527) data 0.000 (0.088) loss 0.1310 (0.1957) acc 96.8750 (95.0000) lr 2.6037e-03 eta 0:07:10\n",
            "epoch [155/200] batch [15/18] time 0.439 (0.498) data 0.000 (0.059) loss 0.1112 (0.1967) acc 100.0000 (95.0000) lr 2.6037e-03 eta 0:06:44\n",
            "epoch [156/200] batch [5/18] time 0.448 (0.612) data 0.000 (0.175) loss 0.2258 (0.1508) acc 93.7500 (96.8750) lr 2.4989e-03 eta 0:08:13\n",
            "epoch [156/200] batch [10/18] time 0.437 (0.525) data 0.000 (0.088) loss 0.1995 (0.1576) acc 93.7500 (96.2500) lr 2.4989e-03 eta 0:07:00\n",
            "epoch [156/200] batch [15/18] time 0.440 (0.496) data 0.000 (0.059) loss 0.2963 (0.2144) acc 93.7500 (95.0000) lr 2.4989e-03 eta 0:06:34\n",
            "epoch [157/200] batch [5/18] time 0.447 (0.609) data 0.000 (0.172) loss 0.0810 (0.1517) acc 96.8750 (95.0000) lr 2.3959e-03 eta 0:07:59\n",
            "epoch [157/200] batch [10/18] time 0.446 (0.524) data 0.000 (0.086) loss 0.1930 (0.1950) acc 96.8750 (94.6875) lr 2.3959e-03 eta 0:06:50\n",
            "epoch [157/200] batch [15/18] time 0.439 (0.496) data 0.000 (0.057) loss 0.1702 (0.1927) acc 96.8750 (94.5833) lr 2.3959e-03 eta 0:06:25\n",
            "epoch [158/200] batch [5/18] time 0.450 (0.611) data 0.000 (0.171) loss 0.1123 (0.1754) acc 96.8750 (95.6250) lr 2.2949e-03 eta 0:07:49\n",
            "epoch [158/200] batch [10/18] time 0.440 (0.525) data 0.000 (0.086) loss 0.1394 (0.1719) acc 96.8750 (95.3125) lr 2.2949e-03 eta 0:06:41\n",
            "epoch [158/200] batch [15/18] time 0.437 (0.497) data 0.000 (0.057) loss 0.2921 (0.2103) acc 84.3750 (94.1667) lr 2.2949e-03 eta 0:06:17\n",
            "epoch [159/200] batch [5/18] time 0.444 (0.615) data 0.000 (0.176) loss 0.1083 (0.1339) acc 96.8750 (96.2500) lr 2.1957e-03 eta 0:07:41\n",
            "epoch [159/200] batch [10/18] time 0.439 (0.526) data 0.000 (0.088) loss 0.3720 (0.1795) acc 87.5000 (95.6250) lr 2.1957e-03 eta 0:06:32\n",
            "epoch [159/200] batch [15/18] time 0.437 (0.497) data 0.000 (0.059) loss 0.1676 (0.1852) acc 96.8750 (95.4167) lr 2.1957e-03 eta 0:06:08\n",
            "epoch [160/200] batch [5/18] time 0.448 (0.616) data 0.000 (0.178) loss 0.2451 (0.1604) acc 90.6250 (94.3750) lr 2.0984e-03 eta 0:07:31\n",
            "epoch [160/200] batch [10/18] time 0.441 (0.527) data 0.000 (0.089) loss 0.1733 (0.1660) acc 93.7500 (94.6875) lr 2.0984e-03 eta 0:06:23\n",
            "epoch [160/200] batch [15/18] time 0.435 (0.497) data 0.000 (0.059) loss 0.1599 (0.1531) acc 96.8750 (95.4167) lr 2.0984e-03 eta 0:05:59\n",
            "epoch [161/200] batch [5/18] time 0.448 (0.615) data 0.000 (0.176) loss 0.2250 (0.1711) acc 96.8750 (95.0000) lr 2.0032e-03 eta 0:07:19\n",
            "epoch [161/200] batch [10/18] time 0.440 (0.525) data 0.000 (0.088) loss 0.3043 (0.1672) acc 93.7500 (95.3125) lr 2.0032e-03 eta 0:06:13\n",
            "epoch [161/200] batch [15/18] time 0.436 (0.496) data 0.000 (0.059) loss 0.1641 (0.1765) acc 96.8750 (95.4167) lr 2.0032e-03 eta 0:05:50\n",
            "epoch [162/200] batch [5/18] time 0.449 (0.612) data 0.000 (0.174) loss 0.2106 (0.2039) acc 96.8750 (96.8750) lr 1.9098e-03 eta 0:07:06\n",
            "epoch [162/200] batch [10/18] time 0.439 (0.525) data 0.000 (0.087) loss 0.3231 (0.2274) acc 93.7500 (95.3125) lr 1.9098e-03 eta 0:06:03\n",
            "epoch [162/200] batch [15/18] time 0.443 (0.496) data 0.000 (0.058) loss 0.4359 (0.2300) acc 84.3750 (94.3750) lr 1.9098e-03 eta 0:05:40\n",
            "epoch [163/200] batch [5/18] time 0.446 (0.618) data 0.000 (0.181) loss 0.2127 (0.1851) acc 96.8750 (96.8750) lr 1.8185e-03 eta 0:06:59\n",
            "epoch [163/200] batch [10/18] time 0.441 (0.528) data 0.000 (0.091) loss 0.1417 (0.1635) acc 96.8750 (96.5625) lr 1.8185e-03 eta 0:05:55\n",
            "epoch [163/200] batch [15/18] time 0.437 (0.498) data 0.000 (0.060) loss 0.2965 (0.1776) acc 93.7500 (95.8333) lr 1.8185e-03 eta 0:05:33\n",
            "epoch [164/200] batch [5/18] time 0.449 (0.609) data 0.000 (0.171) loss 0.0760 (0.1827) acc 96.8750 (95.6250) lr 1.7292e-03 eta 0:06:42\n",
            "epoch [164/200] batch [10/18] time 0.439 (0.523) data 0.000 (0.086) loss 0.2065 (0.2045) acc 96.8750 (95.0000) lr 1.7292e-03 eta 0:05:43\n",
            "epoch [164/200] batch [15/18] time 0.444 (0.495) data 0.000 (0.057) loss 0.1770 (0.2075) acc 96.8750 (94.7917) lr 1.7292e-03 eta 0:05:22\n",
            "epoch [165/200] batch [5/18] time 0.446 (0.603) data 0.000 (0.165) loss 0.1363 (0.2118) acc 96.8750 (93.7500) lr 1.6419e-03 eta 0:06:27\n",
            "epoch [165/200] batch [10/18] time 0.439 (0.520) data 0.000 (0.083) loss 0.2997 (0.2146) acc 87.5000 (94.0625) lr 1.6419e-03 eta 0:05:31\n",
            "epoch [165/200] batch [15/18] time 0.438 (0.493) data 0.000 (0.055) loss 0.2062 (0.2143) acc 96.8750 (93.9583) lr 1.6419e-03 eta 0:05:12\n",
            "epoch [166/200] batch [5/18] time 0.450 (0.609) data 0.000 (0.171) loss 0.0804 (0.2854) acc 100.0000 (91.8750) lr 1.5567e-03 eta 0:06:20\n",
            "epoch [166/200] batch [10/18] time 0.441 (0.524) data 0.001 (0.086) loss 0.2041 (0.2282) acc 96.8750 (94.0625) lr 1.5567e-03 eta 0:05:24\n",
            "epoch [166/200] batch [15/18] time 0.441 (0.496) data 0.000 (0.057) loss 0.2281 (0.2163) acc 93.7500 (94.3750) lr 1.5567e-03 eta 0:05:05\n",
            "epoch [167/200] batch [5/18] time 0.442 (0.634) data 0.000 (0.198) loss 0.3165 (0.2355) acc 90.6250 (93.7500) lr 1.4736e-03 eta 0:06:24\n",
            "epoch [167/200] batch [10/18] time 0.443 (0.538) data 0.000 (0.099) loss 0.2862 (0.2325) acc 87.5000 (92.8125) lr 1.4736e-03 eta 0:05:23\n",
            "epoch [167/200] batch [15/18] time 0.439 (0.505) data 0.000 (0.066) loss 0.1244 (0.2146) acc 96.8750 (93.9583) lr 1.4736e-03 eta 0:05:01\n",
            "epoch [168/200] batch [5/18] time 0.446 (0.618) data 0.000 (0.179) loss 0.2021 (0.2380) acc 96.8750 (94.3750) lr 1.3926e-03 eta 0:06:03\n",
            "epoch [168/200] batch [10/18] time 0.439 (0.529) data 0.000 (0.090) loss 0.0632 (0.1679) acc 96.8750 (96.2500) lr 1.3926e-03 eta 0:05:08\n",
            "epoch [168/200] batch [15/18] time 0.437 (0.499) data 0.000 (0.060) loss 0.2640 (0.1570) acc 90.6250 (96.4583) lr 1.3926e-03 eta 0:04:48\n",
            "epoch [169/200] batch [5/18] time 0.446 (0.623) data 0.000 (0.185) loss 0.0827 (0.1384) acc 100.0000 (97.5000) lr 1.3137e-03 eta 0:05:55\n",
            "epoch [169/200] batch [10/18] time 0.443 (0.531) data 0.000 (0.092) loss 0.1423 (0.1496) acc 93.7500 (96.8750) lr 1.3137e-03 eta 0:05:00\n",
            "epoch [169/200] batch [15/18] time 0.441 (0.500) data 0.000 (0.062) loss 0.3697 (0.1847) acc 93.7500 (95.8333) lr 1.3137e-03 eta 0:04:40\n",
            "epoch [170/200] batch [5/18] time 0.441 (0.613) data 0.000 (0.175) loss 0.1124 (0.1715) acc 96.8750 (95.6250) lr 1.2369e-03 eta 0:05:38\n",
            "epoch [170/200] batch [10/18] time 0.440 (0.526) data 0.000 (0.088) loss 0.3951 (0.1752) acc 81.2500 (94.6875) lr 1.2369e-03 eta 0:04:48\n",
            "epoch [170/200] batch [15/18] time 0.439 (0.497) data 0.000 (0.059) loss 0.3177 (0.1756) acc 90.6250 (95.0000) lr 1.2369e-03 eta 0:04:29\n",
            "epoch [171/200] batch [5/18] time 0.450 (0.612) data 0.000 (0.174) loss 0.2173 (0.1526) acc 93.7500 (95.6250) lr 1.1623e-03 eta 0:05:27\n",
            "epoch [171/200] batch [10/18] time 0.440 (0.525) data 0.000 (0.087) loss 0.1090 (0.1902) acc 96.8750 (95.3125) lr 1.1623e-03 eta 0:04:38\n",
            "epoch [171/200] batch [15/18] time 0.436 (0.496) data 0.000 (0.058) loss 0.0642 (0.1904) acc 100.0000 (95.4167) lr 1.1623e-03 eta 0:04:20\n",
            "epoch [172/200] batch [5/18] time 0.452 (0.601) data 0.000 (0.163) loss 0.1428 (0.1255) acc 100.0000 (96.8750) lr 1.0899e-03 eta 0:05:10\n",
            "epoch [172/200] batch [10/18] time 0.443 (0.520) data 0.000 (0.082) loss 0.2487 (0.1761) acc 90.6250 (95.6250) lr 1.0899e-03 eta 0:04:26\n",
            "epoch [172/200] batch [15/18] time 0.439 (0.493) data 0.000 (0.054) loss 0.1212 (0.1538) acc 96.8750 (96.2500) lr 1.0899e-03 eta 0:04:10\n",
            "epoch [173/200] batch [5/18] time 0.445 (0.616) data 0.000 (0.178) loss 0.2725 (0.1994) acc 90.6250 (93.7500) lr 1.0197e-03 eta 0:05:07\n",
            "epoch [173/200] batch [10/18] time 0.437 (0.527) data 0.000 (0.089) loss 0.0843 (0.1669) acc 100.0000 (95.3125) lr 1.0197e-03 eta 0:04:20\n",
            "epoch [173/200] batch [15/18] time 0.438 (0.498) data 0.001 (0.059) loss 0.1368 (0.1927) acc 96.8750 (95.0000) lr 1.0197e-03 eta 0:04:03\n",
            "epoch [174/200] batch [5/18] time 0.443 (0.607) data 0.000 (0.169) loss 0.2513 (0.1927) acc 90.6250 (95.6250) lr 9.5173e-04 eta 0:04:51\n",
            "epoch [174/200] batch [10/18] time 0.440 (0.523) data 0.000 (0.085) loss 0.1768 (0.1761) acc 93.7500 (95.9375) lr 9.5173e-04 eta 0:04:08\n",
            "epoch [174/200] batch [15/18] time 0.445 (0.495) data 0.000 (0.057) loss 0.3572 (0.1662) acc 84.3750 (95.4167) lr 9.5173e-04 eta 0:03:53\n",
            "epoch [175/200] batch [5/18] time 0.449 (0.611) data 0.000 (0.173) loss 0.2010 (0.1733) acc 96.8750 (94.3750) lr 8.8597e-04 eta 0:04:42\n",
            "epoch [175/200] batch [10/18] time 0.436 (0.524) data 0.000 (0.086) loss 0.1174 (0.1808) acc 93.7500 (94.3750) lr 8.8597e-04 eta 0:04:00\n",
            "epoch [175/200] batch [15/18] time 0.434 (0.495) data 0.000 (0.058) loss 0.1585 (0.1826) acc 93.7500 (94.7917) lr 8.8597e-04 eta 0:03:44\n",
            "epoch [176/200] batch [5/18] time 0.445 (0.600) data 0.000 (0.163) loss 0.3497 (0.2690) acc 93.7500 (92.5000) lr 8.2245e-04 eta 0:04:26\n",
            "epoch [176/200] batch [10/18] time 0.436 (0.517) data 0.000 (0.082) loss 0.0516 (0.1918) acc 96.8750 (94.3750) lr 8.2245e-04 eta 0:03:47\n",
            "epoch [176/200] batch [15/18] time 0.434 (0.490) data 0.000 (0.055) loss 0.0975 (0.1787) acc 96.8750 (95.2083) lr 8.2245e-04 eta 0:03:32\n",
            "epoch [177/200] batch [5/18] time 0.445 (0.613) data 0.000 (0.178) loss 0.2710 (0.1471) acc 93.7500 (96.8750) lr 7.6120e-04 eta 0:04:21\n",
            "epoch [177/200] batch [10/18] time 0.437 (0.523) data 0.000 (0.089) loss 0.3243 (0.2042) acc 87.5000 (94.6875) lr 7.6120e-04 eta 0:03:40\n",
            "epoch [177/200] batch [15/18] time 0.435 (0.494) data 0.000 (0.059) loss 0.2511 (0.1819) acc 90.6250 (95.2083) lr 7.6120e-04 eta 0:03:26\n",
            "epoch [178/200] batch [5/18] time 0.441 (0.610) data 0.000 (0.177) loss 0.1005 (0.1403) acc 96.8750 (95.6250) lr 7.0224e-04 eta 0:04:09\n",
            "epoch [178/200] batch [10/18] time 0.436 (0.522) data 0.000 (0.089) loss 0.1834 (0.1414) acc 96.8750 (96.8750) lr 7.0224e-04 eta 0:03:30\n",
            "epoch [178/200] batch [15/18] time 0.434 (0.492) data 0.000 (0.059) loss 0.1014 (0.1437) acc 96.8750 (96.4583) lr 7.0224e-04 eta 0:03:16\n",
            "epoch [179/200] batch [5/18] time 0.435 (0.612) data 0.000 (0.179) loss 0.1546 (0.1295) acc 96.8750 (96.8750) lr 6.4556e-04 eta 0:03:59\n",
            "epoch [179/200] batch [10/18] time 0.434 (0.522) data 0.000 (0.090) loss 0.1312 (0.1715) acc 100.0000 (95.3125) lr 6.4556e-04 eta 0:03:21\n",
            "epoch [179/200] batch [15/18] time 0.435 (0.492) data 0.000 (0.060) loss 0.2035 (0.1787) acc 96.8750 (95.8333) lr 6.4556e-04 eta 0:03:07\n",
            "epoch [180/200] batch [5/18] time 0.447 (0.604) data 0.000 (0.171) loss 0.2072 (0.2695) acc 93.7500 (91.8750) lr 5.9119e-04 eta 0:03:45\n",
            "epoch [180/200] batch [10/18] time 0.432 (0.517) data 0.000 (0.086) loss 0.2270 (0.2224) acc 93.7500 (93.7500) lr 5.9119e-04 eta 0:03:10\n",
            "epoch [180/200] batch [15/18] time 0.428 (0.489) data 0.000 (0.057) loss 0.1396 (0.2022) acc 96.8750 (94.3750) lr 5.9119e-04 eta 0:02:57\n",
            "epoch [181/200] batch [5/18] time 0.443 (0.605) data 0.000 (0.173) loss 0.1708 (0.1334) acc 93.7500 (96.2500) lr 5.3915e-04 eta 0:03:34\n",
            "epoch [181/200] batch [10/18] time 0.435 (0.518) data 0.000 (0.087) loss 0.1480 (0.1268) acc 96.8750 (96.5625) lr 5.3915e-04 eta 0:03:01\n",
            "epoch [181/200] batch [15/18] time 0.431 (0.489) data 0.000 (0.058) loss 0.1256 (0.1374) acc 96.8750 (96.4583) lr 5.3915e-04 eta 0:02:48\n",
            "epoch [182/200] batch [5/18] time 0.440 (0.615) data 0.000 (0.184) loss 0.1068 (0.2795) acc 96.8750 (91.2500) lr 4.8943e-04 eta 0:03:27\n",
            "epoch [182/200] batch [10/18] time 0.431 (0.522) data 0.000 (0.092) loss 0.2681 (0.2068) acc 93.7500 (94.0625) lr 4.8943e-04 eta 0:02:53\n",
            "epoch [182/200] batch [15/18] time 0.428 (0.492) data 0.000 (0.062) loss 0.2774 (0.1884) acc 87.5000 (93.9583) lr 4.8943e-04 eta 0:02:40\n",
            "epoch [183/200] batch [5/18] time 0.447 (0.607) data 0.000 (0.174) loss 0.0986 (0.2680) acc 96.8750 (93.1250) lr 4.4207e-04 eta 0:03:13\n",
            "epoch [183/200] batch [10/18] time 0.433 (0.519) data 0.000 (0.087) loss 0.1614 (0.2144) acc 93.7500 (94.0625) lr 4.4207e-04 eta 0:02:42\n",
            "epoch [183/200] batch [15/18] time 0.425 (0.489) data 0.000 (0.058) loss 0.1559 (0.2116) acc 96.8750 (94.1667) lr 4.4207e-04 eta 0:02:31\n",
            "epoch [184/200] batch [5/18] time 0.435 (0.597) data 0.000 (0.167) loss 0.1180 (0.2003) acc 96.8750 (94.3750) lr 3.9706e-04 eta 0:02:59\n",
            "epoch [184/200] batch [10/18] time 0.431 (0.513) data 0.000 (0.084) loss 0.2461 (0.1857) acc 90.6250 (95.0000) lr 3.9706e-04 eta 0:02:31\n",
            "epoch [184/200] batch [15/18] time 0.430 (0.485) data 0.000 (0.056) loss 0.2129 (0.1871) acc 90.6250 (94.5833) lr 3.9706e-04 eta 0:02:21\n",
            "epoch [185/200] batch [5/18] time 0.437 (0.595) data 0.000 (0.165) loss 0.2179 (0.1879) acc 93.7500 (95.6250) lr 3.5443e-04 eta 0:02:48\n",
            "epoch [185/200] batch [10/18] time 0.430 (0.512) data 0.000 (0.083) loss 0.2079 (0.1746) acc 93.7500 (95.3125) lr 3.5443e-04 eta 0:02:22\n",
            "epoch [185/200] batch [15/18] time 0.431 (0.484) data 0.000 (0.055) loss 0.0657 (0.1888) acc 100.0000 (95.4167) lr 3.5443e-04 eta 0:02:12\n",
            "epoch [186/200] batch [5/18] time 0.443 (0.618) data 0.000 (0.187) loss 0.1139 (0.1401) acc 96.8750 (96.2500) lr 3.1417e-04 eta 0:02:43\n",
            "epoch [186/200] batch [10/18] time 0.435 (0.523) data 0.000 (0.094) loss 0.1781 (0.2154) acc 96.8750 (94.3750) lr 3.1417e-04 eta 0:02:16\n",
            "epoch [186/200] batch [15/18] time 0.427 (0.492) data 0.000 (0.063) loss 0.1203 (0.1950) acc 96.8750 (95.0000) lr 3.1417e-04 eta 0:02:05\n",
            "epoch [187/200] batch [5/18] time 0.438 (0.612) data 0.000 (0.181) loss 0.1618 (0.1142) acc 96.8750 (98.7500) lr 2.7630e-04 eta 0:02:31\n",
            "epoch [187/200] batch [10/18] time 0.433 (0.521) data 0.000 (0.091) loss 0.3760 (0.1498) acc 93.7500 (97.5000) lr 2.7630e-04 eta 0:02:05\n",
            "epoch [187/200] batch [15/18] time 0.430 (0.490) data 0.000 (0.061) loss 0.1054 (0.1536) acc 100.0000 (96.8750) lr 2.7630e-04 eta 0:01:56\n",
            "epoch [188/200] batch [5/18] time 0.435 (0.598) data 0.000 (0.170) loss 0.0210 (0.1534) acc 100.0000 (95.0000) lr 2.4083e-04 eta 0:02:16\n",
            "epoch [188/200] batch [10/18] time 0.430 (0.513) data 0.000 (0.085) loss 0.2377 (0.1767) acc 93.7500 (95.0000) lr 2.4083e-04 eta 0:01:54\n",
            "epoch [188/200] batch [15/18] time 0.427 (0.484) data 0.000 (0.057) loss 0.2679 (0.1631) acc 93.7500 (95.6250) lr 2.4083e-04 eta 0:01:46\n",
            "epoch [189/200] batch [5/18] time 0.433 (0.605) data 0.000 (0.176) loss 0.1575 (0.1938) acc 90.6250 (92.5000) lr 2.0777e-04 eta 0:02:07\n",
            "epoch [189/200] batch [10/18] time 0.429 (0.516) data 0.000 (0.088) loss 0.2559 (0.2178) acc 93.7500 (93.4375) lr 2.0777e-04 eta 0:01:46\n",
            "epoch [189/200] batch [15/18] time 0.432 (0.487) data 0.000 (0.059) loss 0.1509 (0.2022) acc 96.8750 (93.9583) lr 2.0777e-04 eta 0:01:37\n",
            "epoch [190/200] batch [5/18] time 0.437 (0.601) data 0.000 (0.172) loss 0.1034 (0.1305) acc 96.8750 (97.5000) lr 1.7713e-04 eta 0:01:55\n",
            "epoch [190/200] batch [10/18] time 0.431 (0.514) data 0.000 (0.086) loss 0.3057 (0.1948) acc 93.7500 (96.2500) lr 1.7713e-04 eta 0:01:36\n",
            "epoch [190/200] batch [15/18] time 0.430 (0.485) data 0.000 (0.058) loss 0.0471 (0.1608) acc 96.8750 (96.8750) lr 1.7713e-04 eta 0:01:28\n",
            "epoch [191/200] batch [5/18] time 0.442 (0.615) data 0.000 (0.185) loss 0.1177 (0.1831) acc 100.0000 (96.8750) lr 1.4891e-04 eta 0:01:47\n",
            "epoch [191/200] batch [10/18] time 0.432 (0.520) data 0.000 (0.092) loss 0.2044 (0.2210) acc 93.7500 (95.9375) lr 1.4891e-04 eta 0:01:28\n",
            "epoch [191/200] batch [15/18] time 0.429 (0.489) data 0.000 (0.062) loss 0.3516 (0.2281) acc 90.6250 (95.4167) lr 1.4891e-04 eta 0:01:20\n",
            "epoch [192/200] batch [5/18] time 0.437 (0.595) data 0.000 (0.166) loss 0.0965 (0.2519) acc 96.8750 (93.7500) lr 1.2312e-04 eta 0:01:33\n",
            "epoch [192/200] batch [10/18] time 0.431 (0.511) data 0.000 (0.083) loss 0.1773 (0.2296) acc 96.8750 (94.6875) lr 1.2312e-04 eta 0:01:17\n",
            "epoch [192/200] batch [15/18] time 0.427 (0.483) data 0.000 (0.056) loss 0.1398 (0.2039) acc 93.7500 (95.4167) lr 1.2312e-04 eta 0:01:10\n",
            "epoch [193/200] batch [5/18] time 0.436 (0.599) data 0.000 (0.172) loss 0.1013 (0.2047) acc 100.0000 (95.6250) lr 9.9763e-05 eta 0:01:23\n",
            "epoch [193/200] batch [10/18] time 0.432 (0.513) data 0.000 (0.086) loss 0.3037 (0.2067) acc 96.8750 (95.6250) lr 9.9763e-05 eta 0:01:08\n",
            "epoch [193/200] batch [15/18] time 0.424 (0.484) data 0.000 (0.058) loss 0.4607 (0.2054) acc 90.6250 (96.0417) lr 9.9763e-05 eta 0:01:02\n",
            "epoch [194/200] batch [5/18] time 0.436 (0.606) data 0.000 (0.177) loss 0.1211 (0.1655) acc 100.0000 (96.8750) lr 7.8853e-05 eta 0:01:13\n",
            "epoch [194/200] batch [10/18] time 0.431 (0.516) data 0.000 (0.089) loss 0.1958 (0.1591) acc 93.7500 (96.5625) lr 7.8853e-05 eta 0:00:59\n",
            "epoch [194/200] batch [15/18] time 0.428 (0.487) data 0.000 (0.059) loss 0.2055 (0.1598) acc 96.8750 (96.4583) lr 7.8853e-05 eta 0:00:54\n",
            "epoch [195/200] batch [5/18] time 0.437 (0.609) data 0.000 (0.181) loss 0.1845 (0.1685) acc 96.8750 (96.8750) lr 6.0390e-05 eta 0:01:02\n",
            "epoch [195/200] batch [10/18] time 0.428 (0.517) data 0.000 (0.091) loss 0.0343 (0.1361) acc 100.0000 (96.8750) lr 6.0390e-05 eta 0:00:50\n",
            "epoch [195/200] batch [15/18] time 0.428 (0.487) data 0.000 (0.061) loss 0.2211 (0.1633) acc 90.6250 (95.8333) lr 6.0390e-05 eta 0:00:45\n",
            "epoch [196/200] batch [5/18] time 0.434 (0.606) data 0.000 (0.178) loss 0.0492 (0.1123) acc 100.0000 (98.1250) lr 4.4380e-05 eta 0:00:51\n",
            "epoch [196/200] batch [10/18] time 0.432 (0.516) data 0.000 (0.089) loss 0.1510 (0.1153) acc 93.7500 (97.1875) lr 4.4380e-05 eta 0:00:41\n",
            "epoch [196/200] batch [15/18] time 0.432 (0.487) data 0.000 (0.059) loss 0.1935 (0.1484) acc 96.8750 (96.2500) lr 4.4380e-05 eta 0:00:36\n",
            "epoch [197/200] batch [5/18] time 0.435 (0.609) data 0.000 (0.182) loss 0.4851 (0.1819) acc 90.6250 (95.6250) lr 3.0827e-05 eta 0:00:40\n",
            "epoch [197/200] batch [10/18] time 0.428 (0.517) data 0.000 (0.091) loss 0.2490 (0.1557) acc 93.7500 (96.2500) lr 3.0827e-05 eta 0:00:32\n",
            "epoch [197/200] batch [15/18] time 0.430 (0.487) data 0.000 (0.061) loss 0.2132 (0.1758) acc 93.7500 (95.8333) lr 3.0827e-05 eta 0:00:27\n",
            "epoch [198/200] batch [5/18] time 0.438 (0.621) data 0.000 (0.192) loss 0.0804 (0.2182) acc 100.0000 (93.7500) lr 1.9733e-05 eta 0:00:30\n",
            "epoch [198/200] batch [10/18] time 0.428 (0.523) data 0.000 (0.096) loss 0.0958 (0.2075) acc 100.0000 (95.0000) lr 1.9733e-05 eta 0:00:23\n",
            "epoch [198/200] batch [15/18] time 0.427 (0.491) data 0.000 (0.064) loss 0.1745 (0.2076) acc 93.7500 (95.0000) lr 1.9733e-05 eta 0:00:19\n",
            "epoch [199/200] batch [5/18] time 0.441 (0.615) data 0.000 (0.185) loss 0.1663 (0.1164) acc 96.8750 (97.5000) lr 1.1101e-05 eta 0:00:19\n",
            "epoch [199/200] batch [10/18] time 0.432 (0.520) data 0.000 (0.093) loss 0.1485 (0.1366) acc 96.8750 (96.5625) lr 1.1101e-05 eta 0:00:13\n",
            "epoch [199/200] batch [15/18] time 0.430 (0.489) data 0.000 (0.062) loss 0.0733 (0.1346) acc 100.0000 (96.4583) lr 1.1101e-05 eta 0:00:10\n",
            "epoch [200/200] batch [5/18] time 0.436 (0.597) data 0.000 (0.168) loss 0.1444 (0.1673) acc 96.8750 (95.6250) lr 4.9344e-06 eta 0:00:07\n",
            "epoch [200/200] batch [10/18] time 0.429 (0.511) data 0.000 (0.084) loss 0.2064 (0.1677) acc 96.8750 (95.6250) lr 4.9344e-06 eta 0:00:04\n",
            "epoch [200/200] batch [15/18] time 0.431 (0.483) data 0.000 (0.056) loss 0.3206 (0.1754) acc 90.6250 (95.4167) lr 4.9344e-06 eta 0:00:01\n",
            "Checkpoint saved to output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_16shots/seed1/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:18<00:00,  1.96it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,228\n",
            "* accuracy: 88.0%\n",
            "* error: 12.0%\n",
            "* macro_f1: 87.8%\n",
            "Elapsed: 0:30:43\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets-16shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_16shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0ad11d-3ea8-4e73-ad1c-536200e27a88",
        "id": "sOesWqrdyWJf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 10:54:04.850651: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 10:54:04.870189: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 10:54:04.875985: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 10:54:04.890193: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 10:54:05.889119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '8']\n",
            "output_dir: output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_8shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 200\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_8shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_8-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  296\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "        [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "        [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "        ...,\n",
            "        [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "        [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "        [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-5.2644e-04, -1.1168e-02, -2.1661e-02,  ..., -4.8596e-03,\n",
            "           5.8912e-03,  6.7493e-03],\n",
            "         [ 5.8841e-04, -1.8584e-02, -2.5060e-02,  ..., -3.4987e-03,\n",
            "           5.4105e-03, -1.7861e-03],\n",
            "         [ 1.7309e-03, -7.2606e-03, -2.3481e-02,  ..., -2.1107e-03,\n",
            "           4.9070e-03,  6.0702e-03],\n",
            "         ...,\n",
            "         [ 1.0223e-03, -1.2080e-02, -1.5868e-02,  ..., -1.2882e-02,\n",
            "          -2.0567e-03,  4.8362e-03],\n",
            "         [ 7.8468e-03, -1.1038e-02, -1.0770e-02,  ...,  7.4988e-04,\n",
            "           1.5215e-03, -1.6774e-03],\n",
            "         [ 2.3028e-03, -5.9798e-03, -1.7329e-02,  ...,  5.5724e-04,\n",
            "           1.5857e-02,  4.3269e-03]],\n",
            "\n",
            "        [[ 6.0997e-03,  1.7106e-03, -2.1188e-02,  ...,  2.5523e-03,\n",
            "           2.1776e-03,  7.7525e-03],\n",
            "         [ 6.9275e-03, -6.8095e-03, -1.3799e-02,  ..., -3.6580e-03,\n",
            "           5.6513e-03,  1.9980e-03],\n",
            "         [ 6.2942e-04, -7.5124e-03, -9.4807e-03,  ...,  9.2128e-03,\n",
            "           1.0227e-02,  3.9989e-03],\n",
            "         ...,\n",
            "         [-8.9265e-04, -1.5108e-02, -1.9400e-02,  ..., -3.9164e-03,\n",
            "           4.4435e-03, -3.3369e-04],\n",
            "         [ 9.0713e-03, -1.0043e-02, -2.8737e-02,  ..., -4.9359e-03,\n",
            "           9.1699e-03,  5.4980e-03],\n",
            "         [-8.9653e-05, -7.9620e-03, -2.2422e-02,  ..., -2.5861e-03,\n",
            "           2.1928e-03,  6.9095e-03]],\n",
            "\n",
            "        [[ 2.2506e-04, -1.2583e-02, -1.8306e-02,  ..., -1.1051e-02,\n",
            "           9.2462e-03,  8.1454e-03],\n",
            "         [-9.4987e-04, -1.2930e-02, -2.0111e-02,  ..., -9.7730e-03,\n",
            "           5.3495e-03,  1.8893e-03],\n",
            "         [-2.7428e-03, -8.8699e-03, -1.4280e-02,  ...,  5.0471e-03,\n",
            "           5.3924e-03,  1.0839e-02],\n",
            "         ...,\n",
            "         [ 7.2136e-03, -9.1107e-03, -1.5424e-02,  ...,  1.9115e-03,\n",
            "           6.7415e-03, -8.7823e-04],\n",
            "         [-4.8866e-03, -9.1675e-03, -1.5351e-02,  ..., -5.0427e-03,\n",
            "           8.2830e-03,  1.1029e-02],\n",
            "         [-4.4785e-03, -1.2034e-02, -1.6046e-02,  ..., -1.3892e-03,\n",
            "           1.1470e-02, -4.6686e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1066e-02, -2.1956e-03, -1.7727e-02,  ..., -1.0564e-03,\n",
            "           1.2302e-02,  5.0345e-03],\n",
            "         [ 2.7537e-03, -4.1800e-04, -1.4596e-02,  ..., -1.1298e-03,\n",
            "           6.9791e-03,  3.2216e-03],\n",
            "         [ 3.3331e-03, -1.0128e-02, -1.2788e-02,  ..., -1.2218e-02,\n",
            "           2.8146e-03,  4.3975e-03],\n",
            "         ...,\n",
            "         [ 5.6838e-04, -7.7658e-04, -3.2178e-02,  ..., -6.8700e-03,\n",
            "           4.0220e-03, -3.8805e-04],\n",
            "         [ 9.2410e-04, -7.9189e-03, -1.0762e-02,  ..., -4.6880e-03,\n",
            "           5.9761e-03,  4.5386e-03],\n",
            "         [-2.8230e-04, -1.1029e-02, -1.4386e-02,  ..., -1.5611e-04,\n",
            "           1.0764e-02,  4.3288e-03]],\n",
            "\n",
            "        [[ 5.6610e-03, -6.1458e-03, -2.3435e-02,  ..., -8.1288e-03,\n",
            "           9.4217e-03,  1.0564e-02],\n",
            "         [ 1.7781e-03, -9.1155e-03, -7.3978e-03,  ...,  3.8989e-03,\n",
            "           8.7808e-03, -3.2376e-03],\n",
            "         [ 6.8626e-03, -1.4937e-02, -2.3481e-02,  ...,  1.5681e-03,\n",
            "           1.1096e-02,  2.1168e-03],\n",
            "         ...,\n",
            "         [ 2.3734e-03, -1.0926e-02, -2.4415e-02,  ..., -8.3730e-03,\n",
            "           1.5094e-02,  1.2127e-03],\n",
            "         [ 2.2802e-03, -9.2195e-03, -2.1869e-02,  ..., -2.9556e-03,\n",
            "           8.4852e-03,  4.9182e-03],\n",
            "         [-8.3199e-03, -8.2483e-03, -1.6881e-02,  ..., -4.5459e-03,\n",
            "           6.2860e-03,  5.5418e-04]],\n",
            "\n",
            "        [[ 1.3695e-03, -1.2808e-02, -1.6318e-02,  ...,  3.0864e-03,\n",
            "           4.0411e-03,  6.7378e-03],\n",
            "         [ 4.1695e-03, -1.2923e-02, -2.3946e-02,  ...,  2.9491e-03,\n",
            "           1.2264e-02,  2.7028e-03],\n",
            "         [-3.1091e-04, -1.2759e-02, -1.5500e-02,  ...,  1.6597e-03,\n",
            "           2.8471e-03, -5.2213e-03],\n",
            "         ...,\n",
            "         [ 9.3452e-05, -4.8621e-03, -1.9077e-02,  ..., -5.4662e-03,\n",
            "           5.2866e-03,  2.2937e-03],\n",
            "         [ 5.1670e-03, -1.0069e-02, -1.7373e-02,  ...,  2.5828e-03,\n",
            "           8.2563e-03,  1.3529e-03],\n",
            "         [ 3.2577e-03, -1.7065e-02, -1.7372e-02,  ..., -3.7200e-03,\n",
            "           4.5885e-03, -3.4474e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4769\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_8shots/seed1/tensorboard)\n",
            "epoch [1/200] batch [5/9] time 0.413 (0.811) data 0.000 (0.206) loss 0.6774 (1.2173) acc 81.2500 (60.0000) lr 1.0000e-05 eta 0:24:15\n",
            "epoch [2/200] batch [5/9] time 0.422 (0.597) data 0.000 (0.178) loss 1.0596 (0.9918) acc 68.7500 (70.6250) lr 2.0000e-02 eta 0:17:46\n",
            "epoch [3/200] batch [5/9] time 0.423 (0.596) data 0.000 (0.174) loss 0.7034 (0.8085) acc 87.5000 (77.5000) lr 1.9999e-02 eta 0:17:39\n",
            "epoch [4/200] batch [5/9] time 0.429 (0.606) data 0.000 (0.184) loss 1.0801 (0.8707) acc 68.7500 (76.8750) lr 1.9995e-02 eta 0:17:51\n",
            "epoch [5/200] batch [5/9] time 0.423 (0.605) data 0.000 (0.183) loss 0.6449 (0.6751) acc 78.1250 (81.2500) lr 1.9989e-02 eta 0:17:43\n",
            "epoch [6/200] batch [5/9] time 0.425 (0.601) data 0.000 (0.179) loss 0.5914 (0.7317) acc 81.2500 (80.6250) lr 1.9980e-02 eta 0:17:31\n",
            "epoch [7/200] batch [5/9] time 0.423 (0.593) data 0.000 (0.171) loss 0.7736 (0.6405) acc 84.3750 (83.7500) lr 1.9969e-02 eta 0:17:12\n",
            "epoch [8/200] batch [5/9] time 0.424 (0.598) data 0.000 (0.176) loss 0.7868 (0.7795) acc 84.3750 (79.3750) lr 1.9956e-02 eta 0:17:16\n",
            "epoch [9/200] batch [5/9] time 0.427 (0.606) data 0.000 (0.184) loss 0.5066 (0.5758) acc 90.6250 (86.2500) lr 1.9940e-02 eta 0:17:24\n",
            "epoch [10/200] batch [5/9] time 0.424 (0.593) data 0.000 (0.171) loss 0.4613 (0.5523) acc 87.5000 (83.1250) lr 1.9921e-02 eta 0:16:55\n",
            "epoch [11/200] batch [5/9] time 0.428 (0.615) data 0.000 (0.192) loss 0.5689 (0.5673) acc 84.3750 (85.0000) lr 1.9900e-02 eta 0:17:28\n",
            "epoch [12/200] batch [5/9] time 0.429 (0.591) data 0.000 (0.169) loss 0.3783 (0.4681) acc 90.6250 (88.1250) lr 1.9877e-02 eta 0:16:42\n",
            "epoch [13/200] batch [5/9] time 0.421 (0.585) data 0.000 (0.163) loss 0.9214 (0.5714) acc 68.7500 (80.6250) lr 1.9851e-02 eta 0:16:27\n",
            "epoch [14/200] batch [5/9] time 0.428 (0.614) data 0.000 (0.191) loss 0.4988 (0.4218) acc 87.5000 (91.8750) lr 1.9823e-02 eta 0:17:10\n",
            "epoch [15/200] batch [5/9] time 0.426 (0.602) data 0.000 (0.180) loss 0.2972 (0.3819) acc 90.6250 (87.5000) lr 1.9792e-02 eta 0:16:45\n",
            "epoch [16/200] batch [5/9] time 0.425 (0.585) data 0.000 (0.163) loss 0.5695 (0.4423) acc 87.5000 (89.3750) lr 1.9759e-02 eta 0:16:11\n",
            "epoch [17/200] batch [5/9] time 0.426 (0.599) data 0.000 (0.177) loss 0.4716 (0.4368) acc 81.2500 (87.5000) lr 1.9724e-02 eta 0:16:28\n",
            "epoch [18/200] batch [5/9] time 0.431 (0.605) data 0.000 (0.180) loss 0.4328 (0.5191) acc 84.3750 (86.2500) lr 1.9686e-02 eta 0:16:33\n",
            "epoch [19/200] batch [5/9] time 0.424 (0.603) data 0.000 (0.181) loss 0.1805 (0.3561) acc 96.8750 (90.6250) lr 1.9646e-02 eta 0:16:24\n",
            "epoch [20/200] batch [5/9] time 0.426 (0.608) data 0.000 (0.185) loss 0.3531 (0.3720) acc 93.7500 (91.2500) lr 1.9603e-02 eta 0:16:26\n",
            "epoch [21/200] batch [5/9] time 0.424 (0.591) data 0.000 (0.167) loss 0.3312 (0.4365) acc 87.5000 (88.7500) lr 1.9558e-02 eta 0:15:53\n",
            "epoch [22/200] batch [5/9] time 0.426 (0.601) data 0.000 (0.178) loss 0.2595 (0.3730) acc 93.7500 (91.2500) lr 1.9511e-02 eta 0:16:05\n",
            "epoch [23/200] batch [5/9] time 0.426 (0.598) data 0.000 (0.175) loss 0.3716 (0.3617) acc 93.7500 (93.7500) lr 1.9461e-02 eta 0:15:55\n",
            "epoch [24/200] batch [5/9] time 0.427 (0.602) data 0.000 (0.178) loss 0.1949 (0.4350) acc 96.8750 (88.1250) lr 1.9409e-02 eta 0:15:56\n",
            "epoch [25/200] batch [5/9] time 0.426 (0.603) data 0.000 (0.179) loss 0.3392 (0.4111) acc 93.7500 (88.1250) lr 1.9354e-02 eta 0:15:52\n",
            "epoch [26/200] batch [5/9] time 0.424 (0.593) data 0.000 (0.172) loss 0.4649 (0.4282) acc 81.2500 (88.1250) lr 1.9298e-02 eta 0:15:31\n",
            "epoch [27/200] batch [5/9] time 0.424 (0.595) data 0.000 (0.173) loss 0.4145 (0.3350) acc 87.5000 (93.7500) lr 1.9239e-02 eta 0:15:28\n",
            "epoch [28/200] batch [5/9] time 0.427 (0.607) data 0.000 (0.185) loss 0.2448 (0.4045) acc 93.7500 (90.6250) lr 1.9178e-02 eta 0:15:42\n",
            "epoch [29/200] batch [5/9] time 0.423 (0.599) data 0.000 (0.177) loss 0.3212 (0.4219) acc 90.6250 (88.7500) lr 1.9114e-02 eta 0:15:24\n",
            "epoch [30/200] batch [5/9] time 0.422 (0.601) data 0.000 (0.179) loss 0.3224 (0.3053) acc 96.8750 (95.0000) lr 1.9048e-02 eta 0:15:21\n",
            "epoch [31/200] batch [5/9] time 0.424 (0.606) data 0.000 (0.184) loss 0.3910 (0.3419) acc 96.8750 (92.5000) lr 1.8980e-02 eta 0:15:23\n",
            "epoch [32/200] batch [5/9] time 0.425 (0.591) data 0.000 (0.170) loss 0.1677 (0.2830) acc 93.7500 (94.3750) lr 1.8910e-02 eta 0:14:56\n",
            "epoch [33/200] batch [5/9] time 0.429 (0.606) data 0.000 (0.183) loss 0.3650 (0.4087) acc 90.6250 (88.7500) lr 1.8838e-02 eta 0:15:13\n",
            "epoch [34/200] batch [5/9] time 0.422 (0.608) data 0.000 (0.186) loss 0.7197 (0.4572) acc 84.3750 (88.1250) lr 1.8763e-02 eta 0:15:10\n",
            "epoch [35/200] batch [5/9] time 0.428 (0.594) data 0.000 (0.171) loss 0.5342 (0.4286) acc 84.3750 (88.1250) lr 1.8686e-02 eta 0:14:44\n",
            "epoch [36/200] batch [5/9] time 0.431 (0.597) data 0.000 (0.172) loss 0.2333 (0.3593) acc 96.8750 (92.5000) lr 1.8607e-02 eta 0:14:43\n",
            "epoch [37/200] batch [5/9] time 0.426 (0.609) data 0.000 (0.188) loss 0.4080 (0.2621) acc 90.6250 (94.3750) lr 1.8526e-02 eta 0:14:55\n",
            "epoch [38/200] batch [5/9] time 0.423 (0.593) data 0.000 (0.170) loss 0.1599 (0.2121) acc 100.0000 (96.8750) lr 1.8443e-02 eta 0:14:26\n",
            "epoch [39/200] batch [5/9] time 0.425 (0.602) data 0.000 (0.180) loss 0.2475 (0.3006) acc 93.7500 (92.5000) lr 1.8358e-02 eta 0:14:34\n",
            "epoch [40/200] batch [5/9] time 0.426 (0.616) data 0.000 (0.194) loss 0.3866 (0.2672) acc 93.7500 (95.0000) lr 1.8271e-02 eta 0:14:49\n",
            "epoch [41/200] batch [5/9] time 0.427 (0.593) data 0.000 (0.170) loss 0.4465 (0.4110) acc 93.7500 (90.0000) lr 1.8181e-02 eta 0:14:10\n",
            "epoch [42/200] batch [5/9] time 0.429 (0.589) data 0.000 (0.164) loss 0.6777 (0.4365) acc 75.0000 (86.8750) lr 1.8090e-02 eta 0:13:59\n",
            "epoch [43/200] batch [5/9] time 0.429 (0.605) data 0.000 (0.182) loss 0.2555 (0.3503) acc 93.7500 (91.8750) lr 1.7997e-02 eta 0:14:16\n",
            "epoch [44/200] batch [5/9] time 0.422 (0.605) data 0.000 (0.183) loss 0.2491 (0.2879) acc 87.5000 (90.0000) lr 1.7902e-02 eta 0:14:12\n",
            "epoch [45/200] batch [5/9] time 0.426 (0.589) data 0.000 (0.167) loss 0.3243 (0.2897) acc 87.5000 (92.5000) lr 1.7804e-02 eta 0:13:44\n",
            "epoch [46/200] batch [5/9] time 0.426 (0.608) data 0.000 (0.185) loss 0.3215 (0.3243) acc 96.8750 (92.5000) lr 1.7705e-02 eta 0:14:05\n",
            "epoch [47/200] batch [5/9] time 0.421 (0.582) data 0.000 (0.161) loss 0.3259 (0.2251) acc 90.6250 (95.6250) lr 1.7604e-02 eta 0:13:23\n",
            "epoch [48/200] batch [5/9] time 0.425 (0.601) data 0.000 (0.180) loss 0.3386 (0.2121) acc 93.7500 (96.2500) lr 1.7501e-02 eta 0:13:45\n",
            "epoch [49/200] batch [5/9] time 0.427 (0.604) data 0.000 (0.181) loss 0.2326 (0.2187) acc 93.7500 (93.7500) lr 1.7396e-02 eta 0:13:42\n",
            "epoch [50/200] batch [5/9] time 0.423 (0.594) data 0.000 (0.173) loss 0.1840 (0.2741) acc 93.7500 (94.3750) lr 1.7290e-02 eta 0:13:24\n",
            "epoch [51/200] batch [5/9] time 0.425 (0.588) data 0.000 (0.167) loss 0.4226 (0.3124) acc 90.6250 (91.2500) lr 1.7181e-02 eta 0:13:11\n",
            "epoch [52/200] batch [5/9] time 0.429 (0.596) data 0.000 (0.173) loss 0.3875 (0.2670) acc 84.3750 (92.5000) lr 1.7071e-02 eta 0:13:15\n",
            "epoch [53/200] batch [5/9] time 0.428 (0.595) data 0.000 (0.172) loss 0.4248 (0.3361) acc 90.6250 (93.1250) lr 1.6959e-02 eta 0:13:09\n",
            "epoch [54/200] batch [5/9] time 0.421 (0.599) data 0.000 (0.179) loss 0.2107 (0.2176) acc 93.7500 (94.3750) lr 1.6845e-02 eta 0:13:09\n",
            "epoch [55/200] batch [5/9] time 0.421 (0.600) data 0.000 (0.180) loss 0.2383 (0.2779) acc 90.6250 (93.7500) lr 1.6730e-02 eta 0:13:05\n",
            "epoch [56/200] batch [5/9] time 0.421 (0.597) data 0.000 (0.176) loss 0.1713 (0.1838) acc 96.8750 (96.2500) lr 1.6613e-02 eta 0:12:56\n",
            "epoch [57/200] batch [5/9] time 0.429 (0.616) data 0.000 (0.194) loss 0.1251 (0.1565) acc 96.8750 (95.6250) lr 1.6494e-02 eta 0:13:15\n",
            "epoch [58/200] batch [5/9] time 0.429 (0.608) data 0.000 (0.185) loss 0.1644 (0.2262) acc 96.8750 (95.6250) lr 1.6374e-02 eta 0:12:59\n",
            "epoch [59/200] batch [5/9] time 0.422 (0.593) data 0.000 (0.171) loss 0.4850 (0.2957) acc 87.5000 (92.5000) lr 1.6252e-02 eta 0:12:35\n",
            "epoch [60/200] batch [5/9] time 0.427 (0.600) data 0.000 (0.178) loss 0.2020 (0.2131) acc 96.8750 (95.6250) lr 1.6129e-02 eta 0:12:38\n",
            "epoch [61/200] batch [5/9] time 0.426 (0.602) data 0.000 (0.178) loss 0.3167 (0.2862) acc 93.7500 (90.6250) lr 1.6004e-02 eta 0:12:35\n",
            "epoch [62/200] batch [5/9] time 0.425 (0.593) data 0.000 (0.170) loss 0.2754 (0.2971) acc 90.6250 (89.3750) lr 1.5878e-02 eta 0:12:18\n",
            "epoch [63/200] batch [5/9] time 0.431 (0.595) data 0.000 (0.171) loss 0.2184 (0.2416) acc 93.7500 (93.1250) lr 1.5750e-02 eta 0:12:15\n",
            "epoch [64/200] batch [5/9] time 0.427 (0.593) data 0.000 (0.170) loss 0.2343 (0.2111) acc 96.8750 (95.6250) lr 1.5621e-02 eta 0:12:07\n",
            "epoch [65/200] batch [5/9] time 0.429 (0.600) data 0.000 (0.176) loss 0.1872 (0.1658) acc 93.7500 (96.2500) lr 1.5490e-02 eta 0:12:11\n",
            "epoch [66/200] batch [5/9] time 0.426 (0.599) data 0.000 (0.176) loss 0.1624 (0.2921) acc 96.8750 (91.2500) lr 1.5358e-02 eta 0:12:05\n",
            "epoch [67/200] batch [5/9] time 0.425 (0.593) data 0.000 (0.172) loss 0.4216 (0.3052) acc 84.3750 (91.2500) lr 1.5225e-02 eta 0:11:52\n",
            "epoch [68/200] batch [5/9] time 0.427 (0.593) data 0.000 (0.170) loss 0.3035 (0.2287) acc 90.6250 (93.7500) lr 1.5090e-02 eta 0:11:46\n",
            "epoch [69/200] batch [5/9] time 0.428 (0.612) data 0.000 (0.188) loss 0.2216 (0.2595) acc 93.7500 (92.5000) lr 1.4955e-02 eta 0:12:03\n",
            "epoch [70/200] batch [5/9] time 0.426 (0.592) data 0.000 (0.170) loss 0.3526 (0.2530) acc 90.6250 (93.7500) lr 1.4818e-02 eta 0:11:35\n",
            "epoch [71/200] batch [5/9] time 0.428 (0.600) data 0.000 (0.177) loss 0.5787 (0.2826) acc 87.5000 (93.1250) lr 1.4679e-02 eta 0:11:39\n",
            "epoch [72/200] batch [5/9] time 0.426 (0.629) data 0.000 (0.206) loss 0.4416 (0.2832) acc 87.5000 (91.8750) lr 1.4540e-02 eta 0:12:07\n",
            "epoch [73/200] batch [5/9] time 0.427 (0.590) data 0.000 (0.166) loss 0.1924 (0.2651) acc 100.0000 (94.3750) lr 1.4399e-02 eta 0:11:16\n",
            "epoch [74/200] batch [5/9] time 0.425 (0.587) data 0.000 (0.164) loss 0.1951 (0.1652) acc 96.8750 (96.8750) lr 1.4258e-02 eta 0:11:07\n",
            "epoch [75/200] batch [5/9] time 0.428 (0.597) data 0.000 (0.175) loss 0.2465 (0.2157) acc 93.7500 (94.3750) lr 1.4115e-02 eta 0:11:13\n",
            "epoch [76/200] batch [5/9] time 0.427 (0.592) data 0.000 (0.168) loss 0.1903 (0.2472) acc 93.7500 (93.1250) lr 1.3971e-02 eta 0:11:02\n",
            "epoch [77/200] batch [5/9] time 0.429 (0.593) data 0.000 (0.170) loss 0.2137 (0.3163) acc 93.7500 (93.1250) lr 1.3827e-02 eta 0:10:58\n",
            "epoch [78/200] batch [5/9] time 0.425 (0.607) data 0.000 (0.185) loss 0.1756 (0.2397) acc 93.7500 (92.5000) lr 1.3681e-02 eta 0:11:08\n",
            "epoch [79/200] batch [5/9] time 0.421 (0.601) data 0.000 (0.180) loss 0.2355 (0.3019) acc 90.6250 (90.6250) lr 1.3535e-02 eta 0:10:57\n",
            "epoch [80/200] batch [5/9] time 0.425 (0.611) data 0.000 (0.189) loss 0.3583 (0.2319) acc 87.5000 (93.1250) lr 1.3387e-02 eta 0:11:02\n",
            "epoch [81/200] batch [5/9] time 0.426 (0.591) data 0.000 (0.170) loss 0.1441 (0.2320) acc 100.0000 (95.6250) lr 1.3239e-02 eta 0:10:35\n",
            "epoch [82/200] batch [5/9] time 0.425 (0.604) data 0.001 (0.181) loss 0.2618 (0.2071) acc 93.7500 (95.0000) lr 1.3090e-02 eta 0:10:43\n",
            "epoch [83/200] batch [5/9] time 0.422 (0.595) data 0.000 (0.174) loss 0.3070 (0.2292) acc 96.8750 (95.6250) lr 1.2940e-02 eta 0:10:29\n",
            "epoch [84/200] batch [5/9] time 0.425 (0.605) data 0.000 (0.183) loss 0.4203 (0.2418) acc 90.6250 (95.0000) lr 1.2790e-02 eta 0:10:34\n",
            "epoch [85/200] batch [5/9] time 0.427 (0.587) data 0.000 (0.164) loss 0.1963 (0.2049) acc 93.7500 (93.7500) lr 1.2639e-02 eta 0:10:09\n",
            "epoch [86/200] batch [5/9] time 0.427 (0.611) data 0.000 (0.189) loss 0.2866 (0.2635) acc 93.7500 (92.5000) lr 1.2487e-02 eta 0:10:29\n",
            "epoch [87/200] batch [5/9] time 0.421 (0.598) data 0.000 (0.177) loss 0.0827 (0.1984) acc 96.8750 (94.3750) lr 1.2334e-02 eta 0:10:10\n",
            "epoch [88/200] batch [5/9] time 0.427 (0.593) data 0.000 (0.171) loss 0.1271 (0.2718) acc 100.0000 (94.3750) lr 1.2181e-02 eta 0:10:00\n",
            "epoch [89/200] batch [5/9] time 0.427 (0.593) data 0.000 (0.171) loss 0.1737 (0.3214) acc 96.8750 (91.8750) lr 1.2028e-02 eta 0:09:54\n",
            "epoch [90/200] batch [5/9] time 0.425 (0.595) data 0.000 (0.173) loss 0.1820 (0.1526) acc 96.8750 (96.8750) lr 1.1874e-02 eta 0:09:51\n",
            "epoch [91/200] batch [5/9] time 0.430 (0.594) data 0.000 (0.169) loss 0.1358 (0.1594) acc 96.8750 (95.6250) lr 1.1719e-02 eta 0:09:44\n",
            "epoch [92/200] batch [5/9] time 0.427 (0.610) data 0.000 (0.188) loss 0.3001 (0.2698) acc 93.7500 (93.1250) lr 1.1564e-02 eta 0:09:55\n",
            "epoch [93/200] batch [5/9] time 0.430 (0.604) data 0.000 (0.181) loss 0.1208 (0.2381) acc 96.8750 (92.5000) lr 1.1409e-02 eta 0:09:44\n",
            "epoch [94/200] batch [5/9] time 0.426 (0.590) data 0.000 (0.168) loss 0.1432 (0.1462) acc 96.8750 (97.5000) lr 1.1253e-02 eta 0:09:25\n",
            "epoch [95/200] batch [5/9] time 0.424 (0.608) data 0.000 (0.187) loss 0.4943 (0.2458) acc 84.3750 (93.7500) lr 1.1097e-02 eta 0:09:37\n",
            "epoch [96/200] batch [5/9] time 0.425 (0.601) data 0.000 (0.179) loss 0.4008 (0.2441) acc 81.2500 (94.3750) lr 1.0941e-02 eta 0:09:24\n",
            "epoch [97/200] batch [5/9] time 0.427 (0.595) data 0.000 (0.172) loss 0.1680 (0.1958) acc 93.7500 (93.7500) lr 1.0785e-02 eta 0:09:14\n",
            "epoch [98/200] batch [5/9] time 0.423 (0.596) data 0.000 (0.175) loss 0.1666 (0.2324) acc 96.8750 (93.7500) lr 1.0628e-02 eta 0:09:09\n",
            "epoch [99/200] batch [5/9] time 0.423 (0.590) data 0.000 (0.168) loss 0.1207 (0.1406) acc 100.0000 (97.5000) lr 1.0471e-02 eta 0:08:58\n",
            "epoch [100/200] batch [5/9] time 0.423 (0.587) data 0.000 (0.166) loss 0.1363 (0.1508) acc 96.8750 (97.5000) lr 1.0314e-02 eta 0:08:50\n",
            "epoch [101/200] batch [5/9] time 0.428 (0.594) data 0.000 (0.172) loss 0.3000 (0.2665) acc 93.7500 (93.1250) lr 1.0157e-02 eta 0:08:51\n",
            "epoch [102/200] batch [5/9] time 0.427 (0.599) data 0.000 (0.176) loss 0.4448 (0.2474) acc 87.5000 (93.7500) lr 1.0000e-02 eta 0:08:50\n",
            "epoch [103/200] batch [5/9] time 0.423 (0.609) data 0.000 (0.186) loss 0.2029 (0.2022) acc 100.0000 (96.2500) lr 9.8429e-03 eta 0:08:53\n",
            "epoch [104/200] batch [5/9] time 0.426 (0.604) data 0.000 (0.181) loss 0.1225 (0.1860) acc 96.8750 (96.8750) lr 9.6859e-03 eta 0:08:44\n",
            "epoch [105/200] batch [5/9] time 0.428 (0.594) data 0.000 (0.171) loss 0.3772 (0.2860) acc 90.6250 (93.1250) lr 9.5289e-03 eta 0:08:30\n",
            "epoch [106/200] batch [5/9] time 0.430 (0.609) data 0.000 (0.187) loss 0.2944 (0.1359) acc 93.7500 (98.1250) lr 9.3721e-03 eta 0:08:37\n",
            "epoch [107/200] batch [5/9] time 0.423 (0.600) data 0.000 (0.178) loss 0.3720 (0.1658) acc 93.7500 (96.8750) lr 9.2154e-03 eta 0:08:24\n",
            "epoch [108/200] batch [5/9] time 0.430 (0.600) data 0.000 (0.176) loss 0.2182 (0.1623) acc 93.7500 (95.6250) lr 9.0589e-03 eta 0:08:19\n",
            "epoch [109/200] batch [5/9] time 0.425 (0.600) data 0.000 (0.177) loss 0.0727 (0.2084) acc 100.0000 (95.6250) lr 8.9027e-03 eta 0:08:13\n",
            "epoch [110/200] batch [5/9] time 0.429 (0.589) data 0.000 (0.166) loss 0.2843 (0.2686) acc 90.6250 (90.6250) lr 8.7467e-03 eta 0:07:59\n",
            "epoch [111/200] batch [5/9] time 0.420 (0.593) data 0.000 (0.172) loss 0.2111 (0.1723) acc 96.8750 (96.8750) lr 8.5910e-03 eta 0:07:57\n",
            "epoch [112/200] batch [5/9] time 0.422 (0.603) data 0.000 (0.181) loss 0.2930 (0.2482) acc 90.6250 (93.7500) lr 8.4357e-03 eta 0:08:00\n",
            "epoch [113/200] batch [5/9] time 0.424 (0.599) data 0.000 (0.177) loss 0.2302 (0.1579) acc 93.7500 (95.6250) lr 8.2807e-03 eta 0:07:51\n",
            "epoch [114/200] batch [5/9] time 0.426 (0.605) data 0.000 (0.182) loss 0.0706 (0.1603) acc 100.0000 (96.8750) lr 8.1262e-03 eta 0:07:50\n",
            "epoch [115/200] batch [5/9] time 0.423 (0.621) data 0.000 (0.198) loss 0.1083 (0.1291) acc 100.0000 (97.5000) lr 7.9721e-03 eta 0:07:57\n",
            "epoch [116/200] batch [5/9] time 0.426 (0.595) data 0.000 (0.172) loss 0.2457 (0.1908) acc 90.6250 (94.3750) lr 7.8186e-03 eta 0:07:31\n",
            "epoch [117/200] batch [5/9] time 0.429 (0.595) data 0.000 (0.172) loss 0.6840 (0.3419) acc 78.1250 (90.0000) lr 7.6655e-03 eta 0:07:26\n",
            "epoch [118/200] batch [5/9] time 0.429 (0.610) data 0.000 (0.187) loss 0.2762 (0.2366) acc 93.7500 (95.0000) lr 7.5131e-03 eta 0:07:32\n",
            "epoch [119/200] batch [5/9] time 0.423 (0.593) data 0.000 (0.172) loss 0.3716 (0.2142) acc 90.6250 (95.6250) lr 7.3613e-03 eta 0:07:14\n",
            "epoch [120/200] batch [5/9] time 0.425 (0.594) data 0.001 (0.171) loss 0.3760 (0.1522) acc 90.6250 (97.5000) lr 7.2101e-03 eta 0:07:10\n",
            "epoch [121/200] batch [5/9] time 0.427 (0.605) data 0.000 (0.183) loss 0.3443 (0.1784) acc 90.6250 (95.6250) lr 7.0596e-03 eta 0:07:12\n",
            "epoch [122/200] batch [5/9] time 0.424 (0.598) data 0.000 (0.176) loss 0.3362 (0.1609) acc 90.6250 (96.2500) lr 6.9098e-03 eta 0:07:02\n",
            "epoch [123/200] batch [5/9] time 0.429 (0.597) data 0.000 (0.174) loss 0.2438 (0.2184) acc 96.8750 (93.1250) lr 6.7608e-03 eta 0:06:55\n",
            "epoch [124/200] batch [5/9] time 0.423 (0.595) data 0.000 (0.174) loss 0.4172 (0.2052) acc 87.5000 (93.7500) lr 6.6126e-03 eta 0:06:49\n",
            "epoch [125/200] batch [5/9] time 0.425 (0.590) data 0.000 (0.168) loss 0.1584 (0.1868) acc 96.8750 (95.6250) lr 6.4653e-03 eta 0:06:40\n",
            "epoch [126/200] batch [5/9] time 0.429 (0.606) data 0.000 (0.182) loss 0.3005 (0.1898) acc 93.7500 (95.0000) lr 6.3188e-03 eta 0:06:46\n",
            "epoch [127/200] batch [5/9] time 0.429 (0.604) data 0.000 (0.182) loss 0.0992 (0.1910) acc 100.0000 (96.2500) lr 6.1732e-03 eta 0:06:39\n",
            "epoch [128/200] batch [5/9] time 0.426 (0.597) data 0.000 (0.173) loss 0.4369 (0.1972) acc 90.6250 (95.6250) lr 6.0285e-03 eta 0:06:29\n",
            "epoch [129/200] batch [5/9] time 0.424 (0.607) data 0.000 (0.183) loss 0.1382 (0.1898) acc 96.8750 (95.6250) lr 5.8849e-03 eta 0:06:30\n",
            "epoch [130/200] batch [5/9] time 0.429 (0.600) data 0.000 (0.178) loss 0.0708 (0.1477) acc 100.0000 (96.8750) lr 5.7422e-03 eta 0:06:20\n",
            "epoch [131/200] batch [5/9] time 0.432 (0.597) data 0.000 (0.172) loss 0.2352 (0.1173) acc 93.7500 (97.5000) lr 5.6006e-03 eta 0:06:13\n",
            "epoch [132/200] batch [5/9] time 0.427 (0.618) data 0.000 (0.195) loss 0.3189 (0.2303) acc 96.8750 (96.2500) lr 5.4601e-03 eta 0:06:20\n",
            "epoch [133/200] batch [5/9] time 0.424 (0.596) data 0.000 (0.175) loss 0.1491 (0.1285) acc 96.8750 (96.2500) lr 5.3207e-03 eta 0:06:01\n",
            "epoch [134/200] batch [5/9] time 0.424 (0.592) data 0.000 (0.169) loss 0.2434 (0.1285) acc 90.6250 (96.8750) lr 5.1825e-03 eta 0:05:54\n",
            "epoch [135/200] batch [5/9] time 0.423 (0.617) data 0.000 (0.194) loss 0.1565 (0.1171) acc 93.7500 (98.7500) lr 5.0454e-03 eta 0:06:03\n",
            "epoch [136/200] batch [5/9] time 0.428 (0.600) data 0.000 (0.178) loss 0.0747 (0.1767) acc 100.0000 (96.2500) lr 4.9096e-03 eta 0:05:48\n",
            "epoch [137/200] batch [5/9] time 0.425 (0.601) data 0.000 (0.179) loss 0.1013 (0.1309) acc 96.8750 (96.8750) lr 4.7750e-03 eta 0:05:43\n",
            "epoch [138/200] batch [5/9] time 0.428 (0.619) data 0.000 (0.197) loss 0.0819 (0.1142) acc 100.0000 (98.1250) lr 4.6417e-03 eta 0:05:48\n",
            "epoch [139/200] batch [5/9] time 0.428 (0.593) data 0.000 (0.170) loss 0.0735 (0.1225) acc 96.8750 (96.8750) lr 4.5098e-03 eta 0:05:27\n",
            "epoch [140/200] batch [5/9] time 0.422 (0.595) data 0.000 (0.174) loss 0.2234 (0.1752) acc 96.8750 (96.2500) lr 4.3792e-03 eta 0:05:23\n",
            "epoch [141/200] batch [5/9] time 0.424 (0.605) data 0.000 (0.183) loss 0.2196 (0.2199) acc 90.6250 (93.7500) lr 4.2499e-03 eta 0:05:23\n",
            "epoch [142/200] batch [5/9] time 0.426 (0.593) data 0.000 (0.170) loss 0.3368 (0.1524) acc 84.3750 (95.0000) lr 4.1221e-03 eta 0:05:11\n",
            "epoch [143/200] batch [5/9] time 0.426 (0.597) data 0.000 (0.174) loss 0.1364 (0.2004) acc 100.0000 (96.2500) lr 3.9958e-03 eta 0:05:08\n",
            "epoch [144/200] batch [5/9] time 0.422 (0.598) data 0.000 (0.177) loss 0.0887 (0.1335) acc 100.0000 (97.5000) lr 3.8709e-03 eta 0:05:03\n",
            "epoch [145/200] batch [5/9] time 0.423 (0.594) data 0.000 (0.173) loss 0.2836 (0.1480) acc 93.7500 (96.8750) lr 3.7476e-03 eta 0:04:56\n",
            "epoch [146/200] batch [5/9] time 0.426 (0.603) data 0.000 (0.180) loss 0.1936 (0.1482) acc 96.8750 (96.2500) lr 3.6258e-03 eta 0:04:55\n",
            "epoch [147/200] batch [5/9] time 0.429 (0.601) data 0.000 (0.179) loss 0.0571 (0.1650) acc 100.0000 (96.2500) lr 3.5055e-03 eta 0:04:49\n",
            "epoch [148/200] batch [5/9] time 0.428 (0.599) data 0.000 (0.176) loss 0.2320 (0.1838) acc 93.7500 (95.0000) lr 3.3869e-03 eta 0:04:42\n",
            "epoch [149/200] batch [5/9] time 0.427 (0.602) data 0.000 (0.179) loss 0.3240 (0.1756) acc 90.6250 (95.6250) lr 3.2699e-03 eta 0:04:38\n",
            "epoch [150/200] batch [5/9] time 0.425 (0.605) data 0.000 (0.183) loss 0.0696 (0.1957) acc 100.0000 (96.2500) lr 3.1545e-03 eta 0:04:34\n",
            "epoch [151/200] batch [5/9] time 0.427 (0.589) data 0.000 (0.168) loss 0.2072 (0.1633) acc 96.8750 (96.2500) lr 3.0409e-03 eta 0:04:22\n",
            "epoch [152/200] batch [5/9] time 0.425 (0.605) data 0.000 (0.183) loss 0.2986 (0.2447) acc 93.7500 (93.7500) lr 2.9289e-03 eta 0:04:23\n",
            "epoch [153/200] batch [5/9] time 0.428 (0.598) data 0.000 (0.174) loss 0.2937 (0.2070) acc 93.7500 (94.3750) lr 2.8187e-03 eta 0:04:15\n",
            "epoch [154/200] batch [5/9] time 0.426 (0.587) data 0.000 (0.165) loss 0.2421 (0.2115) acc 90.6250 (94.3750) lr 2.7103e-03 eta 0:04:05\n",
            "epoch [155/200] batch [5/9] time 0.426 (0.598) data 0.000 (0.175) loss 0.1932 (0.1192) acc 93.7500 (97.5000) lr 2.6037e-03 eta 0:04:04\n",
            "epoch [156/200] batch [5/9] time 0.424 (0.591) data 0.000 (0.170) loss 0.1691 (0.1758) acc 93.7500 (95.0000) lr 2.4989e-03 eta 0:03:56\n",
            "epoch [157/200] batch [5/9] time 0.422 (0.589) data 0.000 (0.167) loss 0.0705 (0.1960) acc 100.0000 (95.0000) lr 2.3959e-03 eta 0:03:50\n",
            "epoch [158/200] batch [5/9] time 0.424 (0.605) data 0.000 (0.183) loss 0.0901 (0.2051) acc 100.0000 (96.2500) lr 2.2949e-03 eta 0:03:51\n",
            "epoch [159/200] batch [5/9] time 0.429 (0.599) data 0.000 (0.175) loss 0.1194 (0.1723) acc 100.0000 (96.2500) lr 2.1957e-03 eta 0:03:43\n",
            "epoch [160/200] batch [5/9] time 0.421 (0.591) data 0.000 (0.170) loss 0.1666 (0.1228) acc 96.8750 (98.1250) lr 2.0984e-03 eta 0:03:34\n",
            "epoch [161/200] batch [5/9] time 0.421 (0.608) data 0.000 (0.187) loss 0.0571 (0.1464) acc 100.0000 (96.2500) lr 2.0032e-03 eta 0:03:35\n",
            "epoch [162/200] batch [5/9] time 0.427 (0.600) data 0.000 (0.178) loss 0.2703 (0.2278) acc 96.8750 (93.7500) lr 1.9098e-03 eta 0:03:27\n",
            "epoch [163/200] batch [5/9] time 0.428 (0.602) data 0.000 (0.180) loss 0.1590 (0.1194) acc 90.6250 (96.2500) lr 1.8185e-03 eta 0:03:22\n",
            "epoch [164/200] batch [5/9] time 0.428 (0.603) data 0.001 (0.181) loss 0.1522 (0.1609) acc 96.8750 (96.8750) lr 1.7292e-03 eta 0:03:17\n",
            "epoch [165/200] batch [5/9] time 0.429 (0.597) data 0.000 (0.175) loss 0.2045 (0.2311) acc 93.7500 (94.3750) lr 1.6419e-03 eta 0:03:10\n",
            "epoch [166/200] batch [5/9] time 0.428 (0.607) data 0.000 (0.185) loss 0.2218 (0.1653) acc 96.8750 (96.2500) lr 1.5567e-03 eta 0:03:08\n",
            "epoch [167/200] batch [5/9] time 0.423 (0.600) data 0.000 (0.178) loss 0.2512 (0.1444) acc 96.8750 (98.1250) lr 1.4736e-03 eta 0:03:00\n",
            "epoch [168/200] batch [5/9] time 0.428 (0.613) data 0.000 (0.191) loss 0.1839 (0.1437) acc 96.8750 (96.2500) lr 1.3926e-03 eta 0:02:58\n",
            "epoch [169/200] batch [5/9] time 0.423 (0.599) data 0.000 (0.179) loss 0.0380 (0.1084) acc 100.0000 (96.8750) lr 1.3137e-03 eta 0:02:49\n",
            "epoch [170/200] batch [5/9] time 0.425 (0.598) data 0.000 (0.177) loss 0.0486 (0.1473) acc 100.0000 (97.5000) lr 1.2369e-03 eta 0:02:43\n",
            "epoch [171/200] batch [5/9] time 0.426 (0.597) data 0.000 (0.175) loss 0.0643 (0.1925) acc 100.0000 (95.6250) lr 1.1623e-03 eta 0:02:38\n",
            "epoch [172/200] batch [5/9] time 0.421 (0.600) data 0.000 (0.179) loss 0.1247 (0.1614) acc 100.0000 (96.8750) lr 1.0899e-03 eta 0:02:33\n",
            "epoch [173/200] batch [5/9] time 0.428 (0.594) data 0.000 (0.171) loss 0.1807 (0.1244) acc 96.8750 (98.1250) lr 1.0197e-03 eta 0:02:26\n",
            "epoch [174/200] batch [5/9] time 0.430 (0.585) data 0.000 (0.161) loss 0.1849 (0.1601) acc 96.8750 (95.6250) lr 9.5173e-04 eta 0:02:19\n",
            "epoch [175/200] batch [5/9] time 0.438 (0.613) data 0.000 (0.184) loss 0.1325 (0.2131) acc 96.8750 (93.7500) lr 8.8597e-04 eta 0:02:20\n",
            "epoch [176/200] batch [5/9] time 0.441 (0.614) data 0.000 (0.184) loss 0.1879 (0.1798) acc 96.8750 (96.2500) lr 8.2245e-04 eta 0:02:15\n",
            "epoch [177/200] batch [5/9] time 0.442 (0.605) data 0.000 (0.172) loss 0.1763 (0.1481) acc 96.8750 (96.8750) lr 7.6120e-04 eta 0:02:07\n",
            "epoch [178/200] batch [5/9] time 0.448 (0.633) data 0.000 (0.196) loss 0.1177 (0.1768) acc 96.8750 (95.6250) lr 7.0224e-04 eta 0:02:07\n",
            "epoch [179/200] batch [5/9] time 0.450 (0.613) data 0.000 (0.176) loss 0.2897 (0.1509) acc 87.5000 (96.2500) lr 6.4556e-04 eta 0:01:58\n",
            "epoch [180/200] batch [5/9] time 0.454 (0.614) data 0.000 (0.172) loss 0.1230 (0.1616) acc 96.8750 (95.6250) lr 5.9119e-04 eta 0:01:52\n",
            "epoch [181/200] batch [5/9] time 0.456 (0.632) data 0.000 (0.185) loss 0.2543 (0.1312) acc 96.8750 (96.8750) lr 5.3915e-04 eta 0:01:50\n",
            "epoch [182/200] batch [5/9] time 0.456 (0.616) data 0.000 (0.170) loss 0.1014 (0.1854) acc 96.8750 (95.6250) lr 4.8943e-04 eta 0:01:42\n",
            "epoch [183/200] batch [5/9] time 0.455 (0.620) data 0.000 (0.175) loss 0.0969 (0.1507) acc 100.0000 (96.2500) lr 4.4207e-04 eta 0:01:37\n",
            "epoch [184/200] batch [5/9] time 0.449 (0.624) data 0.000 (0.185) loss 0.2119 (0.1786) acc 96.8750 (95.0000) lr 3.9706e-04 eta 0:01:32\n",
            "epoch [185/200] batch [5/9] time 0.448 (0.622) data 0.000 (0.186) loss 0.2670 (0.1393) acc 96.8750 (96.8750) lr 3.5443e-04 eta 0:01:26\n",
            "epoch [186/200] batch [5/9] time 0.439 (0.623) data 0.000 (0.191) loss 0.1701 (0.1374) acc 96.8750 (97.5000) lr 3.1417e-04 eta 0:01:20\n",
            "epoch [187/200] batch [5/9] time 0.436 (0.601) data 0.000 (0.170) loss 0.3105 (0.1719) acc 93.7500 (96.8750) lr 2.7630e-04 eta 0:01:12\n",
            "epoch [188/200] batch [5/9] time 0.437 (0.609) data 0.000 (0.179) loss 0.0728 (0.0875) acc 100.0000 (99.3750) lr 2.4083e-04 eta 0:01:08\n",
            "epoch [189/200] batch [5/9] time 0.430 (0.617) data 0.000 (0.191) loss 0.1077 (0.1524) acc 100.0000 (96.2500) lr 2.0777e-04 eta 0:01:03\n",
            "epoch [190/200] batch [5/9] time 0.433 (0.594) data 0.000 (0.169) loss 0.1443 (0.1323) acc 96.8750 (98.1250) lr 1.7713e-04 eta 0:00:55\n",
            "epoch [191/200] batch [5/9] time 0.426 (0.593) data 0.000 (0.171) loss 0.1180 (0.1525) acc 100.0000 (96.8750) lr 1.4891e-04 eta 0:00:50\n",
            "epoch [192/200] batch [5/9] time 0.424 (0.601) data 0.000 (0.178) loss 0.0967 (0.1650) acc 100.0000 (96.8750) lr 1.2312e-04 eta 0:00:45\n",
            "epoch [193/200] batch [5/9] time 0.427 (0.604) data 0.000 (0.180) loss 0.0652 (0.2147) acc 100.0000 (95.0000) lr 9.9763e-05 eta 0:00:40\n",
            "epoch [194/200] batch [5/9] time 0.430 (0.597) data 0.000 (0.172) loss 0.0565 (0.1495) acc 100.0000 (96.2500) lr 7.8853e-05 eta 0:00:34\n",
            "epoch [195/200] batch [5/9] time 0.429 (0.618) data 0.000 (0.193) loss 0.3505 (0.1738) acc 90.6250 (95.0000) lr 6.0390e-05 eta 0:00:30\n",
            "epoch [196/200] batch [5/9] time 0.434 (0.606) data 0.000 (0.180) loss 0.0969 (0.1378) acc 100.0000 (98.1250) lr 4.4380e-05 eta 0:00:24\n",
            "epoch [197/200] batch [5/9] time 0.428 (0.588) data 0.000 (0.164) loss 0.4855 (0.2580) acc 90.6250 (94.3750) lr 3.0827e-05 eta 0:00:18\n",
            "epoch [198/200] batch [5/9] time 0.439 (0.630) data 0.000 (0.203) loss 0.1256 (0.1272) acc 96.8750 (97.5000) lr 1.9733e-05 eta 0:00:13\n",
            "epoch [199/200] batch [5/9] time 0.433 (0.606) data 0.000 (0.179) loss 0.1991 (0.1425) acc 93.7500 (96.8750) lr 1.1101e-05 eta 0:00:07\n",
            "epoch [200/200] batch [5/9] time 0.431 (0.598) data 0.000 (0.172) loss 0.2558 (0.1797) acc 93.7500 (96.2500) lr 4.9344e-06 eta 0:00:02\n",
            "Checkpoint saved to output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_8shots/seed1/prompt_learner/model.pth.tar-200\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:18<00:00,  1.95it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,192\n",
            "* accuracy: 87.0%\n",
            "* error: 13.0%\n",
            "* macro_f1: 86.8%\n",
            "Elapsed: 0:16:16\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets-8shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_8shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3843045-9bcf-4e7a-9a8d-279a1eea8067",
        "id": "de1cO_U1yWJf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 11:10:36.276361: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 11:10:36.295870: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 11:10:36.302045: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 11:10:36.316689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 11:10:37.321003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '4']\n",
            "output_dir: output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 4\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_4-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  148\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "        [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "        [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "        ...,\n",
            "        [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "        [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "        [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-5.2644e-04, -1.1168e-02, -2.1661e-02,  ..., -4.8596e-03,\n",
            "           5.8912e-03,  6.7493e-03],\n",
            "         [ 5.8841e-04, -1.8584e-02, -2.5060e-02,  ..., -3.4987e-03,\n",
            "           5.4105e-03, -1.7861e-03],\n",
            "         [ 1.7309e-03, -7.2606e-03, -2.3481e-02,  ..., -2.1107e-03,\n",
            "           4.9070e-03,  6.0702e-03],\n",
            "         ...,\n",
            "         [ 1.0223e-03, -1.2080e-02, -1.5868e-02,  ..., -1.2882e-02,\n",
            "          -2.0567e-03,  4.8362e-03],\n",
            "         [ 7.8468e-03, -1.1038e-02, -1.0770e-02,  ...,  7.4988e-04,\n",
            "           1.5215e-03, -1.6774e-03],\n",
            "         [ 2.3028e-03, -5.9798e-03, -1.7329e-02,  ...,  5.5724e-04,\n",
            "           1.5857e-02,  4.3269e-03]],\n",
            "\n",
            "        [[ 6.0997e-03,  1.7106e-03, -2.1188e-02,  ...,  2.5523e-03,\n",
            "           2.1776e-03,  7.7525e-03],\n",
            "         [ 6.9275e-03, -6.8095e-03, -1.3799e-02,  ..., -3.6580e-03,\n",
            "           5.6513e-03,  1.9980e-03],\n",
            "         [ 6.2942e-04, -7.5124e-03, -9.4807e-03,  ...,  9.2128e-03,\n",
            "           1.0227e-02,  3.9989e-03],\n",
            "         ...,\n",
            "         [-8.9265e-04, -1.5108e-02, -1.9400e-02,  ..., -3.9164e-03,\n",
            "           4.4435e-03, -3.3369e-04],\n",
            "         [ 9.0713e-03, -1.0043e-02, -2.8737e-02,  ..., -4.9359e-03,\n",
            "           9.1699e-03,  5.4980e-03],\n",
            "         [-8.9653e-05, -7.9620e-03, -2.2422e-02,  ..., -2.5861e-03,\n",
            "           2.1928e-03,  6.9095e-03]],\n",
            "\n",
            "        [[ 2.2506e-04, -1.2583e-02, -1.8306e-02,  ..., -1.1051e-02,\n",
            "           9.2462e-03,  8.1454e-03],\n",
            "         [-9.4987e-04, -1.2930e-02, -2.0111e-02,  ..., -9.7730e-03,\n",
            "           5.3495e-03,  1.8893e-03],\n",
            "         [-2.7428e-03, -8.8699e-03, -1.4280e-02,  ...,  5.0471e-03,\n",
            "           5.3924e-03,  1.0839e-02],\n",
            "         ...,\n",
            "         [ 7.2136e-03, -9.1107e-03, -1.5424e-02,  ...,  1.9115e-03,\n",
            "           6.7415e-03, -8.7823e-04],\n",
            "         [-4.8866e-03, -9.1675e-03, -1.5351e-02,  ..., -5.0427e-03,\n",
            "           8.2830e-03,  1.1029e-02],\n",
            "         [-4.4785e-03, -1.2034e-02, -1.6046e-02,  ..., -1.3892e-03,\n",
            "           1.1470e-02, -4.6686e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1066e-02, -2.1956e-03, -1.7727e-02,  ..., -1.0564e-03,\n",
            "           1.2302e-02,  5.0345e-03],\n",
            "         [ 2.7537e-03, -4.1800e-04, -1.4596e-02,  ..., -1.1298e-03,\n",
            "           6.9791e-03,  3.2216e-03],\n",
            "         [ 3.3331e-03, -1.0128e-02, -1.2788e-02,  ..., -1.2218e-02,\n",
            "           2.8146e-03,  4.3975e-03],\n",
            "         ...,\n",
            "         [ 5.6838e-04, -7.7658e-04, -3.2178e-02,  ..., -6.8700e-03,\n",
            "           4.0220e-03, -3.8805e-04],\n",
            "         [ 9.2410e-04, -7.9189e-03, -1.0762e-02,  ..., -4.6880e-03,\n",
            "           5.9761e-03,  4.5386e-03],\n",
            "         [-2.8230e-04, -1.1029e-02, -1.4386e-02,  ..., -1.5611e-04,\n",
            "           1.0764e-02,  4.3288e-03]],\n",
            "\n",
            "        [[ 5.6610e-03, -6.1458e-03, -2.3435e-02,  ..., -8.1288e-03,\n",
            "           9.4217e-03,  1.0564e-02],\n",
            "         [ 1.7781e-03, -9.1155e-03, -7.3978e-03,  ...,  3.8989e-03,\n",
            "           8.7808e-03, -3.2376e-03],\n",
            "         [ 6.8626e-03, -1.4937e-02, -2.3481e-02,  ...,  1.5681e-03,\n",
            "           1.1096e-02,  2.1168e-03],\n",
            "         ...,\n",
            "         [ 2.3734e-03, -1.0926e-02, -2.4415e-02,  ..., -8.3730e-03,\n",
            "           1.5094e-02,  1.2127e-03],\n",
            "         [ 2.2802e-03, -9.2195e-03, -2.1869e-02,  ..., -2.9556e-03,\n",
            "           8.4852e-03,  4.9182e-03],\n",
            "         [-8.3199e-03, -8.2483e-03, -1.6881e-02,  ..., -4.5459e-03,\n",
            "           6.2860e-03,  5.5418e-04]],\n",
            "\n",
            "        [[ 1.3695e-03, -1.2808e-02, -1.6318e-02,  ...,  3.0864e-03,\n",
            "           4.0411e-03,  6.7378e-03],\n",
            "         [ 4.1695e-03, -1.2923e-02, -2.3946e-02,  ...,  2.9491e-03,\n",
            "           1.2264e-02,  2.7028e-03],\n",
            "         [-3.1091e-04, -1.2759e-02, -1.5500e-02,  ...,  1.6597e-03,\n",
            "           2.8471e-03, -5.2213e-03],\n",
            "         ...,\n",
            "         [ 9.3452e-05, -4.8621e-03, -1.9077e-02,  ..., -5.4662e-03,\n",
            "           5.2866e-03,  2.2937e-03],\n",
            "         [ 5.1670e-03, -1.0069e-02, -1.7373e-02,  ...,  2.5828e-03,\n",
            "           8.2563e-03,  1.3529e-03],\n",
            "         [ 3.2577e-03, -1.7065e-02, -1.7372e-02,  ..., -3.7200e-03,\n",
            "           4.5885e-03, -3.4474e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4769\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1/tensorboard)\n",
            "epoch [1/100] batch [1/4] time 2.123 (2.123) data 0.764 (0.764) loss 1.1686 (1.1686) acc 78.1250 (78.1250) lr 1.0000e-05 eta 0:14:07\n",
            "epoch [1/100] batch [2/4] time 0.411 (1.267) data 0.001 (0.382) loss 0.7980 (0.9833) acc 81.2500 (79.6875) lr 1.0000e-05 eta 0:08:24\n",
            "epoch [1/100] batch [3/4] time 0.416 (0.983) data 0.000 (0.255) loss 1.3405 (1.1024) acc 62.5000 (73.9583) lr 1.0000e-05 eta 0:06:30\n",
            "epoch [1/100] batch [4/4] time 0.422 (0.843) data 0.000 (0.191) loss 0.9562 (1.0659) acc 65.6250 (71.8750) lr 2.0000e-02 eta 0:05:33\n",
            "epoch [2/100] batch [1/4] time 1.050 (1.050) data 0.640 (0.640) loss 1.1003 (1.1003) acc 65.6250 (65.6250) lr 2.0000e-02 eta 0:06:54\n",
            "epoch [2/100] batch [2/4] time 0.411 (0.731) data 0.001 (0.320) loss 0.8988 (0.9996) acc 75.0000 (70.3125) lr 2.0000e-02 eta 0:04:47\n",
            "epoch [2/100] batch [3/4] time 0.433 (0.632) data 0.000 (0.214) loss 0.9152 (0.9715) acc 84.3750 (75.0000) lr 2.0000e-02 eta 0:04:08\n",
            "epoch [2/100] batch [4/4] time 0.415 (0.577) data 0.000 (0.160) loss 0.6039 (0.8796) acc 81.2500 (76.5625) lr 1.9995e-02 eta 0:03:46\n",
            "epoch [3/100] batch [1/4] time 1.052 (1.052) data 0.640 (0.640) loss 1.0091 (1.0091) acc 71.8750 (71.8750) lr 1.9995e-02 eta 0:06:51\n",
            "epoch [3/100] batch [2/4] time 0.413 (0.733) data 0.001 (0.320) loss 0.9037 (0.9564) acc 68.7500 (70.3125) lr 1.9995e-02 eta 0:04:45\n",
            "epoch [3/100] batch [3/4] time 0.438 (0.634) data 0.000 (0.214) loss 1.0327 (0.9818) acc 68.7500 (69.7917) lr 1.9995e-02 eta 0:04:06\n",
            "epoch [3/100] batch [4/4] time 0.414 (0.579) data 0.000 (0.160) loss 0.5862 (0.8829) acc 78.1250 (71.8750) lr 1.9980e-02 eta 0:03:44\n",
            "epoch [4/100] batch [1/4] time 1.061 (1.061) data 0.650 (0.650) loss 0.7867 (0.7867) acc 75.0000 (75.0000) lr 1.9980e-02 eta 0:06:50\n",
            "epoch [4/100] batch [2/4] time 0.413 (0.737) data 0.001 (0.325) loss 0.7835 (0.7851) acc 75.0000 (75.0000) lr 1.9980e-02 eta 0:04:44\n",
            "epoch [4/100] batch [3/4] time 0.434 (0.636) data 0.000 (0.217) loss 0.6774 (0.7492) acc 71.8750 (73.9583) lr 1.9980e-02 eta 0:04:04\n",
            "epoch [4/100] batch [4/4] time 0.415 (0.581) data 0.000 (0.163) loss 0.8799 (0.7819) acc 71.8750 (73.4375) lr 1.9956e-02 eta 0:03:42\n",
            "epoch [5/100] batch [1/4] time 1.053 (1.053) data 0.641 (0.641) loss 0.6160 (0.6160) acc 81.2500 (81.2500) lr 1.9956e-02 eta 0:06:43\n",
            "epoch [5/100] batch [2/4] time 0.413 (0.733) data 0.000 (0.321) loss 1.0793 (0.8477) acc 71.8750 (76.5625) lr 1.9956e-02 eta 0:04:39\n",
            "epoch [5/100] batch [3/4] time 0.432 (0.632) data 0.000 (0.214) loss 0.7154 (0.8036) acc 84.3750 (79.1667) lr 1.9956e-02 eta 0:04:00\n",
            "epoch [5/100] batch [4/4] time 0.414 (0.578) data 0.000 (0.160) loss 0.5140 (0.7312) acc 87.5000 (81.2500) lr 1.9921e-02 eta 0:03:39\n",
            "epoch [6/100] batch [1/4] time 1.041 (1.041) data 0.630 (0.630) loss 0.5779 (0.5779) acc 84.3750 (84.3750) lr 1.9921e-02 eta 0:06:34\n",
            "epoch [6/100] batch [2/4] time 0.414 (0.728) data 0.000 (0.315) loss 1.0778 (0.8278) acc 68.7500 (76.5625) lr 1.9921e-02 eta 0:04:35\n",
            "epoch [6/100] batch [3/4] time 0.437 (0.631) data 0.000 (0.210) loss 0.6373 (0.7643) acc 84.3750 (79.1667) lr 1.9921e-02 eta 0:03:57\n",
            "epoch [6/100] batch [4/4] time 0.416 (0.577) data 0.000 (0.158) loss 0.8921 (0.7962) acc 68.7500 (76.5625) lr 1.9877e-02 eta 0:03:37\n",
            "epoch [7/100] batch [1/4] time 1.068 (1.068) data 0.657 (0.657) loss 0.9125 (0.9125) acc 78.1250 (78.1250) lr 1.9877e-02 eta 0:06:40\n",
            "epoch [7/100] batch [2/4] time 0.414 (0.741) data 0.001 (0.329) loss 0.7391 (0.8258) acc 78.1250 (78.1250) lr 1.9877e-02 eta 0:04:37\n",
            "epoch [7/100] batch [3/4] time 0.430 (0.638) data 0.000 (0.219) loss 0.7820 (0.8112) acc 75.0000 (77.0833) lr 1.9877e-02 eta 0:03:57\n",
            "epoch [7/100] batch [4/4] time 0.421 (0.583) data 0.000 (0.165) loss 0.8029 (0.8091) acc 68.7500 (75.0000) lr 1.9823e-02 eta 0:03:36\n",
            "epoch [8/100] batch [1/4] time 1.075 (1.075) data 0.664 (0.664) loss 0.4701 (0.4701) acc 90.6250 (90.6250) lr 1.9823e-02 eta 0:06:38\n",
            "epoch [8/100] batch [2/4] time 0.414 (0.745) data 0.000 (0.332) loss 0.3516 (0.4108) acc 90.6250 (90.6250) lr 1.9823e-02 eta 0:04:35\n",
            "epoch [8/100] batch [3/4] time 0.436 (0.642) data 0.000 (0.222) loss 1.0664 (0.6293) acc 71.8750 (84.3750) lr 1.9823e-02 eta 0:03:56\n",
            "epoch [8/100] batch [4/4] time 0.415 (0.585) data 0.000 (0.166) loss 0.5780 (0.6165) acc 84.3750 (84.3750) lr 1.9759e-02 eta 0:03:35\n",
            "epoch [9/100] batch [1/4] time 1.019 (1.019) data 0.606 (0.606) loss 0.5535 (0.5535) acc 81.2500 (81.2500) lr 1.9759e-02 eta 0:06:13\n",
            "epoch [9/100] batch [2/4] time 0.412 (0.715) data 0.001 (0.303) loss 0.4599 (0.5067) acc 87.5000 (84.3750) lr 1.9759e-02 eta 0:04:21\n",
            "epoch [9/100] batch [3/4] time 0.438 (0.623) data 0.000 (0.202) loss 0.5641 (0.5259) acc 90.6250 (86.4583) lr 1.9759e-02 eta 0:03:47\n",
            "epoch [9/100] batch [4/4] time 0.414 (0.571) data 0.000 (0.152) loss 0.5386 (0.5291) acc 90.6250 (87.5000) lr 1.9686e-02 eta 0:03:27\n",
            "epoch [10/100] batch [1/4] time 1.036 (1.036) data 0.627 (0.627) loss 0.5859 (0.5859) acc 84.3750 (84.3750) lr 1.9686e-02 eta 0:06:15\n",
            "epoch [10/100] batch [2/4] time 0.414 (0.725) data 0.000 (0.314) loss 0.4901 (0.5380) acc 81.2500 (82.8125) lr 1.9686e-02 eta 0:04:22\n",
            "epoch [10/100] batch [3/4] time 0.438 (0.629) data 0.000 (0.209) loss 0.8001 (0.6254) acc 78.1250 (81.2500) lr 1.9686e-02 eta 0:03:47\n",
            "epoch [10/100] batch [4/4] time 0.415 (0.576) data 0.000 (0.157) loss 0.3897 (0.5665) acc 87.5000 (82.8125) lr 1.9603e-02 eta 0:03:27\n",
            "epoch [11/100] batch [1/4] time 1.013 (1.013) data 0.603 (0.603) loss 0.5398 (0.5398) acc 84.3750 (84.3750) lr 1.9603e-02 eta 0:06:03\n",
            "epoch [11/100] batch [2/4] time 0.414 (0.714) data 0.001 (0.302) loss 0.3598 (0.4498) acc 90.6250 (87.5000) lr 1.9603e-02 eta 0:04:15\n",
            "epoch [11/100] batch [3/4] time 0.436 (0.621) data 0.000 (0.201) loss 0.4706 (0.4567) acc 87.5000 (87.5000) lr 1.9603e-02 eta 0:03:41\n",
            "epoch [11/100] batch [4/4] time 0.415 (0.570) data 0.000 (0.151) loss 0.5787 (0.4872) acc 84.3750 (86.7188) lr 1.9511e-02 eta 0:03:22\n",
            "epoch [12/100] batch [1/4] time 0.982 (0.982) data 0.570 (0.570) loss 0.5071 (0.5071) acc 90.6250 (90.6250) lr 1.9511e-02 eta 0:05:48\n",
            "epoch [12/100] batch [2/4] time 0.414 (0.698) data 0.001 (0.285) loss 0.2953 (0.4012) acc 96.8750 (93.7500) lr 1.9511e-02 eta 0:04:06\n",
            "epoch [12/100] batch [3/4] time 0.434 (0.610) data 0.000 (0.190) loss 0.2843 (0.3622) acc 96.8750 (94.7917) lr 1.9511e-02 eta 0:03:35\n",
            "epoch [12/100] batch [4/4] time 0.417 (0.561) data 0.000 (0.143) loss 0.5000 (0.3967) acc 90.6250 (93.7500) lr 1.9409e-02 eta 0:03:17\n",
            "epoch [13/100] batch [1/4] time 1.040 (1.040) data 0.627 (0.627) loss 0.5788 (0.5788) acc 81.2500 (81.2500) lr 1.9409e-02 eta 0:06:05\n",
            "epoch [13/100] batch [2/4] time 0.415 (0.727) data 0.001 (0.314) loss 0.5782 (0.5785) acc 84.3750 (82.8125) lr 1.9409e-02 eta 0:04:14\n",
            "epoch [13/100] batch [3/4] time 0.437 (0.631) data 0.000 (0.209) loss 0.4145 (0.5238) acc 93.7500 (86.4583) lr 1.9409e-02 eta 0:03:40\n",
            "epoch [13/100] batch [4/4] time 0.414 (0.577) data 0.000 (0.157) loss 0.5194 (0.5227) acc 84.3750 (85.9375) lr 1.9298e-02 eta 0:03:20\n",
            "epoch [14/100] batch [1/4] time 1.066 (1.066) data 0.652 (0.652) loss 0.3485 (0.3485) acc 93.7500 (93.7500) lr 1.9298e-02 eta 0:06:09\n",
            "epoch [14/100] batch [2/4] time 0.413 (0.740) data 0.001 (0.326) loss 0.4109 (0.3797) acc 93.7500 (93.7500) lr 1.9298e-02 eta 0:04:15\n",
            "epoch [14/100] batch [3/4] time 0.436 (0.638) data 0.000 (0.218) loss 0.6794 (0.4796) acc 78.1250 (88.5417) lr 1.9298e-02 eta 0:03:40\n",
            "epoch [14/100] batch [4/4] time 0.419 (0.584) data 0.000 (0.163) loss 0.5846 (0.5058) acc 84.3750 (87.5000) lr 1.9178e-02 eta 0:03:20\n",
            "epoch [15/100] batch [1/4] time 1.071 (1.071) data 0.660 (0.660) loss 0.6363 (0.6363) acc 84.3750 (84.3750) lr 1.9178e-02 eta 0:06:07\n",
            "epoch [15/100] batch [2/4] time 0.416 (0.743) data 0.000 (0.330) loss 0.4458 (0.5410) acc 87.5000 (85.9375) lr 1.9178e-02 eta 0:04:14\n",
            "epoch [15/100] batch [3/4] time 0.435 (0.641) data 0.000 (0.220) loss 0.4769 (0.5197) acc 90.6250 (87.5000) lr 1.9178e-02 eta 0:03:38\n",
            "epoch [15/100] batch [4/4] time 0.414 (0.584) data 0.001 (0.165) loss 0.3792 (0.4845) acc 90.6250 (88.2812) lr 1.9048e-02 eta 0:03:18\n",
            "epoch [16/100] batch [1/4] time 1.042 (1.042) data 0.628 (0.628) loss 0.8889 (0.8889) acc 68.7500 (68.7500) lr 1.9048e-02 eta 0:05:53\n",
            "epoch [16/100] batch [2/4] time 0.414 (0.728) data 0.000 (0.314) loss 0.5929 (0.7409) acc 81.2500 (75.0000) lr 1.9048e-02 eta 0:04:06\n",
            "epoch [16/100] batch [3/4] time 0.437 (0.631) data 0.000 (0.210) loss 0.5732 (0.6850) acc 84.3750 (78.1250) lr 1.9048e-02 eta 0:03:32\n",
            "epoch [16/100] batch [4/4] time 0.416 (0.577) data 0.000 (0.157) loss 0.3969 (0.6130) acc 90.6250 (81.2500) lr 1.8910e-02 eta 0:03:13\n",
            "epoch [17/100] batch [1/4] time 1.020 (1.020) data 0.609 (0.609) loss 0.4616 (0.4616) acc 84.3750 (84.3750) lr 1.8910e-02 eta 0:05:41\n",
            "epoch [17/100] batch [2/4] time 0.417 (0.718) data 0.001 (0.305) loss 0.3680 (0.4148) acc 93.7500 (89.0625) lr 1.8910e-02 eta 0:03:59\n",
            "epoch [17/100] batch [3/4] time 0.435 (0.624) data 0.000 (0.203) loss 0.6189 (0.4828) acc 87.5000 (88.5417) lr 1.8910e-02 eta 0:03:27\n",
            "epoch [17/100] batch [4/4] time 0.418 (0.572) data 0.000 (0.152) loss 0.5617 (0.5026) acc 81.2500 (86.7188) lr 1.8763e-02 eta 0:03:10\n",
            "epoch [18/100] batch [1/4] time 1.053 (1.053) data 0.640 (0.640) loss 0.4430 (0.4430) acc 90.6250 (90.6250) lr 1.8763e-02 eta 0:05:48\n",
            "epoch [18/100] batch [2/4] time 0.416 (0.735) data 0.001 (0.320) loss 0.6461 (0.5445) acc 84.3750 (87.5000) lr 1.8763e-02 eta 0:04:02\n",
            "epoch [18/100] batch [3/4] time 0.438 (0.636) data 0.000 (0.214) loss 0.4270 (0.5054) acc 93.7500 (89.5833) lr 1.8763e-02 eta 0:03:29\n",
            "epoch [18/100] batch [4/4] time 0.416 (0.581) data 0.000 (0.160) loss 0.4242 (0.4851) acc 96.8750 (91.4062) lr 1.8607e-02 eta 0:03:10\n",
            "epoch [19/100] batch [1/4] time 1.059 (1.059) data 0.647 (0.647) loss 0.3878 (0.3878) acc 93.7500 (93.7500) lr 1.8607e-02 eta 0:05:46\n",
            "epoch [19/100] batch [2/4] time 0.414 (0.737) data 0.000 (0.324) loss 0.3374 (0.3626) acc 93.7500 (93.7500) lr 1.8607e-02 eta 0:04:00\n",
            "epoch [19/100] batch [3/4] time 0.435 (0.636) data 0.000 (0.216) loss 0.3097 (0.3450) acc 96.8750 (94.7917) lr 1.8607e-02 eta 0:03:26\n",
            "epoch [19/100] batch [4/4] time 0.416 (0.581) data 0.000 (0.162) loss 0.7232 (0.4395) acc 84.3750 (92.1875) lr 1.8443e-02 eta 0:03:08\n",
            "epoch [20/100] batch [1/4] time 1.042 (1.042) data 0.628 (0.628) loss 0.4839 (0.4839) acc 87.5000 (87.5000) lr 1.8443e-02 eta 0:05:36\n",
            "epoch [20/100] batch [2/4] time 0.415 (0.729) data 0.001 (0.314) loss 0.1209 (0.3024) acc 100.0000 (93.7500) lr 1.8443e-02 eta 0:03:54\n",
            "epoch [20/100] batch [3/4] time 0.438 (0.632) data 0.000 (0.210) loss 0.2809 (0.2953) acc 90.6250 (92.7083) lr 1.8443e-02 eta 0:03:22\n",
            "epoch [20/100] batch [4/4] time 0.415 (0.577) data 0.000 (0.157) loss 0.5384 (0.3560) acc 84.3750 (90.6250) lr 1.8271e-02 eta 0:03:04\n",
            "epoch [21/100] batch [1/4] time 1.041 (1.041) data 0.629 (0.629) loss 0.2064 (0.2064) acc 93.7500 (93.7500) lr 1.8271e-02 eta 0:05:32\n",
            "epoch [21/100] batch [2/4] time 0.416 (0.729) data 0.001 (0.315) loss 0.3845 (0.2955) acc 90.6250 (92.1875) lr 1.8271e-02 eta 0:03:51\n",
            "epoch [21/100] batch [3/4] time 0.433 (0.630) data 0.000 (0.210) loss 0.4006 (0.3305) acc 87.5000 (90.6250) lr 1.8271e-02 eta 0:03:19\n",
            "epoch [21/100] batch [4/4] time 0.421 (0.578) data 0.000 (0.157) loss 0.5900 (0.3954) acc 81.2500 (88.2812) lr 1.8090e-02 eta 0:03:02\n",
            "epoch [22/100] batch [1/4] time 1.008 (1.008) data 0.596 (0.596) loss 0.3204 (0.3204) acc 93.7500 (93.7500) lr 1.8090e-02 eta 0:05:17\n",
            "epoch [22/100] batch [2/4] time 0.417 (0.713) data 0.000 (0.298) loss 0.4024 (0.3614) acc 90.6250 (92.1875) lr 1.8090e-02 eta 0:03:43\n",
            "epoch [22/100] batch [3/4] time 0.436 (0.621) data 0.000 (0.199) loss 0.5821 (0.4350) acc 84.3750 (89.5833) lr 1.8090e-02 eta 0:03:14\n",
            "epoch [22/100] batch [4/4] time 0.416 (0.570) data 0.000 (0.149) loss 0.3084 (0.4033) acc 93.7500 (90.6250) lr 1.7902e-02 eta 0:02:57\n",
            "epoch [23/100] batch [1/4] time 1.028 (1.028) data 0.616 (0.616) loss 0.2615 (0.2615) acc 100.0000 (100.0000) lr 1.7902e-02 eta 0:05:19\n",
            "epoch [23/100] batch [2/4] time 0.419 (0.723) data 0.001 (0.308) loss 0.3428 (0.3021) acc 87.5000 (93.7500) lr 1.7902e-02 eta 0:03:44\n",
            "epoch [23/100] batch [3/4] time 0.435 (0.627) data 0.000 (0.206) loss 0.2361 (0.2801) acc 96.8750 (94.7917) lr 1.7902e-02 eta 0:03:13\n",
            "epoch [23/100] batch [4/4] time 0.415 (0.574) data 0.000 (0.154) loss 0.3920 (0.3081) acc 90.6250 (93.7500) lr 1.7705e-02 eta 0:02:56\n",
            "epoch [24/100] batch [1/4] time 1.088 (1.088) data 0.670 (0.670) loss 0.3515 (0.3515) acc 87.5000 (87.5000) lr 1.7705e-02 eta 0:05:33\n",
            "epoch [24/100] batch [2/4] time 0.417 (0.752) data 0.001 (0.335) loss 0.3798 (0.3656) acc 93.7500 (90.6250) lr 1.7705e-02 eta 0:03:50\n",
            "epoch [24/100] batch [3/4] time 0.437 (0.647) data 0.000 (0.224) loss 0.5584 (0.4299) acc 81.2500 (87.5000) lr 1.7705e-02 eta 0:03:17\n",
            "epoch [24/100] batch [4/4] time 0.420 (0.590) data 0.000 (0.168) loss 0.5300 (0.4549) acc 90.6250 (88.2812) lr 1.7501e-02 eta 0:02:59\n",
            "epoch [25/100] batch [1/4] time 1.041 (1.041) data 0.629 (0.629) loss 0.4169 (0.4169) acc 90.6250 (90.6250) lr 1.7501e-02 eta 0:05:15\n",
            "epoch [25/100] batch [2/4] time 0.416 (0.728) data 0.001 (0.315) loss 0.7545 (0.5857) acc 75.0000 (82.8125) lr 1.7501e-02 eta 0:03:39\n",
            "epoch [25/100] batch [3/4] time 0.440 (0.632) data 0.000 (0.210) loss 0.1610 (0.4441) acc 93.7500 (86.4583) lr 1.7501e-02 eta 0:03:10\n",
            "epoch [25/100] batch [4/4] time 0.414 (0.578) data 0.000 (0.158) loss 0.3313 (0.4159) acc 90.6250 (87.5000) lr 1.7290e-02 eta 0:02:53\n",
            "epoch [26/100] batch [1/4] time 1.050 (1.050) data 0.635 (0.635) loss 0.1085 (0.1085) acc 100.0000 (100.0000) lr 1.7290e-02 eta 0:05:13\n",
            "epoch [26/100] batch [2/4] time 0.418 (0.734) data 0.001 (0.318) loss 0.3837 (0.2461) acc 93.7500 (96.8750) lr 1.7290e-02 eta 0:03:38\n",
            "epoch [26/100] batch [3/4] time 0.437 (0.635) data 0.000 (0.212) loss 0.4435 (0.3119) acc 78.1250 (90.6250) lr 1.7290e-02 eta 0:03:08\n",
            "epoch [26/100] batch [4/4] time 0.417 (0.580) data 0.000 (0.159) loss 0.2448 (0.2951) acc 100.0000 (92.9688) lr 1.7071e-02 eta 0:02:51\n",
            "epoch [27/100] batch [1/4] time 1.073 (1.073) data 0.660 (0.660) loss 0.2471 (0.2471) acc 90.6250 (90.6250) lr 1.7071e-02 eta 0:05:16\n",
            "epoch [27/100] batch [2/4] time 0.419 (0.746) data 0.000 (0.330) loss 0.3220 (0.2845) acc 96.8750 (93.7500) lr 1.7071e-02 eta 0:03:39\n",
            "epoch [27/100] batch [3/4] time 0.437 (0.643) data 0.000 (0.220) loss 0.3309 (0.3000) acc 90.6250 (92.7083) lr 1.7071e-02 eta 0:03:08\n",
            "epoch [27/100] batch [4/4] time 0.417 (0.586) data 0.000 (0.165) loss 0.3050 (0.3012) acc 93.7500 (92.9688) lr 1.6845e-02 eta 0:02:51\n",
            "epoch [28/100] batch [1/4] time 1.028 (1.028) data 0.615 (0.615) loss 0.3697 (0.3697) acc 90.6250 (90.6250) lr 1.6845e-02 eta 0:04:59\n",
            "epoch [28/100] batch [2/4] time 0.416 (0.722) data 0.001 (0.308) loss 0.4680 (0.4188) acc 90.6250 (90.6250) lr 1.6845e-02 eta 0:03:29\n",
            "epoch [28/100] batch [3/4] time 0.437 (0.627) data 0.000 (0.205) loss 0.2680 (0.3686) acc 90.6250 (90.6250) lr 1.6845e-02 eta 0:03:01\n",
            "epoch [28/100] batch [4/4] time 0.421 (0.575) data 0.000 (0.154) loss 0.4830 (0.3972) acc 90.6250 (90.6250) lr 1.6613e-02 eta 0:02:45\n",
            "epoch [29/100] batch [1/4] time 1.058 (1.058) data 0.643 (0.643) loss 0.1215 (0.1215) acc 100.0000 (100.0000) lr 1.6613e-02 eta 0:05:03\n",
            "epoch [29/100] batch [2/4] time 0.417 (0.738) data 0.001 (0.322) loss 0.3013 (0.2114) acc 93.7500 (96.8750) lr 1.6613e-02 eta 0:03:30\n",
            "epoch [29/100] batch [3/4] time 0.437 (0.637) data 0.000 (0.215) loss 0.7356 (0.3861) acc 78.1250 (90.6250) lr 1.6613e-02 eta 0:03:01\n",
            "epoch [29/100] batch [4/4] time 0.422 (0.584) data 0.000 (0.161) loss 0.3749 (0.3833) acc 90.6250 (90.6250) lr 1.6374e-02 eta 0:02:45\n",
            "epoch [30/100] batch [1/4] time 1.077 (1.077) data 0.659 (0.659) loss 0.1497 (0.1497) acc 100.0000 (100.0000) lr 1.6374e-02 eta 0:05:04\n",
            "epoch [30/100] batch [2/4] time 0.418 (0.748) data 0.001 (0.330) loss 0.2286 (0.1891) acc 96.8750 (98.4375) lr 1.6374e-02 eta 0:03:30\n",
            "epoch [30/100] batch [3/4] time 0.437 (0.644) data 0.000 (0.220) loss 0.2015 (0.1932) acc 96.8750 (97.9167) lr 1.6374e-02 eta 0:03:00\n",
            "epoch [30/100] batch [4/4] time 0.420 (0.588) data 0.000 (0.165) loss 0.2610 (0.2102) acc 96.8750 (97.6562) lr 1.6129e-02 eta 0:02:44\n",
            "epoch [31/100] batch [1/4] time 1.060 (1.060) data 0.645 (0.645) loss 0.5995 (0.5995) acc 84.3750 (84.3750) lr 1.6129e-02 eta 0:04:55\n",
            "epoch [31/100] batch [2/4] time 0.419 (0.739) data 0.000 (0.323) loss 0.1868 (0.3932) acc 96.8750 (90.6250) lr 1.6129e-02 eta 0:03:25\n",
            "epoch [31/100] batch [3/4] time 0.441 (0.640) data 0.000 (0.215) loss 0.3247 (0.3703) acc 96.8750 (92.7083) lr 1.6129e-02 eta 0:02:57\n",
            "epoch [31/100] batch [4/4] time 0.415 (0.584) data 0.000 (0.162) loss 0.1612 (0.3181) acc 93.7500 (92.9688) lr 1.5878e-02 eta 0:02:41\n",
            "epoch [32/100] batch [1/4] time 1.047 (1.047) data 0.631 (0.631) loss 0.1593 (0.1593) acc 100.0000 (100.0000) lr 1.5878e-02 eta 0:04:48\n",
            "epoch [32/100] batch [2/4] time 0.423 (0.735) data 0.001 (0.316) loss 0.3613 (0.2603) acc 93.7500 (96.8750) lr 1.5878e-02 eta 0:03:21\n",
            "epoch [32/100] batch [3/4] time 0.439 (0.636) data 0.000 (0.211) loss 0.2089 (0.2431) acc 96.8750 (96.8750) lr 1.5878e-02 eta 0:02:53\n",
            "epoch [32/100] batch [4/4] time 0.407 (0.579) data 0.000 (0.158) loss 0.1983 (0.2319) acc 93.7500 (96.0938) lr 1.5621e-02 eta 0:02:37\n",
            "epoch [33/100] batch [1/4] time 1.015 (1.015) data 0.600 (0.600) loss 0.3932 (0.3932) acc 93.7500 (93.7500) lr 1.5621e-02 eta 0:04:35\n",
            "epoch [33/100] batch [2/4] time 0.419 (0.717) data 0.001 (0.300) loss 0.2767 (0.3349) acc 93.7500 (93.7500) lr 1.5621e-02 eta 0:03:13\n",
            "epoch [33/100] batch [3/4] time 0.438 (0.624) data 0.000 (0.200) loss 0.2891 (0.3196) acc 90.6250 (92.7083) lr 1.5621e-02 eta 0:02:47\n",
            "epoch [33/100] batch [4/4] time 0.418 (0.573) data 0.000 (0.150) loss 0.2792 (0.3095) acc 96.8750 (93.7500) lr 1.5358e-02 eta 0:02:33\n",
            "epoch [34/100] batch [1/4] time 1.006 (1.006) data 0.592 (0.592) loss 0.4330 (0.4330) acc 90.6250 (90.6250) lr 1.5358e-02 eta 0:04:28\n",
            "epoch [34/100] batch [2/4] time 0.421 (0.714) data 0.001 (0.296) loss 0.5174 (0.4752) acc 90.6250 (90.6250) lr 1.5358e-02 eta 0:03:09\n",
            "epoch [34/100] batch [3/4] time 0.436 (0.621) data 0.000 (0.198) loss 0.3022 (0.4175) acc 96.8750 (92.7083) lr 1.5358e-02 eta 0:02:44\n",
            "epoch [34/100] batch [4/4] time 0.419 (0.571) data 0.000 (0.148) loss 0.3102 (0.3907) acc 84.3750 (90.6250) lr 1.5090e-02 eta 0:02:30\n",
            "epoch [35/100] batch [1/4] time 1.063 (1.063) data 0.647 (0.647) loss 0.4066 (0.4066) acc 90.6250 (90.6250) lr 1.5090e-02 eta 0:04:39\n",
            "epoch [35/100] batch [2/4] time 0.419 (0.741) data 0.001 (0.324) loss 0.3424 (0.3745) acc 90.6250 (90.6250) lr 1.5090e-02 eta 0:03:14\n",
            "epoch [35/100] batch [3/4] time 0.442 (0.641) data 0.000 (0.216) loss 0.2791 (0.3427) acc 93.7500 (91.6667) lr 1.5090e-02 eta 0:02:47\n",
            "epoch [35/100] batch [4/4] time 0.424 (0.587) data 0.000 (0.162) loss 0.3787 (0.3517) acc 90.6250 (91.4062) lr 1.4818e-02 eta 0:02:32\n",
            "epoch [36/100] batch [1/4] time 1.085 (1.085) data 0.669 (0.669) loss 0.6300 (0.6300) acc 78.1250 (78.1250) lr 1.4818e-02 eta 0:04:40\n",
            "epoch [36/100] batch [2/4] time 0.420 (0.752) data 0.000 (0.335) loss 0.2052 (0.4176) acc 96.8750 (87.5000) lr 1.4818e-02 eta 0:03:14\n",
            "epoch [36/100] batch [3/4] time 0.438 (0.648) data 0.000 (0.223) loss 0.2989 (0.3780) acc 90.6250 (88.5417) lr 1.4818e-02 eta 0:02:46\n",
            "epoch [36/100] batch [4/4] time 0.424 (0.592) data 0.000 (0.167) loss 0.2231 (0.3393) acc 100.0000 (91.4062) lr 1.4540e-02 eta 0:02:31\n",
            "epoch [37/100] batch [1/4] time 1.066 (1.066) data 0.646 (0.646) loss 0.1938 (0.1938) acc 96.8750 (96.8750) lr 1.4540e-02 eta 0:04:31\n",
            "epoch [37/100] batch [2/4] time 0.423 (0.744) data 0.001 (0.323) loss 0.2305 (0.2122) acc 96.8750 (96.8750) lr 1.4540e-02 eta 0:03:09\n",
            "epoch [37/100] batch [3/4] time 0.440 (0.643) data 0.000 (0.216) loss 0.1937 (0.2060) acc 96.8750 (96.8750) lr 1.4540e-02 eta 0:02:42\n",
            "epoch [37/100] batch [4/4] time 0.423 (0.588) data 0.000 (0.162) loss 0.2724 (0.2226) acc 96.8750 (96.8750) lr 1.4258e-02 eta 0:02:28\n",
            "epoch [38/100] batch [1/4] time 1.047 (1.047) data 0.626 (0.626) loss 0.1785 (0.1785) acc 93.7500 (93.7500) lr 1.4258e-02 eta 0:04:22\n",
            "epoch [38/100] batch [2/4] time 0.424 (0.736) data 0.001 (0.313) loss 0.1098 (0.1442) acc 96.8750 (95.3125) lr 1.4258e-02 eta 0:03:03\n",
            "epoch [38/100] batch [3/4] time 0.446 (0.639) data 0.000 (0.209) loss 0.7425 (0.3436) acc 81.2500 (90.6250) lr 1.4258e-02 eta 0:02:39\n",
            "epoch [38/100] batch [4/4] time 0.424 (0.585) data 0.000 (0.157) loss 0.3182 (0.3373) acc 93.7500 (91.4062) lr 1.3971e-02 eta 0:02:25\n",
            "epoch [39/100] batch [1/4] time 1.046 (1.046) data 0.625 (0.625) loss 0.3282 (0.3282) acc 93.7500 (93.7500) lr 1.3971e-02 eta 0:04:18\n",
            "epoch [39/100] batch [2/4] time 0.427 (0.736) data 0.001 (0.313) loss 0.2727 (0.3004) acc 93.7500 (93.7500) lr 1.3971e-02 eta 0:03:01\n",
            "epoch [39/100] batch [3/4] time 0.442 (0.638) data 0.000 (0.209) loss 0.3733 (0.3247) acc 93.7500 (93.7500) lr 1.3971e-02 eta 0:02:36\n",
            "epoch [39/100] batch [4/4] time 0.426 (0.585) data 0.000 (0.157) loss 0.2863 (0.3151) acc 90.6250 (92.9688) lr 1.3681e-02 eta 0:02:22\n",
            "epoch [40/100] batch [1/4] time 1.072 (1.072) data 0.649 (0.649) loss 0.1981 (0.1981) acc 96.8750 (96.8750) lr 1.3681e-02 eta 0:04:20\n",
            "epoch [40/100] batch [2/4] time 0.427 (0.750) data 0.001 (0.325) loss 0.1365 (0.1673) acc 100.0000 (98.4375) lr 1.3681e-02 eta 0:03:01\n",
            "epoch [40/100] batch [3/4] time 0.449 (0.649) data 0.000 (0.217) loss 0.4128 (0.2491) acc 87.5000 (94.7917) lr 1.3681e-02 eta 0:02:36\n",
            "epoch [40/100] batch [4/4] time 0.424 (0.593) data 0.000 (0.163) loss 0.2534 (0.2502) acc 93.7500 (94.5312) lr 1.3387e-02 eta 0:02:22\n",
            "epoch [41/100] batch [1/4] time 1.070 (1.070) data 0.647 (0.647) loss 0.3204 (0.3204) acc 93.7500 (93.7500) lr 1.3387e-02 eta 0:04:15\n",
            "epoch [41/100] batch [2/4] time 0.428 (0.749) data 0.000 (0.324) loss 0.2406 (0.2805) acc 90.6250 (92.1875) lr 1.3387e-02 eta 0:02:58\n",
            "epoch [41/100] batch [3/4] time 0.451 (0.650) data 0.000 (0.216) loss 0.2020 (0.2543) acc 93.7500 (92.7083) lr 1.3387e-02 eta 0:02:33\n",
            "epoch [41/100] batch [4/4] time 0.431 (0.595) data 0.000 (0.162) loss 0.0903 (0.2133) acc 100.0000 (94.5312) lr 1.3090e-02 eta 0:02:20\n",
            "epoch [42/100] batch [1/4] time 1.069 (1.069) data 0.643 (0.643) loss 0.2796 (0.2796) acc 93.7500 (93.7500) lr 1.3090e-02 eta 0:04:11\n",
            "epoch [42/100] batch [2/4] time 0.431 (0.750) data 0.001 (0.322) loss 0.2637 (0.2716) acc 90.6250 (92.1875) lr 1.3090e-02 eta 0:02:55\n",
            "epoch [42/100] batch [3/4] time 0.440 (0.647) data 0.000 (0.215) loss 0.2673 (0.2702) acc 96.8750 (93.7500) lr 1.3090e-02 eta 0:02:30\n",
            "epoch [42/100] batch [4/4] time 0.429 (0.592) data 0.000 (0.161) loss 0.2814 (0.2730) acc 90.6250 (92.9688) lr 1.2790e-02 eta 0:02:17\n",
            "epoch [43/100] batch [1/4] time 1.011 (1.011) data 0.583 (0.583) loss 0.3339 (0.3339) acc 90.6250 (90.6250) lr 1.2790e-02 eta 0:03:53\n",
            "epoch [43/100] batch [2/4] time 0.429 (0.720) data 0.001 (0.292) loss 0.3886 (0.3613) acc 87.5000 (89.0625) lr 1.2790e-02 eta 0:02:45\n",
            "epoch [43/100] batch [3/4] time 0.447 (0.629) data 0.000 (0.195) loss 0.7607 (0.4944) acc 78.1250 (85.4167) lr 1.2790e-02 eta 0:02:23\n",
            "epoch [43/100] batch [4/4] time 0.432 (0.580) data 0.000 (0.146) loss 0.1963 (0.4199) acc 100.0000 (89.0625) lr 1.2487e-02 eta 0:02:12\n",
            "epoch [44/100] batch [1/4] time 1.045 (1.045) data 0.618 (0.618) loss 0.1962 (0.1962) acc 96.8750 (96.8750) lr 1.2487e-02 eta 0:03:57\n",
            "epoch [44/100] batch [2/4] time 0.435 (0.740) data 0.000 (0.309) loss 0.2074 (0.2018) acc 100.0000 (98.4375) lr 1.2487e-02 eta 0:02:47\n",
            "epoch [44/100] batch [3/4] time 0.449 (0.643) data 0.001 (0.206) loss 0.4891 (0.2976) acc 87.5000 (94.7917) lr 1.2487e-02 eta 0:02:24\n",
            "epoch [44/100] batch [4/4] time 0.432 (0.590) data 0.000 (0.155) loss 0.1589 (0.2629) acc 96.8750 (95.3125) lr 1.2181e-02 eta 0:02:12\n",
            "epoch [45/100] batch [1/4] time 1.087 (1.087) data 0.656 (0.656) loss 0.1636 (0.1636) acc 96.8750 (96.8750) lr 1.2181e-02 eta 0:04:02\n",
            "epoch [45/100] batch [2/4] time 0.434 (0.761) data 0.001 (0.328) loss 0.2381 (0.2008) acc 96.8750 (96.8750) lr 1.2181e-02 eta 0:02:48\n",
            "epoch [45/100] batch [3/4] time 0.444 (0.655) data 0.000 (0.219) loss 0.5061 (0.3026) acc 90.6250 (94.7917) lr 1.2181e-02 eta 0:02:24\n",
            "epoch [45/100] batch [4/4] time 0.433 (0.600) data 0.000 (0.164) loss 0.3257 (0.3084) acc 93.7500 (94.5312) lr 1.1874e-02 eta 0:02:11\n",
            "epoch [46/100] batch [1/4] time 1.067 (1.067) data 0.637 (0.637) loss 0.2075 (0.2075) acc 93.7500 (93.7500) lr 1.1874e-02 eta 0:03:53\n",
            "epoch [46/100] batch [2/4] time 0.437 (0.752) data 0.001 (0.319) loss 0.2524 (0.2300) acc 93.7500 (93.7500) lr 1.1874e-02 eta 0:02:43\n",
            "epoch [46/100] batch [3/4] time 0.446 (0.650) data 0.000 (0.213) loss 0.1850 (0.2150) acc 96.8750 (94.7917) lr 1.1874e-02 eta 0:02:21\n",
            "epoch [46/100] batch [4/4] time 0.438 (0.597) data 0.000 (0.159) loss 0.1617 (0.2017) acc 93.7500 (94.5312) lr 1.1564e-02 eta 0:02:08\n",
            "epoch [47/100] batch [1/4] time 1.030 (1.030) data 0.600 (0.600) loss 0.3390 (0.3390) acc 90.6250 (90.6250) lr 1.1564e-02 eta 0:03:41\n",
            "epoch [47/100] batch [2/4] time 0.443 (0.737) data 0.001 (0.300) loss 0.1830 (0.2610) acc 96.8750 (93.7500) lr 1.1564e-02 eta 0:02:37\n",
            "epoch [47/100] batch [3/4] time 0.444 (0.639) data 0.000 (0.200) loss 0.2907 (0.2709) acc 93.7500 (93.7500) lr 1.1564e-02 eta 0:02:16\n",
            "epoch [47/100] batch [4/4] time 0.433 (0.588) data 0.000 (0.150) loss 0.4906 (0.3258) acc 87.5000 (92.1875) lr 1.1253e-02 eta 0:02:04\n",
            "epoch [48/100] batch [1/4] time 1.089 (1.089) data 0.657 (0.657) loss 0.5697 (0.5697) acc 87.5000 (87.5000) lr 1.1253e-02 eta 0:03:49\n",
            "epoch [48/100] batch [2/4] time 0.440 (0.764) data 0.001 (0.329) loss 0.2505 (0.4101) acc 96.8750 (92.1875) lr 1.1253e-02 eta 0:02:40\n",
            "epoch [48/100] batch [3/4] time 0.448 (0.659) data 0.000 (0.219) loss 0.3065 (0.3756) acc 93.7500 (92.7083) lr 1.1253e-02 eta 0:02:17\n",
            "epoch [48/100] batch [4/4] time 0.439 (0.604) data 0.000 (0.165) loss 0.1838 (0.3276) acc 96.8750 (93.7500) lr 1.0941e-02 eta 0:02:05\n",
            "epoch [49/100] batch [1/4] time 1.072 (1.072) data 0.636 (0.636) loss 0.3026 (0.3026) acc 93.7500 (93.7500) lr 1.0941e-02 eta 0:03:41\n",
            "epoch [49/100] batch [2/4] time 0.440 (0.756) data 0.001 (0.318) loss 0.2151 (0.2588) acc 96.8750 (95.3125) lr 1.0941e-02 eta 0:02:35\n",
            "epoch [49/100] batch [3/4] time 0.453 (0.655) data 0.000 (0.212) loss 0.5095 (0.3424) acc 90.6250 (93.7500) lr 1.0941e-02 eta 0:02:14\n",
            "epoch [49/100] batch [4/4] time 0.438 (0.601) data 0.000 (0.159) loss 0.3606 (0.3469) acc 90.6250 (92.9688) lr 1.0628e-02 eta 0:02:02\n",
            "epoch [50/100] batch [1/4] time 1.074 (1.074) data 0.662 (0.662) loss 0.2243 (0.2243) acc 96.8750 (96.8750) lr 1.0628e-02 eta 0:03:37\n",
            "epoch [50/100] batch [2/4] time 0.457 (0.765) data 0.001 (0.331) loss 0.2249 (0.2246) acc 96.8750 (96.8750) lr 1.0628e-02 eta 0:02:34\n",
            "epoch [50/100] batch [3/4] time 0.482 (0.671) data 0.000 (0.221) loss 0.3041 (0.2511) acc 90.6250 (94.7917) lr 1.0628e-02 eta 0:02:14\n",
            "epoch [50/100] batch [4/4] time 0.440 (0.613) data 0.000 (0.166) loss 0.2362 (0.2474) acc 93.7500 (94.5312) lr 1.0314e-02 eta 0:02:02\n",
            "epoch [51/100] batch [1/4] time 1.076 (1.076) data 0.640 (0.640) loss 0.3008 (0.3008) acc 93.7500 (93.7500) lr 1.0314e-02 eta 0:03:34\n",
            "epoch [51/100] batch [2/4] time 0.449 (0.763) data 0.001 (0.320) loss 0.3967 (0.3488) acc 93.7500 (93.7500) lr 1.0314e-02 eta 0:02:30\n",
            "epoch [51/100] batch [3/4] time 0.459 (0.661) data 0.000 (0.213) loss 0.1920 (0.2965) acc 96.8750 (94.7917) lr 1.0314e-02 eta 0:02:10\n",
            "epoch [51/100] batch [4/4] time 0.439 (0.606) data 0.000 (0.160) loss 0.5234 (0.3532) acc 87.5000 (92.9688) lr 1.0000e-02 eta 0:01:58\n",
            "epoch [52/100] batch [1/4] time 1.070 (1.070) data 0.629 (0.629) loss 0.4628 (0.4628) acc 90.6250 (90.6250) lr 1.0000e-02 eta 0:03:28\n",
            "epoch [52/100] batch [2/4] time 0.453 (0.761) data 0.001 (0.315) loss 0.2418 (0.3523) acc 96.8750 (93.7500) lr 1.0000e-02 eta 0:02:27\n",
            "epoch [52/100] batch [3/4] time 0.449 (0.657) data 0.000 (0.210) loss 0.2901 (0.3316) acc 93.7500 (93.7500) lr 1.0000e-02 eta 0:02:06\n",
            "epoch [52/100] batch [4/4] time 0.443 (0.604) data 0.000 (0.157) loss 0.1303 (0.2813) acc 100.0000 (95.3125) lr 9.6859e-03 eta 0:01:55\n",
            "epoch [53/100] batch [1/4] time 1.074 (1.074) data 0.634 (0.634) loss 0.3904 (0.3904) acc 93.7500 (93.7500) lr 9.6859e-03 eta 0:03:25\n",
            "epoch [53/100] batch [2/4] time 0.446 (0.760) data 0.001 (0.317) loss 0.2348 (0.3126) acc 93.7500 (93.7500) lr 9.6859e-03 eta 0:02:24\n",
            "epoch [53/100] batch [3/4] time 0.455 (0.658) data 0.000 (0.212) loss 0.2935 (0.3063) acc 93.7500 (93.7500) lr 9.6859e-03 eta 0:02:04\n",
            "epoch [53/100] batch [4/4] time 0.443 (0.604) data 0.000 (0.159) loss 0.3321 (0.3127) acc 87.5000 (92.1875) lr 9.3721e-03 eta 0:01:53\n",
            "epoch [54/100] batch [1/4] time 1.023 (1.023) data 0.586 (0.586) loss 0.3311 (0.3311) acc 90.6250 (90.6250) lr 9.3721e-03 eta 0:03:11\n",
            "epoch [54/100] batch [2/4] time 0.452 (0.738) data 0.001 (0.293) loss 0.2038 (0.2675) acc 96.8750 (93.7500) lr 9.3721e-03 eta 0:02:17\n",
            "epoch [54/100] batch [3/4] time 0.455 (0.643) data 0.000 (0.196) loss 0.2289 (0.2546) acc 90.6250 (92.7083) lr 9.3721e-03 eta 0:01:59\n",
            "epoch [54/100] batch [4/4] time 0.443 (0.593) data 0.000 (0.147) loss 0.3439 (0.2769) acc 87.5000 (91.4062) lr 9.0589e-03 eta 0:01:49\n",
            "epoch [55/100] batch [1/4] time 1.089 (1.089) data 0.652 (0.652) loss 0.4049 (0.4049) acc 87.5000 (87.5000) lr 9.0589e-03 eta 0:03:19\n",
            "epoch [55/100] batch [2/4] time 0.453 (0.771) data 0.000 (0.326) loss 0.1066 (0.2557) acc 100.0000 (93.7500) lr 9.0589e-03 eta 0:02:20\n",
            "epoch [55/100] batch [3/4] time 0.450 (0.664) data 0.000 (0.218) loss 0.1628 (0.2248) acc 96.8750 (94.7917) lr 9.0589e-03 eta 0:02:00\n",
            "epoch [55/100] batch [4/4] time 0.445 (0.609) data 0.000 (0.163) loss 0.3983 (0.2681) acc 90.6250 (93.7500) lr 8.7467e-03 eta 0:01:49\n",
            "epoch [56/100] batch [1/4] time 1.085 (1.085) data 0.647 (0.647) loss 0.1825 (0.1825) acc 93.7500 (93.7500) lr 8.7467e-03 eta 0:03:14\n",
            "epoch [56/100] batch [2/4] time 0.455 (0.770) data 0.001 (0.324) loss 0.2874 (0.2350) acc 93.7500 (93.7500) lr 8.7467e-03 eta 0:02:17\n",
            "epoch [56/100] batch [3/4] time 0.454 (0.664) data 0.000 (0.216) loss 0.2414 (0.2371) acc 96.8750 (94.7917) lr 8.7467e-03 eta 0:01:57\n",
            "epoch [56/100] batch [4/4] time 0.447 (0.610) data 0.000 (0.162) loss 0.5233 (0.3086) acc 84.3750 (92.1875) lr 8.4357e-03 eta 0:01:47\n",
            "epoch [57/100] batch [1/4] time 1.060 (1.060) data 0.623 (0.623) loss 0.2608 (0.2608) acc 96.8750 (96.8750) lr 8.4357e-03 eta 0:03:05\n",
            "epoch [57/100] batch [2/4] time 0.452 (0.756) data 0.001 (0.312) loss 0.1720 (0.2164) acc 96.8750 (96.8750) lr 8.4357e-03 eta 0:02:11\n",
            "epoch [57/100] batch [3/4] time 0.453 (0.655) data 0.000 (0.208) loss 0.2695 (0.2341) acc 93.7500 (95.8333) lr 8.4357e-03 eta 0:01:53\n",
            "epoch [57/100] batch [4/4] time 0.439 (0.601) data 0.000 (0.156) loss 0.1969 (0.2248) acc 93.7500 (95.3125) lr 8.1262e-03 eta 0:01:43\n",
            "epoch [58/100] batch [1/4] time 1.081 (1.081) data 0.646 (0.646) loss 0.2771 (0.2771) acc 93.7500 (93.7500) lr 8.1262e-03 eta 0:03:04\n",
            "epoch [58/100] batch [2/4] time 0.445 (0.763) data 0.001 (0.323) loss 0.1090 (0.1930) acc 100.0000 (96.8750) lr 8.1262e-03 eta 0:02:09\n",
            "epoch [58/100] batch [3/4] time 0.452 (0.659) data 0.000 (0.216) loss 0.2266 (0.2042) acc 93.7500 (95.8333) lr 8.1262e-03 eta 0:01:51\n",
            "epoch [58/100] batch [4/4] time 0.445 (0.606) data 0.000 (0.162) loss 0.1571 (0.1924) acc 96.8750 (96.0938) lr 7.8186e-03 eta 0:01:41\n",
            "epoch [59/100] batch [1/4] time 1.119 (1.119) data 0.684 (0.684) loss 0.1690 (0.1690) acc 96.8750 (96.8750) lr 7.8186e-03 eta 0:03:06\n",
            "epoch [59/100] batch [2/4] time 0.440 (0.780) data 0.001 (0.342) loss 0.4308 (0.2999) acc 90.6250 (93.7500) lr 7.8186e-03 eta 0:02:09\n",
            "epoch [59/100] batch [3/4] time 0.448 (0.669) data 0.000 (0.228) loss 0.2720 (0.2906) acc 96.8750 (94.7917) lr 7.8186e-03 eta 0:01:50\n",
            "epoch [59/100] batch [4/4] time 0.443 (0.612) data 0.000 (0.171) loss 0.1423 (0.2535) acc 96.8750 (95.3125) lr 7.5131e-03 eta 0:01:40\n",
            "epoch [60/100] batch [1/4] time 1.085 (1.085) data 0.652 (0.652) loss 0.1950 (0.1950) acc 93.7500 (93.7500) lr 7.5131e-03 eta 0:02:56\n",
            "epoch [60/100] batch [2/4] time 0.438 (0.761) data 0.001 (0.326) loss 0.5147 (0.3548) acc 90.6250 (92.1875) lr 7.5131e-03 eta 0:02:03\n",
            "epoch [60/100] batch [3/4] time 0.455 (0.659) data 0.000 (0.218) loss 0.1811 (0.2969) acc 93.7500 (92.7083) lr 7.5131e-03 eta 0:01:46\n",
            "epoch [60/100] batch [4/4] time 0.436 (0.603) data 0.000 (0.163) loss 0.3298 (0.3051) acc 90.6250 (92.1875) lr 7.2101e-03 eta 0:01:36\n",
            "epoch [61/100] batch [1/4] time 1.057 (1.057) data 0.629 (0.629) loss 0.1489 (0.1489) acc 96.8750 (96.8750) lr 7.2101e-03 eta 0:02:48\n",
            "epoch [61/100] batch [2/4] time 0.439 (0.748) data 0.001 (0.315) loss 0.1745 (0.1617) acc 96.8750 (96.8750) lr 7.2101e-03 eta 0:01:58\n",
            "epoch [61/100] batch [3/4] time 0.444 (0.647) data 0.000 (0.210) loss 0.2780 (0.2005) acc 90.6250 (94.7917) lr 7.2101e-03 eta 0:01:41\n",
            "epoch [61/100] batch [4/4] time 0.437 (0.594) data 0.000 (0.158) loss 0.3075 (0.2272) acc 90.6250 (93.7500) lr 6.9098e-03 eta 0:01:32\n",
            "epoch [62/100] batch [1/4] time 1.040 (1.040) data 0.611 (0.611) loss 0.2677 (0.2677) acc 96.8750 (96.8750) lr 6.9098e-03 eta 0:02:41\n",
            "epoch [62/100] batch [2/4] time 0.432 (0.736) data 0.000 (0.306) loss 0.4471 (0.3574) acc 90.6250 (93.7500) lr 6.9098e-03 eta 0:01:53\n",
            "epoch [62/100] batch [3/4] time 0.447 (0.640) data 0.000 (0.204) loss 0.3280 (0.3476) acc 96.8750 (94.7917) lr 6.9098e-03 eta 0:01:37\n",
            "epoch [62/100] batch [4/4] time 0.434 (0.589) data 0.000 (0.153) loss 0.1740 (0.3042) acc 96.8750 (95.3125) lr 6.6126e-03 eta 0:01:29\n",
            "epoch [63/100] batch [1/4] time 1.063 (1.063) data 0.636 (0.636) loss 0.2393 (0.2393) acc 93.7500 (93.7500) lr 6.6126e-03 eta 0:02:40\n",
            "epoch [63/100] batch [2/4] time 0.433 (0.748) data 0.000 (0.318) loss 0.1724 (0.2058) acc 96.8750 (95.3125) lr 6.6126e-03 eta 0:01:52\n",
            "epoch [63/100] batch [3/4] time 0.443 (0.646) data 0.000 (0.212) loss 0.1388 (0.1835) acc 100.0000 (96.8750) lr 6.6126e-03 eta 0:01:36\n",
            "epoch [63/100] batch [4/4] time 0.433 (0.593) data 0.000 (0.159) loss 0.1948 (0.1863) acc 96.8750 (96.8750) lr 6.3188e-03 eta 0:01:27\n",
            "epoch [64/100] batch [1/4] time 1.056 (1.056) data 0.628 (0.628) loss 0.2377 (0.2377) acc 96.8750 (96.8750) lr 6.3188e-03 eta 0:02:35\n",
            "epoch [64/100] batch [2/4] time 0.431 (0.744) data 0.000 (0.314) loss 0.1009 (0.1693) acc 100.0000 (98.4375) lr 6.3188e-03 eta 0:01:48\n",
            "epoch [64/100] batch [3/4] time 0.451 (0.646) data 0.001 (0.210) loss 0.3915 (0.2434) acc 93.7500 (96.8750) lr 6.3188e-03 eta 0:01:33\n",
            "epoch [64/100] batch [4/4] time 0.435 (0.593) data 0.000 (0.157) loss 0.0973 (0.2069) acc 100.0000 (97.6562) lr 6.0285e-03 eta 0:01:25\n",
            "epoch [65/100] batch [1/4] time 1.016 (1.016) data 0.588 (0.588) loss 0.1910 (0.1910) acc 96.8750 (96.8750) lr 6.0285e-03 eta 0:02:25\n",
            "epoch [65/100] batch [2/4] time 0.435 (0.726) data 0.001 (0.294) loss 0.2043 (0.1976) acc 93.7500 (95.3125) lr 6.0285e-03 eta 0:01:43\n",
            "epoch [65/100] batch [3/4] time 0.443 (0.632) data 0.000 (0.196) loss 0.3228 (0.2394) acc 90.6250 (93.7500) lr 6.0285e-03 eta 0:01:29\n",
            "epoch [65/100] batch [4/4] time 0.432 (0.582) data 0.000 (0.147) loss 0.3623 (0.2701) acc 93.7500 (93.7500) lr 5.7422e-03 eta 0:01:21\n",
            "epoch [66/100] batch [1/4] time 1.042 (1.042) data 0.618 (0.618) loss 0.1099 (0.1099) acc 100.0000 (100.0000) lr 5.7422e-03 eta 0:02:24\n",
            "epoch [66/100] batch [2/4] time 0.431 (0.737) data 0.001 (0.309) loss 0.1676 (0.1388) acc 96.8750 (98.4375) lr 5.7422e-03 eta 0:01:41\n",
            "epoch [66/100] batch [3/4] time 0.449 (0.641) data 0.000 (0.206) loss 0.1123 (0.1299) acc 100.0000 (98.9583) lr 5.7422e-03 eta 0:01:27\n",
            "epoch [66/100] batch [4/4] time 0.428 (0.588) data 0.000 (0.155) loss 0.3902 (0.1950) acc 90.6250 (96.8750) lr 5.4601e-03 eta 0:01:19\n",
            "epoch [67/100] batch [1/4] time 1.023 (1.023) data 0.595 (0.595) loss 0.3592 (0.3592) acc 87.5000 (87.5000) lr 5.4601e-03 eta 0:02:18\n",
            "epoch [67/100] batch [2/4] time 0.429 (0.726) data 0.000 (0.298) loss 0.2957 (0.3275) acc 96.8750 (92.1875) lr 5.4601e-03 eta 0:01:37\n",
            "epoch [67/100] batch [3/4] time 0.447 (0.633) data 0.000 (0.199) loss 0.1607 (0.2719) acc 100.0000 (94.7917) lr 5.4601e-03 eta 0:01:24\n",
            "epoch [67/100] batch [4/4] time 0.431 (0.583) data 0.000 (0.149) loss 0.2439 (0.2649) acc 93.7500 (94.5312) lr 5.1825e-03 eta 0:01:16\n",
            "epoch [68/100] batch [1/4] time 1.062 (1.062) data 0.639 (0.639) loss 0.0758 (0.0758) acc 100.0000 (100.0000) lr 5.1825e-03 eta 0:02:19\n",
            "epoch [68/100] batch [2/4] time 0.431 (0.746) data 0.000 (0.320) loss 0.2921 (0.1840) acc 93.7500 (96.8750) lr 5.1825e-03 eta 0:01:37\n",
            "epoch [68/100] batch [3/4] time 0.445 (0.646) data 0.000 (0.213) loss 0.1686 (0.1789) acc 96.8750 (96.8750) lr 5.1825e-03 eta 0:01:23\n",
            "epoch [68/100] batch [4/4] time 0.431 (0.592) data 0.000 (0.160) loss 0.1728 (0.1773) acc 96.8750 (96.8750) lr 4.9096e-03 eta 0:01:15\n",
            "epoch [69/100] batch [1/4] time 1.046 (1.046) data 0.620 (0.620) loss 0.3339 (0.3339) acc 90.6250 (90.6250) lr 4.9096e-03 eta 0:02:12\n",
            "epoch [69/100] batch [2/4] time 0.430 (0.738) data 0.000 (0.310) loss 0.4641 (0.3990) acc 81.2500 (85.9375) lr 4.9096e-03 eta 0:01:32\n",
            "epoch [69/100] batch [3/4] time 0.445 (0.640) data 0.000 (0.207) loss 0.5322 (0.4434) acc 90.6250 (87.5000) lr 4.9096e-03 eta 0:01:20\n",
            "epoch [69/100] batch [4/4] time 0.428 (0.587) data 0.000 (0.155) loss 0.1608 (0.3727) acc 96.8750 (89.8438) lr 4.6417e-03 eta 0:01:12\n",
            "epoch [70/100] batch [1/4] time 1.044 (1.044) data 0.619 (0.619) loss 0.3583 (0.3583) acc 87.5000 (87.5000) lr 4.6417e-03 eta 0:02:08\n",
            "epoch [70/100] batch [2/4] time 0.435 (0.740) data 0.001 (0.310) loss 0.3434 (0.3509) acc 90.6250 (89.0625) lr 4.6417e-03 eta 0:01:30\n",
            "epoch [70/100] batch [3/4] time 0.444 (0.641) data 0.000 (0.206) loss 0.4406 (0.3808) acc 87.5000 (88.5417) lr 4.6417e-03 eta 0:01:17\n",
            "epoch [70/100] batch [4/4] time 0.428 (0.588) data 0.000 (0.155) loss 0.1273 (0.3174) acc 100.0000 (91.4062) lr 4.3792e-03 eta 0:01:10\n",
            "epoch [71/100] batch [1/4] time 1.137 (1.137) data 0.710 (0.710) loss 0.4022 (0.4022) acc 93.7500 (93.7500) lr 4.3792e-03 eta 0:02:15\n",
            "epoch [71/100] batch [2/4] time 0.428 (0.782) data 0.001 (0.356) loss 0.3111 (0.3567) acc 90.6250 (92.1875) lr 4.3792e-03 eta 0:01:32\n",
            "epoch [71/100] batch [3/4] time 0.442 (0.669) data 0.000 (0.237) loss 0.1004 (0.2712) acc 96.8750 (93.7500) lr 4.3792e-03 eta 0:01:18\n",
            "epoch [71/100] batch [4/4] time 0.426 (0.608) data 0.000 (0.178) loss 0.2991 (0.2782) acc 96.8750 (94.5312) lr 4.1221e-03 eta 0:01:10\n",
            "epoch [72/100] batch [1/4] time 1.074 (1.074) data 0.650 (0.650) loss 0.2041 (0.2041) acc 96.8750 (96.8750) lr 4.1221e-03 eta 0:02:03\n",
            "epoch [72/100] batch [2/4] time 0.429 (0.752) data 0.000 (0.325) loss 0.1090 (0.1566) acc 100.0000 (98.4375) lr 4.1221e-03 eta 0:01:25\n",
            "epoch [72/100] batch [3/4] time 0.441 (0.648) data 0.000 (0.217) loss 0.3496 (0.2209) acc 93.7500 (96.8750) lr 4.1221e-03 eta 0:01:13\n",
            "epoch [72/100] batch [4/4] time 0.431 (0.594) data 0.000 (0.163) loss 0.2839 (0.2367) acc 93.7500 (96.0938) lr 3.8709e-03 eta 0:01:06\n",
            "epoch [73/100] batch [1/4] time 1.038 (1.038) data 0.611 (0.611) loss 0.4202 (0.4202) acc 90.6250 (90.6250) lr 3.8709e-03 eta 0:01:55\n",
            "epoch [73/100] batch [2/4] time 0.428 (0.733) data 0.001 (0.306) loss 0.4953 (0.4578) acc 93.7500 (92.1875) lr 3.8709e-03 eta 0:01:20\n",
            "epoch [73/100] batch [3/4] time 0.442 (0.636) data 0.000 (0.204) loss 0.2441 (0.3865) acc 93.7500 (92.7083) lr 3.8709e-03 eta 0:01:09\n",
            "epoch [73/100] batch [4/4] time 0.430 (0.584) data 0.000 (0.153) loss 0.3210 (0.3702) acc 87.5000 (91.4062) lr 3.6258e-03 eta 0:01:03\n",
            "epoch [74/100] batch [1/4] time 1.048 (1.048) data 0.623 (0.623) loss 0.1087 (0.1087) acc 96.8750 (96.8750) lr 3.6258e-03 eta 0:01:52\n",
            "epoch [74/100] batch [2/4] time 0.432 (0.740) data 0.001 (0.312) loss 0.1425 (0.1256) acc 100.0000 (98.4375) lr 3.6258e-03 eta 0:01:18\n",
            "epoch [74/100] batch [3/4] time 0.441 (0.640) data 0.000 (0.208) loss 0.3088 (0.1867) acc 93.7500 (96.8750) lr 3.6258e-03 eta 0:01:07\n",
            "epoch [74/100] batch [4/4] time 0.428 (0.587) data 0.000 (0.156) loss 0.3476 (0.2269) acc 90.6250 (95.3125) lr 3.3869e-03 eta 0:01:01\n",
            "epoch [75/100] batch [1/4] time 1.027 (1.027) data 0.600 (0.600) loss 0.1851 (0.1851) acc 96.8750 (96.8750) lr 3.3869e-03 eta 0:01:45\n",
            "epoch [75/100] batch [2/4] time 0.433 (0.730) data 0.001 (0.300) loss 0.2602 (0.2226) acc 93.7500 (95.3125) lr 3.3869e-03 eta 0:01:14\n",
            "epoch [75/100] batch [3/4] time 0.444 (0.634) data 0.000 (0.200) loss 0.5075 (0.3176) acc 90.6250 (93.7500) lr 3.3869e-03 eta 0:01:04\n",
            "epoch [75/100] batch [4/4] time 0.432 (0.584) data 0.000 (0.150) loss 0.0930 (0.2614) acc 100.0000 (95.3125) lr 3.1545e-03 eta 0:00:58\n",
            "epoch [76/100] batch [1/4] time 1.104 (1.104) data 0.679 (0.679) loss 0.2595 (0.2595) acc 93.7500 (93.7500) lr 3.1545e-03 eta 0:01:49\n",
            "epoch [76/100] batch [2/4] time 0.431 (0.768) data 0.001 (0.340) loss 0.3207 (0.2901) acc 90.6250 (92.1875) lr 3.1545e-03 eta 0:01:15\n",
            "epoch [76/100] batch [3/4] time 0.445 (0.660) data 0.000 (0.227) loss 0.2728 (0.2844) acc 93.7500 (92.7083) lr 3.1545e-03 eta 0:01:04\n",
            "epoch [76/100] batch [4/4] time 0.434 (0.604) data 0.000 (0.170) loss 0.2731 (0.2815) acc 93.7500 (92.9688) lr 2.9289e-03 eta 0:00:57\n",
            "epoch [77/100] batch [1/4] time 1.065 (1.065) data 0.639 (0.639) loss 0.1056 (0.1056) acc 100.0000 (100.0000) lr 2.9289e-03 eta 0:01:41\n",
            "epoch [77/100] batch [2/4] time 0.429 (0.747) data 0.001 (0.320) loss 0.1903 (0.1480) acc 100.0000 (100.0000) lr 2.9289e-03 eta 0:01:10\n",
            "epoch [77/100] batch [3/4] time 0.444 (0.646) data 0.000 (0.213) loss 0.2146 (0.1702) acc 96.8750 (98.9583) lr 2.9289e-03 eta 0:01:00\n",
            "epoch [77/100] batch [4/4] time 0.432 (0.593) data 0.000 (0.160) loss 0.1320 (0.1606) acc 100.0000 (99.2188) lr 2.7103e-03 eta 0:00:54\n",
            "epoch [78/100] batch [1/4] time 1.038 (1.038) data 0.611 (0.611) loss 0.4156 (0.4156) acc 93.7500 (93.7500) lr 2.7103e-03 eta 0:01:34\n",
            "epoch [78/100] batch [2/4] time 0.432 (0.735) data 0.000 (0.306) loss 0.3055 (0.3606) acc 90.6250 (92.1875) lr 2.7103e-03 eta 0:01:06\n",
            "epoch [78/100] batch [3/4] time 0.443 (0.638) data 0.000 (0.204) loss 0.2401 (0.3204) acc 93.7500 (92.7083) lr 2.7103e-03 eta 0:00:56\n",
            "epoch [78/100] batch [4/4] time 0.431 (0.586) data 0.001 (0.153) loss 0.1253 (0.2716) acc 96.8750 (93.7500) lr 2.4989e-03 eta 0:00:51\n",
            "epoch [79/100] batch [1/4] time 1.046 (1.046) data 0.622 (0.622) loss 0.1388 (0.1388) acc 96.8750 (96.8750) lr 2.4989e-03 eta 0:01:31\n",
            "epoch [79/100] batch [2/4] time 0.433 (0.740) data 0.000 (0.311) loss 0.1512 (0.1450) acc 93.7500 (95.3125) lr 2.4989e-03 eta 0:01:03\n",
            "epoch [79/100] batch [3/4] time 0.444 (0.641) data 0.000 (0.208) loss 0.1724 (0.1541) acc 96.8750 (95.8333) lr 2.4989e-03 eta 0:00:54\n",
            "epoch [79/100] batch [4/4] time 0.431 (0.588) data 0.000 (0.156) loss 0.1074 (0.1425) acc 96.8750 (96.0938) lr 2.2949e-03 eta 0:00:49\n",
            "epoch [80/100] batch [1/4] time 1.013 (1.013) data 0.586 (0.586) loss 0.3363 (0.3363) acc 90.6250 (90.6250) lr 2.2949e-03 eta 0:01:24\n",
            "epoch [80/100] batch [2/4] time 0.436 (0.724) data 0.001 (0.293) loss 0.1307 (0.2335) acc 100.0000 (95.3125) lr 2.2949e-03 eta 0:00:59\n",
            "epoch [80/100] batch [3/4] time 0.443 (0.630) data 0.000 (0.196) loss 0.2271 (0.2313) acc 93.7500 (94.7917) lr 2.2949e-03 eta 0:00:51\n",
            "epoch [80/100] batch [4/4] time 0.432 (0.581) data 0.000 (0.147) loss 0.1504 (0.2111) acc 93.7500 (94.5312) lr 2.0984e-03 eta 0:00:46\n",
            "epoch [81/100] batch [1/4] time 1.061 (1.061) data 0.632 (0.632) loss 0.2608 (0.2608) acc 93.7500 (93.7500) lr 2.0984e-03 eta 0:01:23\n",
            "epoch [81/100] batch [2/4] time 0.440 (0.751) data 0.000 (0.316) loss 0.3797 (0.3202) acc 90.6250 (92.1875) lr 2.0984e-03 eta 0:00:58\n",
            "epoch [81/100] batch [3/4] time 0.446 (0.649) data 0.000 (0.211) loss 0.3680 (0.3361) acc 90.6250 (91.6667) lr 2.0984e-03 eta 0:00:49\n",
            "epoch [81/100] batch [4/4] time 0.434 (0.595) data 0.000 (0.158) loss 0.1802 (0.2972) acc 100.0000 (93.7500) lr 1.9098e-03 eta 0:00:45\n",
            "epoch [82/100] batch [1/4] time 1.062 (1.062) data 0.628 (0.628) loss 0.0891 (0.0891) acc 96.8750 (96.8750) lr 1.9098e-03 eta 0:01:19\n",
            "epoch [82/100] batch [2/4] time 0.436 (0.749) data 0.001 (0.314) loss 0.1479 (0.1185) acc 96.8750 (96.8750) lr 1.9098e-03 eta 0:00:55\n",
            "epoch [82/100] batch [3/4] time 0.443 (0.647) data 0.000 (0.210) loss 0.3142 (0.1837) acc 90.6250 (94.7917) lr 1.9098e-03 eta 0:00:47\n",
            "epoch [82/100] batch [4/4] time 0.437 (0.595) data 0.000 (0.157) loss 0.0691 (0.1551) acc 100.0000 (96.0938) lr 1.7292e-03 eta 0:00:42\n",
            "epoch [83/100] batch [1/4] time 1.069 (1.069) data 0.639 (0.639) loss 0.2507 (0.2507) acc 96.8750 (96.8750) lr 1.7292e-03 eta 0:01:15\n",
            "epoch [83/100] batch [2/4] time 0.441 (0.755) data 0.000 (0.320) loss 0.2008 (0.2258) acc 93.7500 (95.3125) lr 1.7292e-03 eta 0:00:52\n",
            "epoch [83/100] batch [3/4] time 0.446 (0.652) data 0.000 (0.213) loss 0.2158 (0.2225) acc 93.7500 (94.7917) lr 1.7292e-03 eta 0:00:44\n",
            "epoch [83/100] batch [4/4] time 0.435 (0.598) data 0.000 (0.160) loss 0.2544 (0.2304) acc 93.7500 (94.5312) lr 1.5567e-03 eta 0:00:40\n",
            "epoch [84/100] batch [1/4] time 1.076 (1.076) data 0.648 (0.648) loss 0.1195 (0.1195) acc 96.8750 (96.8750) lr 1.5567e-03 eta 0:01:12\n",
            "epoch [84/100] batch [2/4] time 0.435 (0.756) data 0.000 (0.324) loss 0.2605 (0.1900) acc 90.6250 (93.7500) lr 1.5567e-03 eta 0:00:49\n",
            "epoch [84/100] batch [3/4] time 0.453 (0.655) data 0.000 (0.216) loss 0.1420 (0.1740) acc 100.0000 (95.8333) lr 1.5567e-03 eta 0:00:42\n",
            "epoch [84/100] batch [4/4] time 0.435 (0.600) data 0.000 (0.162) loss 0.0632 (0.1463) acc 100.0000 (96.8750) lr 1.3926e-03 eta 0:00:38\n",
            "epoch [85/100] batch [1/4] time 1.057 (1.057) data 0.627 (0.627) loss 0.2145 (0.2145) acc 96.8750 (96.8750) lr 1.3926e-03 eta 0:01:06\n",
            "epoch [85/100] batch [2/4] time 0.441 (0.749) data 0.000 (0.314) loss 0.1658 (0.1901) acc 96.8750 (96.8750) lr 1.3926e-03 eta 0:00:46\n",
            "epoch [85/100] batch [3/4] time 0.445 (0.648) data 0.000 (0.209) loss 0.0830 (0.1544) acc 100.0000 (97.9167) lr 1.3926e-03 eta 0:00:39\n",
            "epoch [85/100] batch [4/4] time 0.434 (0.594) data 0.000 (0.157) loss 0.3003 (0.1909) acc 93.7500 (96.8750) lr 1.2369e-03 eta 0:00:35\n",
            "epoch [86/100] batch [1/4] time 1.095 (1.095) data 0.663 (0.663) loss 0.4231 (0.4231) acc 90.6250 (90.6250) lr 1.2369e-03 eta 0:01:04\n",
            "epoch [86/100] batch [2/4] time 0.438 (0.766) data 0.001 (0.332) loss 0.3349 (0.3790) acc 93.7500 (92.1875) lr 1.2369e-03 eta 0:00:44\n",
            "epoch [86/100] batch [3/4] time 0.450 (0.661) data 0.000 (0.221) loss 0.3804 (0.3795) acc 84.3750 (89.5833) lr 1.2369e-03 eta 0:00:37\n",
            "epoch [86/100] batch [4/4] time 0.437 (0.605) data 0.000 (0.166) loss 0.2350 (0.3433) acc 93.7500 (90.6250) lr 1.0899e-03 eta 0:00:33\n",
            "epoch [87/100] batch [1/4] time 1.096 (1.096) data 0.665 (0.665) loss 0.0807 (0.0807) acc 100.0000 (100.0000) lr 1.0899e-03 eta 0:01:00\n",
            "epoch [87/100] batch [2/4] time 0.439 (0.767) data 0.000 (0.333) loss 0.2020 (0.1414) acc 93.7500 (96.8750) lr 1.0899e-03 eta 0:00:41\n",
            "epoch [87/100] batch [3/4] time 0.449 (0.661) data 0.000 (0.222) loss 0.2949 (0.1925) acc 93.7500 (95.8333) lr 1.0899e-03 eta 0:00:35\n",
            "epoch [87/100] batch [4/4] time 0.439 (0.605) data 0.000 (0.166) loss 0.3044 (0.2205) acc 87.5000 (93.7500) lr 9.5173e-04 eta 0:00:31\n",
            "epoch [88/100] batch [1/4] time 1.029 (1.029) data 0.599 (0.599) loss 0.2365 (0.2365) acc 93.7500 (93.7500) lr 9.5173e-04 eta 0:00:52\n",
            "epoch [88/100] batch [2/4] time 0.444 (0.737) data 0.001 (0.300) loss 0.3374 (0.2870) acc 93.7500 (93.7500) lr 9.5173e-04 eta 0:00:36\n",
            "epoch [88/100] batch [3/4] time 0.445 (0.640) data 0.000 (0.200) loss 0.3010 (0.2916) acc 93.7500 (93.7500) lr 9.5173e-04 eta 0:00:31\n",
            "epoch [88/100] batch [4/4] time 0.440 (0.590) data 0.000 (0.150) loss 0.3701 (0.3112) acc 84.3750 (91.4062) lr 8.2245e-04 eta 0:00:28\n",
            "epoch [89/100] batch [1/4] time 1.066 (1.066) data 0.633 (0.633) loss 0.2426 (0.2426) acc 96.8750 (96.8750) lr 8.2245e-04 eta 0:00:50\n",
            "epoch [89/100] batch [2/4] time 0.443 (0.754) data 0.000 (0.316) loss 0.2719 (0.2573) acc 90.6250 (93.7500) lr 8.2245e-04 eta 0:00:34\n",
            "epoch [89/100] batch [3/4] time 0.446 (0.652) data 0.000 (0.211) loss 0.3745 (0.2964) acc 87.5000 (91.6667) lr 8.2245e-04 eta 0:00:29\n",
            "epoch [89/100] batch [4/4] time 0.437 (0.598) data 0.000 (0.158) loss 0.4743 (0.3408) acc 90.6250 (91.4062) lr 7.0224e-04 eta 0:00:26\n",
            "epoch [90/100] batch [1/4] time 1.035 (1.035) data 0.603 (0.603) loss 0.0726 (0.0726) acc 100.0000 (100.0000) lr 7.0224e-04 eta 0:00:44\n",
            "epoch [90/100] batch [2/4] time 0.436 (0.735) data 0.001 (0.302) loss 0.2408 (0.1567) acc 96.8750 (98.4375) lr 7.0224e-04 eta 0:00:30\n",
            "epoch [90/100] batch [3/4] time 0.449 (0.640) data 0.000 (0.201) loss 0.1649 (0.1594) acc 96.8750 (97.9167) lr 7.0224e-04 eta 0:00:26\n",
            "epoch [90/100] batch [4/4] time 0.436 (0.589) data 0.000 (0.151) loss 0.1339 (0.1531) acc 96.8750 (97.6562) lr 5.9119e-04 eta 0:00:23\n",
            "epoch [91/100] batch [1/4] time 1.038 (1.038) data 0.603 (0.603) loss 0.3889 (0.3889) acc 90.6250 (90.6250) lr 5.9119e-04 eta 0:00:40\n",
            "epoch [91/100] batch [2/4] time 0.444 (0.741) data 0.001 (0.302) loss 0.3520 (0.3705) acc 90.6250 (90.6250) lr 5.9119e-04 eta 0:00:28\n",
            "epoch [91/100] batch [3/4] time 0.447 (0.643) data 0.000 (0.201) loss 0.2365 (0.3258) acc 93.7500 (91.6667) lr 5.9119e-04 eta 0:00:23\n",
            "epoch [91/100] batch [4/4] time 0.442 (0.593) data 0.000 (0.151) loss 0.1016 (0.2698) acc 100.0000 (93.7500) lr 4.8943e-04 eta 0:00:21\n",
            "epoch [92/100] batch [1/4] time 1.049 (1.049) data 0.616 (0.616) loss 0.3230 (0.3230) acc 90.6250 (90.6250) lr 4.8943e-04 eta 0:00:36\n",
            "epoch [92/100] batch [2/4] time 0.441 (0.745) data 0.001 (0.308) loss 0.1551 (0.2391) acc 96.8750 (93.7500) lr 4.8943e-04 eta 0:00:25\n",
            "epoch [92/100] batch [3/4] time 0.446 (0.645) data 0.000 (0.206) loss 0.2104 (0.2295) acc 96.8750 (94.7917) lr 4.8943e-04 eta 0:00:21\n",
            "epoch [92/100] batch [4/4] time 0.435 (0.593) data 0.000 (0.154) loss 0.1393 (0.2069) acc 100.0000 (96.0938) lr 3.9706e-04 eta 0:00:18\n",
            "epoch [93/100] batch [1/4] time 1.046 (1.046) data 0.616 (0.616) loss 0.3213 (0.3213) acc 93.7500 (93.7500) lr 3.9706e-04 eta 0:00:32\n",
            "epoch [93/100] batch [2/4] time 0.436 (0.741) data 0.000 (0.308) loss 0.2890 (0.3051) acc 90.6250 (92.1875) lr 3.9706e-04 eta 0:00:22\n",
            "epoch [93/100] batch [3/4] time 0.454 (0.645) data 0.000 (0.206) loss 0.2793 (0.2965) acc 90.6250 (91.6667) lr 3.9706e-04 eta 0:00:18\n",
            "epoch [93/100] batch [4/4] time 0.431 (0.592) data 0.000 (0.154) loss 0.2417 (0.2828) acc 96.8750 (92.9688) lr 3.1417e-04 eta 0:00:16\n",
            "epoch [94/100] batch [1/4] time 1.052 (1.052) data 0.624 (0.624) loss 0.3200 (0.3200) acc 93.7500 (93.7500) lr 3.1417e-04 eta 0:00:28\n",
            "epoch [94/100] batch [2/4] time 0.437 (0.745) data 0.000 (0.312) loss 0.1442 (0.2321) acc 96.8750 (95.3125) lr 3.1417e-04 eta 0:00:19\n",
            "epoch [94/100] batch [3/4] time 0.445 (0.645) data 0.000 (0.208) loss 0.2053 (0.2232) acc 93.7500 (94.7917) lr 3.1417e-04 eta 0:00:16\n",
            "epoch [94/100] batch [4/4] time 0.434 (0.592) data 0.000 (0.156) loss 0.2397 (0.2273) acc 93.7500 (94.5312) lr 2.4083e-04 eta 0:00:14\n",
            "epoch [95/100] batch [1/4] time 1.074 (1.074) data 0.641 (0.641) loss 0.1549 (0.1549) acc 96.8750 (96.8750) lr 2.4083e-04 eta 0:00:24\n",
            "epoch [95/100] batch [2/4] time 0.437 (0.755) data 0.000 (0.321) loss 0.2419 (0.1984) acc 90.6250 (93.7500) lr 2.4083e-04 eta 0:00:16\n",
            "epoch [95/100] batch [3/4] time 0.445 (0.652) data 0.000 (0.214) loss 0.1843 (0.1937) acc 96.8750 (94.7917) lr 2.4083e-04 eta 0:00:13\n",
            "epoch [95/100] batch [4/4] time 0.437 (0.598) data 0.000 (0.161) loss 0.2385 (0.2049) acc 90.6250 (93.7500) lr 1.7713e-04 eta 0:00:11\n",
            "epoch [96/100] batch [1/4] time 1.043 (1.043) data 0.615 (0.615) loss 0.1511 (0.1511) acc 100.0000 (100.0000) lr 1.7713e-04 eta 0:00:19\n",
            "epoch [96/100] batch [2/4] time 0.437 (0.740) data 0.000 (0.308) loss 0.3171 (0.2341) acc 93.7500 (96.8750) lr 1.7713e-04 eta 0:00:13\n",
            "epoch [96/100] batch [3/4] time 0.451 (0.644) data 0.000 (0.205) loss 0.1228 (0.1970) acc 96.8750 (96.8750) lr 1.7713e-04 eta 0:00:10\n",
            "epoch [96/100] batch [4/4] time 0.435 (0.592) data 0.000 (0.154) loss 0.2917 (0.2207) acc 87.5000 (94.5312) lr 1.2312e-04 eta 0:00:09\n",
            "epoch [97/100] batch [1/4] time 1.101 (1.101) data 0.670 (0.670) loss 0.3203 (0.3203) acc 93.7500 (93.7500) lr 1.2312e-04 eta 0:00:16\n",
            "epoch [97/100] batch [2/4] time 0.439 (0.770) data 0.001 (0.335) loss 0.1872 (0.2537) acc 96.8750 (95.3125) lr 1.2312e-04 eta 0:00:10\n",
            "epoch [97/100] batch [3/4] time 0.448 (0.663) data 0.000 (0.224) loss 0.0862 (0.1979) acc 100.0000 (96.8750) lr 1.2312e-04 eta 0:00:08\n",
            "epoch [97/100] batch [4/4] time 0.433 (0.605) data 0.000 (0.168) loss 0.1552 (0.1872) acc 96.8750 (96.8750) lr 7.8853e-05 eta 0:00:07\n",
            "epoch [98/100] batch [1/4] time 1.076 (1.076) data 0.649 (0.649) loss 0.3710 (0.3710) acc 93.7500 (93.7500) lr 7.8853e-05 eta 0:00:11\n",
            "epoch [98/100] batch [2/4] time 0.437 (0.756) data 0.000 (0.325) loss 0.2520 (0.3115) acc 96.8750 (95.3125) lr 7.8853e-05 eta 0:00:07\n",
            "epoch [98/100] batch [3/4] time 0.444 (0.652) data 0.000 (0.217) loss 0.1724 (0.2652) acc 90.6250 (93.7500) lr 7.8853e-05 eta 0:00:05\n",
            "epoch [98/100] batch [4/4] time 0.435 (0.598) data 0.000 (0.163) loss 0.3388 (0.2836) acc 93.7500 (93.7500) lr 4.4380e-05 eta 0:00:04\n",
            "epoch [99/100] batch [1/4] time 1.064 (1.064) data 0.636 (0.636) loss 0.2819 (0.2819) acc 93.7500 (93.7500) lr 4.4380e-05 eta 0:00:07\n",
            "epoch [99/100] batch [2/4] time 0.432 (0.748) data 0.000 (0.318) loss 0.1719 (0.2269) acc 93.7500 (93.7500) lr 4.4380e-05 eta 0:00:04\n",
            "epoch [99/100] batch [3/4] time 0.452 (0.649) data 0.000 (0.212) loss 0.3491 (0.2676) acc 90.6250 (92.7083) lr 4.4380e-05 eta 0:00:03\n",
            "epoch [99/100] batch [4/4] time 0.435 (0.596) data 0.000 (0.159) loss 0.2062 (0.2523) acc 96.8750 (93.7500) lr 1.9733e-05 eta 0:00:02\n",
            "epoch [100/100] batch [1/4] time 1.069 (1.069) data 0.644 (0.644) loss 0.1110 (0.1110) acc 100.0000 (100.0000) lr 1.9733e-05 eta 0:00:03\n",
            "epoch [100/100] batch [2/4] time 0.432 (0.750) data 0.000 (0.322) loss 0.2370 (0.1740) acc 93.7500 (96.8750) lr 1.9733e-05 eta 0:00:01\n",
            "epoch [100/100] batch [3/4] time 0.448 (0.650) data 0.000 (0.215) loss 0.1193 (0.1557) acc 100.0000 (97.9167) lr 1.9733e-05 eta 0:00:00\n",
            "epoch [100/100] batch [4/4] time 0.435 (0.596) data 0.000 (0.161) loss 0.1978 (0.1663) acc 93.7500 (96.8750) lr 4.9344e-06 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:19<00:00,  1.90it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,271\n",
            "* accuracy: 89.2%\n",
            "* error: 10.8%\n",
            "* macro_f1: 89.0%\n",
            "Elapsed: 0:04:23\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets-4shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep100_4shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3068c9-27b6-4756-8fb7-a6156e2f18b1",
        "id": "pI7Ymr7FyWJg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 11:15:15.617468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 11:15:15.636922: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 11:15:15.642868: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 11:15:15.656803: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 11:15:16.635261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep100.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '2']\n",
            "output_dir: output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 2\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_2-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  74\n",
            "# val      74\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "        [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "        [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "        ...,\n",
            "        [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "        [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "        [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-5.2644e-04, -1.1168e-02, -2.1661e-02,  ..., -4.8596e-03,\n",
            "           5.8912e-03,  6.7493e-03],\n",
            "         [ 5.8841e-04, -1.8584e-02, -2.5060e-02,  ..., -3.4987e-03,\n",
            "           5.4105e-03, -1.7861e-03],\n",
            "         [ 1.7309e-03, -7.2606e-03, -2.3481e-02,  ..., -2.1107e-03,\n",
            "           4.9070e-03,  6.0702e-03],\n",
            "         ...,\n",
            "         [ 1.0223e-03, -1.2080e-02, -1.5868e-02,  ..., -1.2882e-02,\n",
            "          -2.0567e-03,  4.8362e-03],\n",
            "         [ 7.8468e-03, -1.1038e-02, -1.0770e-02,  ...,  7.4988e-04,\n",
            "           1.5215e-03, -1.6774e-03],\n",
            "         [ 2.3028e-03, -5.9798e-03, -1.7329e-02,  ...,  5.5724e-04,\n",
            "           1.5857e-02,  4.3269e-03]],\n",
            "\n",
            "        [[ 6.0997e-03,  1.7106e-03, -2.1188e-02,  ...,  2.5523e-03,\n",
            "           2.1776e-03,  7.7525e-03],\n",
            "         [ 6.9275e-03, -6.8095e-03, -1.3799e-02,  ..., -3.6580e-03,\n",
            "           5.6513e-03,  1.9980e-03],\n",
            "         [ 6.2942e-04, -7.5124e-03, -9.4807e-03,  ...,  9.2128e-03,\n",
            "           1.0227e-02,  3.9989e-03],\n",
            "         ...,\n",
            "         [-8.9265e-04, -1.5108e-02, -1.9400e-02,  ..., -3.9164e-03,\n",
            "           4.4435e-03, -3.3369e-04],\n",
            "         [ 9.0713e-03, -1.0043e-02, -2.8737e-02,  ..., -4.9359e-03,\n",
            "           9.1699e-03,  5.4980e-03],\n",
            "         [-8.9653e-05, -7.9620e-03, -2.2422e-02,  ..., -2.5861e-03,\n",
            "           2.1928e-03,  6.9095e-03]],\n",
            "\n",
            "        [[ 2.2506e-04, -1.2583e-02, -1.8306e-02,  ..., -1.1051e-02,\n",
            "           9.2462e-03,  8.1454e-03],\n",
            "         [-9.4987e-04, -1.2930e-02, -2.0111e-02,  ..., -9.7730e-03,\n",
            "           5.3495e-03,  1.8893e-03],\n",
            "         [-2.7428e-03, -8.8699e-03, -1.4280e-02,  ...,  5.0471e-03,\n",
            "           5.3924e-03,  1.0839e-02],\n",
            "         ...,\n",
            "         [ 7.2136e-03, -9.1107e-03, -1.5424e-02,  ...,  1.9115e-03,\n",
            "           6.7415e-03, -8.7823e-04],\n",
            "         [-4.8866e-03, -9.1675e-03, -1.5351e-02,  ..., -5.0427e-03,\n",
            "           8.2830e-03,  1.1029e-02],\n",
            "         [-4.4785e-03, -1.2034e-02, -1.6046e-02,  ..., -1.3892e-03,\n",
            "           1.1470e-02, -4.6686e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1066e-02, -2.1956e-03, -1.7727e-02,  ..., -1.0564e-03,\n",
            "           1.2302e-02,  5.0345e-03],\n",
            "         [ 2.7537e-03, -4.1800e-04, -1.4596e-02,  ..., -1.1298e-03,\n",
            "           6.9791e-03,  3.2216e-03],\n",
            "         [ 3.3331e-03, -1.0128e-02, -1.2788e-02,  ..., -1.2218e-02,\n",
            "           2.8146e-03,  4.3975e-03],\n",
            "         ...,\n",
            "         [ 5.6838e-04, -7.7658e-04, -3.2178e-02,  ..., -6.8700e-03,\n",
            "           4.0220e-03, -3.8805e-04],\n",
            "         [ 9.2410e-04, -7.9189e-03, -1.0762e-02,  ..., -4.6880e-03,\n",
            "           5.9761e-03,  4.5386e-03],\n",
            "         [-2.8230e-04, -1.1029e-02, -1.4386e-02,  ..., -1.5611e-04,\n",
            "           1.0764e-02,  4.3288e-03]],\n",
            "\n",
            "        [[ 5.6610e-03, -6.1458e-03, -2.3435e-02,  ..., -8.1288e-03,\n",
            "           9.4217e-03,  1.0564e-02],\n",
            "         [ 1.7781e-03, -9.1155e-03, -7.3978e-03,  ...,  3.8989e-03,\n",
            "           8.7808e-03, -3.2376e-03],\n",
            "         [ 6.8626e-03, -1.4937e-02, -2.3481e-02,  ...,  1.5681e-03,\n",
            "           1.1096e-02,  2.1168e-03],\n",
            "         ...,\n",
            "         [ 2.3734e-03, -1.0926e-02, -2.4415e-02,  ..., -8.3730e-03,\n",
            "           1.5094e-02,  1.2127e-03],\n",
            "         [ 2.2802e-03, -9.2195e-03, -2.1869e-02,  ..., -2.9556e-03,\n",
            "           8.4852e-03,  4.9182e-03],\n",
            "         [-8.3199e-03, -8.2483e-03, -1.6881e-02,  ..., -4.5459e-03,\n",
            "           6.2860e-03,  5.5418e-04]],\n",
            "\n",
            "        [[ 1.3695e-03, -1.2808e-02, -1.6318e-02,  ...,  3.0864e-03,\n",
            "           4.0411e-03,  6.7378e-03],\n",
            "         [ 4.1695e-03, -1.2923e-02, -2.3946e-02,  ...,  2.9491e-03,\n",
            "           1.2264e-02,  2.7028e-03],\n",
            "         [-3.1091e-04, -1.2759e-02, -1.5500e-02,  ...,  1.6597e-03,\n",
            "           2.8471e-03, -5.2213e-03],\n",
            "         ...,\n",
            "         [ 9.3452e-05, -4.8621e-03, -1.9077e-02,  ..., -5.4662e-03,\n",
            "           5.2866e-03,  2.2937e-03],\n",
            "         [ 5.1670e-03, -1.0069e-02, -1.7373e-02,  ...,  2.5828e-03,\n",
            "           8.2563e-03,  1.3529e-03],\n",
            "         [ 3.2577e-03, -1.7065e-02, -1.7372e-02,  ..., -3.7200e-03,\n",
            "           4.5885e-03, -3.4474e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4769\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1/tensorboard)\n",
            "epoch [1/100] batch [1/2] time 2.027 (2.027) data 0.712 (0.712) loss 0.8947 (0.8947) acc 68.7500 (68.7500) lr 1.0000e-05 eta 0:06:43\n",
            "epoch [1/100] batch [2/2] time 0.421 (1.224) data 0.000 (0.356) loss 1.5817 (1.2382) acc 59.3750 (64.0625) lr 2.0000e-02 eta 0:04:02\n",
            "epoch [2/100] batch [1/2] time 0.968 (0.968) data 0.547 (0.547) loss 1.3151 (1.3151) acc 68.7500 (68.7500) lr 2.0000e-02 eta 0:03:10\n",
            "epoch [2/100] batch [2/2] time 0.423 (0.696) data 0.000 (0.274) loss 1.5046 (1.4099) acc 43.7500 (56.2500) lr 1.9995e-02 eta 0:02:16\n",
            "epoch [3/100] batch [1/2] time 0.925 (0.925) data 0.503 (0.503) loss 1.0688 (1.0688) acc 75.0000 (75.0000) lr 1.9995e-02 eta 0:03:00\n",
            "epoch [3/100] batch [2/2] time 0.428 (0.676) data 0.000 (0.252) loss 0.6406 (0.8547) acc 87.5000 (81.2500) lr 1.9980e-02 eta 0:02:11\n",
            "epoch [4/100] batch [1/2] time 1.005 (1.005) data 0.583 (0.583) loss 0.8734 (0.8734) acc 71.8750 (71.8750) lr 1.9980e-02 eta 0:03:13\n",
            "epoch [4/100] batch [2/2] time 0.427 (0.716) data 0.000 (0.292) loss 1.0780 (0.9757) acc 62.5000 (67.1875) lr 1.9956e-02 eta 0:02:17\n",
            "epoch [5/100] batch [1/2] time 1.068 (1.068) data 0.645 (0.645) loss 0.9351 (0.9351) acc 65.6250 (65.6250) lr 1.9956e-02 eta 0:03:24\n",
            "epoch [5/100] batch [2/2] time 0.429 (0.749) data 0.000 (0.323) loss 1.0211 (0.9781) acc 65.6250 (65.6250) lr 1.9921e-02 eta 0:02:22\n",
            "epoch [6/100] batch [1/2] time 1.035 (1.035) data 0.611 (0.611) loss 0.9690 (0.9690) acc 65.6250 (65.6250) lr 1.9921e-02 eta 0:03:15\n",
            "epoch [6/100] batch [2/2] time 0.425 (0.730) data 0.001 (0.306) loss 0.5747 (0.7719) acc 87.5000 (76.5625) lr 1.9877e-02 eta 0:02:17\n",
            "epoch [7/100] batch [1/2] time 1.014 (1.014) data 0.587 (0.587) loss 1.0931 (1.0931) acc 62.5000 (62.5000) lr 1.9877e-02 eta 0:03:09\n",
            "epoch [7/100] batch [2/2] time 0.434 (0.724) data 0.000 (0.294) loss 0.9215 (1.0073) acc 78.1250 (70.3125) lr 1.9823e-02 eta 0:02:14\n",
            "epoch [8/100] batch [1/2] time 0.963 (0.963) data 0.537 (0.537) loss 0.8164 (0.8164) acc 75.0000 (75.0000) lr 1.9823e-02 eta 0:02:58\n",
            "epoch [8/100] batch [2/2] time 0.434 (0.699) data 0.001 (0.269) loss 0.4123 (0.6144) acc 93.7500 (84.3750) lr 1.9759e-02 eta 0:02:08\n",
            "epoch [9/100] batch [1/2] time 0.948 (0.948) data 0.522 (0.522) loss 0.6883 (0.6883) acc 84.3750 (84.3750) lr 1.9759e-02 eta 0:02:53\n",
            "epoch [9/100] batch [2/2] time 0.430 (0.689) data 0.001 (0.261) loss 0.5739 (0.6311) acc 84.3750 (84.3750) lr 1.9686e-02 eta 0:02:05\n",
            "epoch [10/100] batch [1/2] time 1.004 (1.004) data 0.576 (0.576) loss 0.5091 (0.5091) acc 87.5000 (87.5000) lr 1.9686e-02 eta 0:03:01\n",
            "epoch [10/100] batch [2/2] time 0.436 (0.720) data 0.000 (0.288) loss 0.8862 (0.6977) acc 65.6250 (76.5625) lr 1.9603e-02 eta 0:02:09\n",
            "epoch [11/100] batch [1/2] time 1.009 (1.009) data 0.580 (0.580) loss 0.5439 (0.5439) acc 84.3750 (84.3750) lr 1.9603e-02 eta 0:03:00\n",
            "epoch [11/100] batch [2/2] time 0.436 (0.723) data 0.000 (0.290) loss 0.7181 (0.6310) acc 81.2500 (82.8125) lr 1.9511e-02 eta 0:02:08\n",
            "epoch [12/100] batch [1/2] time 0.969 (0.969) data 0.540 (0.540) loss 0.5132 (0.5132) acc 87.5000 (87.5000) lr 1.9511e-02 eta 0:02:51\n",
            "epoch [12/100] batch [2/2] time 0.434 (0.701) data 0.000 (0.270) loss 0.7731 (0.6431) acc 84.3750 (85.9375) lr 1.9409e-02 eta 0:02:03\n",
            "epoch [13/100] batch [1/2] time 0.995 (0.995) data 0.568 (0.568) loss 0.6543 (0.6543) acc 84.3750 (84.3750) lr 1.9409e-02 eta 0:02:54\n",
            "epoch [13/100] batch [2/2] time 0.437 (0.716) data 0.000 (0.284) loss 0.6528 (0.6536) acc 84.3750 (84.3750) lr 1.9298e-02 eta 0:02:04\n",
            "epoch [14/100] batch [1/2] time 1.037 (1.037) data 0.606 (0.606) loss 0.3537 (0.3537) acc 93.7500 (93.7500) lr 1.9298e-02 eta 0:02:59\n",
            "epoch [14/100] batch [2/2] time 0.438 (0.737) data 0.000 (0.303) loss 0.7239 (0.5388) acc 71.8750 (82.8125) lr 1.9178e-02 eta 0:02:06\n",
            "epoch [15/100] batch [1/2] time 1.058 (1.058) data 0.625 (0.625) loss 0.5998 (0.5998) acc 87.5000 (87.5000) lr 1.9178e-02 eta 0:03:00\n",
            "epoch [15/100] batch [2/2] time 0.442 (0.750) data 0.000 (0.313) loss 0.6021 (0.6009) acc 90.6250 (89.0625) lr 1.9048e-02 eta 0:02:07\n",
            "epoch [16/100] batch [1/2] time 0.982 (0.982) data 0.550 (0.550) loss 0.7035 (0.7035) acc 78.1250 (78.1250) lr 1.9048e-02 eta 0:02:46\n",
            "epoch [16/100] batch [2/2] time 0.440 (0.711) data 0.000 (0.275) loss 0.4658 (0.5846) acc 93.7500 (85.9375) lr 1.8910e-02 eta 0:01:59\n",
            "epoch [17/100] batch [1/2] time 0.978 (0.978) data 0.547 (0.547) loss 0.3304 (0.3304) acc 96.8750 (96.8750) lr 1.8910e-02 eta 0:02:43\n",
            "epoch [17/100] batch [2/2] time 0.444 (0.711) data 0.000 (0.274) loss 0.4047 (0.3675) acc 87.5000 (92.1875) lr 1.8763e-02 eta 0:01:57\n",
            "epoch [18/100] batch [1/2] time 1.030 (1.030) data 0.595 (0.595) loss 0.4871 (0.4871) acc 81.2500 (81.2500) lr 1.8763e-02 eta 0:02:49\n",
            "epoch [18/100] batch [2/2] time 0.440 (0.735) data 0.001 (0.298) loss 0.4337 (0.4604) acc 90.6250 (85.9375) lr 1.8607e-02 eta 0:02:00\n",
            "epoch [19/100] batch [1/2] time 1.011 (1.011) data 0.575 (0.575) loss 0.3882 (0.3882) acc 90.6250 (90.6250) lr 1.8607e-02 eta 0:02:44\n",
            "epoch [19/100] batch [2/2] time 0.441 (0.726) data 0.000 (0.288) loss 0.3307 (0.3594) acc 93.7500 (92.1875) lr 1.8443e-02 eta 0:01:57\n",
            "epoch [20/100] batch [1/2] time 0.989 (0.989) data 0.555 (0.555) loss 0.4697 (0.4697) acc 93.7500 (93.7500) lr 1.8443e-02 eta 0:02:39\n",
            "epoch [20/100] batch [2/2] time 0.445 (0.717) data 0.000 (0.278) loss 0.3282 (0.3989) acc 90.6250 (92.1875) lr 1.8271e-02 eta 0:01:54\n",
            "epoch [21/100] batch [1/2] time 0.967 (0.967) data 0.533 (0.533) loss 0.3072 (0.3072) acc 93.7500 (93.7500) lr 1.8271e-02 eta 0:02:33\n",
            "epoch [21/100] batch [2/2] time 0.449 (0.708) data 0.001 (0.267) loss 0.7053 (0.5063) acc 81.2500 (87.5000) lr 1.8090e-02 eta 0:01:51\n",
            "epoch [22/100] batch [1/2] time 0.980 (0.980) data 0.544 (0.544) loss 0.4708 (0.4708) acc 90.6250 (90.6250) lr 1.8090e-02 eta 0:02:33\n",
            "epoch [22/100] batch [2/2] time 0.446 (0.713) data 0.000 (0.272) loss 0.6464 (0.5586) acc 84.3750 (87.5000) lr 1.7902e-02 eta 0:01:51\n",
            "epoch [23/100] batch [1/2] time 1.043 (1.043) data 0.608 (0.608) loss 0.2742 (0.2742) acc 93.7500 (93.7500) lr 1.7902e-02 eta 0:02:41\n",
            "epoch [23/100] batch [2/2] time 0.447 (0.745) data 0.001 (0.304) loss 0.3603 (0.3173) acc 93.7500 (93.7500) lr 1.7705e-02 eta 0:01:54\n",
            "epoch [24/100] batch [1/2] time 1.007 (1.007) data 0.572 (0.572) loss 0.4588 (0.4588) acc 87.5000 (87.5000) lr 1.7705e-02 eta 0:02:34\n",
            "epoch [24/100] batch [2/2] time 0.450 (0.729) data 0.000 (0.286) loss 0.5007 (0.4797) acc 87.5000 (87.5000) lr 1.7501e-02 eta 0:01:50\n",
            "epoch [25/100] batch [1/2] time 1.038 (1.038) data 0.600 (0.600) loss 0.2622 (0.2622) acc 93.7500 (93.7500) lr 1.7501e-02 eta 0:02:36\n",
            "epoch [25/100] batch [2/2] time 0.457 (0.747) data 0.000 (0.300) loss 0.4741 (0.3681) acc 87.5000 (90.6250) lr 1.7290e-02 eta 0:01:52\n",
            "epoch [26/100] batch [1/2] time 0.996 (0.996) data 0.556 (0.556) loss 0.1291 (0.1291) acc 100.0000 (100.0000) lr 1.7290e-02 eta 0:02:28\n",
            "epoch [26/100] batch [2/2] time 0.454 (0.725) data 0.000 (0.278) loss 0.4847 (0.3069) acc 87.5000 (93.7500) lr 1.7071e-02 eta 0:01:47\n",
            "epoch [27/100] batch [1/2] time 0.978 (0.978) data 0.537 (0.537) loss 0.4550 (0.4550) acc 84.3750 (84.3750) lr 1.7071e-02 eta 0:02:23\n",
            "epoch [27/100] batch [2/2] time 0.444 (0.711) data 0.000 (0.269) loss 0.5333 (0.4942) acc 81.2500 (82.8125) lr 1.6845e-02 eta 0:01:43\n",
            "epoch [28/100] batch [1/2] time 1.013 (1.013) data 0.574 (0.574) loss 0.2937 (0.2937) acc 93.7500 (93.7500) lr 1.6845e-02 eta 0:02:26\n",
            "epoch [28/100] batch [2/2] time 0.465 (0.739) data 0.000 (0.287) loss 0.4787 (0.3862) acc 87.5000 (90.6250) lr 1.6613e-02 eta 0:01:46\n",
            "epoch [29/100] batch [1/2] time 0.999 (0.999) data 0.559 (0.559) loss 0.3209 (0.3209) acc 93.7500 (93.7500) lr 1.6613e-02 eta 0:02:22\n",
            "epoch [29/100] batch [2/2] time 0.454 (0.726) data 0.001 (0.280) loss 0.3446 (0.3327) acc 93.7500 (93.7500) lr 1.6374e-02 eta 0:01:43\n",
            "epoch [30/100] batch [1/2] time 0.996 (0.996) data 0.556 (0.556) loss 0.2825 (0.2825) acc 93.7500 (93.7500) lr 1.6374e-02 eta 0:02:20\n",
            "epoch [30/100] batch [2/2] time 0.459 (0.728) data 0.001 (0.279) loss 0.3874 (0.3350) acc 90.6250 (92.1875) lr 1.6129e-02 eta 0:01:41\n",
            "epoch [31/100] batch [1/2] time 1.040 (1.040) data 0.602 (0.602) loss 0.3553 (0.3553) acc 84.3750 (84.3750) lr 1.6129e-02 eta 0:02:24\n",
            "epoch [31/100] batch [2/2] time 0.453 (0.747) data 0.000 (0.301) loss 0.3424 (0.3488) acc 93.7500 (89.0625) lr 1.5878e-02 eta 0:01:43\n",
            "epoch [32/100] batch [1/2] time 1.040 (1.040) data 0.600 (0.600) loss 0.4873 (0.4873) acc 87.5000 (87.5000) lr 1.5878e-02 eta 0:02:22\n",
            "epoch [32/100] batch [2/2] time 0.442 (0.741) data 0.000 (0.300) loss 0.3907 (0.4390) acc 87.5000 (87.5000) lr 1.5621e-02 eta 0:01:40\n",
            "epoch [33/100] batch [1/2] time 1.020 (1.020) data 0.584 (0.584) loss 0.1506 (0.1506) acc 96.8750 (96.8750) lr 1.5621e-02 eta 0:02:17\n",
            "epoch [33/100] batch [2/2] time 0.450 (0.735) data 0.000 (0.292) loss 0.5852 (0.3679) acc 87.5000 (92.1875) lr 1.5358e-02 eta 0:01:38\n",
            "epoch [34/100] batch [1/2] time 0.988 (0.988) data 0.551 (0.551) loss 0.3927 (0.3927) acc 87.5000 (87.5000) lr 1.5358e-02 eta 0:02:11\n",
            "epoch [34/100] batch [2/2] time 0.444 (0.716) data 0.000 (0.276) loss 0.1977 (0.2952) acc 96.8750 (92.1875) lr 1.5090e-02 eta 0:01:34\n",
            "epoch [35/100] batch [1/2] time 0.969 (0.969) data 0.534 (0.534) loss 0.6729 (0.6729) acc 81.2500 (81.2500) lr 1.5090e-02 eta 0:02:06\n",
            "epoch [35/100] batch [2/2] time 0.445 (0.707) data 0.000 (0.267) loss 0.3669 (0.5199) acc 90.6250 (85.9375) lr 1.4818e-02 eta 0:01:31\n",
            "epoch [36/100] batch [1/2] time 0.998 (0.998) data 0.565 (0.565) loss 0.5209 (0.5209) acc 87.5000 (87.5000) lr 1.4818e-02 eta 0:02:08\n",
            "epoch [36/100] batch [2/2] time 0.446 (0.722) data 0.000 (0.283) loss 0.2332 (0.3770) acc 100.0000 (93.7500) lr 1.4540e-02 eta 0:01:32\n",
            "epoch [37/100] batch [1/2] time 0.990 (0.990) data 0.559 (0.559) loss 0.2690 (0.2690) acc 93.7500 (93.7500) lr 1.4540e-02 eta 0:02:05\n",
            "epoch [37/100] batch [2/2] time 0.441 (0.715) data 0.000 (0.280) loss 0.3578 (0.3134) acc 93.7500 (93.7500) lr 1.4258e-02 eta 0:01:30\n",
            "epoch [38/100] batch [1/2] time 0.969 (0.969) data 0.538 (0.538) loss 0.2118 (0.2118) acc 93.7500 (93.7500) lr 1.4258e-02 eta 0:02:01\n",
            "epoch [38/100] batch [2/2] time 0.436 (0.702) data 0.000 (0.269) loss 0.4132 (0.3125) acc 87.5000 (90.6250) lr 1.3971e-02 eta 0:01:27\n",
            "epoch [39/100] batch [1/2] time 1.059 (1.059) data 0.629 (0.629) loss 0.1847 (0.1847) acc 96.8750 (96.8750) lr 1.3971e-02 eta 0:02:10\n",
            "epoch [39/100] batch [2/2] time 0.438 (0.748) data 0.000 (0.315) loss 0.4003 (0.2925) acc 90.6250 (93.7500) lr 1.3681e-02 eta 0:01:31\n",
            "epoch [40/100] batch [1/2] time 1.006 (1.006) data 0.574 (0.574) loss 0.4791 (0.4791) acc 87.5000 (87.5000) lr 1.3681e-02 eta 0:02:01\n",
            "epoch [40/100] batch [2/2] time 0.439 (0.722) data 0.001 (0.287) loss 0.2634 (0.3712) acc 93.7500 (90.6250) lr 1.3387e-02 eta 0:01:26\n",
            "epoch [41/100] batch [1/2] time 0.982 (0.982) data 0.557 (0.557) loss 0.4785 (0.4785) acc 90.6250 (90.6250) lr 1.3387e-02 eta 0:01:56\n",
            "epoch [41/100] batch [2/2] time 0.436 (0.709) data 0.000 (0.279) loss 0.3016 (0.3901) acc 96.8750 (93.7500) lr 1.3090e-02 eta 0:01:23\n",
            "epoch [42/100] batch [1/2] time 0.949 (0.949) data 0.519 (0.519) loss 0.4719 (0.4719) acc 87.5000 (87.5000) lr 1.3090e-02 eta 0:01:50\n",
            "epoch [42/100] batch [2/2] time 0.435 (0.692) data 0.001 (0.260) loss 0.3732 (0.4226) acc 93.7500 (90.6250) lr 1.2790e-02 eta 0:01:20\n",
            "epoch [43/100] batch [1/2] time 0.946 (0.946) data 0.517 (0.517) loss 0.2786 (0.2786) acc 93.7500 (93.7500) lr 1.2790e-02 eta 0:01:48\n",
            "epoch [43/100] batch [2/2] time 0.433 (0.690) data 0.000 (0.259) loss 0.3073 (0.2929) acc 90.6250 (92.1875) lr 1.2487e-02 eta 0:01:18\n",
            "epoch [44/100] batch [1/2] time 1.015 (1.015) data 0.588 (0.588) loss 0.2143 (0.2143) acc 96.8750 (96.8750) lr 1.2487e-02 eta 0:01:54\n",
            "epoch [44/100] batch [2/2] time 0.431 (0.723) data 0.000 (0.294) loss 0.2401 (0.2272) acc 96.8750 (96.8750) lr 1.2181e-02 eta 0:01:20\n",
            "epoch [45/100] batch [1/2] time 1.018 (1.018) data 0.593 (0.593) loss 0.2971 (0.2971) acc 93.7500 (93.7500) lr 1.2181e-02 eta 0:01:52\n",
            "epoch [45/100] batch [2/2] time 0.429 (0.724) data 0.000 (0.297) loss 0.2148 (0.2559) acc 96.8750 (95.3125) lr 1.1874e-02 eta 0:01:19\n",
            "epoch [46/100] batch [1/2] time 0.986 (0.986) data 0.560 (0.560) loss 0.1947 (0.1947) acc 93.7500 (93.7500) lr 1.1874e-02 eta 0:01:47\n",
            "epoch [46/100] batch [2/2] time 0.431 (0.708) data 0.000 (0.280) loss 0.8160 (0.5053) acc 78.1250 (85.9375) lr 1.1564e-02 eta 0:01:16\n",
            "epoch [47/100] batch [1/2] time 0.957 (0.957) data 0.534 (0.534) loss 0.2947 (0.2947) acc 93.7500 (93.7500) lr 1.1564e-02 eta 0:01:42\n",
            "epoch [47/100] batch [2/2] time 0.426 (0.692) data 0.000 (0.267) loss 0.1272 (0.2109) acc 96.8750 (95.3125) lr 1.1253e-02 eta 0:01:13\n",
            "epoch [48/100] batch [1/2] time 1.012 (1.012) data 0.589 (0.589) loss 0.1640 (0.1640) acc 96.8750 (96.8750) lr 1.1253e-02 eta 0:01:46\n",
            "epoch [48/100] batch [2/2] time 0.432 (0.722) data 0.000 (0.295) loss 0.2529 (0.2085) acc 96.8750 (96.8750) lr 1.0941e-02 eta 0:01:15\n",
            "epoch [49/100] batch [1/2] time 1.011 (1.011) data 0.588 (0.588) loss 0.4751 (0.4751) acc 90.6250 (90.6250) lr 1.0941e-02 eta 0:01:44\n",
            "epoch [49/100] batch [2/2] time 0.431 (0.721) data 0.000 (0.294) loss 0.4134 (0.4442) acc 87.5000 (89.0625) lr 1.0628e-02 eta 0:01:13\n",
            "epoch [50/100] batch [1/2] time 1.053 (1.053) data 0.632 (0.632) loss 0.1403 (0.1403) acc 96.8750 (96.8750) lr 1.0628e-02 eta 0:01:46\n",
            "epoch [50/100] batch [2/2] time 0.429 (0.741) data 0.000 (0.316) loss 0.1651 (0.1527) acc 96.8750 (96.8750) lr 1.0314e-02 eta 0:01:14\n",
            "epoch [51/100] batch [1/2] time 0.970 (0.970) data 0.547 (0.547) loss 0.3875 (0.3875) acc 87.5000 (87.5000) lr 1.0314e-02 eta 0:01:36\n",
            "epoch [51/100] batch [2/2] time 0.426 (0.698) data 0.000 (0.274) loss 0.4514 (0.4195) acc 90.6250 (89.0625) lr 1.0000e-02 eta 0:01:08\n",
            "epoch [52/100] batch [1/2] time 0.985 (0.985) data 0.565 (0.565) loss 0.2202 (0.2202) acc 90.6250 (90.6250) lr 1.0000e-02 eta 0:01:35\n",
            "epoch [52/100] batch [2/2] time 0.426 (0.705) data 0.000 (0.283) loss 0.4242 (0.3222) acc 84.3750 (87.5000) lr 9.6859e-03 eta 0:01:07\n",
            "epoch [53/100] batch [1/2] time 1.025 (1.025) data 0.604 (0.604) loss 0.5105 (0.5105) acc 90.6250 (90.6250) lr 9.6859e-03 eta 0:01:37\n",
            "epoch [53/100] batch [2/2] time 0.423 (0.724) data 0.001 (0.302) loss 0.3470 (0.4287) acc 90.6250 (90.6250) lr 9.3721e-03 eta 0:01:08\n",
            "epoch [54/100] batch [1/2] time 0.987 (0.987) data 0.568 (0.568) loss 0.1638 (0.1638) acc 96.8750 (96.8750) lr 9.3721e-03 eta 0:01:31\n",
            "epoch [54/100] batch [2/2] time 0.424 (0.705) data 0.000 (0.284) loss 0.1898 (0.1768) acc 96.8750 (96.8750) lr 9.0589e-03 eta 0:01:04\n",
            "epoch [55/100] batch [1/2] time 0.962 (0.962) data 0.541 (0.541) loss 0.2614 (0.2614) acc 93.7500 (93.7500) lr 9.0589e-03 eta 0:01:27\n",
            "epoch [55/100] batch [2/2] time 0.424 (0.693) data 0.001 (0.271) loss 0.1235 (0.1925) acc 100.0000 (96.8750) lr 8.7467e-03 eta 0:01:02\n",
            "epoch [56/100] batch [1/2] time 0.992 (0.992) data 0.572 (0.572) loss 0.1844 (0.1844) acc 93.7500 (93.7500) lr 8.7467e-03 eta 0:01:28\n",
            "epoch [56/100] batch [2/2] time 0.424 (0.708) data 0.000 (0.286) loss 0.2603 (0.2223) acc 93.7500 (93.7500) lr 8.4357e-03 eta 0:01:02\n",
            "epoch [57/100] batch [1/2] time 1.030 (1.030) data 0.609 (0.609) loss 0.1894 (0.1894) acc 93.7500 (93.7500) lr 8.4357e-03 eta 0:01:29\n",
            "epoch [57/100] batch [2/2] time 0.423 (0.727) data 0.001 (0.305) loss 0.3398 (0.2646) acc 90.6250 (92.1875) lr 8.1262e-03 eta 0:01:02\n",
            "epoch [58/100] batch [1/2] time 1.021 (1.021) data 0.601 (0.601) loss 0.2291 (0.2291) acc 93.7500 (93.7500) lr 8.1262e-03 eta 0:01:26\n",
            "epoch [58/100] batch [2/2] time 0.424 (0.722) data 0.001 (0.301) loss 0.2211 (0.2251) acc 100.0000 (96.8750) lr 7.8186e-03 eta 0:01:00\n",
            "epoch [59/100] batch [1/2] time 0.988 (0.988) data 0.568 (0.568) loss 0.2381 (0.2381) acc 93.7500 (93.7500) lr 7.8186e-03 eta 0:01:22\n",
            "epoch [59/100] batch [2/2] time 0.421 (0.705) data 0.000 (0.284) loss 0.4497 (0.3439) acc 84.3750 (89.0625) lr 7.5131e-03 eta 0:00:57\n",
            "epoch [60/100] batch [1/2] time 0.961 (0.961) data 0.541 (0.541) loss 0.2853 (0.2853) acc 93.7500 (93.7500) lr 7.5131e-03 eta 0:01:17\n",
            "epoch [60/100] batch [2/2] time 0.421 (0.691) data 0.001 (0.271) loss 0.1322 (0.2088) acc 100.0000 (96.8750) lr 7.2101e-03 eta 0:00:55\n",
            "epoch [61/100] batch [1/2] time 0.984 (0.984) data 0.566 (0.566) loss 0.2710 (0.2710) acc 93.7500 (93.7500) lr 7.2101e-03 eta 0:01:17\n",
            "epoch [61/100] batch [2/2] time 0.421 (0.703) data 0.000 (0.283) loss 0.3103 (0.2906) acc 93.7500 (93.7500) lr 6.9098e-03 eta 0:00:54\n",
            "epoch [62/100] batch [1/2] time 0.931 (0.931) data 0.513 (0.513) loss 0.2367 (0.2367) acc 93.7500 (93.7500) lr 6.9098e-03 eta 0:01:11\n",
            "epoch [62/100] batch [2/2] time 0.421 (0.676) data 0.000 (0.257) loss 0.4689 (0.3528) acc 84.3750 (89.0625) lr 6.6126e-03 eta 0:00:51\n",
            "epoch [63/100] batch [1/2] time 0.939 (0.939) data 0.519 (0.519) loss 0.4013 (0.4013) acc 84.3750 (84.3750) lr 6.6126e-03 eta 0:01:10\n",
            "epoch [63/100] batch [2/2] time 0.424 (0.681) data 0.001 (0.260) loss 0.2150 (0.3081) acc 96.8750 (90.6250) lr 6.3188e-03 eta 0:00:50\n",
            "epoch [64/100] batch [1/2] time 0.974 (0.974) data 0.557 (0.557) loss 0.2554 (0.2554) acc 93.7500 (93.7500) lr 6.3188e-03 eta 0:01:11\n",
            "epoch [64/100] batch [2/2] time 0.420 (0.697) data 0.001 (0.279) loss 0.3792 (0.3173) acc 90.6250 (92.1875) lr 6.0285e-03 eta 0:00:50\n",
            "epoch [65/100] batch [1/2] time 1.005 (1.005) data 0.589 (0.589) loss 0.2278 (0.2278) acc 93.7500 (93.7500) lr 6.0285e-03 eta 0:01:11\n",
            "epoch [65/100] batch [2/2] time 0.423 (0.714) data 0.000 (0.295) loss 0.3636 (0.2957) acc 84.3750 (89.0625) lr 5.7422e-03 eta 0:00:50\n",
            "epoch [66/100] batch [1/2] time 1.023 (1.023) data 0.602 (0.602) loss 0.2357 (0.2357) acc 96.8750 (96.8750) lr 5.7422e-03 eta 0:01:10\n",
            "epoch [66/100] batch [2/2] time 0.420 (0.722) data 0.001 (0.301) loss 0.4285 (0.3321) acc 93.7500 (95.3125) lr 5.4601e-03 eta 0:00:49\n",
            "epoch [67/100] batch [1/2] time 0.985 (0.985) data 0.565 (0.565) loss 0.2092 (0.2092) acc 96.8750 (96.8750) lr 5.4601e-03 eta 0:01:05\n",
            "epoch [67/100] batch [2/2] time 0.423 (0.704) data 0.001 (0.283) loss 0.1702 (0.1897) acc 96.8750 (96.8750) lr 5.1825e-03 eta 0:00:46\n",
            "epoch [68/100] batch [1/2] time 0.970 (0.970) data 0.553 (0.553) loss 0.0917 (0.0917) acc 100.0000 (100.0000) lr 5.1825e-03 eta 0:01:03\n",
            "epoch [68/100] batch [2/2] time 0.420 (0.695) data 0.000 (0.277) loss 0.1479 (0.1198) acc 96.8750 (98.4375) lr 4.9096e-03 eta 0:00:44\n",
            "epoch [69/100] batch [1/2] time 0.976 (0.976) data 0.557 (0.557) loss 0.1756 (0.1756) acc 100.0000 (100.0000) lr 4.9096e-03 eta 0:01:01\n",
            "epoch [69/100] batch [2/2] time 0.421 (0.699) data 0.000 (0.279) loss 0.3797 (0.2776) acc 93.7500 (96.8750) lr 4.6417e-03 eta 0:00:43\n",
            "epoch [70/100] batch [1/2] time 0.973 (0.973) data 0.555 (0.555) loss 0.1132 (0.1132) acc 100.0000 (100.0000) lr 4.6417e-03 eta 0:00:59\n",
            "epoch [70/100] batch [2/2] time 0.424 (0.699) data 0.000 (0.278) loss 0.4180 (0.2656) acc 84.3750 (92.1875) lr 4.3792e-03 eta 0:00:41\n",
            "epoch [71/100] batch [1/2] time 0.917 (0.917) data 0.499 (0.499) loss 0.1939 (0.1939) acc 96.8750 (96.8750) lr 4.3792e-03 eta 0:00:54\n",
            "epoch [71/100] batch [2/2] time 0.419 (0.668) data 0.000 (0.250) loss 0.1682 (0.1810) acc 93.7500 (95.3125) lr 4.1221e-03 eta 0:00:38\n",
            "epoch [72/100] batch [1/2] time 0.954 (0.954) data 0.538 (0.538) loss 0.2231 (0.2231) acc 90.6250 (90.6250) lr 4.1221e-03 eta 0:00:54\n",
            "epoch [72/100] batch [2/2] time 0.421 (0.688) data 0.000 (0.269) loss 0.1657 (0.1944) acc 96.8750 (93.7500) lr 3.8709e-03 eta 0:00:38\n",
            "epoch [73/100] batch [1/2] time 0.975 (0.975) data 0.556 (0.556) loss 0.1486 (0.1486) acc 100.0000 (100.0000) lr 3.8709e-03 eta 0:00:53\n",
            "epoch [73/100] batch [2/2] time 0.424 (0.700) data 0.000 (0.278) loss 0.1869 (0.1678) acc 93.7500 (96.8750) lr 3.6258e-03 eta 0:00:37\n",
            "epoch [74/100] batch [1/2] time 0.993 (0.993) data 0.574 (0.574) loss 0.2013 (0.2013) acc 96.8750 (96.8750) lr 3.6258e-03 eta 0:00:52\n",
            "epoch [74/100] batch [2/2] time 0.421 (0.707) data 0.000 (0.287) loss 0.0599 (0.1306) acc 100.0000 (98.4375) lr 3.3869e-03 eta 0:00:36\n",
            "epoch [75/100] batch [1/2] time 1.002 (1.002) data 0.582 (0.582) loss 0.2311 (0.2311) acc 96.8750 (96.8750) lr 3.3869e-03 eta 0:00:51\n",
            "epoch [75/100] batch [2/2] time 0.422 (0.712) data 0.001 (0.291) loss 0.2363 (0.2337) acc 93.7500 (95.3125) lr 3.1545e-03 eta 0:00:35\n",
            "epoch [76/100] batch [1/2] time 1.013 (1.013) data 0.594 (0.594) loss 0.1675 (0.1675) acc 93.7500 (93.7500) lr 3.1545e-03 eta 0:00:49\n",
            "epoch [76/100] batch [2/2] time 0.418 (0.716) data 0.001 (0.297) loss 0.2133 (0.1904) acc 96.8750 (95.3125) lr 2.9289e-03 eta 0:00:34\n",
            "epoch [77/100] batch [1/2] time 1.008 (1.008) data 0.590 (0.590) loss 0.0765 (0.0765) acc 100.0000 (100.0000) lr 2.9289e-03 eta 0:00:47\n",
            "epoch [77/100] batch [2/2] time 0.422 (0.715) data 0.000 (0.295) loss 0.1509 (0.1137) acc 100.0000 (100.0000) lr 2.7103e-03 eta 0:00:32\n",
            "epoch [78/100] batch [1/2] time 0.991 (0.991) data 0.574 (0.574) loss 0.1991 (0.1991) acc 93.7500 (93.7500) lr 2.7103e-03 eta 0:00:44\n",
            "epoch [78/100] batch [2/2] time 0.422 (0.707) data 0.000 (0.287) loss 0.4251 (0.3121) acc 87.5000 (90.6250) lr 2.4989e-03 eta 0:00:31\n",
            "epoch [79/100] batch [1/2] time 0.969 (0.969) data 0.552 (0.552) loss 0.1092 (0.1092) acc 100.0000 (100.0000) lr 2.4989e-03 eta 0:00:41\n",
            "epoch [79/100] batch [2/2] time 0.421 (0.695) data 0.000 (0.276) loss 0.2160 (0.1626) acc 93.7500 (96.8750) lr 2.2949e-03 eta 0:00:29\n",
            "epoch [80/100] batch [1/2] time 0.959 (0.959) data 0.542 (0.542) loss 0.2967 (0.2967) acc 90.6250 (90.6250) lr 2.2949e-03 eta 0:00:39\n",
            "epoch [80/100] batch [2/2] time 0.422 (0.690) data 0.000 (0.271) loss 0.1366 (0.2167) acc 96.8750 (93.7500) lr 2.0984e-03 eta 0:00:27\n",
            "epoch [81/100] batch [1/2] time 0.953 (0.953) data 0.535 (0.535) loss 0.3155 (0.3155) acc 90.6250 (90.6250) lr 2.0984e-03 eta 0:00:37\n",
            "epoch [81/100] batch [2/2] time 0.420 (0.686) data 0.001 (0.268) loss 0.3243 (0.3199) acc 93.7500 (92.1875) lr 1.9098e-03 eta 0:00:26\n",
            "epoch [82/100] batch [1/2] time 0.949 (0.949) data 0.532 (0.532) loss 0.2119 (0.2119) acc 96.8750 (96.8750) lr 1.9098e-03 eta 0:00:35\n",
            "epoch [82/100] batch [2/2] time 0.420 (0.684) data 0.000 (0.266) loss 0.2546 (0.2332) acc 93.7500 (95.3125) lr 1.7292e-03 eta 0:00:24\n",
            "epoch [83/100] batch [1/2] time 0.989 (0.989) data 0.569 (0.569) loss 0.6033 (0.6033) acc 78.1250 (78.1250) lr 1.7292e-03 eta 0:00:34\n",
            "epoch [83/100] batch [2/2] time 0.422 (0.706) data 0.001 (0.285) loss 0.2446 (0.4239) acc 93.7500 (85.9375) lr 1.5567e-03 eta 0:00:23\n",
            "epoch [84/100] batch [1/2] time 1.061 (1.061) data 0.643 (0.643) loss 0.2365 (0.2365) acc 93.7500 (93.7500) lr 1.5567e-03 eta 0:00:35\n",
            "epoch [84/100] batch [2/2] time 0.420 (0.741) data 0.001 (0.322) loss 0.1935 (0.2150) acc 93.7500 (93.7500) lr 1.3926e-03 eta 0:00:23\n",
            "epoch [85/100] batch [1/2] time 1.012 (1.012) data 0.594 (0.594) loss 0.3372 (0.3372) acc 87.5000 (87.5000) lr 1.3926e-03 eta 0:00:31\n",
            "epoch [85/100] batch [2/2] time 0.421 (0.716) data 0.001 (0.297) loss 0.0814 (0.2093) acc 100.0000 (93.7500) lr 1.2369e-03 eta 0:00:21\n",
            "epoch [86/100] batch [1/2] time 0.983 (0.983) data 0.563 (0.563) loss 0.3528 (0.3528) acc 93.7500 (93.7500) lr 1.2369e-03 eta 0:00:28\n",
            "epoch [86/100] batch [2/2] time 0.419 (0.701) data 0.000 (0.282) loss 0.1768 (0.2648) acc 96.8750 (95.3125) lr 1.0899e-03 eta 0:00:19\n",
            "epoch [87/100] batch [1/2] time 0.967 (0.967) data 0.549 (0.549) loss 0.1724 (0.1724) acc 100.0000 (100.0000) lr 1.0899e-03 eta 0:00:26\n",
            "epoch [87/100] batch [2/2] time 0.421 (0.694) data 0.000 (0.275) loss 0.1083 (0.1404) acc 100.0000 (100.0000) lr 9.5173e-04 eta 0:00:18\n",
            "epoch [88/100] batch [1/2] time 0.973 (0.973) data 0.556 (0.556) loss 0.1960 (0.1960) acc 96.8750 (96.8750) lr 9.5173e-04 eta 0:00:24\n",
            "epoch [88/100] batch [2/2] time 0.422 (0.698) data 0.000 (0.278) loss 0.2795 (0.2378) acc 90.6250 (93.7500) lr 8.2245e-04 eta 0:00:16\n",
            "epoch [89/100] batch [1/2] time 0.980 (0.980) data 0.561 (0.561) loss 0.1902 (0.1902) acc 90.6250 (90.6250) lr 8.2245e-04 eta 0:00:22\n",
            "epoch [89/100] batch [2/2] time 0.420 (0.700) data 0.000 (0.281) loss 0.2475 (0.2189) acc 93.7500 (92.1875) lr 7.0224e-04 eta 0:00:15\n",
            "epoch [90/100] batch [1/2] time 1.015 (1.015) data 0.597 (0.597) loss 0.1220 (0.1220) acc 96.8750 (96.8750) lr 7.0224e-04 eta 0:00:21\n",
            "epoch [90/100] batch [2/2] time 0.419 (0.717) data 0.000 (0.299) loss 0.1533 (0.1377) acc 93.7500 (95.3125) lr 5.9119e-04 eta 0:00:14\n",
            "epoch [91/100] batch [1/2] time 0.955 (0.955) data 0.538 (0.538) loss 0.2413 (0.2413) acc 93.7500 (93.7500) lr 5.9119e-04 eta 0:00:18\n",
            "epoch [91/100] batch [2/2] time 0.419 (0.687) data 0.001 (0.269) loss 0.4499 (0.3456) acc 84.3750 (89.0625) lr 4.8943e-04 eta 0:00:12\n",
            "epoch [92/100] batch [1/2] time 0.975 (0.975) data 0.558 (0.558) loss 0.2090 (0.2090) acc 93.7500 (93.7500) lr 4.8943e-04 eta 0:00:16\n",
            "epoch [92/100] batch [2/2] time 0.421 (0.698) data 0.000 (0.279) loss 0.2710 (0.2400) acc 96.8750 (95.3125) lr 3.9706e-04 eta 0:00:11\n",
            "epoch [93/100] batch [1/2] time 1.007 (1.007) data 0.591 (0.591) loss 0.2016 (0.2016) acc 96.8750 (96.8750) lr 3.9706e-04 eta 0:00:15\n",
            "epoch [93/100] batch [2/2] time 0.421 (0.714) data 0.001 (0.296) loss 0.1253 (0.1635) acc 100.0000 (98.4375) lr 3.1417e-04 eta 0:00:09\n",
            "epoch [94/100] batch [1/2] time 1.048 (1.048) data 0.632 (0.632) loss 0.4609 (0.4609) acc 87.5000 (87.5000) lr 3.1417e-04 eta 0:00:13\n",
            "epoch [94/100] batch [2/2] time 0.420 (0.734) data 0.001 (0.316) loss 0.3127 (0.3868) acc 93.7500 (90.6250) lr 2.4083e-04 eta 0:00:08\n",
            "epoch [95/100] batch [1/2] time 1.008 (1.008) data 0.591 (0.591) loss 0.1175 (0.1175) acc 100.0000 (100.0000) lr 2.4083e-04 eta 0:00:11\n",
            "epoch [95/100] batch [2/2] time 0.417 (0.713) data 0.000 (0.296) loss 0.2690 (0.1933) acc 93.7500 (96.8750) lr 1.7713e-04 eta 0:00:07\n",
            "epoch [96/100] batch [1/2] time 0.993 (0.993) data 0.574 (0.574) loss 0.2119 (0.2119) acc 93.7500 (93.7500) lr 1.7713e-04 eta 0:00:08\n",
            "epoch [96/100] batch [2/2] time 0.419 (0.706) data 0.000 (0.287) loss 0.1173 (0.1646) acc 100.0000 (96.8750) lr 1.2312e-04 eta 0:00:05\n",
            "epoch [97/100] batch [1/2] time 0.948 (0.948) data 0.532 (0.532) loss 0.0816 (0.0816) acc 100.0000 (100.0000) lr 1.2312e-04 eta 0:00:06\n",
            "epoch [97/100] batch [2/2] time 0.422 (0.685) data 0.000 (0.266) loss 0.1866 (0.1341) acc 96.8750 (98.4375) lr 7.8853e-05 eta 0:00:04\n",
            "epoch [98/100] batch [1/2] time 0.950 (0.950) data 0.532 (0.532) loss 0.1002 (0.1002) acc 100.0000 (100.0000) lr 7.8853e-05 eta 0:00:04\n",
            "epoch [98/100] batch [2/2] time 0.420 (0.685) data 0.000 (0.266) loss 0.3313 (0.2157) acc 87.5000 (93.7500) lr 4.4380e-05 eta 0:00:02\n",
            "epoch [99/100] batch [1/2] time 0.978 (0.978) data 0.563 (0.563) loss 0.3615 (0.3615) acc 93.7500 (93.7500) lr 4.4380e-05 eta 0:00:02\n",
            "epoch [99/100] batch [2/2] time 0.419 (0.699) data 0.001 (0.282) loss 0.1545 (0.2580) acc 96.8750 (95.3125) lr 1.9733e-05 eta 0:00:01\n",
            "epoch [100/100] batch [1/2] time 1.012 (1.012) data 0.596 (0.596) loss 0.2844 (0.2844) acc 93.7500 (93.7500) lr 1.9733e-05 eta 0:00:01\n",
            "epoch [100/100] batch [2/2] time 0.420 (0.716) data 0.000 (0.298) loss 0.1468 (0.2156) acc 96.8750 (95.3125) lr 4.9344e-06 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1/prompt_learner/model.pth.tar-100\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:18<00:00,  1.98it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 2,929\n",
            "* accuracy: 79.8%\n",
            "* error: 20.2%\n",
            "* macro_f1: 77.8%\n",
            "Elapsed: 0:02:49\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets-2shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep100.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep100_2shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06369a9-f594-46f1-a129-b5c88e158354",
        "id": "nrSGXtSCyWJg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 11:18:20.629972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 11:18:20.649591: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 11:18:20.655614: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 11:18:20.669761: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 11:18:21.656782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  37\n",
            "# val      37\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "Layer: vis_ctx, Weights: Parameter containing:\n",
            "tensor([[ 0.0162, -0.5607,  0.1720,  ...,  0.5048,  0.4164,  0.0351],\n",
            "        [ 0.0178, -0.5682,  0.1701,  ...,  0.4977,  0.4271,  0.0418],\n",
            "        [ 0.0250, -0.5695,  0.1778,  ...,  0.5012,  0.4111,  0.0415],\n",
            "        ...,\n",
            "        [ 0.0148, -0.5727,  0.1671,  ...,  0.5045,  0.4128,  0.0407],\n",
            "        [ 0.0169, -0.5608,  0.1709,  ...,  0.4957,  0.4173,  0.0417],\n",
            "        [ 0.0201, -0.5724,  0.1660,  ...,  0.5018,  0.4213,  0.0332]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "Layer: txt_ctx, Weights: Parameter containing:\n",
            "tensor([[[-5.2644e-04, -1.1168e-02, -2.1661e-02,  ..., -4.8596e-03,\n",
            "           5.8912e-03,  6.7493e-03],\n",
            "         [ 5.8841e-04, -1.8584e-02, -2.5060e-02,  ..., -3.4987e-03,\n",
            "           5.4105e-03, -1.7861e-03],\n",
            "         [ 1.7309e-03, -7.2606e-03, -2.3481e-02,  ..., -2.1107e-03,\n",
            "           4.9070e-03,  6.0702e-03],\n",
            "         ...,\n",
            "         [ 1.0223e-03, -1.2080e-02, -1.5868e-02,  ..., -1.2882e-02,\n",
            "          -2.0567e-03,  4.8362e-03],\n",
            "         [ 7.8468e-03, -1.1038e-02, -1.0770e-02,  ...,  7.4988e-04,\n",
            "           1.5215e-03, -1.6774e-03],\n",
            "         [ 2.3028e-03, -5.9798e-03, -1.7329e-02,  ...,  5.5724e-04,\n",
            "           1.5857e-02,  4.3269e-03]],\n",
            "\n",
            "        [[ 6.0997e-03,  1.7106e-03, -2.1188e-02,  ...,  2.5523e-03,\n",
            "           2.1776e-03,  7.7525e-03],\n",
            "         [ 6.9275e-03, -6.8095e-03, -1.3799e-02,  ..., -3.6580e-03,\n",
            "           5.6513e-03,  1.9980e-03],\n",
            "         [ 6.2942e-04, -7.5124e-03, -9.4807e-03,  ...,  9.2128e-03,\n",
            "           1.0227e-02,  3.9989e-03],\n",
            "         ...,\n",
            "         [-8.9265e-04, -1.5108e-02, -1.9400e-02,  ..., -3.9164e-03,\n",
            "           4.4435e-03, -3.3369e-04],\n",
            "         [ 9.0713e-03, -1.0043e-02, -2.8737e-02,  ..., -4.9359e-03,\n",
            "           9.1699e-03,  5.4980e-03],\n",
            "         [-8.9653e-05, -7.9620e-03, -2.2422e-02,  ..., -2.5861e-03,\n",
            "           2.1928e-03,  6.9095e-03]],\n",
            "\n",
            "        [[ 2.2506e-04, -1.2583e-02, -1.8306e-02,  ..., -1.1051e-02,\n",
            "           9.2462e-03,  8.1454e-03],\n",
            "         [-9.4987e-04, -1.2930e-02, -2.0111e-02,  ..., -9.7730e-03,\n",
            "           5.3495e-03,  1.8893e-03],\n",
            "         [-2.7428e-03, -8.8699e-03, -1.4280e-02,  ...,  5.0471e-03,\n",
            "           5.3924e-03,  1.0839e-02],\n",
            "         ...,\n",
            "         [ 7.2136e-03, -9.1107e-03, -1.5424e-02,  ...,  1.9115e-03,\n",
            "           6.7415e-03, -8.7823e-04],\n",
            "         [-4.8866e-03, -9.1675e-03, -1.5351e-02,  ..., -5.0427e-03,\n",
            "           8.2830e-03,  1.1029e-02],\n",
            "         [-4.4785e-03, -1.2034e-02, -1.6046e-02,  ..., -1.3892e-03,\n",
            "           1.1470e-02, -4.6686e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1066e-02, -2.1956e-03, -1.7727e-02,  ..., -1.0564e-03,\n",
            "           1.2302e-02,  5.0345e-03],\n",
            "         [ 2.7537e-03, -4.1800e-04, -1.4596e-02,  ..., -1.1298e-03,\n",
            "           6.9791e-03,  3.2216e-03],\n",
            "         [ 3.3331e-03, -1.0128e-02, -1.2788e-02,  ..., -1.2218e-02,\n",
            "           2.8146e-03,  4.3975e-03],\n",
            "         ...,\n",
            "         [ 5.6838e-04, -7.7658e-04, -3.2178e-02,  ..., -6.8700e-03,\n",
            "           4.0220e-03, -3.8805e-04],\n",
            "         [ 9.2410e-04, -7.9189e-03, -1.0762e-02,  ..., -4.6880e-03,\n",
            "           5.9761e-03,  4.5386e-03],\n",
            "         [-2.8230e-04, -1.1029e-02, -1.4386e-02,  ..., -1.5611e-04,\n",
            "           1.0764e-02,  4.3288e-03]],\n",
            "\n",
            "        [[ 5.6610e-03, -6.1458e-03, -2.3435e-02,  ..., -8.1288e-03,\n",
            "           9.4217e-03,  1.0564e-02],\n",
            "         [ 1.7781e-03, -9.1155e-03, -7.3978e-03,  ...,  3.8989e-03,\n",
            "           8.7808e-03, -3.2376e-03],\n",
            "         [ 6.8626e-03, -1.4937e-02, -2.3481e-02,  ...,  1.5681e-03,\n",
            "           1.1096e-02,  2.1168e-03],\n",
            "         ...,\n",
            "         [ 2.3734e-03, -1.0926e-02, -2.4415e-02,  ..., -8.3730e-03,\n",
            "           1.5094e-02,  1.2127e-03],\n",
            "         [ 2.2802e-03, -9.2195e-03, -2.1869e-02,  ..., -2.9556e-03,\n",
            "           8.4852e-03,  4.9182e-03],\n",
            "         [-8.3199e-03, -8.2483e-03, -1.6881e-02,  ..., -4.5459e-03,\n",
            "           6.2860e-03,  5.5418e-04]],\n",
            "\n",
            "        [[ 1.3695e-03, -1.2808e-02, -1.6318e-02,  ...,  3.0864e-03,\n",
            "           4.0411e-03,  6.7378e-03],\n",
            "         [ 4.1695e-03, -1.2923e-02, -2.3946e-02,  ...,  2.9491e-03,\n",
            "           1.2264e-02,  2.7028e-03],\n",
            "         [-3.1091e-04, -1.2759e-02, -1.5500e-02,  ...,  1.6597e-03,\n",
            "           2.8471e-03, -5.2213e-03],\n",
            "         ...,\n",
            "         [ 9.3452e-05, -4.8621e-03, -1.9077e-02,  ..., -5.4662e-03,\n",
            "           5.2866e-03,  2.2937e-03],\n",
            "         [ 5.1670e-03, -1.0069e-02, -1.7373e-02,  ...,  2.5828e-03,\n",
            "           8.2563e-03,  1.3529e-03],\n",
            "         [ 3.2577e-03, -1.7065e-02, -1.7372e-02,  ..., -3.7200e-03,\n",
            "           4.5885e-03, -3.4474e-03]]], device='cuda:0', requires_grad=True)\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4769\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
            "epoch [1/50] batch [1/1] time 1.936 (1.936) data 0.623 (0.623) loss 1.2018 (1.2018) acc 71.8750 (71.8750) lr 2.0000e-02 eta 0:01:34\n",
            "epoch [2/50] batch [1/1] time 0.900 (0.900) data 0.491 (0.491) loss 0.8620 (0.8620) acc 75.0000 (75.0000) lr 1.9980e-02 eta 0:00:43\n",
            "epoch [3/50] batch [1/1] time 0.885 (0.885) data 0.476 (0.476) loss 1.2613 (1.2613) acc 71.8750 (71.8750) lr 1.9921e-02 eta 0:00:41\n",
            "epoch [4/50] batch [1/1] time 0.911 (0.911) data 0.500 (0.500) loss 1.1321 (1.1321) acc 65.6250 (65.6250) lr 1.9823e-02 eta 0:00:41\n",
            "epoch [5/50] batch [1/1] time 0.884 (0.884) data 0.478 (0.478) loss 0.7639 (0.7639) acc 81.2500 (81.2500) lr 1.9686e-02 eta 0:00:39\n",
            "epoch [6/50] batch [1/1] time 0.891 (0.891) data 0.483 (0.483) loss 0.9538 (0.9538) acc 75.0000 (75.0000) lr 1.9511e-02 eta 0:00:39\n",
            "epoch [7/50] batch [1/1] time 0.924 (0.924) data 0.515 (0.515) loss 0.7365 (0.7365) acc 81.2500 (81.2500) lr 1.9298e-02 eta 0:00:39\n",
            "epoch [8/50] batch [1/1] time 0.917 (0.917) data 0.507 (0.507) loss 0.9773 (0.9773) acc 71.8750 (71.8750) lr 1.9048e-02 eta 0:00:38\n",
            "epoch [9/50] batch [1/1] time 0.936 (0.936) data 0.525 (0.525) loss 0.5263 (0.5263) acc 84.3750 (84.3750) lr 1.8763e-02 eta 0:00:38\n",
            "epoch [10/50] batch [1/1] time 0.916 (0.916) data 0.505 (0.505) loss 0.5738 (0.5738) acc 84.3750 (84.3750) lr 1.8443e-02 eta 0:00:36\n",
            "epoch [11/50] batch [1/1] time 0.878 (0.878) data 0.470 (0.470) loss 0.8011 (0.8011) acc 84.3750 (84.3750) lr 1.8090e-02 eta 0:00:34\n",
            "epoch [12/50] batch [1/1] time 0.942 (0.942) data 0.531 (0.531) loss 0.8350 (0.8350) acc 84.3750 (84.3750) lr 1.7705e-02 eta 0:00:35\n",
            "epoch [13/50] batch [1/1] time 0.892 (0.892) data 0.485 (0.485) loss 0.5600 (0.5600) acc 81.2500 (81.2500) lr 1.7290e-02 eta 0:00:33\n",
            "epoch [14/50] batch [1/1] time 0.912 (0.912) data 0.501 (0.501) loss 0.4834 (0.4834) acc 87.5000 (87.5000) lr 1.6845e-02 eta 0:00:32\n",
            "epoch [15/50] batch [1/1] time 0.910 (0.910) data 0.502 (0.502) loss 0.5973 (0.5973) acc 84.3750 (84.3750) lr 1.6374e-02 eta 0:00:31\n",
            "epoch [16/50] batch [1/1] time 0.880 (0.880) data 0.470 (0.470) loss 0.6177 (0.6177) acc 78.1250 (78.1250) lr 1.5878e-02 eta 0:00:29\n",
            "epoch [17/50] batch [1/1] time 0.907 (0.907) data 0.498 (0.498) loss 0.6092 (0.6092) acc 87.5000 (87.5000) lr 1.5358e-02 eta 0:00:29\n",
            "epoch [18/50] batch [1/1] time 0.889 (0.889) data 0.481 (0.481) loss 0.5759 (0.5759) acc 84.3750 (84.3750) lr 1.4818e-02 eta 0:00:28\n",
            "epoch [19/50] batch [1/1] time 0.930 (0.930) data 0.520 (0.520) loss 0.5548 (0.5548) acc 84.3750 (84.3750) lr 1.4258e-02 eta 0:00:28\n",
            "epoch [20/50] batch [1/1] time 0.898 (0.898) data 0.489 (0.489) loss 0.6240 (0.6240) acc 84.3750 (84.3750) lr 1.3681e-02 eta 0:00:26\n",
            "epoch [21/50] batch [1/1] time 0.959 (0.959) data 0.550 (0.550) loss 0.3958 (0.3958) acc 93.7500 (93.7500) lr 1.3090e-02 eta 0:00:27\n",
            "epoch [22/50] batch [1/1] time 0.930 (0.930) data 0.520 (0.520) loss 0.9103 (0.9103) acc 78.1250 (78.1250) lr 1.2487e-02 eta 0:00:26\n",
            "epoch [23/50] batch [1/1] time 0.895 (0.895) data 0.486 (0.486) loss 0.4234 (0.4234) acc 87.5000 (87.5000) lr 1.1874e-02 eta 0:00:24\n",
            "epoch [24/50] batch [1/1] time 0.946 (0.946) data 0.537 (0.537) loss 0.6172 (0.6172) acc 84.3750 (84.3750) lr 1.1253e-02 eta 0:00:24\n",
            "epoch [25/50] batch [1/1] time 0.912 (0.912) data 0.503 (0.503) loss 0.7157 (0.7157) acc 81.2500 (81.2500) lr 1.0628e-02 eta 0:00:22\n",
            "epoch [26/50] batch [1/1] time 0.906 (0.906) data 0.499 (0.499) loss 0.3089 (0.3089) acc 93.7500 (93.7500) lr 1.0000e-02 eta 0:00:21\n",
            "epoch [27/50] batch [1/1] time 0.904 (0.904) data 0.493 (0.493) loss 0.4797 (0.4797) acc 90.6250 (90.6250) lr 9.3721e-03 eta 0:00:20\n",
            "epoch [28/50] batch [1/1] time 0.885 (0.885) data 0.477 (0.477) loss 0.4010 (0.4010) acc 87.5000 (87.5000) lr 8.7467e-03 eta 0:00:19\n",
            "epoch [29/50] batch [1/1] time 0.921 (0.921) data 0.512 (0.512) loss 0.2679 (0.2679) acc 96.8750 (96.8750) lr 8.1262e-03 eta 0:00:19\n",
            "epoch [30/50] batch [1/1] time 0.885 (0.885) data 0.477 (0.477) loss 0.2682 (0.2682) acc 100.0000 (100.0000) lr 7.5131e-03 eta 0:00:17\n",
            "epoch [31/50] batch [1/1] time 0.893 (0.893) data 0.483 (0.483) loss 0.7607 (0.7607) acc 84.3750 (84.3750) lr 6.9098e-03 eta 0:00:16\n",
            "epoch [32/50] batch [1/1] time 0.911 (0.911) data 0.501 (0.501) loss 0.4882 (0.4882) acc 84.3750 (84.3750) lr 6.3188e-03 eta 0:00:16\n",
            "epoch [33/50] batch [1/1] time 0.941 (0.941) data 0.531 (0.531) loss 0.4378 (0.4378) acc 93.7500 (93.7500) lr 5.7422e-03 eta 0:00:16\n",
            "epoch [34/50] batch [1/1] time 0.945 (0.945) data 0.534 (0.534) loss 0.3343 (0.3343) acc 87.5000 (87.5000) lr 5.1825e-03 eta 0:00:15\n",
            "epoch [35/50] batch [1/1] time 0.911 (0.911) data 0.501 (0.501) loss 0.6306 (0.6306) acc 84.3750 (84.3750) lr 4.6417e-03 eta 0:00:13\n",
            "epoch [36/50] batch [1/1] time 0.887 (0.887) data 0.476 (0.476) loss 0.2345 (0.2345) acc 96.8750 (96.8750) lr 4.1221e-03 eta 0:00:12\n",
            "epoch [37/50] batch [1/1] time 0.909 (0.909) data 0.499 (0.499) loss 0.4313 (0.4313) acc 90.6250 (90.6250) lr 3.6258e-03 eta 0:00:11\n",
            "epoch [38/50] batch [1/1] time 0.904 (0.904) data 0.497 (0.497) loss 0.6589 (0.6589) acc 87.5000 (87.5000) lr 3.1545e-03 eta 0:00:10\n",
            "epoch [39/50] batch [1/1] time 0.926 (0.926) data 0.520 (0.520) loss 0.2252 (0.2252) acc 96.8750 (96.8750) lr 2.7103e-03 eta 0:00:10\n",
            "epoch [40/50] batch [1/1] time 0.875 (0.875) data 0.466 (0.466) loss 0.3553 (0.3553) acc 96.8750 (96.8750) lr 2.2949e-03 eta 0:00:08\n",
            "epoch [41/50] batch [1/1] time 0.895 (0.895) data 0.488 (0.488) loss 0.2731 (0.2731) acc 93.7500 (93.7500) lr 1.9098e-03 eta 0:00:08\n",
            "epoch [42/50] batch [1/1] time 0.907 (0.907) data 0.497 (0.497) loss 0.4874 (0.4874) acc 87.5000 (87.5000) lr 1.5567e-03 eta 0:00:07\n",
            "epoch [43/50] batch [1/1] time 0.886 (0.886) data 0.475 (0.475) loss 0.3126 (0.3126) acc 93.7500 (93.7500) lr 1.2369e-03 eta 0:00:06\n",
            "epoch [44/50] batch [1/1] time 0.908 (0.908) data 0.501 (0.501) loss 0.3309 (0.3309) acc 93.7500 (93.7500) lr 9.5173e-04 eta 0:00:05\n",
            "epoch [45/50] batch [1/1] time 0.889 (0.889) data 0.481 (0.481) loss 0.4837 (0.4837) acc 93.7500 (93.7500) lr 7.0224e-04 eta 0:00:04\n",
            "epoch [46/50] batch [1/1] time 0.934 (0.934) data 0.526 (0.526) loss 0.4681 (0.4681) acc 90.6250 (90.6250) lr 4.8943e-04 eta 0:00:03\n",
            "epoch [47/50] batch [1/1] time 0.946 (0.946) data 0.540 (0.540) loss 0.2957 (0.2957) acc 96.8750 (96.8750) lr 3.1417e-04 eta 0:00:02\n",
            "epoch [48/50] batch [1/1] time 0.933 (0.933) data 0.529 (0.529) loss 0.1482 (0.1482) acc 100.0000 (100.0000) lr 1.7713e-04 eta 0:00:01\n",
            "epoch [49/50] batch [1/1] time 0.909 (0.909) data 0.501 (0.501) loss 0.1670 (0.1670) acc 96.8750 (96.8750) lr 7.8853e-05 eta 0:00:00\n",
            "epoch [50/50] batch [1/1] time 0.889 (0.889) data 0.481 (0.481) loss 0.4092 (0.4092) acc 90.6250 (90.6250) lr 1.9733e-05 eta 0:00:00\n",
            "Checkpoint saved to output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:18<00:00,  2.03it/s]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,235\n",
            "* accuracy: 88.2%\n",
            "* error: 11.8%\n",
            "* macro_f1: 88.0%\n",
            "Elapsed: 0:01:08\n"
          ]
        }
      ],
      "source": [
        "#oxford_pets-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##visulaization"
      ],
      "metadata": {
        "id": "ayFcbuy1Bq7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#oxford_pets-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/vis/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nkv7BVGBttZ",
        "outputId": "5d55dc4c-fdc2-4c43-ac89-0e699cadea26"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 11:50:29.356146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 11:50:29.375935: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 11:50:29.382212: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 11:50:29.400459: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 11:50:30.412163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/vis/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/vis/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  37\n",
            "# val      37\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4769\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/vis/1207_new_diff_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
            "epoch [1/50] batch [1/1] time 1.987 (1.987) data 0.631 (0.631) loss 1.2018 (1.2018) acc 71.8750 (71.8750) lr 2.0000e-02 eta 0:01:37\n",
            "epoch [2/50] batch [1/1] time 0.896 (0.896) data 0.504 (0.504) loss 0.8620 (0.8620) acc 75.0000 (75.0000) lr 1.9980e-02 eta 0:00:43\n",
            "epoch [3/50] batch [1/1] time 0.890 (0.890) data 0.496 (0.496) loss 1.2613 (1.2613) acc 71.8750 (71.8750) lr 1.9921e-02 eta 0:00:41\n",
            "epoch [4/50] batch [1/1] time 0.890 (0.890) data 0.499 (0.499) loss 1.1330 (1.1330) acc 65.6250 (65.6250) lr 1.9823e-02 eta 0:00:40\n",
            "epoch [5/50] batch [1/1] time 0.859 (0.859) data 0.467 (0.467) loss 0.7634 (0.7634) acc 81.2500 (81.2500) lr 1.9686e-02 eta 0:00:38\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 211, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 151, in main\n",
            "    trainer.train()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 385, in train\n",
            "    super().train(self.start_epoch, self.max_epoch)\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 249, in train\n",
            "    self.run_epoch()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 593, in run_epoch\n",
            "    loss_summary = self.forward_backward(batch)\n",
            "  File \"/content/drive/MyDrive/DAPT/trainers/dapt.py\", line 562, in forward_backward\n",
            "    accuracy = compute_accuracy(output, label)[0].item()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#oxford_pets-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/vis/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP59ng0JHEs5",
        "outputId": "6cec4350-d401-484d-92ee-86ec5bab5c80"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 11:52:00.607643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 11:52:00.627951: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 11:52:00.634763: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 11:52:00.651165: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 11:52:01.664892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/vis/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/vis/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  10\n",
            "# val      10\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4553\n",
            "  Max: 0.4769\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/vis/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 211, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 151, in main\n",
            "    trainer.train()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 385, in train\n",
            "    super().train(self.start_epoch, self.max_epoch)\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 249, in train\n",
            "    self.run_epoch()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 593, in run_epoch\n",
            "    loss_summary = self.forward_backward(batch)\n",
            "  File \"/content/drive/MyDrive/DAPT/trainers/dapt.py\", line 534, in forward_backward\n",
            "    output, image_features, text_features = self.model(image)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/DAPT/trainers/dapt.py\", line 440, in forward\n",
            "    text_prompts = self.prompt_learner.forward_txt()\n",
            "  File \"/content/drive/MyDrive/DAPT/trainers/dapt.py\", line 409, in forward_txt\n",
            "    prompts = torch.cat(\n",
            "RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 10 but got size 37 for tensor number 1 in the list.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#oxford_pets-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/vis/1207_new_same_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtFlQZFPH_i4",
        "outputId": "16c1ebb0-95a8-4826-af19-8ed5d05d9dd4"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 11:56:21.645793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 11:56:21.665573: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 11:56:21.671725: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 11:56:21.685998: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 11:56:22.673954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/vis/1207_new_same_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/vis/1207_new_same_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  37\n",
            "# val      37\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/vis/1207_new_same_init/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
            "epoch [1/50] batch [1/1] time 1.965 (1.965) data 0.614 (0.614) loss 1.1925 (1.1925) acc 71.8750 (71.8750) lr 2.0000e-02 eta 0:01:36\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1448, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1402, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1243, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/queue.py\", line 180, in get\n",
            "    self.not_empty.wait(remaining)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 324, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 211, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 151, in main\n",
            "    trainer.train()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 385, in train\n",
            "    super().train(self.start_epoch, self.max_epoch)\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 249, in train\n",
            "    self.run_epoch()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 591, in run_epoch\n",
            "    for self.batch_idx, batch in enumerate(self.train_loader_x):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 697, in __next__\n",
            "    with torch.autograd.profiler.record_function(self._profile_name):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py\", line 750, in __exit__\n",
            "    torch.ops.profiler._record_function_exit._RecordFunction(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 939, in __call__\n",
            "    def __call__(self, /, *args, **kwargs):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#oxford_pets-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/vis/1207_new_same_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU0a1sNhIIYw",
        "outputId": "787c9ced-32ef-43c4-d768-df147b8c5788"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 11:57:11.399494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 11:57:11.419190: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 11:57:11.425217: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 11:57:11.439304: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 11:57:12.466677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/vis/1207_new_same_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/vis/1207_new_same_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  10\n",
            "# val      10\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([37, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/vis/1207_new_same_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 211, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 151, in main\n",
            "    trainer.train()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 385, in train\n",
            "    super().train(self.start_epoch, self.max_epoch)\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 249, in train\n",
            "    self.run_epoch()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 593, in run_epoch\n",
            "    loss_summary = self.forward_backward(batch)\n",
            "  File \"/content/drive/MyDrive/DAPT/trainers/dapt.py\", line 537, in forward_backward\n",
            "    output, image_features, text_features = self.model(image)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/DAPT/trainers/dapt.py\", line 443, in forward\n",
            "    text_prompts = self.prompt_learner.forward_txt()\n",
            "  File \"/content/drive/MyDrive/DAPT/trainers/dapt.py\", line 412, in forward_txt\n",
            "    prompts = torch.cat(\n",
            "RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 10 but got size 37 for tensor number 1 in the list.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#oxford_pets-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/vis/original/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVCMzmiUJbvz",
        "outputId": "534d9a91-828f-48fc-f914-9c42b88d1c87"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 12:09:42.625014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 12:09:42.644936: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 12:09:42.650899: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 12:09:42.665323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 12:09:43.691867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/vis/original/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.02\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/vis/original/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 0.1\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 10.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_zhou_OxfordPets.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/oxford_pets/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  37\n",
            "# val      37\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0359\n",
            "  Min: -0.0625\n",
            "  Max: 0.0624\n",
            "------------------------------\n",
            "  Mean: 0.0005\n",
            "  Std: 0.0198\n",
            "  Min: -0.0718\n",
            "  Max: 0.0731\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/vis/original/oxford_pets/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 211, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 151, in main\n",
            "    trainer.train()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 385, in train\n",
            "    super().train(self.start_epoch, self.max_epoch)\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 249, in train\n",
            "    self.run_epoch()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 593, in run_epoch\n",
            "    loss_summary = self.forward_backward(batch)\n",
            "  File \"/content/drive/MyDrive/DAPT/trainers/dapt.py\", line 389, in forward_backward\n",
            "    output, image_features, text_features = self.model(image)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/DAPT/trainers/dapt.py\", line 324, in forward\n",
            "    text_prompts = self.prompt_learner.forward_txt()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1918, in __getattr__\n",
            "    def __getattr__(self, name: str) -> Any:\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#oxford_pets-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/vis/original/eurosat/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY_2RM6lJiU-",
        "outputId": "c8d5ca22-ee45-4ba1-9d60-60d8aded7a13"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 12:10:01.283808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 12:10:01.303612: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 12:10:01.309472: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 12:10:01.323362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 12:10:02.323712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/vis/original/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/vis/original/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  10\n",
            "# val      10\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0359\n",
            "  Min: -0.0625\n",
            "  Max: 0.0624\n",
            "------------------------------\n",
            "  Mean: 0.0005\n",
            "  Std: 0.0198\n",
            "  Min: -0.0718\n",
            "  Max: 0.0731\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/vis/original/eurosat/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
            "epoch [1/50] batch [1/1] time 1.420 (1.420) data 0.386 (0.386) loss 11.6680 (11.6680) acc 10.0000 (10.0000) lr 2.0000e+01 eta 0:01:09\n",
            "epoch [2/50] batch [1/1] time 0.393 (0.393) data 0.268 (0.268) loss 11.6191 (11.6191) acc 10.0000 (10.0000) lr 1.9980e+01 eta 0:00:18\n",
            "Exception ignored in: <function _releaseLock at 0x7cd6072b6e60>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
            "    def _releaseLock():\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1243, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/queue.py\", line 179, in get\n",
            "    raise Empty\n",
            "_queue.Empty\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 211, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/DAPT/train.py\", line 151, in main\n",
            "    trainer.train()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 385, in train\n",
            "    super().train(self.start_epoch, self.max_epoch)\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 249, in train\n",
            "    self.run_epoch()\n",
            "  File \"/content/drive/My Drive/DAPT/Dassl.pytorch/dassl/engine/trainer.py\", line 591, in run_epoch\n",
            "    for self.batch_idx, batch in enumerate(self.train_loader_x):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1448, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1402, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1256, in _try_get_data\n",
            "    raise RuntimeError(\n",
            "RuntimeError: DataLoader worker (pid(s) 153272, 153273, 153274, 153275, 153276) exited unexpectedly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#oxford_pets-1shots-seed1\n",
        "!python train.py \\\n",
        "        --root /content/drive/MyDrive/DAPT/DATA/ \\\n",
        "        --seed 1 \\\n",
        "        --trainer DAPT \\\n",
        "        --dataset-config-file configs/datasets/eurosat.yaml \\\n",
        "        --config-file configs/trainers/DAPT/vit_b16_ep50.yaml \\\n",
        "        --output-dir output/visfsf/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1 \\\n",
        "        DATASET.NUM_SHOTS 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZLUN2w9PXcT",
        "outputId": "6d93687f-3d0a-49f6-d3b3-63e1932a48fd"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 12:30:25.301034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-08 12:30:25.321091: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-08 12:30:25.326964: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-08 12:30:25.340858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-08 12:30:26.329670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/DAPT/vit_b16_ep50.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/visfsf/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
            "prototype_gen: False\n",
            "resume: \n",
            "root: /content/drive/MyDrive/DAPT/DATA/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: DAPT\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: /content/drive/MyDrive/DAPT/DATA/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 20.0\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/visfsf/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAPT:\n",
            "    PROTOTYPE_GEN: False\n",
            "    TXT_BETA: 10.0\n",
            "    TXT_NUM_TOKENS: 16\n",
            "    TXT_RBF_T: 2.0\n",
            "    VIS_BETA: 100.0\n",
            "    VIS_DROPOUT: 0.0\n",
            "    VIS_NUM_TOKENS: 16\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: DAPT\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               8\n",
            "On-line CPU(s) list:                  0-7\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                79\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   4\n",
            "Socket(s):                            1\n",
            "Stepping:                             0\n",
            "BogoMIPS:                             4399.99\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            128 KiB (4 instances)\n",
            "L1i cache:                            128 KiB (4 instances)\n",
            "L2 cache:                             1 MiB (4 instances)\n",
            "L3 cache:                             55 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-7\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchcam==0.4.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: DAPT\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/drive/MyDrive/DAPT/DATA/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/drive/MyDrive/DAPT/DATA/eurosat/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  10\n",
            "# train_x  10\n",
            "# val      10\n",
            "# test     8,100\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing context from CLIP embeddings\n",
            "Visual context shape: torch.Size([1, 16, 768])\n",
            "Text context shape: torch.Size([10, 16, 512])\n",
            "Number of context words (tokens): 16\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Turning off gradients in both the image and the text encoder\n",
            "=== Prompt Learner Weights ===\n",
            "  Mean: 0.0025\n",
            "  Std: 0.2339\n",
            "  Min: -1.5597\n",
            "  Max: 1.0103\n",
            "------------------------------\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0399\n",
            "  Min: -0.4488\n",
            "  Max: 0.4736\n",
            "------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/visfsf/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1/tensorboard)\n",
            "epoch [1/50] batch [1/1] time 1.296 (1.296) data 0.387 (0.387) loss 9.2002 (9.2002) acc 20.0000 (20.0000) lr 2.0000e+01 eta 0:01:03\n",
            "epoch [2/50] batch [1/1] time 0.440 (0.440) data 0.302 (0.302) loss 8.9286 (8.9286) acc 20.0000 (20.0000) lr 1.9980e+01 eta 0:00:21\n",
            "epoch [3/50] batch [1/1] time 0.433 (0.433) data 0.295 (0.295) loss 8.3344 (8.3344) acc 10.0000 (10.0000) lr 1.9921e+01 eta 0:00:20\n",
            "epoch [4/50] batch [1/1] time 0.414 (0.414) data 0.292 (0.292) loss 4.2929 (4.2929) acc 20.0000 (20.0000) lr 1.9823e+01 eta 0:00:19\n",
            "epoch [5/50] batch [1/1] time 0.420 (0.420) data 0.298 (0.298) loss 5.0954 (5.0954) acc 10.0000 (10.0000) lr 1.9686e+01 eta 0:00:18\n",
            "epoch [6/50] batch [1/1] time 0.414 (0.414) data 0.294 (0.294) loss 5.0362 (5.0362) acc 30.0000 (30.0000) lr 1.9511e+01 eta 0:00:18\n",
            "epoch [7/50] batch [1/1] time 0.428 (0.428) data 0.307 (0.307) loss 3.9601 (3.9601) acc 60.0000 (60.0000) lr 1.9298e+01 eta 0:00:18\n",
            "epoch [8/50] batch [1/1] time 0.421 (0.421) data 0.298 (0.298) loss 2.7531 (2.7531) acc 60.0000 (60.0000) lr 1.9048e+01 eta 0:00:17\n",
            "epoch [9/50] batch [1/1] time 0.420 (0.420) data 0.298 (0.298) loss 2.4187 (2.4187) acc 50.0000 (50.0000) lr 1.8763e+01 eta 0:00:17\n",
            "epoch [10/50] batch [1/1] time 0.423 (0.423) data 0.303 (0.303) loss 2.3854 (2.3854) acc 50.0000 (50.0000) lr 1.8443e+01 eta 0:00:16\n",
            "epoch [11/50] batch [1/1] time 0.414 (0.414) data 0.293 (0.293) loss 2.1413 (2.1413) acc 60.0000 (60.0000) lr 1.8090e+01 eta 0:00:16\n",
            "epoch [12/50] batch [1/1] time 0.420 (0.420) data 0.301 (0.301) loss 2.2140 (2.2140) acc 60.0000 (60.0000) lr 1.7705e+01 eta 0:00:15\n",
            "epoch [13/50] batch [1/1] time 0.416 (0.416) data 0.296 (0.296) loss 1.5810 (1.5810) acc 70.0000 (70.0000) lr 1.7290e+01 eta 0:00:15\n",
            "epoch [14/50] batch [1/1] time 0.420 (0.420) data 0.298 (0.298) loss 1.4352 (1.4352) acc 90.0000 (90.0000) lr 1.6845e+01 eta 0:00:15\n",
            "epoch [15/50] batch [1/1] time 0.411 (0.411) data 0.291 (0.291) loss 1.3899 (1.3899) acc 80.0000 (80.0000) lr 1.6374e+01 eta 0:00:14\n",
            "epoch [16/50] batch [1/1] time 0.436 (0.436) data 0.314 (0.314) loss 1.6673 (1.6673) acc 70.0000 (70.0000) lr 1.5878e+01 eta 0:00:14\n",
            "epoch [17/50] batch [1/1] time 0.416 (0.416) data 0.293 (0.293) loss 1.4826 (1.4826) acc 60.0000 (60.0000) lr 1.5358e+01 eta 0:00:13\n",
            "epoch [18/50] batch [1/1] time 0.416 (0.416) data 0.294 (0.294) loss 1.2263 (1.2263) acc 80.0000 (80.0000) lr 1.4818e+01 eta 0:00:13\n",
            "epoch [19/50] batch [1/1] time 0.405 (0.405) data 0.292 (0.292) loss 1.4129 (1.4129) acc 80.0000 (80.0000) lr 1.4258e+01 eta 0:00:12\n",
            "epoch [20/50] batch [1/1] time 0.453 (0.453) data 0.331 (0.331) loss 1.0658 (1.0658) acc 80.0000 (80.0000) lr 1.3681e+01 eta 0:00:13\n",
            "epoch [21/50] batch [1/1] time 0.422 (0.422) data 0.300 (0.300) loss 0.9668 (0.9668) acc 90.0000 (90.0000) lr 1.3090e+01 eta 0:00:12\n",
            "epoch [22/50] batch [1/1] time 0.424 (0.424) data 0.303 (0.303) loss 1.4909 (1.4909) acc 80.0000 (80.0000) lr 1.2487e+01 eta 0:00:11\n",
            "epoch [23/50] batch [1/1] time 0.434 (0.434) data 0.313 (0.313) loss 0.6907 (0.6907) acc 100.0000 (100.0000) lr 1.1874e+01 eta 0:00:11\n",
            "epoch [24/50] batch [1/1] time 0.461 (0.461) data 0.338 (0.338) loss 0.8029 (0.8029) acc 100.0000 (100.0000) lr 1.1253e+01 eta 0:00:11\n",
            "epoch [25/50] batch [1/1] time 0.433 (0.433) data 0.310 (0.310) loss 1.1563 (1.1563) acc 80.0000 (80.0000) lr 1.0628e+01 eta 0:00:10\n",
            "epoch [26/50] batch [1/1] time 0.421 (0.421) data 0.301 (0.301) loss 0.8908 (0.8908) acc 90.0000 (90.0000) lr 1.0000e+01 eta 0:00:10\n",
            "epoch [27/50] batch [1/1] time 0.422 (0.422) data 0.301 (0.301) loss 0.6465 (0.6465) acc 100.0000 (100.0000) lr 9.3721e+00 eta 0:00:09\n",
            "epoch [28/50] batch [1/1] time 0.421 (0.421) data 0.300 (0.300) loss 0.8888 (0.8888) acc 80.0000 (80.0000) lr 8.7467e+00 eta 0:00:09\n",
            "epoch [29/50] batch [1/1] time 0.414 (0.414) data 0.293 (0.293) loss 0.5481 (0.5481) acc 100.0000 (100.0000) lr 8.1262e+00 eta 0:00:08\n",
            "epoch [30/50] batch [1/1] time 0.425 (0.425) data 0.304 (0.304) loss 0.8285 (0.8285) acc 80.0000 (80.0000) lr 7.5131e+00 eta 0:00:08\n",
            "epoch [31/50] batch [1/1] time 0.418 (0.418) data 0.296 (0.296) loss 0.5606 (0.5606) acc 100.0000 (100.0000) lr 6.9098e+00 eta 0:00:07\n",
            "epoch [32/50] batch [1/1] time 0.417 (0.417) data 0.295 (0.295) loss 0.5034 (0.5034) acc 100.0000 (100.0000) lr 6.3188e+00 eta 0:00:07\n",
            "epoch [33/50] batch [1/1] time 0.421 (0.421) data 0.299 (0.299) loss 0.5956 (0.5956) acc 90.0000 (90.0000) lr 5.7422e+00 eta 0:00:07\n",
            "epoch [34/50] batch [1/1] time 0.436 (0.436) data 0.313 (0.313) loss 0.5300 (0.5300) acc 100.0000 (100.0000) lr 5.1825e+00 eta 0:00:06\n",
            "epoch [35/50] batch [1/1] time 0.418 (0.418) data 0.296 (0.296) loss 0.7514 (0.7514) acc 90.0000 (90.0000) lr 4.6417e+00 eta 0:00:06\n",
            "epoch [36/50] batch [1/1] time 0.421 (0.421) data 0.298 (0.298) loss 0.5098 (0.5098) acc 100.0000 (100.0000) lr 4.1221e+00 eta 0:00:05\n",
            "epoch [37/50] batch [1/1] time 0.437 (0.437) data 0.316 (0.316) loss 0.4779 (0.4779) acc 100.0000 (100.0000) lr 3.6258e+00 eta 0:00:05\n",
            "epoch [38/50] batch [1/1] time 0.428 (0.428) data 0.305 (0.305) loss 0.5159 (0.5159) acc 100.0000 (100.0000) lr 3.1545e+00 eta 0:00:05\n",
            "epoch [39/50] batch [1/1] time 0.427 (0.427) data 0.307 (0.307) loss 0.6506 (0.6506) acc 100.0000 (100.0000) lr 2.7103e+00 eta 0:00:04\n",
            "epoch [40/50] batch [1/1] time 0.427 (0.427) data 0.305 (0.305) loss 0.4488 (0.4488) acc 100.0000 (100.0000) lr 2.2949e+00 eta 0:00:04\n",
            "epoch [41/50] batch [1/1] time 0.437 (0.437) data 0.316 (0.316) loss 0.6091 (0.6091) acc 100.0000 (100.0000) lr 1.9098e+00 eta 0:00:03\n",
            "epoch [42/50] batch [1/1] time 0.427 (0.427) data 0.306 (0.306) loss 0.4380 (0.4380) acc 100.0000 (100.0000) lr 1.5567e+00 eta 0:00:03\n",
            "epoch [43/50] batch [1/1] time 0.417 (0.417) data 0.295 (0.295) loss 0.4148 (0.4148) acc 100.0000 (100.0000) lr 1.2369e+00 eta 0:00:02\n",
            "epoch [44/50] batch [1/1] time 0.422 (0.422) data 0.300 (0.300) loss 0.4310 (0.4310) acc 100.0000 (100.0000) lr 9.5173e-01 eta 0:00:02\n",
            "epoch [45/50] batch [1/1] time 0.441 (0.441) data 0.319 (0.319) loss 0.4510 (0.4510) acc 100.0000 (100.0000) lr 7.0224e-01 eta 0:00:02\n",
            "epoch [46/50] batch [1/1] time 0.429 (0.429) data 0.306 (0.306) loss 0.9363 (0.9363) acc 90.0000 (90.0000) lr 4.8943e-01 eta 0:00:01\n",
            "epoch [47/50] batch [1/1] time 0.432 (0.432) data 0.311 (0.311) loss 0.6077 (0.6077) acc 80.0000 (80.0000) lr 3.1417e-01 eta 0:00:01\n",
            "epoch [48/50] batch [1/1] time 0.456 (0.456) data 0.334 (0.334) loss 0.4165 (0.4165) acc 100.0000 (100.0000) lr 1.7713e-01 eta 0:00:00\n",
            "epoch [49/50] batch [1/1] time 0.432 (0.432) data 0.310 (0.310) loss 0.3871 (0.3871) acc 100.0000 (100.0000) lr 7.8853e-02 eta 0:00:00\n",
            "epoch [50/50] batch [1/1] time 0.411 (0.411) data 0.290 (0.290) loss 0.9637 (0.9637) acc 90.0000 (90.0000) lr 1.9733e-02 eta 0:00:00\n",
            "Checkpoint saved to output/visfsf/1207_new_diff_init/eurosat/DAPT/vit_b16_ep50_1shots/seed1/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 81/81 [00:27<00:00,  2.90it/s]\n",
            "=> result\n",
            "* total: 8,100\n",
            "* correct: 3,162\n",
            "* accuracy: 39.0%\n",
            "* error: 61.0%\n",
            "* macro_f1: 37.1%\n",
            "Elapsed: 0:00:54\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}